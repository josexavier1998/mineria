,page,text,link,categories,topic
0,Data Science,"Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains. Data science is related to data mining, machine learning and big data.
Data science is a ""concept to unify statistics, data analysis, informatics, and their related methods"" in order to ""understand and analyze actual phenomena"" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. Turing award winner Jim Gray imagined data science as a ""fourth paradigm"" of science (empirical, theoretical, computational and now data-driven) and asserted that ""everything about science is changing because of the impact of information technology"" and the data deluge.

Foundations
Data science is an interdisciplinary field focused on extracting knowledge from data sets, which are typically large (see big data), and applying the knowledge and actionable insights from data to solve problems in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, information visualization, data integration, graphic design, complex systems, communication and business. Statistician Nathan Yau, drawing on Ben Fry, also links data science to human-computer interaction: users should be able to intuitively control and explore data. In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.

Relationship to statistics
Many statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics. Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g. images) and emphasizes prediction and action. Andrew Gelman of Columbia University and data scientist Vincent Granville have described statistics as a nonessential part of data science.
Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing, and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data science program. He describes data science as an applied field growing out of traditional statistics. 
In summary, data science can be therefore described as an applied branch of statistics.

Etymology
Early usage
In 1962, John Tukey described a field he called “data analysis,” which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C.F. Jeff Wu used the term Data Science for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.The term “data science” has been traced back to 1974, when Peter Naur proposed it as an alternative name for computer science. In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic. However, the definition was still in flux. After the 1985 lecture in the Chinese Academy of Sciences in Beijing, in 1997 C.F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting, or limited to describing data. In 1998, Chikio Hayashi argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.During the 1990s, popular terms for the process of finding patterns in datasets (which were increasingly large) included “knowledge discovery” and “data mining”.

Modern usage
The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland. In a 2001 paper, he advocated an expansion of statistics beyond theory into technical areas; because this would significantly change the field, it warranted a new name. ""Data science"" became more widely used in the next few years: in 2002, the Committee on Data for Science and Technology launched Data Science Journal. In 2003, Columbia University launched The Journal of Data Science. In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.The professional title of “data scientist” has been attributed to DJ Patil and Jeff Hammerbacher in 2008. Though it was used by the National Science Board in their 2005 report, ""Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century,"" it referred broadly to any key role in managing a digital data collection.There is still no consensus on the definition of data science and it is considered by some to be a buzzword.

Impact
Big data is very quickly becoming a vital tool for businesses and companies of all sizes. The availability and interpretation of big data has altered the business models of old industries and enabled the creation of new ones. Data-driven businesses are worth $1.2 trillion collectively in 2020, an increase from $333 billion in the year 2015. Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations. As big data continues to have a major impact on the world, data science does as well due to the close relationship between the two.

Technologies and techniques
There are a variety of different technologies and techniques that are used for data science which depend on the application. More recently, full-featured, end-to-end platforms have been developed and heavily used for data science and machine learning.

Techniques

Linear Regression
Logistic Regression
Decision tree is used as prediction models for classification and data fitting. The decision tree structure can be used to generate rules able to classify or predict target/class/label variable based on the observation attributes.
Support Vector Machine (SVM)
Clustering is a technique used to group data together.
Dimensionality reduction is used to reduce the complexity of data computation so that it can be performed more quickly.
Machine learning is a technique used to perform tasks by inferencing patterns from data


== References ==",https://en.wikipedia.org/wiki/Data_science,"['Articles with short description', 'CS1 maint: others', 'Computational fields of study', 'Computer occupations', 'Data analysis', 'Information science', 'Short description matches Wikidata', 'Use dmy dates from December 2012']",Data Science
1,Association rule learning,"Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule 
  
    
      
        {
        
          o
          n
          i
          o
          n
          s
          ,
          p
          o
          t
          a
          t
          o
          e
          s
        
        }
        ⇒
        {
        
          b
          u
          r
          g
          e
          r
        
        }
      
    
    {\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}
   found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements.
In addition to the above example from market basket analysis association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.

Definition
Following the original definition by Agrawal, Imieliński, Swami the problem of association rule mining is defined as:
Let 
  
    
      
        I
        =
        {
        
          i
          
            1
          
        
        ,
        
          i
          
            2
          
        
        ,
        …
        ,
        
          i
          
            n
          
        
        }
      
    
    {\displaystyle I=\{i_{1},i_{2},\ldots ,i_{n}\}}
   be a set of 
  
    
      
        n
      
    
    {\displaystyle n}
   binary attributes called items.
Let 
  
    
      
        D
        =
        {
        
          t
          
            1
          
        
        ,
        
          t
          
            2
          
        
        ,
        …
        ,
        
          t
          
            m
          
        
        }
      
    
    {\displaystyle D=\{t_{1},t_{2},\ldots ,t_{m}\}}
   be a set of transactions called the database.
Each transaction in 
  
    
      
        D
      
    
    {\displaystyle D}
   has a unique transaction ID and contains a subset of the items in 
  
    
      
        I
      
    
    {\displaystyle I}
  .
A rule is defined as an implication of the form:

  
    
      
        X
        ⇒
        Y
      
    
    {\displaystyle X\Rightarrow Y}
  , where 
  
    
      
        X
        ,
        Y
        ⊆
        I
      
    
    {\displaystyle X,Y\subseteq I}
  .
In Agrawal, Imieliński, Swami a rule is defined only between a set and a single item, 
  
    
      
        X
        ⇒
        
          i
          
            j
          
        
      
    
    {\displaystyle X\Rightarrow i_{j}}
   for 
  
    
      
        
          i
          
            j
          
        
        ∈
        I
      
    
    {\displaystyle i_{j}\in I}
  .
Every rule is composed by two different sets of items, also known as itemsets, 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
  , where 
  
    
      
        X
      
    
    {\displaystyle X}
   is called antecedent or left-hand-side (LHS) and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   consequent or right-hand-side (RHS).
To illustrate the concepts, we use a small example from the supermarket domain. The set of items is 
  
    
      
        I
        =
        {
        
          m
          i
          l
          k
          ,
          b
          r
          e
          a
          d
          ,
          b
          u
          t
          t
          e
          r
          ,
          b
          e
          e
          r
          ,
          d
          i
          a
          p
          e
          r
          s
        
        }
      
    
    {\displaystyle I=\{\mathrm {milk,bread,butter,beer,diapers} \}}
   and in the table is shown a small database containing the items, where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represents the absence of an item in that transaction.
An example rule for the supermarket could be 
  
    
      
        {
        
          b
          u
          t
          t
          e
          r
          ,
          b
          r
          e
          a
          d
        
        }
        ⇒
        {
        
          m
          i
          l
          k
        
        }
      
    
    {\displaystyle \{\mathrm {butter,bread} \}\Rightarrow \{\mathrm {milk} \}}
   meaning that if butter and bread are bought, customers also buy milk.
Note: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.

Useful Concepts
In order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.
Let 
  
    
      
        X
        ,
        Y
      
    
    {\displaystyle X,Y}
   be itemsets, 
  
    
      
        X
        ⇒
        Y
      
    
    {\displaystyle X\Rightarrow Y}
   an association rule and 
  
    
      
        T
      
    
    {\displaystyle T}
   a set of transactions of a given database.

Support
Support is an indication of how frequently the itemset appears in the dataset.
The support of 
  
    
      
        X
      
    
    {\displaystyle X}
   with respect to 
  
    
      
        T
      
    
    {\displaystyle T}
   is defined as the proportion of transactions 
  
    
      
        t
      
    
    {\displaystyle t}
   in the dataset which contains the itemset 
  
    
      
        X
      
    
    {\displaystyle X}
  .

  
    
      
        
          s
          u
          p
          p
        
        (
        X
        )
        =
        
          
            
              
                |
              
              {
              t
              ∈
              T
              ;
              X
              ⊆
              t
              }
              
                |
              
            
            
              
                |
              
              T
              
                |
              
            
          
        
      
    
    {\displaystyle \mathrm {supp} (X)={\frac {|\{t\in T;X\subseteq t\}|}{|T|}}}
  
In the example dataset, the itemset 
  
    
      
        X
        =
        {
        
          b
          e
          e
          r
          ,
          d
          i
          a
          p
          e
          r
          s
        
        }
      
    
    {\displaystyle X=\{\mathrm {beer,diapers} \}}
   has a support of 
  
    
      
        1
        
          /
        
        5
        =
        0.2
      
    
    {\displaystyle 1/5=0.2}
   since it occurs in 20% of all transactions (1 out of 5 transactions). The argument of 
  
    
      
        
          s
          u
          p
          p
        
        (
        )
      
    
    {\displaystyle \mathrm {supp} ()}
   is a set of preconditions, and thus becomes more restrictive as it grows (instead of more inclusive).Furthermore, the itemset 
  
    
      
        Y
        =
        {
        
          m
          i
          l
          k
          ,
          b
          r
          e
          a
          d
          ,
          b
          u
          t
          t
          e
          r
        
        }
      
    
    {\displaystyle Y=\{\mathrm {milk,bread,butter} \}}
   has a support of 
  
    
      
        1
        
          /
        
        5
        =
        0.2
      
    
    {\displaystyle 1/5=0.2}
   as it appears in 20% of all transactions as well.

Confidence
Confidence is an indication of how often the rule has been found to be true.
The confidence value of a rule, 
  
    
      
        X
        ⇒
        Y
      
    
    {\displaystyle X\Rightarrow Y}
   , with respect to a set of transactions 
  
    
      
        T
      
    
    {\displaystyle T}
  , is the proportion of the transactions that contains 
  
    
      
        X
      
    
    {\displaystyle X}
   which also contains 
  
    
      
        Y
      
    
    {\displaystyle Y}
  .
Confidence is defined as:

  
    
      
        
          c
          o
          n
          f
        
        (
        X
        ⇒
        Y
        )
        =
        
          s
          u
          p
          p
        
        (
        X
        ∪
        Y
        )
        
          /
        
        
          s
          u
          p
          p
        
        (
        X
        )
      
    
    {\displaystyle \mathrm {conf} (X\Rightarrow Y)=\mathrm {supp} (X\cup Y)/\mathrm {supp} (X)}
  
For example, the rule 
  
    
      
        {
        
          b
          u
          t
          t
          e
          r
          ,
          b
          r
          e
          a
          d
        
        }
        ⇒
        {
        
          m
          i
          l
          k
        
        }
      
    
    {\displaystyle \{\mathrm {butter,bread} \}\Rightarrow \{\mathrm {milk} \}}
   has a confidence of 
  
    
      
        0.2
        
          /
        
        0.2
        =
        1.0
      
    
    {\displaystyle 0.2/0.2=1.0}
   in the database, which means that for 100% of the transactions containing butter and bread the rule is correct (100% of the times a customer buys butter and bread, milk is bought as well).
Note that 
  
    
      
        
          s
          u
          p
          p
        
        (
        X
        ∪
        Y
        )
      
    
    {\displaystyle \mathrm {supp} (X\cup Y)}
   means the support of the union of the items in X and Y. This is somewhat confusing since we normally think in terms of probabilities of events and not sets of items. We can rewrite 
  
    
      
        
          s
          u
          p
          p
        
        (
        X
        ∪
        Y
        )
      
    
    {\displaystyle \mathrm {supp} (X\cup Y)}
   as the probability 
  
    
      
        P
        (
        
          E
          
            X
          
        
        ∩
        
          E
          
            Y
          
        
        )
      
    
    {\displaystyle P(E_{X}\cap E_{Y})}
  , where 
  
    
      
        
          E
          
            X
          
        
      
    
    {\displaystyle E_{X}}
   and 
  
    
      
        
          E
          
            Y
          
        
      
    
    {\displaystyle E_{Y}}
   are the events that a transaction contains itemset 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
  , respectively.Thus confidence can be interpreted as an estimate of the conditional probability 
  
    
      
        P
        (
        
          E
          
            Y
          
        
        
          |
        
        
          E
          
            X
          
        
        )
      
    
    {\displaystyle P(E_{Y}|E_{X})}
  , the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.

Lift
The lift of a rule is defined as:

  
    
      
        
          l
          i
          f
          t
        
        (
        X
        ⇒
        Y
        )
        =
        
          
            
              
                s
                u
                p
                p
              
              (
              X
              ∪
              Y
              )
            
            
              
                s
                u
                p
                p
              
              (
              X
              )
              ×
              
                s
                u
                p
                p
              
              (
              Y
              )
            
          
        
      
    
    {\displaystyle \mathrm {lift} (X\Rightarrow Y)={\frac {\mathrm {supp} (X\cup Y)}{\mathrm {supp} (X)\times \mathrm {supp} (Y)}}}
  
or the ratio of the observed support to that expected if X and Y were independent.
For example, the rule 
  
    
      
        {
        
          m
          i
          l
          k
          ,
          b
          r
          e
          a
          d
        
        }
        ⇒
        {
        
          b
          u
          t
          t
          e
          r
        
        }
      
    
    {\displaystyle \{\mathrm {milk,bread} \}\Rightarrow \{\mathrm {butter} \}}
   has a lift of 
  
    
      
        
          
            0.2
            
              0.4
              ×
              0.4
            
          
        
        =
        1.25
      
    
    {\displaystyle {\frac {0.2}{0.4\times 0.4}}=1.25}
  .
If the rule had a lift of 1, it would imply that the probability of occurrence of the antecedent and that of the consequent are independent of each other. When two events are independent of each other, no rule can be drawn involving those two events.
If the lift is > 1, that lets us know the degree to which those two occurrences are dependent on one another, and makes those rules potentially useful for predicting the consequent in future data sets.
If the lift is < 1, that lets us know the items are substitute to each other. This means that presence of one item has negative effect on presence of other item and vice versa.
The value of lift is that it considers both the support of the rule and the overall data set.

Conviction
The conviction of a rule is defined as 
  
    
      
        
          c
          o
          n
          v
        
        (
        X
        ⇒
        Y
        )
        =
        
          
            
              1
              −
              
                s
                u
                p
                p
              
              (
              Y
              )
            
            
              1
              −
              
                c
                o
                n
                f
              
              (
              X
              ⇒
              Y
              )
            
          
        
      
    
    {\displaystyle \mathrm {conv} (X\Rightarrow Y)={\frac {1-\mathrm {supp} (Y)}{1-\mathrm {conf} (X\Rightarrow Y)}}}
  .For example, the rule 
  
    
      
        {
        
          m
          i
          l
          k
          ,
          b
          r
          e
          a
          d
        
        }
        ⇒
        {
        
          b
          u
          t
          t
          e
          r
        
        }
      
    
    {\displaystyle \{\mathrm {milk,bread} \}\Rightarrow \{\mathrm {butter} \}}
   has a conviction of 
  
    
      
        
          
            
              1
              −
              0.4
            
            
              1
              −
              0.5
            
          
        
        =
        1.2
      
    
    {\displaystyle {\frac {1-0.4}{1-0.5}}=1.2}
  , and can be interpreted as the ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. In this example, the conviction value of 1.2 shows that the rule 
  
    
      
        {
        
          m
          i
          l
          k
          ,
          b
          r
          e
          a
          d
        
        }
        ⇒
        {
        
          b
          u
          t
          t
          e
          r
        
        }
      
    
    {\displaystyle \{\mathrm {milk,bread} \}\Rightarrow \{\mathrm {butter} \}}
   would be incorrect 20% more often (1.2 times as often) if the association between X and Y was purely random chance.

Alternative measures of interestingness
In addition to confidence, other measures of interestingness for rules have been proposed. Some popular measures are:

All-confidence
Collective strength
LeverageSeveral more measures are presented and compared by Tan et al. and by Hahsler. Looking for techniques that can model what the user has known (and using these models as interestingness measures) is currently an active research trend under the name of ""Subjective Interestingness.""

Process
Association rules are usually required to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Association rule generation is usually split up into two separate steps:
A minimum support threshold is applied to find all frequent itemsets in a database.
A minimum confidence constraint is applied to these frequent itemsets in order to form rules.While the second step is straightforward, the first step needs more attention.
Finding all frequent itemsets in a database is difficult since it involves searching all possible itemsets (item combinations). The set of possible itemsets is the power set over 
  
    
      
        I
      
    
    {\displaystyle I}
   and has size 
  
    
      
        
          2
          
            n
          
        
        −
        1
      
    
    {\displaystyle 2^{n}-1}
   (excluding the empty set which is not a valid itemset). Although the size of the power-set grows exponentially in the number of items 
  
    
      
        n
      
    
    {\displaystyle n}
   in 
  
    
      
        I
      
    
    {\displaystyle I}
  , efficient search is possible using the downward-closure property of support (also called anti-monotonicity) which guarantees that for a frequent itemset, all its subsets are also frequent and thus no infrequent itemset can be a subset of a frequent itemset. Exploiting this property, efficient algorithms (e.g., Apriori and Eclat) can find all frequent itemsets.

History
The concept of association rules was popularised particularly due to the 1993 article of Agrawal et al., which has acquired more than 18,000 citations according to Google Scholar, as of August 2015, and is thus one of the most cited papers in the Data Mining field. However, what is now called ""association rules"" is introduced already in the 1966 paper on GUHA, a general data mining method developed by Petr Hájek et al.An early (circa 1989) use of minimum support and confidence to find all association rules is the Feature Based Modeling framework, which found all rules with 
  
    
      
        
          s
          u
          p
          p
        
        (
        X
        )
      
    
    {\displaystyle \mathrm {supp} (X)}
   and 
  
    
      
        
          c
          o
          n
          f
        
        (
        X
        ⇒
        Y
        )
      
    
    {\displaystyle \mathrm {conf} (X\Rightarrow Y)}
   greater than user defined constraints.

Statistically sound associations
One limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations. These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance. For example, suppose we are considering a collection of 10,000 items and looking for rules containing two items in the left-hand-side and 1 item in the right-hand-side. There are approximately 1,000,000,000,000 such rules. If we apply a statistical test for independence with a significance level of 0.05 it means there is only a 5% chance of accepting a rule if there is no association. If we assume there are no associations, we should nonetheless expect to find 50,000,000,000 rules. Statistically sound association discovery controls this risk, in most cases reducing the risk of finding any spurious associations to a user-specified significance level.

Algorithms
Many algorithms for generating association rules have been proposed.
Some well-known algorithms are Apriori, Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.

Apriori algorithm
Apriori uses a breadth-first search strategy to count the support of itemsets and uses a candidate generation function which exploits the downward closure property of support.

Eclat algorithm
Eclat (alt. ECLAT, stands for Equivalence Class Transformation) is a depth-first search algorithm based on set intersection. It is suitable for both sequential as well as parallel execution with locality-enhancing properties.

FP-growth algorithm
FP stands for frequent pattern.In the first pass, the algorithm counts the occurrences of items (attribute-value pairs) in the dataset of transactions, and stores these counts in a 'header table'. In the second pass, it builds the FP-tree structure by inserting transactions into a trie.
Items in each transaction have to be sorted by descending order of their frequency in the dataset before being inserted so that the tree can be processed quickly.
Items in each transaction that do not meet the minimum support requirement are discarded.
If many transactions share most frequent items, the FP-tree provides high compression close to tree root.
Recursive processing of this compressed version of the main dataset grows frequent item sets directly, instead of generating candidate items and testing them against the entire database (as in the apriori algorithm).
Growth begins from the bottom of the header table i.e. the item with the smallest support by finding all sorted transactions that end in that item. Call this item 
  
    
      
        I
      
    
    {\displaystyle I}
  .
A new conditional tree is created which is the original FP-tree projected onto 
  
    
      
        I
      
    
    {\displaystyle I}
  . The supports of all nodes in the projected tree are re-counted with each node getting the sum of its children counts. Nodes (and hence subtrees) that do not meet the minimum support are pruned. Recursive growth ends when no individual items conditional on 
  
    
      
        I
      
    
    {\displaystyle I}
   meet the minimum support threshold. The resulting paths from root to 
  
    
      
        I
      
    
    {\displaystyle I}
   will be frequent itemsets. After this step, processing continues with the next least-supported header item of the original FP-tree.
Once the recursive process has completed, all frequent item sets will have been found, and association rule creation begins.

Others
ASSOC
The ASSOC procedure is a GUHA method which mines for generalized association rules using fast bitstrings operations. The association rules mined by this method are more general than those output by apriori, for example ""items"" can be connected both with conjunction and disjunctions and the relation between antecedent and consequent of the rule is not restricted to setting minimum support and confidence as in apriori: an arbitrary combination of supported interest measures can be used.

OPUS search
OPUS is an efficient algorithm for rule discovery that, in contrast to most alternatives, does not require either monotone or anti-monotone constraints such as minimum support. Initially used to find rules for a fixed consequent it has subsequently been extended to find rules with any item as a consequent. OPUS search is the core technology in the popular Magnum Opus association discovery system.

Lore
A famous story about association rule mining is the ""beer and diaper"" story. A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer. This anecdote became popular as an example of how unexpected association rules might be found from everyday data. There are varying opinions as to how much of the story is true. Daniel Powers says:
In 1992, Thomas Blischok, manager of a retail consulting group at Teradata, and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis ""did discover that between 5:00 and 7:00 p.m. that consumers bought beer and diapers"". Osco managers did NOT exploit the beer and diapers relationship by moving the products closer together on the shelves.

Other types of association rule mining
Multi-Relation Association Rules: Multi-Relation Association Rules (MRAR) are association rules where each item may have several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations live in, nearby and humid: “Those who live in a place which is nearby a city with humid climate type and also are younger than 20 -> their health condition is good”. Such association rules are extractable from RDBMS data or semantic web data.Contrast set learning is a form of associative learning. Contrast set learners use rules that differ meaningfully in their distribution across subsets.Weighted class learning is another form of associative learning in which weight may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results.
High-order pattern discovery facilitate the capture of high-order (polythetic) patterns or event associations that are intrinsic to complex real-world data.
K-optimal pattern discovery provides an alternative to the standard approach to association rule learning that requires that each pattern appear frequently in the data.
Approximate Frequent Itemset mining is a relaxed version of Frequent Itemset mining that allows some of the items in some of the rows to be 0.Generalized Association Rules hierarchical taxonomy (concept hierarchy)
Quantitative Association Rules categorical and quantitative data
Interval Data Association Rules e.g. partition the age into 5-year-increment ranged
Sequential pattern mining  discovers subsequences that are common to more than minsup sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.Subspace Clustering, a specific type of Clustering high-dimensional data, is in many variants also based on the downward-closure property for specific clustering models.Warmr is shipped as part of the ACE data mining suite. It allows association rule learning for first order relational rules.

See also
Sequence mining
Production system (computer science)
Learning classifier system
Rule-based machine learning

References
Bibliographies
Annotated Bibliography on Association Rules by M. Hahsler",https://en.wikipedia.org/wiki/Association_rule_learning,"['All articles with unsourced statements', 'Articles prone to spam from February 2016', 'Articles with unsourced statements from March 2021', 'CS1: long volume value', 'CS1 errors: missing periodical', 'Data management', 'Data mining', 'Webarchive template wayback links', 'Wikipedia articles needing clarification from October 2019', 'Wikipedia articles needing page number citations from January 2019']",Data Science
2,Andrew Gelman,"Andrew Gelman (born February 11, 1965) is an American statistician, professor of statistics and political science at Columbia University.  He earned an S.B. in mathematics and in physics from MIT, where he was a National Merit Scholar, in 1986. He then earned his Ph.D. in statistics from Harvard University in 1990 under the supervision of Donald Rubin.He has received the Outstanding Statistical Application award from the American Statistical Association three times. He is an elected fellow of the American Statistical Association and the Institute of Mathematical Statistics. He was elected fellow of the American Academy of Arts and Sciences (AAAS) in 2020.

Personal life
Gelman married Caroline Rosenthal in 2002 and has three children.The psychologist Susan Gelman is his older sister.  The cartoonist Woody Gelman was his uncle.

Work
Gelman is currently a professor of political science and statistics at Columbia University. Gelman is a practitioner of Bayesian statistics, and hierarchical models.He is a major contributor to the statistical programming framework Stan.

Popular press
Gelman is notable for his efforts to make political science and statistics more accessible to journalists and to the public. He is one of the primary authors of ""The Monkey Cage"", blog published by The Washington Post. The blog is dedicated to providing informed commentary on politics and making political science more accessible.Gelman also keeps his own blog which deals with statistical practices in social science. He frequently writes about Bayesian statistics, displaying data, and interesting trends in social science. According to The New York Times, on the blog ""he posts his thoughts on best statistical practices in the sciences, with a frequent emphasis on what he sees as the absurd and unscientific... He is respected enough that his posts are well read; he is cutting enough that many of his critiques are enjoyed with a strong sense of schadenfreude.""Gelman has been prominent as a critic of alleged poor methodological work in the replication crisis.

Bibliography
Andrew Gelman, David Park, Boris Shor, and Jeronimo Cortina. ""Red State, Blue State, Rich State, Poor State: Why Americans Vote the Way They Do (2nd edition). Princeton University Press, 2009.
Andrew Gelman and Jennifer Hill. ""Data Analysis Using Regression and Multilevel/Hierarchical Models"". Cambridge University Press, 2006. ISBN 978-0-521-68689-1
Andrew Gelman and Deborah Nolan. ""Teaching Statistics: A Bag of Tricks"". Oxford University Press, 2002. ISBN 978-0-19-857224-4
Andrew Gelman, John B. Carlin, Hal S. Stern, David Dunson, Aki Vehtari, and Donald B. Rubin. ""Bayesian Data Analysis"" (3rd edition). Chapman & Hall/CRC, 2013.
Andrew Gelman, Jennifer Hill, and Aki Vehtari. ""Regression and Other Stories"". Cambridge University Press, 2020. ISBN 978-1107023987

References
External links
Home page
Statistical Modeling, Causal Inference, and Social Science. Andrew Gelman's research blog.",https://en.wikipedia.org/wiki/Andrew_Gelman,"['1965 births', 'American political scientists', 'American social scientists', 'American statisticians', 'Articles with hCards', 'Articles with short description', 'Bayesian statisticians', 'Columbia University staff', 'Commons category link is on Wikidata', 'Fellows of the American Academy of Arts and Sciences', 'Fellows of the American Statistical Association', 'Fellows of the Institute of Mathematical Statistics', 'Harvard University alumni', 'Living people', 'Massachusetts Institute of Technology School of Science alumni', 'Pages using infobox scientist with unknown parameters', 'People with Tourette syndrome', 'Short description matches Wikidata', 'Webarchive template wayback links', 'Wikipedia articles with BIBSYS identifiers', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with CANTIC identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MGP identifiers', 'Wikipedia articles with NKC identifiers', 'Wikipedia articles with NTA identifiers', 'Wikipedia articles with ORCID identifiers', 'Wikipedia articles with SNAC-ID identifiers', 'Wikipedia articles with SUDOC identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers']",Data Science
3,Anomaly detection,"In data analysis, anomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro clusters formed by these patterns.Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the learnt model.

Applications
Anomaly detection is applicable in a variety of domains, such as intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, and detecting ecosystem disturbances. It is often used in preprocessing to remove anomalous data from the dataset. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy.

Popular techniques
Several anomaly detection techniques have been proposed in literature. Some of the popular techniques are:

Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept).
Subspace-, correlation-based and tensor-based  outlier detection for high-dimensional data.
One-class support vector machines.
Replicator neural networks., autoencoders, variational autoencoders, long short-term memory neural networks
Bayesian networks.
Hidden Markov models (HMMs).
Cluster analysis-based outlier detection.
Deviations from association rules and frequent itemsets.
Fuzzy logic-based outlier detection.
Ensemble techniques, using feature bagging, score normalization and different sources of diversity.The performance of different methods depends a lot on the data set and parameters, and methods have little systematic advantages over another when compared across many data sets and parameters.

Application to data security
Anomaly detection was proposed for intrusion detection systems (IDS) by Dorothy Denning in 1986. Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning. Types of statistics proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations.  The counterpart of anomaly detection in intrusion detection is misuse detection.

In data pre-processing
In supervised learning, anomaly detection is often an important step in data pre-processing to provide the learning algorithm a proper dataset to learn on. This is also known as Data cleansing.  After detecting anomalous samples classifiers remove them, however, at times corrupted data can still provide useful samples for learning. A common method for finding appropriate samples to use is identifying Noisy data. One approach to find noisy values is to create a probabilistic model from data using models of uncorrupted data and corrupted data.Below is an example of the Iris flower data set with an anomaly added. With an anomaly included, classification algorithm may have difficulties properly finding patterns, or run into errors. 

By removing the anomaly, training will be enabled to find patterns in classifications more easily.
In data mining, high-dimensional data will also propose high computing challenges with intensely large sets of data. By removing numerous samples that can find itself irrelevant to a classifier or detection algorithm, runtime can be significantly reduced on even the largest sets of data.

Software
ELKI is an open-source Java data mining toolkit that contains several anomaly detection algorithms, as well as index acceleration for them.
Scikit-Learn is an open-source Python library that has built functionality to provide unsupervised anomaly detection.

Datasets
Anomaly detection benchmark data repository of the Ludwig-Maximilians-Universität München; Mirror at University of São Paulo.
ODDS – ODDS: A large collection of publicly available outlier detection datasets with ground truth in different domains.
Unsupervised Anomaly Detection Benchmark at Harvard Dataverse: Datasets for Unsupervised Anomaly Detection with ground truth.

See also
Change detection
Statistical process control
Novelty detection
Hierarchical temporal memory


== References ==",https://en.wikipedia.org/wiki/Anomaly_detection,"['CS1: long volume value', 'Data mining', 'Data security', 'Machine learning', 'Statistical outliers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers']",Data Science
4,Artificial neural network,"Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems vaguely inspired by the biological neural networks that constitute animal brains.
An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The ""signal"" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.

Training
Neural networks learn (or are trained) by processing examples, each of which contains a known ""input"" and ""result,"" forming probability-weighted associations between the two, which are stored within the data structure of the net itself. The training of a neural network from a given example is usually conducted by determining the difference between the processed output of the network (often a prediction) and a target output. This is the error. The network then adjusts its weighted associations according to a learning rule and using this error value. Successive adjustments will cause the neural network to produce output which is increasingly similar to the target output. After a sufficient number of these adjustments the training can be terminated based upon certain criteria. This is known as supervised learning.
Such systems ""learn"" to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the results to identify cats in other images. They do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers, and cat-like faces. Instead, they automatically generate identifying characteristics from the examples that they process.

History
Warren McCulloch and Walter Pitts (1943) opened the subject by creating a computational model for neural networks. In the late 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Farley and Wesley A. Clark (1954) first used computational machines, then called ""calculators"", to simulate a Hebbian network. Rosenblatt (1958) created the perceptron. The first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, as the Group Method of Data Handling. The basics of continuous backpropagation were derived in the context of control theory by Kelley in 1960 and by Bryson in 1961, using principles of dynamic programming.
In 1970, Seppo Linnainmaa published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions. In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. Werbos's (1975) backpropagation algorithm enabled practical training of multi-layer networks. In 1982, he applied Linnainmaa's AD method to neural networks in the way that became widely used. Thereafter research stagnated following Minsky and Papert (1969), who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks.
The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.In 1986 Rumelhart, Hinton and Williams showed that backpropagation  learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In 1992, max-pooling was introduced to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation.Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as ""deep learning"".Ciresan and colleagues (2010) showed that despite the vanishing gradient problem, GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012, ANNs began winning prizes in ANN contests, approaching human level performance on various tasks, initially in pattern recognition and machine learning. For example, the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012).

Models
ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, mostly abandoning attempts to remain true to their biological precursors. Neurons are connected to each other in various patterns, to allow the output of some neurons to become the input of others. The network forms a directed, weighted graph.An artificial neural network consists of a collection of simulated neurons. Each neuron is a node which is connected to other nodes via links that correspond to biological axon-synapse-dendrite connections. Each link has a weight, which determines the strength of one node's influence on another.

Components of ANNs
Neurons
ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.
To find the output of the neuron, first we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image.

Connections and weights
The network consists of connections, each connection providing the output of one neuron as an input to another neuron. Each connection is assigned a weight that represents its relative importance. A given neuron can have multiple input and output connections.

Propagation function
The propagation function computes the input to a neuron from the outputs of its predecessor neurons and their connections as a weighted sum. A bias term can be added to the result of the propagation.

Organization
The neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be fully connected, with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connect to a single neuron in the next layer, thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.

Hyperparameter
A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.

Learning
Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.

Learning rate
The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.

Cost function
While it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) or because it arises from the model (e.g. in a probabilistic model the model's posterior probability can be used as an inverse cost).

Backpropagation
Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backprop calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as Extreme Learning Machines, ""No-prop"" networks, training without backtracking, ""weightless"" networks, and non-connectionist neural networks.

Learning paradigms
The three major learning paradigms are supervised learning, unsupervised learning and reinforcement learning. They each correspond to a particular learning task

Supervised learning
Supervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for hand writing, speech and gesture recognition). This can be thought of as learning with a ""teacher"", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.

Unsupervised learning
In unsupervised learning, input data is given along with the cost function, some function of the data 
  
    
      
        
          x
        
      
    
    {\displaystyle \textstyle x}
   and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model 
  
    
      
        
          f
          (
          x
          )
          =
          a
        
      
    
    {\displaystyle \textstyle f(x)=a}
   where 
  
    
      
        
          a
        
      
    
    {\displaystyle \textstyle a}
   is a constant and the cost 
  
    
      
        
          C
          =
          E
          [
          (
          x
          −
          f
          (
          x
          )
          
            )
            
              2
            
          
          ]
        
      
    
    {\displaystyle \textstyle C=E[(x-f(x))^{2}]}
  . Minimizing this cost produces a value of 
  
    
      
        
          a
        
      
    
    {\displaystyle \textstyle a}
   that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between 
  
    
      
        
          x
        
      
    
    {\displaystyle \textstyle x}
   and 
  
    
      
        
          f
          (
          x
          )
        
      
    
    {\displaystyle \textstyle f(x)}
  , whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.

Reinforcement learning
In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.
Formally the environment is modeled as a Markov decision process (MDP) with states 
  
    
      
        
          
            
              s
              
                1
              
            
            ,
            .
            .
            .
            ,
            
              s
              
                n
              
            
          
          ∈
          S
        
      
    
    {\displaystyle \textstyle {s_{1},...,s_{n}}\in S}
   and actions 
  
    
      
        
          
            
              a
              
                1
              
            
            ,
            .
            .
            .
            ,
            
              a
              
                m
              
            
          
          ∈
          A
        
      
    
    {\displaystyle \textstyle {a_{1},...,a_{m}}\in A}
  . Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution 
  
    
      
        
          P
          (
          
            c
            
              t
            
          
          
            |
          
          
            s
            
              t
            
          
          )
        
      
    
    {\displaystyle \textstyle P(c_{t}|s_{t})}
  , the observation distribution 
  
    
      
        
          P
          (
          
            x
            
              t
            
          
          
            |
          
          
            s
            
              t
            
          
          )
        
      
    
    {\displaystyle \textstyle P(x_{t}|s_{t})}
   and the transition distribution 
  
    
      
        
          P
          (
          
            s
            
              t
              +
              1
            
          
          
            |
          
          
            s
            
              t
            
          
          ,
          
            a
            
              t
            
          
          )
        
      
    
    {\displaystyle \textstyle P(s_{t+1}|s_{t},a_{t})}
  , while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.
ANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.

Self learning
Self learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named Crossbar Adaptive Array (CAA). It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given memory matrix W =||w(a,s)||, the crossbar self learning algorithm in each iteration performs the following computation:

  In situation s perform action a;
  Receive consequence situation s';
  Compute emotion of being in consequence situation v(s');
  Update crossbar memory w'(a,s) = w(a,s) + v(s').

The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.

Other
In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.

Modes
Two modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces ""noise"" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use ""mini-batches"", small batches with samples in each batch selected stochastically from the entire data set.

Types
ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains.  The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter are much more complicated, but can shorten learning periods and produce better results. Some types allow/require learning to be ""supervised"" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.
Some of the main breakthroughs include: convolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data; long short-term memory avoid the vanishing gradient problem and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition, text-to-speech synthesis, and photo-real talking heads; competitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game or on deceiving the opponent about the authenticity of an input.

Network design
Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras.Design issues include deciding the number, type and connectedness of network layers, as well as the size of each and the connection type (full, pooling, ...).
Hyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc.

Use
Using Artificial neural networks requires an understanding of their characteristics.

Choice of model: This depends on the data representation and the application. Overly complex models slow learning.
Learning algorithm: Numerous trade-offs exist between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular data set. However, selecting and tuning an algorithm for training on unseen data requires significant experimentation.
Robustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust.ANN capabilities fall within the following broad categories:
Function approximation, or regression analysis, including time series prediction, fitness approximation and modeling.
Classification, including pattern and sequence recognition, novelty detection and sequential decision making.
Data processing, including filtering, clustering, blind source separation and compression.
Robotics, including directing manipulators and prostheses.

Applications
Because of their ability to reproduce and model nonlinear processes, Artificial neural networks have found applications in many disciplines. Application areas include system identification and control (vehicle control, trajectory prediction, process control, natural resource management), quantum chemistry, general game playing, pattern recognition (radar systems, face identification, signal classification, 3D reconstruction, object recognition and more), sequence recognition (gesture, speech, handwritten and printed text recognition), medical diagnosis, finance (e.g. automated trading systems), data mining, visualization, machine translation, social network filtering and e-mail spam filtering. ANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.
ANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.

Theoretical properties
Computational power
The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.
A specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.

Capacity
A model's ""capacity"" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.
Two notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover. The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form.  As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a Perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.

Convergence
Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.
The convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models. Another example is when parameters are small, it is observed that ANNs often fits target functions from low to high frequencies. This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method.

Generalization and statistics
Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error.
The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.

Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.
By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications.
The softmax activation function is:

  
    
      
        
          y
          
            i
          
        
        =
        
          
            
              e
              
                
                  x
                  
                    i
                  
                
              
            
            
              
                ∑
                
                  j
                  =
                  1
                
                
                  c
                
              
              
                e
                
                  
                    x
                    
                      j
                    
                  
                
              
            
          
        
      
    
    {\displaystyle y_{i}={\frac {e^{x_{i}}}{\sum _{j=1}^{c}e^{x_{j}}}}}

Criticism
Training
A common criticism of neural networks, particularly in robotics, is that they require too much training for real-world operation. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.

Theory
A fundamental objection is that ANNs do not sufficiently reflect neuronal function. Backpropagation is a critical step, although no such mechanism exists in biological neural networks. How information is coded by real neurons is not known. Sensor neurons fire action potentials more frequently with sensor activation and muscle cells pull more strongly when their associated motor neurons receive action potentials more frequently. Other than the case of relaying information from a sensor neuron to a motor neuron, almost nothing of the principles of how information is handled by biological neural networks is known.
A central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. Alexander Dewdney commented that, as a result, artificial neural networks have a ""something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything"". One response to Dewdney is that neural networks handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.
Technology writer Roger Bridgman commented:

Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque, unreadable table...valueless as a scientific resource"".
In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.

Biological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.

Hardware
Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons –  which require enormous CPU power and time.
Schmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.Neuromorphic engineering addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.

Practical counterexamples
Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.

Hybrid approaches
Advocates of hybrid models (combining neural networks and symbolic approaches), claim that such a mixture can better capture the mechanisms of the human mind.

Gallery
See also
References
Bibliography
External links
The Neural Network Zoo – a compilation of neural network types
The Stilwell Brain – a Mind Field episode featuring an experiment in which humans act as individual neurons in a neural network that classifies handwritten digits",https://en.wikipedia.org/wiki/Artificial_neural_network,"['All Wikipedia articles needing clarification', 'All articles lacking in-text citations', 'All articles needing additional references', 'All articles with unsourced statements', 'Articles lacking in-text citations from August 2019', 'Articles needing additional references from November 2020', 'Articles with excessive see also sections from March 2018', 'Articles with short description', 'Articles with unsourced statements from June 2017', 'Articles with unsourced statements from November 2014', 'Artificial neural networks', 'CS1 Finnish-language sources (fi)', 'CS1 German-language sources (de)', 'CS1 errors: missing periodical', 'Classification algorithms', 'Computational neuroscience', 'Computational statistics', 'Market research', 'Mathematical and quantitative methods (economics)', 'Mathematical psychology', 'Short description matches Wikidata', 'Use dmy dates from December 2020', 'Webarchive template wayback links', 'Wikipedia articles needing clarification from April 2017', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NDL identifiers']",Data Science
5,American Statistical Association,"The American Statistical Association (ASA) is the main professional organization for statisticians and related professionals in the United States. It was founded in Boston, Massachusetts on November 27, 1839, and is the second oldest continuously operating professional society in the US (only the Massachusetts Medical Society, founded in 1781, is older). The ASA services statisticians, quantitative scientists, and users of statistics across many academic areas and applications. The association publishes a variety of journals and sponsors several international conferences every year.

Mission
The organization's mission is to promote good application of statistical science, specifically to:
support excellence in statistical practice, research, journals, and meetings
work for the improvement of statistical education at all levels
promote the proper application of statistics
anticipate and meet member needs
use the discipline of statistics to enhance human welfare
seek opportunities to advance the statistics profession

Membership
ASA has about 18,000 members, found in government, academia, and the private sector. The membership is involved in a wide variety of activities including:
research in medical areas such as AIDS
environmental risk assessment
the development of new therapeutic drugs
the exploration of space
quality assurance in industry
the examination of social issues such as the homeless and the poor
analytic research on current business problems and economic forecasting
the setting of standards for statistics used at all levels of government
the promotion and development of statistical education for the public and the profession, and
the expansion of methods and the use of computers and graphics to advance the science of statistics

Fellowship
New Fellowships of the ASA are granted annually by the ASA Committee on Fellows. Candidates must have been members for the preceding three years but may be nominated by anyone. The maximum number of recipients each year is one-third of one percent of the ASA membership.

Organizational structure
ASA is organized in Sections, Chapters and Committees. Chapters are arranged geographically, representing 78 areas across the US and Canada. Sections are subject-area and industry-area interest groups covering 22 sub-disciplines. ASA has more than 60 committees coordinating meetings, publications, education, careers, and special-interest topics involving statisticians.

Accredited Professional Statistician
As of April 2010, the ASA offers the Accredited Professional Statistician status (PStat), to members who meet the ASA's credentialing requirements, which include an advanced degree in statistics or related quantitative field, five years of documented experience, and evidence of professional competence. A list of current members with PStat status is available.The ASA also offers the Graduate Statistician status (GStat) as of April 2014. It serves as a preparatory accreditation suitable for graduate students.

Publications
The ASA publishes several scientific journals:

Journal of the American Statistical Association (JASA)
The American Statistician (TAS)
Journal of Business & Economic Statistics (JBES)
Journal of Agricultural, Biological and Environmental Statistics (JABES)
Journal of Computational and Graphical Statistics (JCGS)
Technometrics (TECH)Online-only journals:

Journal of Statistics Education (JSE)
Journal of Statistical Software (JSS)The ASA co-sponsors the Current Index to Statistics (CIS)
Quarterly magazine: Chance
The monthly magazine for members Amstat News is available online.Historical publications include:

Edward Jarvis, William Brigham and John Wingate Thornton, Memorial Of The American Statistical Association Praying The Adoption Of Measures For The Correction Of Errors In The Census, 1844
Publications of the American Statistical Association, 1888-1919 (Vols. 1-16) and Quarterly Publications of the American Statistical Association, 1920-1921

Meetings
Meetings provide a platform for scholars and practitioners to exchange research, job opportunities and ideas with each other. ASA holds an annual meeting called Joint Statistical Meetings (JSM), a conference on statistical methodologies and applications called Spring Research Conference (SRC), Conference on Statistical Practice (CSP), and sponsors multiple international meetings and special-interest group meetings.

See also
American Mathematical Society
COPSS Presidents' Award
Fellows of the American Statistical Association
President of the American Statistical Association
Statistics Without Borders (SWB)

References
External links
American Statistical Association
The ASA: the First 160 years by Robert L. Mason
MacTutor: American Statistical Association",https://en.wikipedia.org/wiki/American_Statistical_Association,"['1839 establishments in the United States', 'All articles containing potentially dated statements', 'American Statistical Association', 'Articles containing potentially dated statements from April 2010', 'Articles with short description', 'Commons category link is on Wikidata', 'Professional associations based in the United States', 'Short description is different from Wikidata', 'Statistical organizations in the United States', 'Wikipedia articles with BIBSYS identifiers', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NKC identifiers', 'Wikipedia articles with NLA identifiers', 'Wikipedia articles with SUDOC identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers', 'Wikipedia articles with multiple identifiers']",Data Science
6,Autoencoder,"An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learned, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Variants exist, aiming to force the learned representations to assume useful properties. Examples are regularized autoencoders (Sparse, Denoising and Contractive), which are effective in learning representations for subsequent classification tasks, and Variational autoencoders, with applications as generative models. Autoencoders are applied to many problems, from facial recognition to acquiring the semantic meaning of words.

Introduction
An autoencoder  is a neural network that learns to copy its input to its output. It has an internal (hidden) layer that describes a code used to represent the input, and it is constituted by two main parts: an encoder that maps the input into the code, and a decoder that maps the code to a reconstruction of the input.
Performing the copying task perfectly would simply duplicate the signal, and this is why autoencoders usually are restricted in ways that force them to reconstruct the input approximately, preserving only the most relevant aspects of the data in the copy.
The idea of autoencoders has been popular in the field of neural networks for decades. The first applications date to the 1980s. Their most traditional application was dimensionality reduction or feature learning, but the autoencoder concept became more widely used for learning generative models of data. Some of the most powerful AIs in the 2010s involved sparse autoencoders stacked inside deep neural networks.

Basic architecture
The simplest form of an autoencoder is a feedforward, non-recurrent neural network similar to single layer perceptrons that participate in multilayer perceptrons (MLP) – employing an input layer and an output layer connected by one or more hidden layers. The output layer has the same number of nodes (neurons) as the input layer. Its purpose is to reconstruct its inputs (minimizing the difference between the input and the output) instead of predicting a target value 
  
    
      
        Y
      
    
    {\displaystyle Y}
   given inputs 
  
    
      
        X
      
    
    {\displaystyle X}
  . Therefore, autoencoders are unsupervised learning models. (They do not require labeled inputs to enable learning).
An autoencoder consists of two parts, the encoder and the decoder, which can be defined as transitions 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   and 
  
    
      
        ψ
        ,
      
    
    {\displaystyle \psi ,}
   such that:

  
    
      
        ϕ
        :
        
          
            X
          
        
        →
        
          
            F
          
        
      
    
    {\displaystyle \phi :{\mathcal {X}}\rightarrow {\mathcal {F}}}
  

  
    
      
        ψ
        :
        
          
            F
          
        
        →
        
          
            X
          
        
      
    
    {\displaystyle \psi :{\mathcal {F}}\rightarrow {\mathcal {X}}}
  

  
    
      
        ϕ
        ,
        ψ
        =
        
          
            
              a
              r
              g
              
              m
              i
              n
            
            
              ϕ
              ,
              ψ
            
          
        
        
        ‖
        X
        −
        (
        ψ
        ∘
        ϕ
        )
        X
        
          ‖
          
            2
          
        
      
    
    {\displaystyle \phi ,\psi ={\underset {\phi ,\psi }{\operatorname {arg\,min} }}\,\|X-(\psi \circ \phi )X\|^{2}}
  In the simplest case, given one hidden layer, the encoder stage of an autoencoder takes the input 
  
    
      
        
          x
        
        ∈
        
          
            R
          
          
            d
          
        
        =
        
          
            X
          
        
      
    
    {\displaystyle \mathbf {x} \in \mathbb {R} ^{d}={\mathcal {X}}}
   and maps it to 
  
    
      
        
          h
        
        ∈
        
          
            R
          
          
            p
          
        
        =
        
          
            F
          
        
      
    
    {\displaystyle \mathbf {h} \in \mathbb {R} ^{p}={\mathcal {F}}}
  :

  
    
      
        
          h
        
        =
        σ
        (
        
          W
          x
        
        +
        
          b
        
        )
      
    
    {\displaystyle \mathbf {h} =\sigma (\mathbf {Wx} +\mathbf {b} )}
  This image 
  
    
      
        
          h
        
      
    
    {\displaystyle \mathbf {h} }
   is usually referred to as code, latent variables, or latent representation. Here, 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
   is an element-wise activation function such as a sigmoid function or a rectified linear unit.  
  
    
      
        
          W
        
      
    
    {\displaystyle \mathbf {W} }
   is a weight matrix and 
  
    
      
        
          b
        
      
    
    {\displaystyle \mathbf {b} }
   is a bias vector. Weights and biases are usually initialized randomly, and then updated iteratively during training through backpropagation. After that, the decoder stage of the autoencoder maps 
  
    
      
        
          h
        
      
    
    {\displaystyle \mathbf {h} }
   to the reconstruction 
  
    
      
        
          
            x
            ′
          
        
      
    
    {\displaystyle \mathbf {x'} }
   of the same shape as 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
  :

  
    
      
        
          
            x
            ′
          
        
        =
        
          σ
          ′
        
        (
        
          
            W
            ′
          
          h
        
        +
        
          
            b
            ′
          
        
        )
      
    
    {\displaystyle \mathbf {x'} =\sigma '(\mathbf {W'h} +\mathbf {b'} )}
  where 
  
    
      
        
          
            σ
            ′
          
        
        ,
        
          
            W
            ′
          
        
        ,
        
           and 
        
        
          
            b
            ′
          
        
      
    
    {\displaystyle \mathbf {\sigma '} ,\mathbf {W'} ,{\text{ and }}\mathbf {b'} }
   for the decoder may be unrelated to the corresponding 
  
    
      
        
          σ
        
        ,
        
          W
        
        ,
        
           and 
        
        
          b
        
      
    
    {\displaystyle \mathbf {\sigma } ,\mathbf {W} ,{\text{ and }}\mathbf {b} }
   for the encoder.
Autoencoders are trained to minimise reconstruction errors (such as squared errors), often referred to as the ""loss"":

  
    
      
        
          
            L
          
        
        (
        
          x
        
        ,
        
          
            x
            ′
          
        
        )
        =
        ‖
        
          x
        
        −
        
          
            x
            ′
          
        
        
          ‖
          
            2
          
        
        =
        ‖
        
          x
        
        −
        
          σ
          ′
        
        (
        
          
            W
            ′
          
        
        (
        σ
        (
        
          W
          x
        
        +
        
          b
        
        )
        )
        +
        
          
            b
            ′
          
        
        )
        
          ‖
          
            2
          
        
      
    
    {\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )=\|\mathbf {x} -\mathbf {x'} \|^{2}=\|\mathbf {x} -\sigma '(\mathbf {W'} (\sigma (\mathbf {Wx} +\mathbf {b} ))+\mathbf {b'} )\|^{2}}
  where 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   is usually averaged over some input training set.
As mentioned before, the training of an autoencoder is performed through backpropagation of the error, just like a regular feedforward neural network.
Should the feature space 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   have lower dimensionality than the input space 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
  , the feature vector 
  
    
      
        ϕ
        (
        x
        )
      
    
    {\displaystyle \phi (x)}
   can be regarded as a compressed representation of the input 
  
    
      
        x
      
    
    {\displaystyle x}
  . This is the case of undercomplete autoencoders. If the hidden layers are larger than (overcomplete autoencoders), or equal to, the input layer, or the hidden units are given enough capacity, an autoencoder can potentially learn the identity function and become useless. However, experimental results have shown that autoencoders might still learn useful features in these cases. In the ideal setting, one should be able to tailor the code dimension and the model capacity on the basis of the complexity of the data distribution to be modeled. One way to do so is to exploit the model variants known as Regularized Autoencoders.

Variations
Regularized autoencoders
Various techniques exist to prevent autoencoders from learning the identity function and to improve their ability to capture important information and learn richer representations.

Sparse autoencoder (SAE)
When representations are learned in a way that encourages sparsity, improved performance is obtained on classification tasks. Sparse autoencoder may include more (rather than fewer) hidden units than inputs, but only a small number of the hidden units are allowed to be active at the same time. This sparsity constraint forces the model to respond to the unique statistical features of the training data.
Specifically, a sparse autoencoder is an autoencoder whose training criterion involves a sparsity penalty 
  
    
      
        Ω
        (
        
          h
        
        )
      
    
    {\displaystyle \Omega ({\boldsymbol {h}})}
   on the code layer 
  
    
      
        
          h
        
      
    
    {\displaystyle {\boldsymbol {h}}}
  .

  
    
      
        
          
            L
          
        
        (
        
          x
        
        ,
        
          
            x
            ′
          
        
        )
        +
        Ω
        (
        
          h
        
        )
      
    
    {\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\Omega ({\boldsymbol {h}})}
  
Recalling that 
  
    
      
        
          h
        
        =
        f
        (
        
          W
        
        
          x
        
        +
        
          b
        
        )
      
    
    {\displaystyle {\boldsymbol {h}}=f({\boldsymbol {W}}{\boldsymbol {x}}+{\boldsymbol {b}})}
  , the penalty encourages the model to activate (i.e. output value close to 1) specific areas of the network on the basis of the input data, while inactivating all other neurons (i.e. to have an output value close to 0).This sparsity can be achieved by formulating the penalty terms in different ways.

One way is to exploit the Kullback-Leibler (KL) divergence.  Let
  
    
      
        
          
            
              
                ρ
                
                  j
                
              
              ^
            
          
        
        =
        
          
            1
            m
          
        
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        [
        
          h
          
            j
          
        
        (
        
          x
          
            i
          
        
        )
        ]
      
    
    {\displaystyle {\hat {\rho _{j}}}={\frac {1}{m}}\sum _{i=1}^{m}[h_{j}(x_{i})]}
  
be the average activation of the hidden unit 
  
    
      
        j
      
    
    {\displaystyle j}
   (averaged over the  
  
    
      
        m
      
    
    {\displaystyle m}
   training examples). The notation 
  
    
      
        
          h
          
            j
          
        
        (
        
          x
          
            i
          
        
        )
      
    
    {\displaystyle h_{j}(x_{i})}
   identifies the input value that triggered the activation. To encourage most of the neurons to be inactive, 
  
    
      
        
          
            
              
                ρ
                
                  j
                
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {\rho _{j}}}}
   needs to be close to 0. Therefore, this method enforces the constraint 
  
    
      
        
          
            
              
                ρ
                
                  j
                
              
              ^
            
          
        
        =
        ρ
      
    
    {\displaystyle {\hat {\rho _{j}}}=\rho }
    where 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
   is the sparsity parameter, a value close to zero. The penalty term 
  
    
      
        Ω
        (
        
          h
        
        )
      
    
    {\displaystyle \Omega ({\boldsymbol {h}})}
   takes a form that penalizes 
  
    
      
        
          
            
              
                ρ
                
                  j
                
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {\rho _{j}}}}
   for deviating significantly from 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
  , exploiting the KL divergence:

  
    
      
        
          ∑
          
            j
            =
            1
          
          
            s
          
        
        K
        L
        (
        ρ
        
          |
        
        
          |
        
        
          
            
              
                ρ
                
                  j
                
              
              ^
            
          
        
        )
        =
        
          ∑
          
            j
            =
            1
          
          
            s
          
        
        
          [
          
            ρ
            log
            ⁡
            
              
                ρ
                
                  
                    
                      ρ
                      
                        j
                      
                    
                    ^
                  
                
              
            
            +
            (
            1
            −
            ρ
            )
            log
            ⁡
            
              
                
                  1
                  −
                  ρ
                
                
                  1
                  −
                  
                    
                      
                        
                          ρ
                          
                            j
                          
                        
                        ^
                      
                    
                  
                
              
            
          
          ]
        
      
    
    {\displaystyle \sum _{j=1}^{s}KL(\rho ||{\hat {\rho _{j}}})=\sum _{j=1}^{s}\left[\rho \log {\frac {\rho }{\hat {\rho _{j}}}}+(1-\rho )\log {\frac {1-\rho }{1-{\hat {\rho _{j}}}}}\right]}
    where 
  
    
      
        j
      
    
    {\displaystyle j}
   is summing over the 
  
    
      
        s
      
    
    {\displaystyle s}
   hidden nodes in the hidden layer, and 
  
    
      
        K
        L
        (
        ρ
        
          |
        
        
          |
        
        
          
            
              
                ρ
                
                  j
                
              
              ^
            
          
        
        )
      
    
    {\displaystyle KL(\rho ||{\hat {\rho _{j}}})}
   is the KL-divergence between a Bernoulli random variable with mean 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
   and a Bernoulli random variable with mean 
  
    
      
        
          
            
              
                ρ
                
                  j
                
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {\rho _{j}}}}
  .
Another way to achieve sparsity is by applying L1 or L2 regularization terms on the activation, scaled by a certain parameter 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
  . For instance, in the case of L1 the loss function becomes
  
    
      
        
          
            L
          
        
        (
        
          x
        
        ,
        
          
            x
            ′
          
        
        )
        +
        λ
        
          ∑
          
            i
          
        
        
          |
        
        
          h
          
            i
          
        
        
          |
        
      
    
    {\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\lambda \sum _{i}|h_{i}|}
  

A further proposed strategy to force sparsity is to manually zero all but the strongest hidden unit activations (k-sparse autoencoder). The k-sparse autoencoder is based on a linear autoencoder (i.e. with linear activation function) and tied weights. The identification of the strongest activations can be achieved by sorting the activities and keeping only the first k values, or by using ReLU hidden units with thresholds that are adaptively adjusted until the k largest activities are identified. This selection acts like the previously mentioned regularization terms in that it prevents the model from reconstructing the input using too many neurons.

Denoising autoencoder (DAE)
Denoising autoencoders (DAE) try to achieve a good representation by changing the reconstruction criterion.Indeed, DAEs take a partially corrupted input and are trained to recover the original undistorted input.  In practice, the objective of denoising autoencoders is that of cleaning the corrupted input, or denoising. Two assumptions are inherent to this approach:

Higher level representations are relatively stable and robust to the corruption of the input;
To perform denoising well, the model needs to extract features that capture useful structure in the input distribution.In other words, denoising is advocated as a training criterion for learning to extract useful features that will constitute better higher level representations of the input.The training process of a DAE works as follows:

The initial input 
  
    
      
        x
      
    
    {\displaystyle x}
   is corrupted into 
  
    
      
        
          
            
              x
              ~
            
          
        
      
    
    {\displaystyle {\boldsymbol {\tilde {x}}}}
   through stochastic mapping 
  
    
      
        
          
            
              x
              ~
            
          
        
        ∼
        
          q
          
            D
          
        
        (
        
          
            
              x
              ~
            
          
        
        
          |
        
        
          x
        
        )
      
    
    {\displaystyle {\boldsymbol {\tilde {x}}}\thicksim q_{D}({\boldsymbol {\tilde {x}}}|{\boldsymbol {x}})}
  .
The corrupted input 
  
    
      
        
          
            
              x
              ~
            
          
        
      
    
    {\displaystyle {\boldsymbol {\tilde {x}}}}
   is then mapped to a hidden representation with the same process of the standard autoencoder, 
  
    
      
        
          h
        
        =
        
          f
          
            θ
          
        
        (
        
          
            
              x
              ~
            
          
        
        )
        =
        s
        (
        
          W
        
        
          
            
              x
              ~
            
          
        
        +
        
          b
        
        )
      
    
    {\displaystyle {\boldsymbol {h}}=f_{\theta }({\boldsymbol {\tilde {x}}})=s({\boldsymbol {W}}{\boldsymbol {\tilde {x}}}+{\boldsymbol {b}})}
  .
From the hidden representation the model reconstructs 
  
    
      
        
          z
        
        =
        
          g
          
            
              θ
              ′
            
          
        
        (
        
          h
        
        )
      
    
    {\displaystyle {\boldsymbol {z}}=g_{\theta '}({\boldsymbol {h}})}
  .The model's parameters 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   and 
  
    
      
        
          θ
          ′
        
      
    
    {\displaystyle \theta '}
   are trained to minimize the average reconstruction error over the training data, specifically, minimizing the difference between 
  
    
      
        
          z
        
      
    
    {\displaystyle {\boldsymbol {z}}}
   and the original uncorrupted input  
  
    
      
        
          x
        
      
    
    {\displaystyle {\boldsymbol {x}}}
  . Note that each time a random example 
  
    
      
        
          x
        
      
    
    {\displaystyle {\boldsymbol {x}}}
   is presented to the model, a new corrupted version is generated stochastically on the basis of 
  
    
      
        
          q
          
            D
          
        
        (
        
          
            
              x
              ~
            
          
        
        
          |
        
        
          x
        
        )
      
    
    {\displaystyle q_{D}({\boldsymbol {\tilde {x}}}|{\boldsymbol {x}})}
  .
The above-mentioned training process could be applied with any kind of corruption process. Some examples might be additive isotropic Gaussian noise, Masking noise (a fraction of the input chosen at random for each example is forced to 0) or Salt-and-pepper noise (a fraction of the input chosen at random for each example is set to its minimum or maximum value with uniform probability).The corruption of the input is performed only during training. Once the model has learnt the optimal parameters, in order to extract the representations from the original data no corruption is added.

Contractive autoencoder (CAE)
The contractive autoencoder adds an explicit regularizer in its objective function that forces the model to learn an encoding robust to slight variations of input values. This regularizer corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. Since the penalty is applied to training examples only, this term forces the model to learn useful information about the training distribution. The final objective function has the following form:

  
    
      
        
          
            L
          
        
        (
        
          x
        
        ,
        
          
            x
            ′
          
        
        )
        +
        λ
        
          ∑
          
            i
          
        
        
          |
        
        
          |
        
        
          ∇
          
            x
          
        
        
          h
          
            i
          
        
        
          |
        
        
          
            |
          
          
            2
          
        
      
    
    {\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\lambda \sum _{i}||\nabla _{x}h_{i}||^{2}}
  The autoencoder is termed contractive because CAE is encouraged to map a neighborhood of input points to a smaller neighborhood of output points.DAE is connected to CAE: in the limit of small Gaussian input noise, DAEs make the reconstruction function resist small but finite-sized input perturbations, while CAEs make the extracted features resist infinitesimal input perturbations.

Concrete autoencoder
The concrete autoencoder is variation of the standard autoencoder architecture that is designed for discrete feature selection. Unlike a standard autoencoder, which learns a latent representation that is combination of potentially all of the input features, the concrete autoencoder enforces the latent space to consist only of a number of features that is user-specified. The concrete autoencoder uses a continuous relaxation of the Categorical distribution to allow gradients to pass through the feature selector layer, which makes it possible to use standard backpropagation to learn an optimal subset of input features that minimize reconstruction loss.

Variational autoencoder (VAE)
Variational autoencoders (VAEs) are generative models, akin to generative adversarial networks. Their association with this group of models derives mainly from the architectural affinity with the basic autoencoder (the final training objective has an encoder and a decoder), but their mathematical formulation differs significantly. VAEs are directed probabilistic graphical models (DPGM) whose posterior is approximated by a neural network, forming an autoencoder-like architecture. Unlike discriminative modeling that aims to learn a predictor given observation, generative modeling tries to learn how the data is generated, and to reflect the underlying causal relations. Causal relations have the potential for generalizability.Variational autoencoder models make strong assumptions concerning the distribution of latent variables. They use a variational approach for latent representation learning, which results in an additional loss component and a specific estimator for the training algorithm called the Stochastic Gradient Variational Bayes (SGVB) estimator. It assumes that the data is generated by a directed graphical model 
  
    
      
        
          p
          
            θ
          
        
        (
        
          x
        
        
          |
        
        
          h
        
        )
      
    
    {\displaystyle p_{\theta }(\mathbf {x} |\mathbf {h} )}
   and that the encoder is learning an approximation 
  
    
      
        
          q
          
            ϕ
          
        
        (
        
          h
        
        
          |
        
        
          x
        
        )
      
    
    {\displaystyle q_{\phi }(\mathbf {h} |\mathbf {x} )}
   to the posterior distribution 
  
    
      
        
          p
          
            θ
          
        
        (
        
          h
        
        
          |
        
        
          x
        
        )
      
    
    {\displaystyle p_{\theta }(\mathbf {h} |\mathbf {x} )}
   where 
  
    
      
        
          ϕ
        
      
    
    {\displaystyle \mathbf {\phi } }
   and 
  
    
      
        
          θ
        
      
    
    {\displaystyle \mathbf {\theta } }
   denote the parameters of the encoder (recognition model) and decoder (generative model) respectively. The probability distribution of the latent vector of a VAE typically matches that of the training data much closer than a standard autoencoder. The objective of VAE has the following form:

  
    
      
        
          
            L
          
        
        (
        
          ϕ
        
        ,
        
          θ
        
        ,
        
          x
        
        )
        =
        
          D
          
            
              K
              L
            
          
        
        (
        
          q
          
            ϕ
          
        
        (
        
          h
        
        
          |
        
        
          x
        
        )
        ‖
        
          p
          
            θ
          
        
        (
        
          h
        
        )
        )
        −
        
          
            E
          
          
            
              q
              
                ϕ
              
            
            (
            
              h
            
            
              |
            
            
              x
            
            )
          
        
        
          
            (
          
        
        log
        ⁡
        
          p
          
            θ
          
        
        (
        
          x
        
        
          |
        
        
          h
        
        )
        
          
            )
          
        
      
    
    {\displaystyle {\mathcal {L}}(\mathbf {\phi } ,\mathbf {\theta } ,\mathbf {x} )=D_{\mathrm {KL} }(q_{\phi }(\mathbf {h} |\mathbf {x} )\Vert p_{\theta }(\mathbf {h} ))-\mathbb {E} _{q_{\phi }(\mathbf {h} |\mathbf {x} )}{\big (}\log p_{\theta }(\mathbf {x} |\mathbf {h} ){\big )}}
  Here, 
  
    
      
        
          D
          
            
              K
              L
            
          
        
      
    
    {\displaystyle D_{\mathrm {KL} }}
   stands for the Kullback–Leibler divergence. The prior over the latent variables is usually set to be the centred isotropic multivariate Gaussian 
  
    
      
        
          p
          
            θ
          
        
        (
        
          h
        
        )
        =
        
          
            N
          
        
        (
        
          0
          ,
          I
        
        )
      
    
    {\displaystyle p_{\theta }(\mathbf {h} )={\mathcal {N}}(\mathbf {0,I} )}
  ; however, alternative configurations have been considered.Commonly, the shape of the variational and the likelihood distributions are chosen such that they are factorized Gaussians:

  
    
      
        
          
            
              
                
                  q
                  
                    ϕ
                  
                
                (
                
                  h
                
                
                  |
                
                
                  x
                
                )
              
              
                
                =
                
                  
                    N
                  
                
                (
                
                  ρ
                
                (
                
                  x
                
                )
                ,
                
                  
                    ω
                  
                  
                    2
                  
                
                (
                
                  x
                
                )
                
                  I
                
                )
                ,
              
            
            
              
                
                  p
                  
                    θ
                  
                
                (
                
                  x
                
                
                  |
                
                
                  h
                
                )
              
              
                
                =
                
                  
                    N
                  
                
                (
                
                  μ
                
                (
                
                  h
                
                )
                ,
                
                  
                    σ
                  
                  
                    2
                  
                
                (
                
                  h
                
                )
                
                  I
                
                )
                ,
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}q_{\phi }(\mathbf {h} |\mathbf {x} )&={\mathcal {N}}({\boldsymbol {\rho }}(\mathbf {x} ),{\boldsymbol {\omega }}^{2}(\mathbf {x} )\mathbf {I} ),\\p_{\theta }(\mathbf {x} |\mathbf {h} )&={\mathcal {N}}({\boldsymbol {\mu }}(\mathbf {h} ),{\boldsymbol {\sigma }}^{2}(\mathbf {h} )\mathbf {I} ),\end{aligned}}}
  where 
  
    
      
        
          ρ
        
        (
        
          x
        
        )
      
    
    {\displaystyle {\boldsymbol {\rho }}(\mathbf {x} )}
   and  
  
    
      
        
          
            ω
          
          
            2
          
        
        (
        
          x
        
        )
      
    
    {\displaystyle {\boldsymbol {\omega }}^{2}(\mathbf {x} )}
   are the encoder outputs, while 
  
    
      
        
          μ
        
        (
        
          h
        
        )
      
    
    {\displaystyle {\boldsymbol {\mu }}(\mathbf {h} )}
   and  
  
    
      
        
          
            σ
          
          
            2
          
        
        (
        
          h
        
        )
      
    
    {\displaystyle {\boldsymbol {\sigma }}^{2}(\mathbf {h} )}
   are the decoder outputs. This choice is justified by the simplifications that it produces when evaluating both the KL divergence and the likelihood term in variational objective defined above.
VAE have been criticized because they generate blurry images. However, researchers employing this model were showing only the mean of the distributions, 
  
    
      
        
          μ
        
        (
        
          h
        
        )
      
    
    {\displaystyle {\boldsymbol {\mu }}(\mathbf {h} )}
  , rather than a sample of the learned Gaussian distribution

  
    
      
        
          x
        
        ∼
        
          
            N
          
        
        (
        
          μ
        
        (
        
          h
        
        )
        ,
        
          
            σ
          
          
            2
          
        
        (
        
          h
        
        )
        
          I
        
        )
      
    
    {\displaystyle \mathbf {x} \sim {\mathcal {N}}({\boldsymbol {\mu }}(\mathbf {h} ),{\boldsymbol {\sigma }}^{2}(\mathbf {h} )\mathbf {I} )}
  .These samples were shown to be overly noisy due to the choice of a factorized Gaussian distribution. Employing a Gaussian distribution with a full covariance matrix,

  
    
      
        
          p
          
            θ
          
        
        (
        
          x
        
        
          |
        
        
          h
        
        )
        =
        
          
            N
          
        
        (
        
          μ
        
        (
        
          h
        
        )
        ,
        
          Σ
        
        (
        
          h
        
        )
        )
        ,
      
    
    {\displaystyle p_{\theta }(\mathbf {x} |\mathbf {h} )={\mathcal {N}}({\boldsymbol {\mu }}(\mathbf {h} ),{\boldsymbol {\Sigma }}(\mathbf {h} )),}
  could solve this issue, but is computationally intractable and numerically unstable, as it requires estimating a covariance matrix from a single data sample. However, later research showed that a restricted approach where the inverse matrix 
  
    
      
        
          
            Σ
          
          
            −
            1
          
        
        (
        
          h
        
        )
      
    
    {\displaystyle {\boldsymbol {\Sigma }}^{-1}(\mathbf {h} )}
   is sparse could generate images with high-frequency details.
Large-scale VAE models have been developed in different domains to represent data in a compact probabilistic latent space. For example, VQ-VAE for image generation and Optimus  for language modeling.

Advantages of depth
Autoencoders are often trained with a single layer encoder and a single layer decoder, but using deep (many-layered) encoders and decoders offers many advantages.
Depth can exponentially reduce the computational cost of representing some functions.
Depth can exponentially decrease the amount of training data needed to learn some functions.
Experimentally, deep autoencoders yield better compression compared to shallow or linear autoencoders.

Training
Geoffrey Hinton developed a technique for training many-layered deep autoencoders. His method involves treating each neighbouring set of two layers as a restricted Boltzmann machine so that pretraining approximates a good solution, then using backpropagation to fine-tune the results. This model takes the name of deep belief network.
Researchers have debated whether joint training (i.e. training the whole architecture together with a single global reconstruction objective to optimize) would be better for deep auto-encoders. A 2015 study showed that joint training learns better data models along with more representative features for classification as compared to the layerwise method. However, their experiments showed that the success of joint training depends heavily on the regularization strategies adopted.

Applications
The two main applications of autoencoders are dimensionality reduction and information retrieval, but modern variations were proven successful when applied to different tasks.

Dimensionality reduction
Dimensionality reduction was one of the first deep learning applications, and one of the early motivations to study autoencoders.  The objective is to find a proper projection method that maps data from high feature space to low feature space.One milestone paper on the subject was Hinton's 2006 paper: in that study, he pretrained a multi-layer autoencoder with a stack of RBMs and then used their weights to initialize a deep autoencoder with gradually smaller hidden layers until hitting a bottleneck of 30 neurons. The resulting 30 dimensions of the code yielded a smaller reconstruction error compared to the first 30 components of a principal component analysis (PCA), and learned a representation that was qualitatively easier to interpret, clearly separating data clusters.Representing data in a lower-dimensional space can improve performance on tasks such as classification. Indeed, many forms of dimensionality reduction place semantically related examples near each other, aiding generalization.

Principal component analysis
If linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to principal component analysis (PCA). The weights of an autoencoder with a single hidden layer of size 
  
    
      
        p
      
    
    {\displaystyle p}
   (where 
  
    
      
        p
      
    
    {\displaystyle p}
   is less than the size of the input) span the same vector subspace as the one spanned by the first 
  
    
      
        p
      
    
    {\displaystyle p}
   principal components, and the output of the autoencoder is an orthogonal projection onto this subspace. The autoencoder weights are not equal to the principal components, and are generally not orthogonal, yet the principal components may be recovered from them using the singular value decomposition.However, the potential of autoencoders resides in their non-linearity, allowing the model to learn more powerful generalizations compared to PCA, and to reconstruct the input with significantly lower information loss.

Information retrieval
Information retrieval benefits particularly from dimensionality reduction in that search can become more efficient in certain kinds of low dimensional spaces. Autoencoders were indeed applied to semantic hashing, proposed by Salakhutdinov and Hinton in 2007. By training the algorithm to produce a low-dimensional binary code, all database entries could be stored in a hash table mapping binary code vectors to entries. This table would then support information retrieval by returning all entries with the same binary code as the query, or slightly less similar entries by flipping some bits from the query encoding.

Anomaly detection
Another application for autoencoders is anomaly detection. By learning to replicate the most salient features in the training data under some of the constraints described previously, the model is encouraged to learn to precisely reproduce the most frequently observed characteristics. When facing anomalies, the model should worsen its reconstruction performance. In most cases, only data with normal instances are used to train the autoencoder; in others, the frequency of anomalies is small compared to the observation set so that its contribution to the learned representation could be ignored. After training, the autoencoder will accurately reconstruct ""normal"" data, while failing to do so with unfamiliar anomalous data. Reconstruction error (the error between the original data and its low dimensional reconstruction) is used as an anomaly score to detect anomalies.Recent literature has however shown that certain autoencoding models can, counterintuitively, be very good at reconstructing anomalous examples and consequently not able to reliably perform anomaly detection.

Image processing
The characteristics of autoencoders are useful in image processing.
One example can be found in lossy image compression, where autoencoders outperformed other approaches and proved competitive against JPEG 2000.Another useful application of autoencoders in image preprocessing is image denoising.Autoencoders found use in more demanding contexts such as medical imaging where they have been used for image denoising as well as super-resolution In image-assisted diagnosis, experiments have applied autoencoders for breast cancer detection and for modelling the relation between the cognitive decline of Alzheimer's Disease and the latent features of an autoencoder trained with MRI.

Drug discovery
In 2019 molecules generated with variational autoencoders were validated experimentally in mice.

Popularity prediction
Recently, a stacked autoencoder framework produced promising results in predicting popularity of social media posts, which is helpful for online advertising strategies.

Machine Translation
Autoencoder has been applied to machine translation, which is usually referred to as neural machine translation (NMT). In NMT, texts are treated as sequences to be encoded into the learning procedure, while on the decoder side the target languages are generated. Language-specific autoencoders incorporate linguistic features into the learning procedure, such as Chinese decomposition features.

See also
Representation learning
Sparse dictionary learning
Deep learning


== References ==",https://en.wikipedia.org/wiki/Autoencoder,"['All articles to be split', 'All articles with unsourced statements', 'Articles to be split from May 2020', 'Articles with short description', 'Articles with unsourced statements from February 2021', 'Artificial neural networks', 'CS1 errors: missing periodical', 'Dimension reduction', 'Short description matches Wikidata', 'Unsupervised learning', 'Use dmy dates from March 2020']",Data Science
7,Automated machine learning,"Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. AutoML covers the complete pipeline from the raw dataset to the deployable machine learning model. AutoML was proposed as an artificial intelligence-based solution to the ever-growing challenge of applying machine learning. The high degree of automation in AutoML allows non-experts to make use of machine learning models and techniques without requiring them to become experts in the field first. 
Automating the process of applying machine learning end-to-end, additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models.

Comparison to the standard approach
In a typical machine learning application, practitioners have a set of input data points to be used for training. The raw data may not be in a form that all algorithms can be applied to it. To make the data amenable for machine learning, an expert may have to apply appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods. After these steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their model. Each of these steps may be challenging, resulting in significant hurdles to using machine learning.
AutoML dramatically simplifies these steps for non-experts.

Targets of automation
Automated machine learning can target various stages of the machine learning process.  Steps to automate are:

Data preparation and ingestion (from raw data and miscellaneous formats)
Column type detection; e.g., boolean, discrete numerical, continuous numerical, or text
Column intent detection; e.g., target/label, stratification field, numerical feature, categorical text feature, or free text feature
Task detection; e.g., binary classification, regression, clustering, or ranking
Feature engineering
Feature selection
Feature extraction
Meta learning and transfer learning
Detection and handling of skewed data and/or missing values
Model selection
Hyperparameter optimization of the learning algorithm and featurization
Pipeline selection under time, memory, and complexity constraints
Selection of evaluation metrics and validation procedures
Problem checking
Leakage detection
Misconfiguration detection
Analysis of obtained results
Creating user interfaces and visualizations

See also
Neural architecture search
Neuroevolution
Self-tuning
Neural Network Intelligence
AutoAI
ModelOps

References
Further reading
""Open Source AutoML Tools: AutoGluon, TransmogrifAI, Auto-sklearn, and NNI"". Bizety. 2020-06-16.

External links
Azure ML documentation – What is AutoML? – Microsoft Azure cloud service documentation
Google Cloud AutoML, AutoML solution on Google Cloud Platform
AutoAI with IBM Watson Studio: automation of data preparation, model development, feature engineering, and hyper-parameter optimization in IBM Watson Studio
The Oracle AutoML Pipeline, documentation of Oracle Accelerated Data Science (ADS) SDK, a Python library included as part of the Oracle Cloud Infrastructure Data Science service",https://en.wikipedia.org/wiki/Automated_machine_learning,"['All Wikipedia articles that are incomprehensible', 'All articles that are too technical', 'All pages needing cleanup', 'Articles needing cleanup from March 2018', 'Articles with multiple maintenance issues', 'Articles with sections that need to be turned into prose from March 2018', 'Articles with short description', 'Artificial intelligence', 'Machine learning', 'Short description is different from Wikidata', 'Wikipedia articles that are incomprehensible from November 2018', 'Wikipedia articles that are too technical from November 2018']",Data Science
8,BIRCH,"BIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets. With modifications it can also be used to accelerate k-means clustering and Gaussian mixture modeling with the expectation–maximization algorithm. An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database.
Its inventors claim BIRCH to be the ""first clustering algorithm proposed in the database area to handle 'noise' (data points that are not part of the underlying pattern) effectively"", beating DBSCAN by two months. The BIRCH algorithm received the SIGMOD 10 year test of time award in 2006.

Problem with previous methods
Previous clustering algorithms performed less effectively over very large databases and did not adequately consider the case wherein a data-set was too large to fit in main memory. As a result, there was a lot of overhead maintaining high clustering quality while minimizing the cost of additional IO (input/output) operations. Furthermore, most of BIRCH's predecessors inspect all data points (or all currently existing clusters) equally for each 'clustering decision' and do not perform heuristic weighting based on the distance between these data points.

Advantages with BIRCH
It is local in that each clustering decision is made without scanning all data points and currently existing clusters.
It exploits the observation that the data space is not usually uniformly occupied and not every data point is equally important.
It makes full use of available memory to derive the finest possible sub-clusters while minimizing I/O costs.
It is also an incremental method that does not require the whole data set in advance.

Algorithm
The BIRCH algorithm takes as input a set of N data points, represented as real-valued vectors, and a desired number of clusters K. It operates in four phases, the second of which is optional.
The first phase builds a clustering feature (
  
    
      
        C
        F
      
    
    {\displaystyle CF}
  ) tree out of the data points, a height-balanced tree data structure, defined as follows:

Given a set of N d-dimensional data points, the clustering feature 
  
    
      
        C
        F
      
    
    {\displaystyle CF}
   of the set is defined as the triple 
  
    
      
        C
        F
        =
        (
        N
        ,
        
          
            
              L
              S
            
            →
          
        
        ,
        S
        S
        )
      
    
    {\displaystyle CF=(N,{\overrightarrow {LS}},SS)}
  , where 
  
    
      
        
          
            
              L
              S
            
            →
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        
          
            
              X
              
                i
              
            
            →
          
        
      
    
    {\displaystyle {\overrightarrow {LS}}=\sum _{i=1}^{N}{\overrightarrow {X_{i}}}}
   is the linear sum and 
  
    
      
        S
        S
        =
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        (
        
          
            
              X
              
                i
              
            
            →
          
        
        
          )
          
            2
          
        
      
    
    {\displaystyle SS=\sum _{i=1}^{N}({\overrightarrow {X_{i}}})^{2}}
   is the square sum of data points.
Clustering features are organized in a CF tree, a height-balanced tree with two parameters: branching factor 
  
    
      
        B
      
    
    {\displaystyle B}
   and threshold 
  
    
      
        T
      
    
    {\displaystyle T}
  . Each non-leaf node contains at most 
  
    
      
        B
      
    
    {\displaystyle B}
   entries of the form 
  
    
      
        [
        C
        
          F
          
            i
          
        
        ,
        c
        h
        i
        l
        
          d
          
            i
          
        
        ]
      
    
    {\displaystyle [CF_{i},child_{i}]}
  , where 
  
    
      
        c
        h
        i
        l
        
          d
          
            i
          
        
      
    
    {\displaystyle child_{i}}
   is a pointer to its 
  
    
      
        i
      
    
    {\displaystyle i}
  th child node and 
  
    
      
        C
        
          F
          
            i
          
        
      
    
    {\displaystyle CF_{i}}
   the clustering feature representing the associated subcluster. A leaf node contains at most 
  
    
      
        L
      
    
    {\displaystyle L}
   entries each of the form 
  
    
      
        [
        C
        
          F
          
            i
          
        
        ]
      
    
    {\displaystyle [CF_{i}]}
   . It also has two pointers prev and next which are used to chain all leaf nodes together. The tree size depends on the parameter 
  
    
      
        T
      
    
    {\displaystyle T}
  . A node is required to fit in a page of size 
  
    
      
        P
      
    
    {\displaystyle P}
  . 
  
    
      
        B
      
    
    {\displaystyle B}
   and 
  
    
      
        L
      
    
    {\displaystyle L}
   are determined by 
  
    
      
        P
      
    
    {\displaystyle P}
  . So 
  
    
      
        P
      
    
    {\displaystyle P}
   can be varied for performance tuning. It is a very compact representation of the dataset because each entry in a leaf node is not a single data point but a subcluster.In the second step, the algorithm scans all the leaf entries in the initial 
  
    
      
        C
        F
      
    
    {\displaystyle CF}
   tree to rebuild a smaller 
  
    
      
        C
        F
      
    
    {\displaystyle CF}
   tree, while removing outliers and grouping crowded subclusters into larger ones. This step is marked optional in the original presentation of BIRCH.
In step three an existing clustering algorithm is used to cluster all leaf entries. Here an agglomerative hierarchical clustering algorithm is applied directly to the subclusters represented by their 
  
    
      
        C
        F
      
    
    {\displaystyle CF}
   vectors. It also provides the flexibility of allowing the user to specify either the desired number of clusters or the desired diameter threshold for clusters. After this step a set of clusters is obtained that captures major distribution pattern in the data. However, there might exist minor and localized inaccuracies which can be handled by an optional step 4. In step 4 the centroids of the clusters produced in step 3 are used as seeds and redistribute the data points to its closest seeds to obtain a new set of clusters. Step 4 also provides us with an option of discarding outliers. That is a point which is too far from its closest seed can be treated as an outlier.

Calculations with the clustering features
Given only the clustering feature 
  
    
      
        C
        F
        =
        [
        N
        ,
        
          
            
              L
              S
            
            →
          
        
        ,
        S
        S
        ]
      
    
    {\displaystyle CF=[N,{\overrightarrow {LS}},SS]}
  , the same measures can be calculated without the knowledge of the underlying actual values.

Centroid: 
  
    
      
        
          
            C
            →
          
        
        =
        
          
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  N
                
              
              
                
                  
                    X
                    
                      i
                    
                  
                  →
                
              
            
            N
          
        
        =
        
          
            
              
                L
                S
              
              →
            
            N
          
        
      
    
    {\displaystyle {\overrightarrow {C}}={\frac {\sum _{i=1}^{N}{\overrightarrow {X_{i}}}}{N}}={\frac {\overrightarrow {LS}}{N}}}
  
Radius: 
  
    
      
        R
        =
        
          
            
              
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    N
                  
                
                (
                
                  
                    
                      X
                      
                        i
                      
                    
                    →
                  
                
                −
                
                  
                    C
                    →
                  
                
                
                  )
                  
                    2
                  
                
              
              N
            
          
        
        =
        
          
            
              
                N
                ⋅
                
                  
                    
                      C
                      →
                    
                  
                  
                    2
                  
                
                +
                S
                S
                −
                2
                ⋅
                
                  
                    C
                    →
                  
                
                ⋅
                
                  
                    
                      L
                      S
                    
                    →
                  
                
              
              N
            
          
        
        =
        
          
            
              
                
                  S
                  S
                
                N
              
            
            −
            (
            
              
                
                  
                    L
                    S
                  
                  →
                
                N
              
            
            
              )
              
                2
              
            
          
        
      
    
    {\displaystyle R={\sqrt {\frac {\sum _{i=1}^{N}({\overrightarrow {X_{i}}}-{\overrightarrow {C}})^{2}}{N}}}={\sqrt {\frac {N\cdot {\overrightarrow {C}}^{2}+SS-2\cdot {\overrightarrow {C}}\cdot {\overrightarrow {LS}}}{N}}}={\sqrt {{\frac {SS}{N}}-({\frac {\overrightarrow {LS}}{N}})^{2}}}}
  
Average Linkage Distance between clusters 
  
    
      
        C
        
          F
          
            1
          
        
        =
        [
        
          N
          
            1
          
        
        ,
        
          
            
              L
              
                S
                
                  1
                
              
            
            →
          
        
        ,
        S
        
          S
          
            1
          
        
        ]
      
    
    {\displaystyle CF_{1}=[N_{1},{\overrightarrow {LS_{1}}},SS_{1}]}
   and 
  
    
      
        C
        
          F
          
            2
          
        
        =
        [
        
          N
          
            2
          
        
        ,
        
          
            
              L
              
                S
                
                  2
                
              
            
            →
          
        
        ,
        S
        
          S
          
            2
          
        
        ]
      
    
    {\displaystyle CF_{2}=[N_{2},{\overrightarrow {LS_{2}}},SS_{2}]}
  :
  
    
      
        
          D
          
            2
          
        
        =
        
          
            
              
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    
                      N
                      
                        1
                      
                    
                  
                
                
                  ∑
                  
                    j
                    =
                    1
                  
                  
                    
                      N
                      
                        2
                      
                    
                  
                
                (
                
                  
                    
                      X
                      
                        i
                      
                    
                    →
                  
                
                −
                
                  
                    
                      Y
                      
                        j
                      
                    
                    →
                  
                
                
                  )
                  
                    2
                  
                
              
              
                
                  N
                  
                    1
                  
                
                ⋅
                
                  N
                  
                    2
                  
                
              
            
          
        
        =
        
          
            
              
                
                  N
                  
                    1
                  
                
                ⋅
                S
                
                  S
                  
                    2
                  
                
                +
                
                  N
                  
                    2
                  
                
                ⋅
                S
                
                  S
                  
                    1
                  
                
                −
                2
                ⋅
                
                  
                    
                      L
                      
                        S
                        
                          1
                        
                      
                    
                    →
                  
                
                ⋅
                
                  
                    
                      L
                      
                        S
                        
                          2
                        
                      
                    
                    →
                  
                
              
              
                
                  N
                  
                    1
                  
                
                ⋅
                
                  N
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle D_{2}={\sqrt {\frac {\sum _{i=1}^{N_{1}}\sum _{j=1}^{N_{2}}({\overrightarrow {X_{i}}}-{\overrightarrow {Y_{j}}})^{2}}{N_{1}\cdot N_{2}}}}={\sqrt {\frac {N_{1}\cdot SS_{2}+N_{2}\cdot SS_{1}-2\cdot {\overrightarrow {LS_{1}}}\cdot {\overrightarrow {LS_{2}}}}{N_{1}\cdot N_{2}}}}}
  In multidimensional cases the square root should be replaced with a suitable norm.

Numerical issues in BIRCH clustering features
Unfortunately, there are numerical issues associated with the use of the term 
  
    
      
        S
        S
      
    
    {\displaystyle SS}
   in BIRCH. When subtracting 
  
    
      
        
          
            
              S
              S
            
            N
          
        
        −
        
          
            (
          
        
        
          
            
              
                
                  L
                  S
                
                →
              
            
            N
          
        
        
          
            
              )
            
          
          
            2
          
        
      
    
    {\displaystyle {\frac {SS}{N}}-{\big (}{\frac {\vec {LS}}{N}}{\big )}^{2}}
   or similar in the other distances such as 
  
    
      
        
          D
          
            2
          
        
      
    
    {\displaystyle D_{2}}
  , catastrophic cancellation can occur and yield a poor precision, and which can in some cases even cause the result to be negative (and the square root then become undefined). This can be resolved by using BETULA cluster features 
  
    
      
        C
        F
        =
        (
        N
        ,
        μ
        ,
        S
        )
      
    
    {\displaystyle CF=(N,\mu ,S)}
   instead, which store the count 
  
    
      
        N
      
    
    {\displaystyle N}
  , mean 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  , and sum of squared deviations instead based on numerically more reliable online algorithms to calculate variance. For these features, a similar additivity theorem holds. When storing a vector respectively a matrix for the squared deviations, the resulting BIRCH CF-tree can also be used to accelerate Gaussian Mixture Modeling with the expectation–maximization algorithm, besides k-means clustering and hierarchical agglomerative clustering.


== Notes ==",https://en.wikipedia.org/wiki/BIRCH,"['Cluster analysis algorithms', 'Wikipedia articles needing clarification from December 2014']",Data Science
9,Astronomical survey,"An astronomical survey is a general map or image of a region of the sky that lacks a specific observational target.  Alternatively, an astronomical survey may comprise a set of many images or spectra of objects that share a common type or feature. Surveys are often restricted to one band of the electromagnetic spectrum due to instrumental limitations, although multiwavelength surveys can be made by using multiple detectors, each sensitive to a different bandwidth.Surveys have generally been performed as part of the production of an astronomical catalog. They may also search for transient astronomical events. They often use wide-field astrographs.

Scientific value
Sky surveys, unlike targeted observation of a specific object, allow astronomers to catalog celestial objects and perform statistical analyses on them without making prohibitively lengthy observations.  In some cases, an astronomer interested in a particular object will find that survey images are sufficient to make telescope time entirely unnecessary.
Surveys also help astronomers choose targets for closer study using larger, more powerful telescopes. If previous observations support a hypothesis, a telescope scheduling committee is more likely to approve new, more detailed observations to test it.
The wide scope of surveys makes them ideal for finding foreground objects that move, such as asteroids and comets. An astronomer can compare existing survey images to current observations to identify changes; this task can even be performed automatically using image analysis software. Besides science, these surveys also detect potentially hazardous objects. Similarly, images of the same object taken by different surveys can be compared to detect transient astronomical events such as variable stars.

List of sky surveys
Optical
Hipparchus - created the first known star catalogue with more than 850 stars.  The data was incorporated into the Almagest along with the first list of stellar magnitudes and was the primary astronomical reference until modern times, 190-120 BC.
Astrographic Catalogue - first international astronomical survey of the entire sky. The survey was performed by  18 observatories using over 22,000 photographic plates. The results have been the basis of comparison for all subsequent surveys, 1887-1975.
Catalina Sky Survey - an astronomical survey to discover comets and asteroids.
Pan-Andromeda Archaeological Survey
National Geographic Society – Palomar Observatory Sky Survey (NGS–POSS) – survey of the northern sky on photographic plates, 1948–1958
CfA Redshift Survey –  A program from Harvard-Smithonian Center for Astrophysics.  It began in 1977 to 1982 then from 1985 to 1995.
Digitized Sky Survey – optical all-sky survey created from digitized photographic plates, 1994
2dF Galaxy Redshift Survey (2dfGRS) – redshift survey conducted by the Anglo-Australian Observatory between 1997 and 2002
Sloan Digital Sky Survey (SDSS) – an optical and spectroscopic survey, 2000–2006 (first pass)
Photopic Sky Survey – a survey with 37,440 individual exposures, 2010–2011.
DEEP2 Redshift Survey (DEEP2) – Used Keck Telescopes to measure redshift of 50,000 galaxies
VIMOS-VLT Deep Survey (VVDS) – Franco-Italian study using the Very Large Telescope at Paranal Observatory
Palomar Distant Solar System Survey (PDSSS)
WiggleZ Dark Energy Survey (2006–2011) used the Australian Astronomical Observatory
Dark Energy Survey (DES) is a survey about one-tenth of the sky to find clues to the characteristics of dark energy.-
Calar Alto Legacy Integral Field Area Survey (CALIFA) – a spectroscopic survey of galaxies
SAGES Legacy Unifying Globulars and GalaxieS (SAGES Legacy Unifying Globulars and GalaxieS Survey (SLUGGS) survey – a near-infrared spectro-photometric survey of 25 nearby early-type galaxies (2014)
Large Sky Area Multi-Object Fiber Spectroscopic Telescope (LAMOST) – an extra-galactic and stellar spectroscopic survey
IPHAS and VPHAS+ – surveys of the Galactic bulge and inner disk using the Isaac Newton Telescope (north) and VLT Survey Telescope (south) in u, g, r, Hα, and i bands, 2003–present
Pan-STARRS – a proposed 4-telescope large-field survey system to look for transient and variable sources
Optical Gravitational Lensing Experiment (OGLE) – large-scale variability sky survey (in I and V bands), 1992-present
DESI Legacy Imaging Surveys (Legacy Surveys) - large imaging survey of the extragalactic sky, in three bands and covering one third of the sky, 2013-present
Infrared 
Infrared Astronomical Satellite did an all sky survey at 12, 25, 60, and 100 μm, 1983
The 2-micron All-Sky Survey (2MASS), a ground-based all sky survey at J, H, and Ks bands (1.25, 1.65, and 2.17 μm) 1997–2001
Akari (Astro-F) a Japanese mid and far infrared all-sky survey satellite, 2006–2008
Wide-field Infrared Survey Explorer (WISE) was launched in December 2009 to begin a survey of 99% of the sky at wavelengths of 3.3, 4.7, 12, and 23 μm.  The telescope is over a thousand times as sensitive as previous infrared surveys.  The initial survey, consisting of each sky position imaged at least eight times, was completed by July 2010.
UKIRT Infrared Deep Sky Survey (UKIDSS) – a collection of ground based northern hemisphere surveys (GPS, GCS, LAS, DXS, UDS) using the WFCAM camera on UKIRT, some wide and some very deep, in Z, Y, J, H, & K bands 2005–
VISTA public surveys – a collection of ground based southern hemisphere surveys (VVV, VMC, VHS, VIKING, VIDEO, UltraVISTA), of various areas and depths, in Z, Y, J, H, & Ks bands, 2009–present
SCUBA-2 All Sky Survey
Radio
HIPASS – Radio survey, the first blind HI survey to cover the entire southern sky.   1997–2002
Ohio Sky Survey – Over 19,000 radio sources at 1415 MHz. 1965–1973.
NVSS – Survey at 1.4 GHz mapping the sky north of −40 deg
FIRST – Survey to look for faint radio sources at twenty cms.
 SUMSS - Survey at 843 MHz, mapping the sky south of -30 deg with similar sensitivity and resolution to the northern NVSS 
PALFA Survey – On-going 1.4 GHz survey for radio pulsars using the Arecibo Observatory.
GALEX Arecibo SDSS Survey GASS designed to measure the neutral hydrogen content of a representative sample of ~1000 massive, galaxies
C-BASS – On-going 5 GHz all sky survey to aid in the subtraction of galactic foregrounds from maps of the Cosmic Microwave Background
EMU – A large radio continuum survey covering 3/4 of the sky, expected to discover about 70 million galaxies
GMRT - The Giant Metrewave Radio Telescope's TGSS ADR  mapped the sky at 150 MHz.
HTRU – A pulsar and radio transients survey of the northern and southern sky using the Parkes Radio Telescope and the Effelsberg telescope.Gamma-ray
Fermi Gamma-ray Space Telescope, formerly referred to as the ""Gamma-ray Large Area Space Telescope (GLAST)."" 2008–present; the goal for the telescope's lifetime is 10 years.
Multi-wavelength surveys
GAMA – the Galaxy And Mass Assembly survey combines data from a number of ground- and space-based observatories together with a large redshift survey, performed at the Anglo-Australian Telescope.  The resulting dataset aims to be a comprehensive resource for studying the physics of the galaxy population and underlying mass structures in the recent universe.
GOODS – The Great Observatories Origins Deep Survey.
COSMOS – The Cosmic Evolution Survey
(The latter two surveys are joining together observations obtained from space with the Hubble Space Telescope, the Spitzer Space Telescope, the Chandra X-ray Observatory and the XMM-Newton satellite, with a large set of observations obtained with ground-based telescopes).
Atlas 3d Survey – sample of 260 galaxies for the Astrophysics project.
Planned
Vera C. Rubin Observatory – a proposed very large telescope designed to repeatedly survey the whole sky that is visible from its location
ASKAP HI All Sky Survey (WALLABY) – PI Bärbel Koribalski

Surveys of the Magellanic Clouds
MCELS (Magellanic Cloud Emission-line Survey)
The Magellanic Clouds Photometric Survey – UBVI (optical)
Deep Near Infrared Survey (DENIS) – near-IR

See also
See astronomical catalogue for a more detailed description of astronomical surveys and the production of astronomical catalogues
Redshift surveys are astronomical surveys devoted to mapping the cosmos in three dimensions
Category:astronomical catalogues—List of astronomical catalogues on Wikipedia
Astrograph for a type of instrument used in Astronomical surveys.
Timeline of astronomical maps, catalogs, and surveys


== References ==",https://en.wikipedia.org/wiki/Astronomical_survey,"['Articles with short description', 'Astronomical imaging', 'Astronomical surveys', 'CS1 maint: uses authors parameter', 'Commons category link is on Wikidata', 'Observational astronomy', 'Short description matches Wikidata', 'Works about astronomy']",Data Science
10,Basic research,"Basic research, also called pure research or fundamental research, is a type of scientific research with the aim of improving scientific theories for better understanding and prediction of natural or other phenomena.  In contrast, applied research uses scientific theories to develop technology or techniques which can be used to intervene and alter natural or other phenomena. Though often driven simply by curiosity, basic research often fuels the technological innovations of applied science.  The two aims are often practiced simultaneously in coordinated research and development.

Overview
Basic research advances fundamental knowledge about the world. It focuses on creating and refuting or supporting theories that explain observed phenomena. Pure research is the source of most new scientific ideas and ways of thinking about the world. It can be exploratory, descriptive, or explanatory; however, explanatory research is the most common.Basic research generates new ideas, principles, and theories, which may not be immediately utilized but nonetheless form the basis of progress and development in different fields. Today's computers, for example, could not exist without research in pure mathematics conducted over a century ago, for which there was no known practical application at the time. Basic research rarely helps practitioners directly with their everyday concerns; nevertheless, it stimulates new ways of thinking that have the potential to revolutionize and dramatically improve how practitioners deal with a problem in the future.

By country
In the United States, basic research is funded mainly by federal government and done mainly at universities and institutes.  As government funding has diminished in the 2010s, however, private funding is increasingly important.

Basic versus applied science
Applied science focuses on the development of technology and techniques. In contrast, basic science develops scientific knowledge and predictions, principally in natural sciences but also in other empirical sciences, which are used as the scientific foundation for applied science. Basic science develops and establishes information to predict phenomena and perhaps to understand nature, whereas applied science uses portions of basic science to develop interventions via technology or technique to alter events or outcomes. Applied and basic sciences can interface closely in research and development. The interface between basic research and applied research has been studied by the National Science Foundation.  A worker in basic scientific research is motivated by a driving curiosity about the unknown. When his explorations yield new knowledge, he experiences the satisfaction of those who first attain the summit of a mountain or the upper reaches of a river flowing through unmapped territory. Discovery of truth and understanding of nature are his objectives. His professional standing among his fellows depends upon the originality and soundness of his work. Creativeness in science is of a cloth with that of the poet or painter.It conducted a study in which it traced the relationship between basic scientific research efforts and the development of major innovations, such as oral contraceptives and videotape recorders.  This study found that basic research played a key role in the development in all of the innovations.  The number of basic science research that assisted in the production of a given innovation peaked between 20 and 30 years before the innovation itself.  While most innovation takes the form of applied science and most innovation occurs in the private sector, basic research is a necessary precursor to almost all applied science and associated instances of innovation.  Roughly 76% of basic research is conducted by universities.A distinction can be made between basic science and disciplines such as medicine and technology.  They can be grouped as STM (science, technology, and medicine; not to be confused with STEM [science, technology, engineering, and mathematics]) or STS (science, technology, and society). These groups are interrelated and influence each other, although they may differ in the specifics such as methods and standards.The Nobel Prize mixes basic with applied sciences for its award in Physiology or Medicine. In contrast, the Royal Society of London awards distinguish natural science from applied science.

See also
Blue skies research
Hard and soft science
Metascience
Normative science
Physics
Precautionary principle
Pure mathematics
Pure ChemistryPolio==References==

Polio==Further reading==

Levy, David M. (2002). ""Research and Development"".  In David R. Henderson (ed.). Concise Encyclopedia of Economics (1st ed.). Library of Economics and Liberty. OCLC 317650570, 50016270, 163149563",https://en.wikipedia.org/wiki/Basic_research,"['All articles with unsourced statements', 'Articles with limited geographic scope from October 2017', 'Articles with short description', 'Articles with unsourced statements from May 2018', 'Research', 'Short description matches Wikidata', 'Webarchive template wayback links', 'Wikipedia articles needing clarification from May 2017', 'Wikipedia articles with GND identifiers']",Data Science
11,Bayesian network,"A Bayesian network (also known as a Bayes network, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.
Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.

Graphical model
Formally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Edges represent conditional dependencies; nodes that are not connected (no path connects one node to another) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if 
  
    
      
        m
      
    
    {\displaystyle m}
   parent nodes represent 
  
    
      
        m
      
    
    {\displaystyle m}
   Boolean variables, then the probability function could be represented by a table of 
  
    
      
        
          2
          
            m
          
        
      
    
    {\displaystyle 2^{m}}
   entries, one entry for each of the 
  
    
      
        
          2
          
            m
          
        
      
    
    {\displaystyle 2^{m}}
   possible parent combinations. Similar ideas may be applied to undirected, and possibly cyclic, graphs such as Markov networks.

Example
Two events can cause grass to be wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler usually is not active). This situation can be modeled with a Bayesian network (shown to the right). Each variable has two possible values, T (for true) and F (for false).
The joint probability function is:

  
    
      
        Pr
        (
        G
        ,
        S
        ,
        R
        )
        =
        Pr
        (
        G
        ∣
        S
        ,
        R
        )
        Pr
        (
        S
        ∣
        R
        )
        Pr
        (
        R
        )
      
    
    {\displaystyle \Pr(G,S,R)=\Pr(G\mid S,R)\Pr(S\mid R)\Pr(R)}
  where G = ""Grass wet (true/false)"", S = ""Sprinkler turned on (true/false)"", and R = ""Raining (true/false)"".
The model can answer questions about the presence of a cause given the presence of an effect (so-called inverse probability) like ""What is the probability that it is raining, given the grass is wet?"" by using the conditional probability formula and summing over all nuisance variables:

  
    
      
        Pr
        (
        R
        =
        T
        ∣
        G
        =
        T
        )
        =
        
          
            
              Pr
              (
              G
              =
              T
              ,
              R
              =
              T
              )
            
            
              Pr
              (
              G
              =
              T
              )
            
          
        
        =
        
          
            
              
                ∑
                
                  x
                  ∈
                  {
                  T
                  ,
                  F
                  }
                
              
              Pr
              (
              G
              =
              T
              ,
              S
              =
              x
              ,
              R
              =
              T
              )
            
            
              
                ∑
                
                  x
                  ,
                  y
                  ∈
                  {
                  T
                  ,
                  F
                  }
                
              
              Pr
              (
              G
              =
              T
              ,
              S
              =
              x
              ,
              R
              =
              y
              )
            
          
        
      
    
    {\displaystyle \Pr(R=T\mid G=T)={\frac {\Pr(G=T,R=T)}{\Pr(G=T)}}={\frac {\sum _{x\in \{T,F\}}\Pr(G=T,S=x,R=T)}{\sum _{x,y\in \{T,F\}}\Pr(G=T,S=x,R=y)}}}
  Using the expansion for the joint probability function 
  
    
      
        Pr
        (
        G
        ,
        S
        ,
        R
        )
      
    
    {\displaystyle \Pr(G,S,R)}
   and the conditional probabilities from the conditional probability tables (CPTs) stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example,

  
    
      
        
          
            
              
                Pr
                (
                G
                =
                T
                ,
                S
                =
                T
                ,
                R
                =
                T
                )
              
              
                
                =
                Pr
                (
                G
                =
                T
                ∣
                S
                =
                T
                ,
                R
                =
                T
                )
                Pr
                (
                S
                =
                T
                ∣
                R
                =
                T
                )
                Pr
                (
                R
                =
                T
                )
              
            
            
              
              
                
                =
                0.99
                ×
                0.01
                ×
                0.2
              
            
            
              
              
                
                =
                0.00198.
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\Pr(G=T,S=T,R=T)&=\Pr(G=T\mid S=T,R=T)\Pr(S=T\mid R=T)\Pr(R=T)\\&=0.99\times 0.01\times 0.2\\&=0.00198.\end{aligned}}}
  Then the numerical results (subscripted by the associated variable values) are

  
    
      
        Pr
        (
        R
        =
        T
        ∣
        G
        =
        T
        )
        =
        
          
            
              
                0.00198
                
                  T
                  T
                  T
                
              
              +
              
                0.1584
                
                  T
                  F
                  T
                
              
            
            
              
                0.00198
                
                  T
                  T
                  T
                
              
              +
              
                0.288
                
                  T
                  T
                  F
                
              
              +
              
                0.1584
                
                  T
                  F
                  T
                
              
              +
              
                0.0
                
                  T
                  F
                  F
                
              
            
          
        
        =
        
          
            891
            2491
          
        
        ≈
        35.77
        %
        .
      
    
    {\displaystyle \Pr(R=T\mid G=T)={\frac {0.00198_{TTT}+0.1584_{TFT}}{0.00198_{TTT}+0.288_{TTF}+0.1584_{TFT}+0.0_{TFF}}}={\frac {891}{2491}}\approx 35.77\%.}
  To answer an interventional question, such as ""What is the probability that it would rain, given that we wet the grass?"" the answer is governed by the post-intervention joint distribution function

  
    
      
        Pr
        (
        S
        ,
        R
        ∣
        
          do
        
        (
        G
        =
        T
        )
        )
        =
        Pr
        (
        S
        ∣
        R
        )
        Pr
        (
        R
        )
      
    
    {\displaystyle \Pr(S,R\mid {\text{do}}(G=T))=\Pr(S\mid R)\Pr(R)}
  obtained by removing the factor 
  
    
      
        Pr
        (
        G
        ∣
        S
        ,
        R
        )
      
    
    {\displaystyle \Pr(G\mid S,R)}
   from the pre-intervention distribution. The do operator forces the value of G to be true. The probability of rain is unaffected by the action:

  
    
      
        Pr
        (
        R
        ∣
        
          do
        
        (
        G
        =
        T
        )
        )
        =
        Pr
        (
        R
        )
        .
      
    
    {\displaystyle \Pr(R\mid {\text{do}}(G=T))=\Pr(R).}
  To predict the impact of turning the sprinkler on:

  
    
      
        Pr
        (
        R
        ,
        G
        ∣
        
          do
        
        (
        S
        =
        T
        )
        )
        =
        Pr
        (
        R
        )
        Pr
        (
        G
        ∣
        R
        ,
        S
        =
        T
        )
      
    
    {\displaystyle \Pr(R,G\mid {\text{do}}(S=T))=\Pr(R)\Pr(G\mid R,S=T)}
  with the term 
  
    
      
        Pr
        (
        S
        =
        T
        ∣
        R
        )
      
    
    {\displaystyle \Pr(S=T\mid R)}
   removed, showing that the action affects the grass but not the rain.
These predictions may not be feasible given unobserved variables, as in most policy evaluation problems. The effect of the action 
  
    
      
        
          do
        
        (
        x
        )
      
    
    {\displaystyle {\text{do}}(x)}
   can still be predicted, however, whenever the back-door criterion is satisfied. It states that, if a set Z of nodes can be observed that d-separates (or blocks) all back-door paths from X to Y then

  
    
      
        Pr
        (
        Y
        ,
        Z
        ∣
        
          do
        
        (
        x
        )
        )
        =
        
          
            
              Pr
              (
              Y
              ,
              Z
              ,
              X
              =
              x
              )
            
            
              Pr
              (
              X
              =
              x
              ∣
              Z
              )
            
          
        
        .
      
    
    {\displaystyle \Pr(Y,Z\mid {\text{do}}(x))={\frac {\Pr(Y,Z,X=x)}{\Pr(X=x\mid Z)}}.}
  A back-door path is one that ends with an arrow into X. Sets that satisfy the back-door criterion are called ""sufficient"" or ""admissible."" For example, the set Z = R is admissible for predicting the effect of S = T on G, because R d-separates the (only) back-door path S ← R → G. However, if S is not observed, no other set d-separates this path and the effect of turning the sprinkler on (S = T) on the grass (G) cannot be predicted from passive observations. In that case P(G | do(S = T)) is not ""identified"". This reflects the fact that, lacking interventional data, the observed dependence between S and G is due to a causal connection or is spurious
(apparent dependence arising from a common cause, R). (see Simpson's paradox)
To determine whether a causal relation is identified from an arbitrary Bayesian network with unobserved variables, one can use the three rules of ""do-calculus"" and test whether all do terms can be removed from the expression of that relation, thus confirming that the desired quantity is estimable from frequency data.Using a Bayesian network can save considerable amounts of memory over exhaustive probability tables, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for 
  
    
      
        
          2
          
            10
          
        
        =
        1024
      
    
    {\displaystyle 2^{10}=1024}
   values. If no variable's local distribution depends on more than three parent variables, the Bayesian network representation stores at most 
  
    
      
        10
        ⋅
        
          2
          
            3
          
        
        =
        80
      
    
    {\displaystyle 10\cdot 2^{3}=80}
   values.
One advantage of Bayesian networks is that it is intuitively easier for a human to understand (a sparse set of) direct dependencies and local distributions than complete joint distributions.

Inference and learning
Bayesian networks perform three main inference tasks:

Inferring unobserved variables
Because a Bayesian network is a complete model for its variables and their relationships, it can be used to answer probabilistic queries about them. For example, the network can be used to update knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the posterior distribution of variables given evidence is called probabilistic inference. The posterior gives a universal sufficient statistic for detection applications, when choosing values for the variable subset that minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems.
The most common exact inference methods are: variable elimination, which eliminates (by integration or summation) the non-observed non-query variables one by one by distributing the sum over the product; clique tree propagation, which caches the computation so that many variables can be queried at one time and new evidence can be propagated quickly; and recursive conditioning and AND/OR search, which allow for a space–time tradeoff and match the efficiency of variable elimination when enough space is used. All of these methods have complexity that is exponential in the network's treewidth. The most common approximate inference algorithms are importance sampling, stochastic MCMC simulation, mini-bucket elimination, loopy belief propagation, generalized belief propagation and variational methods.

Parameter learning
In order to fully specify the Bayesian network and thus fully represent the joint probability distribution, it is necessary to specify for each node X the probability distribution for X conditional upon X's parents. The distribution of X conditional upon its parents may have any form. It is common to work with discrete or Gaussian distributions since that simplifies calculations. Sometimes only constraints on a distribution are known; one can then use the principle of maximum entropy to determine a single distribution, the one with the greatest entropy given the constraints. (Analogously, in the specific context of a dynamic Bayesian network, the conditional distribution for the hidden state's temporal evolution is commonly specified to maximize the entropy rate of the implied stochastic process.)
Often these conditional distributions include parameters that are unknown and must be estimated from data, e.g., via the maximum likelihood approach. Direct maximization of the likelihood (or of the posterior probability) is often complex given unobserved variables. A classical approach to this problem is the expectation-maximization algorithm, which alternates computing expected values of the unobserved variables conditional on observed data, with maximizing the complete likelihood (or posterior) assuming that previously computed expected values are correct. Under mild regularity conditions this process converges on maximum likelihood (or maximum posterior) values for parameters.
A more fully Bayesian approach to parameters is to treat them as additional unobserved variables and to compute a full posterior distribution over all nodes conditional upon observed data, then to integrate out the parameters. This approach can be expensive and lead to large dimension models, making classical parameter-setting approaches more tractable.

Structure learning
In the simplest case, a Bayesian network is specified by an expert and is then used to perform inference. In other applications the task of defining the network is too complex for humans. In this case, the network structure and the parameters of the local distributions must be learned from data.
Automatically learning the graph structure of a Bayesian network (BN) is a challenge pursued within machine learning. The basic idea goes back to a recovery algorithm developed by Rebane and Pearl and rests on the distinction between the three possible patterns allowed in a 3-node DAG:

The first 2 represent the same dependencies (
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Z
      
    
    {\displaystyle Z}
   are independent given 
  
    
      
        Y
      
    
    {\displaystyle Y}
  ) and are, therefore, indistinguishable. The collider, however, can be uniquely identified, since 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Z
      
    
    {\displaystyle Z}
   are marginally independent and all other pairs are dependent. Thus, while the skeletons (the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies when 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Z
      
    
    {\displaystyle Z}
   have common parents, except that one must first condition on those parents. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independences observed.An alternative method of structural learning uses optimization-based search. It requires a scoring function and a search strategy. A common scoring function is posterior probability of the structure given the training data, like the BIC or the BDeu. The time requirement of an exhaustive search returning a structure that maximizes the score is superexponential in the number of variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm like Markov chain Monte Carlo can avoid getting trapped in local minima. Friedman et al. discuss using mutual information between variables and finding a structure that maximizes this. They do this by restricting the parent candidate set to k nodes and exhaustively searching therein.
A particularly fast method for exact BN learning is to cast the problem as an optimization problem, and solve it using integer programming. Acyclicity constraints are added to the integer program (IP) during solving in the form of cutting planes. Such method can handle problems with up to 100 variables.
In order to deal with problems with thousands of variables, a different approach is necessary. One is to first sample one ordering, and then find the optimal BN structure with respect to that ordering. This implies working on the search space of the possible orderings, which is convenient as it is smaller than the space of network structures. Multiple orderings are then sampled and evaluated. This method has been proven to be the best available in literature when the number of variables is huge.Another method consists of focusing on the sub-class of decomposable models, for which the MLE have a closed form. It is then possible to discover a consistent structure for hundreds of variables.Learning Bayesian networks with bounded treewidth is necessary to allow exact, tractable inference, since the worst-case inference complexity is exponential in the treewidth k (under the exponential time hypothesis). Yet, as a global property of the graph, it considerably increases the difficulty of the learning process. In this context it is possible to use K-tree for effective learning.

Statistical introduction
Given data 
  
    
      
        x
        
        
      
    
    {\displaystyle x\,\!}
   and parameter 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  , a simple Bayesian analysis starts with a prior probability (prior) 
  
    
      
        p
        (
        θ
        )
      
    
    {\displaystyle p(\theta )}
   and likelihood 
  
    
      
        p
        (
        x
        ∣
        θ
        )
      
    
    {\displaystyle p(x\mid \theta )}
   to compute a posterior probability 
  
    
      
        p
        (
        θ
        ∣
        x
        )
        ∝
        p
        (
        x
        ∣
        θ
        )
        p
        (
        θ
        )
      
    
    {\displaystyle p(\theta \mid x)\propto p(x\mid \theta )p(\theta )}
  .
Often the prior on 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   depends in turn on other parameters 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
   that are not mentioned in the likelihood. So, the prior 
  
    
      
        p
        (
        θ
        )
      
    
    {\displaystyle p(\theta )}
   must be replaced by a likelihood 
  
    
      
        p
        (
        θ
        ∣
        φ
        )
      
    
    {\displaystyle p(\theta \mid \varphi )}
  , and a prior 
  
    
      
        p
        (
        φ
        )
      
    
    {\displaystyle p(\varphi )}
   on the newly introduced parameters 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
   is required, resulting in a posterior probability

  
    
      
        p
        (
        θ
        ,
        φ
        ∣
        x
        )
        ∝
        p
        (
        x
        ∣
        θ
        )
        p
        (
        θ
        ∣
        φ
        )
        p
        (
        φ
        )
        .
      
    
    {\displaystyle p(\theta ,\varphi \mid x)\propto p(x\mid \theta )p(\theta \mid \varphi )p(\varphi ).}
  This is the simplest example of a hierarchical Bayes model.The process may be repeated; for example, the parameters 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
   may depend in turn on additional parameters 
  
    
      
        ψ
        
        
      
    
    {\displaystyle \psi \,\!}
  , which require their own prior. Eventually the process must terminate, with priors that do not depend on unmentioned parameters.

Introductory examples
Given the measured quantities 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        
        
      
    
    {\displaystyle x_{1},\dots ,x_{n}\,\!}
  each with normally distributed errors of known standard deviation 
  
    
      
        σ
        
        
      
    
    {\displaystyle \sigma \,\!}
  ,

  
    
      
        
          x
          
            i
          
        
        ∼
        N
        (
        
          θ
          
            i
          
        
        ,
        
          σ
          
            2
          
        
        )
      
    
    {\displaystyle x_{i}\sim N(\theta _{i},\sigma ^{2})}
  Suppose we are interested in estimating the 
  
    
      
        
          θ
          
            i
          
        
      
    
    {\displaystyle \theta _{i}}
  . An approach would be to estimate the 
  
    
      
        
          θ
          
            i
          
        
      
    
    {\displaystyle \theta _{i}}
   using a maximum likelihood approach; since the observations are independent, the likelihood factorizes and the maximum likelihood estimate is simply

  
    
      
        
          θ
          
            i
          
        
        =
        
          x
          
            i
          
        
        .
      
    
    {\displaystyle \theta _{i}=x_{i}.}
  However, if the quantities are related, so that for example the individual 
  
    
      
        
          θ
          
            i
          
        
      
    
    {\displaystyle \theta _{i}}
  have themselves been drawn from an underlying distribution, then this relationship destroys the independence and suggests a more complex model, e.g.,

  
    
      
        
          x
          
            i
          
        
        ∼
        N
        (
        
          θ
          
            i
          
        
        ,
        
          σ
          
            2
          
        
        )
        ,
      
    
    {\displaystyle x_{i}\sim N(\theta _{i},\sigma ^{2}),}
  

  
    
      
        
          θ
          
            i
          
        
        ∼
        N
        (
        φ
        ,
        
          τ
          
            2
          
        
        )
        ,
      
    
    {\displaystyle \theta _{i}\sim N(\varphi ,\tau ^{2}),}
  with improper priors 
  
    
      
        φ
        ∼
        
          flat
        
      
    
    {\displaystyle \varphi \sim {\text{flat}}}
  , 
  
    
      
        τ
        ∼
        
          flat
        
        ∈
        (
        0
        ,
        ∞
        )
      
    
    {\displaystyle \tau \sim {\text{flat}}\in (0,\infty )}
  . When 
  
    
      
        n
        ≥
        3
      
    
    {\displaystyle n\geq 3}
  , this is an identified model (i.e. there exists a unique solution for the model's parameters), and the posterior distributions of the individual 
  
    
      
        
          θ
          
            i
          
        
      
    
    {\displaystyle \theta _{i}}
   will tend to move, or shrink away from the maximum likelihood estimates towards their common mean. This shrinkage is a typical behavior in hierarchical Bayes models.

Restrictions on priors
Some care is needed when choosing priors in a hierarchical model, particularly on scale variables at higher levels of the hierarchy such as the variable 
  
    
      
        τ
        
        
      
    
    {\displaystyle \tau \,\!}
   in the example. The usual priors such as the Jeffreys prior often do not work, because the posterior distribution will not be normalizable and estimates made by minimizing the expected loss will be inadmissible.

Definitions and concepts
Several equivalent definitions of a Bayesian network have been offered. For the following, let G = (V,E) be a directed acyclic graph (DAG) and let X = (Xv), v ∈ V be a set of random variables indexed by V.

Factorization definition
X is a Bayesian network with respect to G if its joint probability density function (with respect to a product measure) can be written as a product of the individual density functions, conditional on their parent variables:

  
    
      
        p
        (
        x
        )
        =
        
          ∏
          
            v
            ∈
            V
          
        
        p
        
          (
          
            
              x
              
                v
              
            
            
            
              
                |
              
            
            
            
              x
              
                pa
                ⁡
                (
                v
                )
              
            
          
          )
        
      
    
    {\displaystyle p(x)=\prod _{v\in V}p\left(x_{v}\,{\big |}\,x_{\operatorname {pa} (v)}\right)}
  where pa(v) is the set of parents of v (i.e. those vertices pointing directly to v via a single edge).
For any set of random variables, the probability of any member of a joint distribution can be calculated from conditional probabilities using the chain rule (given a topological ordering of X) as follows:

  
    
      
        P
        ⁡
        (
        
          X
          
            1
          
        
        =
        
          x
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        =
        
          x
          
            n
          
        
        )
        =
        
          ∏
          
            v
            =
            1
          
          
            n
          
        
        P
        ⁡
        
          (
          
            
              X
              
                v
              
            
            =
            
              x
              
                v
              
            
            ∣
            
              X
              
                v
                +
                1
              
            
            =
            
              x
              
                v
                +
                1
              
            
            ,
            …
            ,
            
              X
              
                n
              
            
            =
            
              x
              
                n
              
            
          
          )
        
      
    
    {\displaystyle \operatorname {P} (X_{1}=x_{1},\ldots ,X_{n}=x_{n})=\prod _{v=1}^{n}\operatorname {P} \left(X_{v}=x_{v}\mid X_{v+1}=x_{v+1},\ldots ,X_{n}=x_{n}\right)}
  Using the definition above, this can be written as:

  
    
      
        P
        ⁡
        (
        
          X
          
            1
          
        
        =
        
          x
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        =
        
          x
          
            n
          
        
        )
        =
        
          ∏
          
            v
            =
            1
          
          
            n
          
        
        P
        ⁡
        (
        
          X
          
            v
          
        
        =
        
          x
          
            v
          
        
        ∣
        
          X
          
            j
          
        
        =
        
          x
          
            j
          
        
        
           for each 
        
        
          X
          
            j
          
        
        
        
           that is a parent of 
        
        
          X
          
            v
          
        
        
        )
      
    
    {\displaystyle \operatorname {P} (X_{1}=x_{1},\ldots ,X_{n}=x_{n})=\prod _{v=1}^{n}\operatorname {P} (X_{v}=x_{v}\mid X_{j}=x_{j}{\text{ for each }}X_{j}\,{\text{ that is a parent of }}X_{v}\,)}
  The difference between the two expressions is the conditional independence of the variables from any of their non-descendants, given the values of their parent variables.

Local Markov property
X is a Bayesian network with respect to G if it satisfies the local Markov property: each variable is conditionally independent of its non-descendants given its parent variables:

  
    
      
        
          X
          
            v
          
        
        ⊥
        
        
        
        ⊥
        
          X
          
            V
            
            ∖
            
            de
            ⁡
            (
            v
            )
          
        
        ∣
        
          X
          
            pa
            ⁡
            (
            v
            )
          
        
        
        
          for all 
        
        v
        ∈
        V
      
    
    {\displaystyle X_{v}\perp \!\!\!\perp X_{V\,\smallsetminus \,\operatorname {de} (v)}\mid X_{\operatorname {pa} (v)}\quad {\text{for all }}v\in V}
  where de(v) is the set of descendants and V \ de(v) is the set of non-descendants of v.
This can be expressed in terms similar to the first definition, as

  
    
      
        
          
            
              
              
                P
                ⁡
                (
                
                  X
                  
                    v
                  
                
                =
                
                  x
                  
                    v
                  
                
                ∣
                
                  X
                  
                    i
                  
                
                =
                
                  x
                  
                    i
                  
                
                
                   for each 
                
                
                  X
                  
                    i
                  
                
                
                   that is not a descendant of 
                
                
                  X
                  
                    v
                  
                
                
                )
              
            
            
              
                =
                

                
              
              
                P
                (
                
                  X
                  
                    v
                  
                
                =
                
                  x
                  
                    v
                  
                
                ∣
                
                  X
                  
                    j
                  
                
                =
                
                  x
                  
                    j
                  
                
                
                   for each 
                
                
                  X
                  
                    j
                  
                
                
                   that is a parent of 
                
                
                  X
                  
                    v
                  
                
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&\operatorname {P} (X_{v}=x_{v}\mid X_{i}=x_{i}{\text{ for each }}X_{i}{\text{ that is not a descendant of }}X_{v}\,)\\[6pt]={}&P(X_{v}=x_{v}\mid X_{j}=x_{j}{\text{ for each }}X_{j}{\text{ that is a parent of }}X_{v}\,)\end{aligned}}}
  The set of parents is a subset of the set of non-descendants because the graph is acyclic.

Developing Bayesian networks
Developing a Bayesian network often begins with creating a DAG G such that X satisfies the local Markov property with respect to G. Sometimes this is a causal DAG. The conditional probability distributions of each variable given its parents in G are assessed. In many cases, in particular in the case where the variables are discrete, if the joint distribution of X is the product of these conditional distributions, then X is a Bayesian network with respect to G.

Markov blanket
The Markov blanket of a node is the set of nodes consisting of its parents, its children, and any other parents of its children. The Markov blanket renders the node independent of the rest of the network; the joint distribution of the variables in the Markov blanket of a node is sufficient knowledge for calculating the distribution of the node. X is a Bayesian network with respect to G if every node is conditionally independent of all other nodes in the network, given its Markov blanket.

d-separation
This definition can be made more general by defining the ""d""-separation of two nodes, where d stands for directional. We first define the ""d""-separation of a trail and then we will define the ""d""-separation of two nodes in terms of that.
Let P be a trail from node u to v. A trail is a loop-free, undirected (i.e. all edge directions are ignored) path between two nodes. Then P is said to be d-separated by a set of nodes Z if any of the following conditions holds:

P contains (but does not need to be entirely) a directed chain, 
  
    
      
        u
        ⋯
        ←
        m
        ←
        ⋯
        v
      
    
    {\displaystyle u\cdots \leftarrow m\leftarrow \cdots v}
   or 
  
    
      
        u
        ⋯
        →
        m
        →
        ⋯
        v
      
    
    {\displaystyle u\cdots \rightarrow m\rightarrow \cdots v}
  , such that the middle node m is in Z,
P contains a fork, 
  
    
      
        u
        ⋯
        ←
        m
        →
        ⋯
        v
      
    
    {\displaystyle u\cdots \leftarrow m\rightarrow \cdots v}
  , such that the middle node m is in Z, or
P contains an inverted fork (or collider), 
  
    
      
        u
        ⋯
        →
        m
        ←
        ⋯
        v
      
    
    {\displaystyle u\cdots \rightarrow m\leftarrow \cdots v}
  , such that the middle node m is not in Z and no descendant of m is in Z.The nodes u and v are d-separated by Z if all trails between them are d-separated. If u and v are not d-separated, they are d-connected.
X is a Bayesian network with respect to G if, for any two nodes u, v:

  
    
      
        
          X
          
            u
          
        
        ⊥
        
        
        
        ⊥
        
          X
          
            v
          
        
        ∣
        
          X
          
            Z
          
        
      
    
    {\displaystyle X_{u}\perp \!\!\!\perp X_{v}\mid X_{Z}}
  where Z is a set which d-separates u and v. (The Markov blanket is the minimal set of nodes which d-separates node v from all other nodes.)

Causal networks
Although Bayesian networks are often used to represent causal relationships, this need not be the case: a directed edge from u to v does not require that Xv be causally dependent on Xu. This is demonstrated by the fact that Bayesian networks on the graphs:

  
    
      
        a
        →
        b
        →
        c
        
        
          and
        
        
        a
        ←
        b
        ←
        c
      
    
    {\displaystyle a\rightarrow b\rightarrow c\qquad {\text{and}}\qquad a\leftarrow b\leftarrow c}
  are equivalent: that is they impose exactly the same conditional independence requirements.
A causal network is a Bayesian network with the requirement that the relationships be causal. The additional semantics of causal networks specify that if a node X is actively caused to be in a given state x (an action written as do(X = x)), then the probability density function changes to that of the network obtained by cutting the links from the parents of X to X, and setting X to the caused value x. Using these semantics, the impact of external interventions from data obtained prior to intervention can be predicted.

Inference complexity and approximation algorithms
In 1990, while working at Stanford University on large bioinformatic applications, Cooper proved that exact inference in Bayesian networks is NP-hard. This result prompted research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference. In 1993, Dagum and Luby proved two surprising results on the complexity of approximation of probabilistic inference in Bayesian networks. First, they proved that no tractable deterministic algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2. Second, they proved that no tractable randomized algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2 with confidence probability greater than 1/2.
At about the same time, Roth proved that exact inference in Bayesian networks is in fact #P-complete (and thus as hard as counting the number of satisfying assignments of a conjunctive normal form formula (CNF) and that approximate inference within a factor 2n1−ɛ for every ɛ > 0, even for Bayesian networks with restricted architecture, is NP-hard.In practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by 1/p(n) where p(n) was any polynomial on the number of nodes in the network n.

Software
Notable software for Bayesian networks include:

Just another Gibbs sampler (JAGS) – Open-source alternative to WinBUGS. Uses Gibbs sampling.
OpenBUGS – Open-source development of WinBUGS.
SPSS Modeler – Commercial software that includes an implementation for Bayesian networks.
Stan (software) – Stan is an open-source package for obtaining Bayesian inference using the No-U-Turn sampler (NUTS), a variant of Hamiltonian Monte Carlo.
PyMC3 – A Python library implementing an embedded domain specific language to represent bayesian networks, and a variety of samplers (including NUTS)
WinBUGS – One of the first computational implementations of MCMC samplers. No longer maintained.

History
The term Bayesian network was coined by Judea Pearl in 1985 to emphasize:
the often subjective nature of the input information
the reliance on Bayes' conditioning as the basis for updating information
the distinction between causal and evidential modes of reasoningIn the late 1980s Pearl's Probabilistic Reasoning in Intelligent Systems and Neapolitan's Probabilistic Reasoning in Expert Systems summarized their properties and established them as a field of study.

See also
Notes
References
Further reading
Conrady S, Jouffe L (2015-07-01). Bayesian Networks and BayesiaLab – A practical introduction for researchers. Franklin, Tennessee: Bayesian USA. ISBN 978-0-9965333-0-0.
Charniak E (Winter 1991). ""Bayesian networks without tears"" (PDF). AI Magazine.
Kruse R, Borgelt C, Klawonn F, Moewes C, Steinbrecher M, Held P (2013). Computational Intelligence A Methodological Introduction. London: Springer-Verlag. ISBN 978-1-4471-5012-1.
Borgelt C, Steinbrecher M, Kruse R (2009). Graphical Models – Representations for Learning, Reasoning and Data Mining (Second ed.). Chichester: Wiley. ISBN 978-0-470-74956-2.

External links
An Introduction to Bayesian Networks and their Contemporary Applications
On-line Tutorial on Bayesian nets and probability
Web-App to create Bayesian nets and run it with a Monte Carlo method
Continuous Time Bayesian Networks
Bayesian Networks: Explanation and Analogy
A live tutorial on learning Bayesian networks
A hierarchical Bayes Model for handling sample heterogeneity in classification problems, provides a classification model taking into consideration the uncertainty associated with measuring replicate samples.
Hierarchical Naive Bayes Model for handling sample uncertainty, shows how to perform classification and learning with continuous and discrete variables with replicated measurements.",https://en.wikipedia.org/wiki/Bayesian_network,"['All articles lacking in-text citations', 'All articles to be expanded', 'Articles lacking in-text citations from February 2011', 'Articles to be expanded from March 2009', 'Articles using small message boxes', 'Articles with short description', 'Bayesian networks', 'CS1 errors: missing periodical', 'Causal inference', 'Causality', 'Graphical models', 'Harv and Sfn no-target errors', 'Short description matches Wikidata', 'Wikipedia articles needing clarification from October 2009']",Data Science
12,Bias–variance tradeoff,"In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters.
The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:
The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.

Motivation
The bias-variance tradeoff is a central problem in supervised learning. Ideally, one wants to choose a model that both accurately captures the regularities in its training data, but also generalizes well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that may fail to capture important regularities (i.e. underfit) in the data.
It is an often made fallacy to assume that complex models must have high variance; High variance models are 'complex' in some sense, but the reverse needs not be true.
In addition one has to be careful how to define complexity: In particular, the number of parameters used to describe the model is a poor measure of complexity. This is illustrated by an example adapted from: The model 
  
    
      
        
          f
          
            a
            ,
            b
          
        
        (
        x
        )
        =
        a
        sin
        ⁡
        (
        b
        x
        )
      
    
    {\displaystyle f_{a,b}(x)=a\sin(bx)}
   has only two parameters (
  
    
      
        a
        ,
        b
      
    
    {\displaystyle a,b}
  ) but it can interpolate any number of points by oscillating with a high enough frequency, resulting in both a high bias and high variance.
Intuitively, bias is reduced by using only local information, whereas variance can only be reduced by averaging over multiple observations, which inherently means using information from a larger region. For an enlightening example, see the section on k-nearest neighbors or the figure on the right.
To balance how much information is used from neighboring observations, a model can be smoothed via explicit regularization, such as shrinkage.

Bias–variance decomposition of mean squared error
Suppose that we have a training set consisting of a set of points 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},\dots ,x_{n}}
   and real values 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
   associated with each point 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  . We assume that there is a function with noise 
  
    
      
        y
        =
        f
        (
        x
        )
        +
        ε
      
    
    {\displaystyle y=f(x)+\varepsilon }
  , where the noise, 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  , has zero mean and variance 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\displaystyle \sigma ^{2}}
  .
We want to find a function 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
      
    
    {\displaystyle {\hat {f}}(x;D)}
  , that approximates the true function 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   as well as possible, by means of some learning algorithm based on a training dataset (sample) 
  
    
      
        D
        =
        {
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        …
        ,
        (
        
          x
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
        }
      
    
    {\displaystyle D=\{(x_{1},y_{1})\dots ,(x_{n},y_{n})\}}
  . We make ""as well as possible"" precise by measuring the mean squared error between 
  
    
      
        y
      
    
    {\displaystyle y}
   and 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
      
    
    {\displaystyle {\hat {f}}(x;D)}
  : we want 
  
    
      
        (
        y
        −
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          )
          
            2
          
        
      
    
    {\displaystyle (y-{\hat {f}}(x;D))^{2}}
   to be minimal, both for 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},\dots ,x_{n}}
   and for points outside of our sample. Of course, we cannot hope to do so perfectly, since the 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
   contain noise 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  ; this means we must be prepared to accept an irreducible error in any function we come up with.
Finding an 
  
    
      
        
          
            
              f
              ^
            
          
        
      
    
    {\displaystyle {\hat {f}}}
   that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function 
  
    
      
        
          
            
              f
              ^
            
          
        
      
    
    {\displaystyle {\hat {f}}}
   we select, we can decompose its expected error on an unseen sample 
  
    
      
        x
      
    
    {\displaystyle x}
   as follows:

  
    
      
        
          E
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            (
          
        
        y
        −
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            
              )
            
          
          
            2
          
        
        
          
            ]
          
        
        =
        
          
            (
          
        
        
          Bias
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            ]
          
        
        
          
            
              )
            
          
          
            2
          
        
        +
        
          Var
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            ]
          
        
        +
        
          σ
          
            2
          
        
      
    
    {\displaystyle \operatorname {E} _{D}{\Big [}{\big (}y-{\hat {f}}(x;D){\big )}^{2}{\Big ]}={\Big (}\operatorname {Bias} _{D}{\big [}{\hat {f}}(x;D){\big ]}{\Big )}^{2}+\operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}+\sigma ^{2}}
  where

  
    
      
        
          Bias
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            ]
          
        
        =
        
          E
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            ]
          
        
        −
        f
        (
        x
        )
      
    
    {\displaystyle \operatorname {Bias} _{D}{\big [}{\hat {f}}(x;D){\big ]}=\operatorname {E} _{D}{\big [}{\hat {f}}(x;D){\big ]}-f(x)}
  and

  
    
      
        
          Var
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            ]
          
        
        =
        
          E
          
            D
          
        
        ⁡
        [
        
          
            (
          
        
        
          E
          
            D
          
        
        ⁡
        [
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        ]
        −
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            
              )
            
          
          
            2
          
        
        ]
        .
      
    
    {\displaystyle \operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}=\operatorname {E} _{D}[{\big (}\operatorname {E} _{D}[{\hat {f}}(x;D)]-{\hat {f}}(x;D){\big )}^{2}].}
  The expectation ranges over different choices of the training set 
  
    
      
        D
        =
        {
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        …
        ,
        (
        
          x
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
        }
      
    
    {\displaystyle D=\{(x_{1},y_{1})\dots ,(x_{n},y_{n})\}}
  , all sampled from the same joint distribution 
  
    
      
        P
        (
        x
        ,
        y
        )
      
    
    {\displaystyle P(x,y)}
  . The three terms represent:

the square of the bias of the learning method, which can be thought of as the error caused by the simplifying assumptions built into the method. E.g., when approximating a non-linear function 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   using a learning method for linear models, there will be error in the estimates 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        )
      
    
    {\displaystyle {\hat {f}}(x)}
   due to this assumption;
the variance of the learning method, or, intuitively, how much the learning method 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        )
      
    
    {\displaystyle {\hat {f}}(x)}
   will move around its mean;
the irreducible error 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\displaystyle \sigma ^{2}}
  .Since all three terms are non-negative, this forms a lower bound on the expected error on unseen samples.The more complex the model 
  
    
      
        
          
            
              f
              ^
            
          
        
        (
        x
        )
      
    
    {\displaystyle {\hat {f}}(x)}
   is, the more data points it will capture, and the lower the bias will be. However, complexity will make the model ""move"" more to capture the data points, and hence its variance will be larger.

Derivation
The derivation of the bias–variance decomposition for squared error proceeds as follows. For notational convenience, we abbreviate 
  
    
      
        f
        =
        f
        (
        x
        )
      
    
    {\displaystyle f=f(x)}
  , 
  
    
      
        
          
            
              f
              ^
            
          
        
        =
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
      
    
    {\displaystyle {\hat {f}}={\hat {f}}(x;D)}
   and we drop the 
  
    
      
        D
      
    
    {\displaystyle D}
   subscript on our expectation operators. First, recall that, by definition, for any random variable 
  
    
      
        X
      
    
    {\displaystyle X}
  , we have

  
    
      
        Var
        ⁡
        [
        X
        ]
        =
        E
        ⁡
        [
        
          X
          
            2
          
        
        ]
        −
        E
        ⁡
        [
        X
        
          ]
          
            2
          
        
        .
      
    
    {\displaystyle \operatorname {Var} [X]=\operatorname {E} [X^{2}]-\operatorname {E} [X]^{2}.}
  Rearranging, we get:

  
    
      
        E
        ⁡
        [
        
          X
          
            2
          
        
        ]
        =
        Var
        ⁡
        [
        X
        ]
        +
        E
        ⁡
        [
        X
        
          ]
          
            2
          
        
        .
      
    
    {\displaystyle \operatorname {E} [X^{2}]=\operatorname {Var} [X]+\operatorname {E} [X]^{2}.}
  Since 
  
    
      
        f
      
    
    {\displaystyle f}
   is deterministic, i.e. independent of 
  
    
      
        D
      
    
    {\displaystyle D}
  ,

  
    
      
        E
        ⁡
        [
        f
        ]
        =
        f
        .
      
    
    {\displaystyle \operatorname {E} [f]=f.}
  Thus, given 
  
    
      
        y
        =
        f
        +
        ε
      
    
    {\displaystyle y=f+\varepsilon }
   and 
  
    
      
        E
        ⁡
        [
        ε
        ]
        =
        0
      
    
    {\displaystyle \operatorname {E} [\varepsilon ]=0}
   (because 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   is noise), implies 
  
    
      
        E
        ⁡
        [
        y
        ]
        =
        E
        ⁡
        [
        f
        +
        ε
        ]
        =
        E
        ⁡
        [
        f
        ]
        =
        f
        .
      
    
    {\displaystyle \operatorname {E} [y]=\operatorname {E} [f+\varepsilon ]=\operatorname {E} [f]=f.}
  
Also, since 
  
    
      
        Var
        ⁡
        [
        ε
        ]
        =
        
          σ
          
            2
          
        
        ,
      
    
    {\displaystyle \operatorname {Var} [\varepsilon ]=\sigma ^{2},}
  

  
    
      
        Var
        ⁡
        [
        y
        ]
        =
        E
        ⁡
        [
        (
        y
        −
        E
        ⁡
        [
        y
        ]
        
          )
          
            2
          
        
        ]
        =
        E
        ⁡
        [
        (
        y
        −
        f
        
          )
          
            2
          
        
        ]
        =
        E
        ⁡
        [
        (
        f
        +
        ε
        −
        f
        
          )
          
            2
          
        
        ]
        =
        E
        ⁡
        [
        
          ε
          
            2
          
        
        ]
        =
        Var
        ⁡
        [
        ε
        ]
        +
        E
        ⁡
        [
        ε
        
          ]
          
            2
          
        
        =
        
          σ
          
            2
          
        
        +
        
          0
          
            2
          
        
        =
        
          σ
          
            2
          
        
        .
      
    
    {\displaystyle \operatorname {Var} [y]=\operatorname {E} [(y-\operatorname {E} [y])^{2}]=\operatorname {E} [(y-f)^{2}]=\operatorname {E} [(f+\varepsilon -f)^{2}]=\operatorname {E} [\varepsilon ^{2}]=\operatorname {Var} [\varepsilon ]+\operatorname {E} [\varepsilon ]^{2}=\sigma ^{2}+0^{2}=\sigma ^{2}.}
  Thus, since 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   and 
  
    
      
        
          
            
              f
              ^
            
          
        
      
    
    {\displaystyle {\hat {f}}}
   are independent, we can write

  
    
      
        
          
            
              
                E
                ⁡
                
                  
                    [
                  
                
                (
                y
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                
                  )
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
              
                
                =
                E
                ⁡
                
                  
                    [
                  
                
                (
                f
                +
                ε
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                
                  )
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                E
                ⁡
                
                  
                    [
                  
                
                (
                f
                +
                ε
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                +
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                −
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                
                  )
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                E
                ⁡
                
                  
                    [
                  
                
                (
                f
                −
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                
                  )
                  
                    2
                  
                
                
                  
                    ]
                  
                
                +
                E
                ⁡
                [
                
                  ε
                  
                    2
                  
                
                ]
                +
                E
                ⁡
                
                  
                    [
                  
                
                (
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                
                  )
                  
                    2
                  
                
                
                  
                    ]
                  
                
                +
                2
                E
                ⁡
                
                  
                    [
                  
                
                (
                f
                −
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                )
                ε
                
                  
                    ]
                  
                
                +
                2
                E
                ⁡
                
                  
                    [
                  
                
                ε
                (
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                )
                
                  
                    ]
                  
                
                +
                2
                E
                ⁡
                
                  
                    [
                  
                
                (
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                )
                (
                f
                −
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                )
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                (
                f
                −
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                
                  )
                  
                    2
                  
                
                +
                E
                ⁡
                [
                
                  ε
                  
                    2
                  
                
                ]
                +
                E
                ⁡
                
                  
                    [
                  
                
                (
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                
                  )
                  
                    2
                  
                
                
                  
                    ]
                  
                
                +
                2
                (
                f
                −
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                )
                E
                ⁡
                [
                ε
                ]
                +
                2
                E
                ⁡
                [
                ε
                ]
                E
                ⁡
                
                  
                    [
                  
                
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                
                  
                    ]
                  
                
                +
                2
                E
                ⁡
                
                  
                    [
                  
                
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                
                  
                    ]
                  
                
                (
                f
                −
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                )
              
            
            
              
              
                
                =
                (
                f
                −
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                
                  )
                  
                    2
                  
                
                +
                E
                ⁡
                [
                
                  ε
                  
                    2
                  
                
                ]
                +
                E
                ⁡
                
                  
                    [
                  
                
                (
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                
                  )
                  
                    2
                  
                
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                (
                f
                −
                E
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                ]
                
                  )
                  
                    2
                  
                
                +
                Var
                ⁡
                [
                ε
                ]
                +
                Var
                ⁡
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                Bias
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                
                  ]
                  
                    2
                  
                
                +
                Var
                ⁡
                [
                ε
                ]
                +
                Var
                ⁡
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                
                  
                    ]
                  
                
              
            
            
              
              
                
                =
                Bias
                ⁡
                [
                
                  
                    
                      f
                      ^
                    
                  
                
                
                  ]
                  
                    2
                  
                
                +
                
                  σ
                  
                    2
                  
                
                +
                Var
                ⁡
                
                  
                    [
                  
                
                
                  
                    
                      f
                      ^
                    
                  
                
                
                  
                    ]
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\operatorname {E} {\big [}(y-{\hat {f}})^{2}{\big ]}&=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}})^{2}{\big ]}\\[5pt]&=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}}+\operatorname {E} [{\hat {f}}]-\operatorname {E} [{\hat {f}}])^{2}{\big ]}\\[5pt]&=\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])^{2}{\big ]}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])\varepsilon {\big ]}+2\operatorname {E} {\big [}\varepsilon (\operatorname {E} [{\hat {f}}]-{\hat {f}}){\big ]}+2\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})(f-\operatorname {E} [{\hat {f}}]){\big ]}\\[5pt]&=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2(f-\operatorname {E} [{\hat {f}}])\operatorname {E} [\varepsilon ]+2\operatorname {E} [\varepsilon ]\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}+2\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}(f-\operatorname {E} [{\hat {f}}])\\[5pt]&=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}\\[5pt]&=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {Var} [\varepsilon ]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\[5pt]&=\operatorname {Bias} [{\hat {f}}]^{2}+\operatorname {Var} [\varepsilon ]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\[5pt]&=\operatorname {Bias} [{\hat {f}}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}{\hat {f}}{\big ]}.\end{aligned}}}
  Finally, MSE loss function (or negative log-likelihood) is obtained by taking the expectation value over 
  
    
      
        x
        ∼
        P
      
    
    {\displaystyle x\sim P}
  :

  
    
      
        
          MSE
        
        =
        
          E
          
            x
          
        
        ⁡
        
          
            {
          
        
        
          Bias
          
            D
          
        
        ⁡
        [
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          ]
          
            2
          
        
        +
        
          Var
          
            D
          
        
        ⁡
        
          
            [
          
        
        
          
            
              f
              ^
            
          
        
        (
        x
        ;
        D
        )
        
          
            ]
          
        
        
          
            }
          
        
        +
        
          σ
          
            2
          
        
        .
      
    
    {\displaystyle {\text{MSE}}=\operatorname {E} _{x}{\bigg \{}\operatorname {Bias} _{D}[{\hat {f}}(x;D)]^{2}+\operatorname {Var} _{D}{\big [}{\hat {f}}(x;D){\big ]}{\bigg \}}+\sigma ^{2}.}

Approaches
Dimensionality reduction and feature selection can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance; for example,

linear  and Generalized linear models can be regularized to decrease their variance at the cost of increasing their bias.
In artificial neural networks, the variance increases and the bias decreases as the number of hidden units increase, although this classical assumption has been the subject of recent debate. Like in GLMs, regularization is typically applied.
In k-nearest neighbor models, a high value of k leads to high bias and low variance (see below).
In instance-based learning, regularization can be achieved varying the mixture of prototypes and exemplars.
In decision trees, the depth of the tree determines the variance. Decision trees are commonly pruned to control variance.One way of resolving the trade-off is to use mixture models and ensemble learning. For example, boosting combines many ""weak"" (high bias) models in an ensemble that has lower bias than the individual models, while bagging combines ""strong"" learners in a way that reduces their variance.  
Model validation methods such as cross-validation (statistics) can be used to tune models so as to optimize the trade-off.

k-nearest neighbors
In the case of k-nearest neighbors regression, when the expectation is taken over the possible labeling of a fixed training set, a closed-form expression exists that relates the bias–variance decomposition to the parameter k:

  
    
      
        E
        ⁡
        [
        (
        y
        −
        
          
            
              f
              ^
            
          
        
        (
        x
        )
        
          )
          
            2
          
        
        ∣
        X
        =
        x
        ]
        =
        
          
            (
            
              f
              (
              x
              )
              −
              
                
                  1
                  k
                
              
              
                ∑
                
                  i
                  =
                  1
                
                
                  k
                
              
              f
              (
              
                N
                
                  i
                
              
              (
              x
              )
              )
            
            )
          
          
            2
          
        
        +
        
          
            
              σ
              
                2
              
            
            k
          
        
        +
        
          σ
          
            2
          
        
      
    
    {\displaystyle \operatorname {E} [(y-{\hat {f}}(x))^{2}\mid X=x]=\left(f(x)-{\frac {1}{k}}\sum _{i=1}^{k}f(N_{i}(x))\right)^{2}+{\frac {\sigma ^{2}}{k}}+\sigma ^{2}}
  where 
  
    
      
        
          N
          
            1
          
        
        (
        x
        )
        ,
        …
        ,
        
          N
          
            k
          
        
        (
        x
        )
      
    
    {\displaystyle N_{1}(x),\dots ,N_{k}(x)}
   are the k nearest neighbors of x in the training set. The bias (first term) is a monotone rising function of k, while the variance (second term) drops off as k is increased. In fact, under ""reasonable assumptions"" the bias of the first-nearest neighbor (1-NN) estimator vanishes entirely as the size of the training set approaches infinity.

Applications
In regression
The bias–variance decomposition forms the conceptual basis for regression regularization methods such as Lasso and ridge regression. Regularization methods introduce bias into the regression solution that can reduce variance considerably relative to the ordinary least squares (OLS) solution.  Although the OLS solution provides non-biased regression estimates, the lower variance solutions produced by regularization techniques provide superior MSE performance.

In classification
The bias–variance decomposition was originally formulated for least-squares regression. For the case of classification under the 0-1 loss (misclassification rate), it is possible to find a similar decomposition. Alternatively, if the classification problem can be phrased as probabilistic classification, then the expected squared error of the predicted probabilities with respect to the true probabilities can be decomposed as before.

In reinforcement learning
Even though the bias–variance decomposition does not directly apply in reinforcement learning, a similar tradeoff can also characterize generalization. When an agent has limited information on its environment, the suboptimality of an RL algorithm can be decomposed into the sum of two terms: a term related to an asymptotic bias and a term due to overfitting. The asymptotic bias is directly related to the learning algorithm (independently of the quantity of data) while the overfitting term comes from the fact that the amount of data is limited.

In human learning
While widely discussed in the context of machine learning, the bias-variance dilemma has been examined in the context of human cognition, most notably by Gerd Gigerenzer and co-workers in the context of learned heuristics. They have argued (see references below) that the human brain resolves the dilemma in the case of the typically sparse, poorly-characterised training-sets provided by experience by adopting high-bias/low variance heuristics. This reflects the fact that a zero-bias approach has poor generalisability to new situations, and also unreasonably presumes precise knowledge of the true state of the world. The resulting heuristics are relatively simple, but produce better inferences in a wider variety of situations.Geman et al. argue that the bias-variance dilemma implies that abilities such as generic object recognition cannot be learned from scratch, but require a certain degree of “hard wiring”   that is later tuned by experience.  This is because model-free approaches to inference require impractically large training sets if they are to avoid high variance.

See also


== References ==",https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff,"['Articles with short description', 'Dilemmas', 'Machine learning', 'Model selection', 'Short description is different from Wikidata', 'Statistical classification']",Data Science
13,Big data,"Big data is a field that treats ways to analyze, systematically extract information from, or otherwise deal with data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many fields (columns) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Therefore, big data often includes data with sizes that exceed the capacity of traditional software to process within an acceptable time and value.
Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. ""There is little doubt that the quantities of data now available are indeed large, but that's not the most relevant characteristic of this new data ecosystem.""
Analysis of data sets can find new correlations to ""spot business trends, prevent diseases, combat crime and so on"". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics.  Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.The size and number of available data sets has grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.5×260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require ""massively parallel software running on tens, hundreds, or even thousands of servers"". What qualifies as ""big data"" varies depending on the capabilities of those analyzing it and their tools.  Furthermore, expanding capabilities make big data a moving target. ""For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.""

Definition
The term big data has been in use since the 1990s, with some giving credit to John Mashey for popularizing the term.
Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time. Big data philosophy encompasses unstructured, semi-structured and structured data, however the main focus is on unstructured data. Big data ""size"" is a constantly moving target; as of 2012 ranging from a few dozen terabytes to many zettabytes of data.
Big data requires a set of techniques and technologies with new forms of integration to reveal insights from data-sets that are diverse, complex, and of a massive scale.""Variety"", ""veracity"", and various other ""Vs"" are added by some organizations to describe it, a revision challenged by some industry authorities.  The Vs of big data were often referred to as the ""three Vs"", ""four Vs"", and ""five Vs"".  They represented the qualities of big data in volume, variety, velocity, veracity, and value.  Variability is often included as an additional quality of big data.
A 2018 definition states ""Big data is where parallel computing tools are needed to handle data"", and notes, ""This represents a distinct and clearly defined change in the computer science used, via parallel programming theories, and losses of
some of the guarantees and capabilities made by Codd's relational model.""

Big data vs. business intelligence
The growing maturity of the concept more starkly delineates the difference between ""big data"" and ""business intelligence"":
Business intelligence uses applied mathematics tools and descriptive statistics with data with high information density to measure things, detect trends, etc.
Big data uses mathematical analysis, optimization, inductive statistics, and concepts from nonlinear system identification to infer laws (regressions, nonlinear relationships, and causal effects) from large sets of data with low information density to reveal relationships and dependencies, or to perform predictions of outcomes and behaviors.

Characteristics
Big data can be described by the following characteristics:

Volume
The quantity of generated and stored data. The size of the data determines the value and potential insight, and whether it can be considered big data or not. The size of big data is usually larger than terabytes and petabytes.Variety
The type and nature of the data. The earlier technologies like RDBMSs were capable to handle structured data efficiently and effectively. However, the change in type and nature from structured to semi-structured or unstructured challenged the existing tools and technologies. The big data technologies evolved with the prime intention to capture, store, and process the semi-structured and unstructured (variety) data generated with high speed (velocity), and huge in size (volume). Later, these tools and technologies were explored and used for handling structured data also but preferable for storage. Eventually, the processing of structured data was still kept as optional, either using big data or traditional RDBMSs. This helps in analyzing data towards effective usage of the hidden insights exposed from the data collected via social media, log files, sensors, etc. Big data draws from text, images, audio, video; plus it completes missing pieces through data fusion.Velocity
The speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development. Big data is often available in real-time. Compared to small data, big data is produced more continually. Two kinds of velocity related to big data are the frequency of generation and the frequency of handling, recording, and publishing.Veracity
The truthfulness or reliability of the data, which refers to the data quality and the data value. Big data must not only be large in size, but also must be reliable in order to achieve value in the analysis of it. The data quality of captured data can vary greatly, affecting an accurate analysis.Value
The worth in information that can be achieved by the processing and analysis of large datasets.  Value also can be measured by an assessment of the other qualities of big data.  Value may also represent the profitability of information that is retrieved from the analysis of big data.Variability
The characteristic of the changing formats, structure, or sources of big data.  Big data can include structured, unstructured, or combinations of structured and unstructured data.  Big data analysis may integrate raw data from multiple sources.  The processing of raw data may also involve transformations of unstructured data to structured data.Other possible characteristics of big data are:
Exhaustive
Whether the entire system (i.e.,  
  
    
      
        n
      
    
    {\textstyle n}
  =all) is captured or recorded or not.  Big data may or may not include all the available data from sources.Fine-grained and uniquely lexical
Respectively, the proportion of specific data of each element per element collected and if the element and its characteristics are properly indexed or identified.Relational
If the data collected contains common fields that would enable a conjoining, or meta-analysis, of different data sets.Extensional
If new fields in each element of the data collected can be added or changed easily.Scalability
If the size of the big data storage system can expand rapidly.

Architecture
Big data repositories have existed in many forms, often built by corporations with a special need.  Commercial vendors historically offered parallel database management systems for big data beginning in the 1990s.  For many years, WinterCorp published the largest database report.Teradata Corporation in 1984 marketed the parallel processing DBC 1012 system. Teradata systems were the first to store and analyze 1 terabyte of data in 1992. Hard disk drives were 2.5 GB in 1991 so the definition of big data continuously evolves according to Kryder's law. Teradata installed the first petabyte class RDBMS based system in 2007. As of 2017, there are a few dozen petabyte class Teradata relational databases installed, the largest of which exceeds 50 PB. Systems up until 2008 were 100% structured relational data.  Since then, Teradata has added unstructured data types including XML, JSON, and Avro.
In 2000, Seisint Inc. (now LexisNexis Risk Solutions) developed a C++-based distributed platform for data processing and querying known as the HPCC Systems platform. This system automatically partitions, distributes, stores and delivers structured, semi-structured, and unstructured data across multiple commodity servers.  Users can write data processing pipelines and queries in a declarative dataflow programming language called ECL. Data analysts working in ECL are not required to define data schemas upfront and can rather focus on the particular problem at hand, reshaping data in the best possible manner as they develop the solution. In 2004, LexisNexis acquired Seisint Inc. and their high-speed parallel processing platform and successfully used this platform to integrate the data systems of Choicepoint Inc. when they acquired that company in 2008. In 2011, the HPCC systems platform was open-sourced under the Apache v2.0 License.
CERN and other physics experiments have collected big data sets for many decades, usually analyzed via high-throughput computing rather than the map-reduce architectures usually meant by the current ""big data"" movement.
In 2004, Google published a paper on a process called MapReduce that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data.  With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the ""map"" step). The results are then gathered and delivered (the ""reduce"" step). The framework was very successful, so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named ""Hadoop"". Apache Spark was developed in 2012 in response to limitations in the MapReduce paradigm, as it adds the ability to set up many operations (not just map followed by reducing).
MIKE2.0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled ""Big Data Solution Offering"". The methodology addresses handling big data in terms of useful permutations of data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records.Studies in 2012 showed that a multiple-layer architecture was one option to address the issues that big data presents. A distributed parallel architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end-user by using a front-end application server.The data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.

Technologies
A 2011 McKinsey Global Institute report characterizes the main components and ecosystem of big data as follows:
Techniques for analyzing data, such as A/B testing, machine learning, and natural language processing
Big data technologies, like business intelligence, cloud computing, and databases
Visualization, such as charts, graphs, and other displays of the dataMultidimensional big data can also be represented as OLAP data cubes or, mathematically, tensors. Array database systems have set out to provide storage and high-level query support on this data type.
Additional technologies being applied to big data include efficient tensor-based computation, such as multilinear subspace learning, massively parallel-processing (MPP) databases, search-based applications, data mining, distributed file systems, distributed cache (e.g., burst buffer and Memcached), distributed databases, cloud and HPC-based infrastructure (applications, storage and computing resources), and the Internet. Although, many approaches and technologies have been developed, it still remains difficult to carry out machine learning with big data.Some MPP relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the RDBMS.DARPA's Topological Data Analysis program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called ""Ayasdi"".The practitioners of big data analytics processes are generally hostile to slower shared storage, preferring direct-attached storage (DAS) in its various forms from solid state drive (SSD) to high capacity SATA disk buried inside parallel processing nodes. The perception of shared storage architectures—storage area network (SAN) and network-attached storage (NAS)— is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.
Real or near-real-time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in direct-attached memory or disk is good—data on memory or disk at the other end of an FC SAN connection is not. The cost of an SAN at the scale needed for analytics applications is much higher than other storage techniques.
There are advantages as well as disadvantages to shared storage in big data analytics, but big data analytics practitioners as of 2011 did not favor it.

Applications
Big data has increased the demand of information management specialists so much so that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP, and Dell have spent more than $15 billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100 billion and was growing at almost 10 percent a year: about twice as fast as the software business as a whole.Developed economies increasingly use data-intensive technologies. There are 4.6 billion mobile-phone subscriptions worldwide, and between 1 billion and 2 billion people accessing the internet. Between 1990 and 2005, more than 1 billion people worldwide entered the middle class, which means more people became more literate, which in turn led to information growth. The world's effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986, 471 petabytes in 1993, 2.2 exabytes in 2000, 65 exabytes in 2007 and predictions put the amount of internet traffic at 667 exabytes annually by 2014. According to one estimate, one-third of the globally stored information is in the form of alphanumeric text and still image data, which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).
While many vendors offer off-the-shelf solutions for big data, experts recommend the development of in-house solutions custom-tailored to solve the company's problem at hand if the company has sufficient technical capabilities.

Government
The use and adoption of big data within governmental processes allows efficiencies in terms of cost, productivity, and innovation, but does not come without its flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. A common government organization that makes use of big data is the National Security Administration (NSA), who monitor the activities of the Internet constantly in search for potential patterns of suspicious or illegal activities their system may pick up.
Civil registration and vital statistics (CRVS) collects all certificates status from birth to death. CRVS is a source of big data for governments.

International development
Research on the effective usage of information and communication technologies for development (also known as ""ICT4D"") suggests that big data technology can make important contributions but also present unique challenges to international development. Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as health care, employment, economic productivity, crime, security, and natural disaster and resource management. Additionally, user-generated data offers new opportunities to give the unheard a voice. However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues. The challenge of ""big data for development"" is currently evolving toward the application of this data through machine learning, known as ""artificial intelligence for development (AI4D).

Benefits
A major practical application of big data for development has been ""fighting poverty with data"". In 2015, Blumenstock and colleagues estimated predicted poverty and wealth from mobile phone metadata  and in 2016 Jean and colleagues combined satellite imagery and machine learning to predict poverty. Using digital trace data to study the labor market and the digital economy in Latin America, Hilbert and colleagues  argue that digital trace data has several benefits such as:

Thematic coverage: including areas that were previously difficult or impossible to measure
Geographical coverage: our international sources provided sizable and comparable data for almost all countries, including many small countries that usually are not included in international inventories
Level of detail: providing fine-grained data with many interrelated variables, and new aspects, like network connections
Timeliness and timeseries: graphs can be produced within days of being collected

Challenges
At the same time, working with digital trace data instead of traditional survey data does not eliminate the traditional challenges involved when working in the field of international quantitative analysis. Priorities change, but the basic discussions remain the same. Among the main challenges are:

Representativeness. While traditional development statistics is mainly concerned with the representativeness of random survey samples, digital trace data is never a random sample.
Generalizability. While observational data always represents this source very well, it only represents what it represents, and nothing more. While it is tempting to generalize from specific observations of one platform to broader settings, this is often very deceptive.
Harmonization. Digital trace data still requires international harmonization of indicators. It adds the challenge of so-called ""data-fusion"", the harmonization of different sources.
Data overload. Analysts and institutions are not used to effectively deal with a large number of variables, which is efficiently done with interactive dashboards. Practitioners still lack a standard workflow that would allow researchers, users and policymakers to efficiently and effectively.

Healthcare
Big data analytics has helped healthcare improve by providing personalized medicine and prescriptive analytics, clinical risk intervention and predictive analytics, waste and care variability reduction, automated external and internal reporting of patient data, standardized medical terms and patient registries and fragmented point solutions. Some areas of improvement are more aspirational than actually implemented. The level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearable technologies the volume of data will continue to increase. This includes electronic health record data, imaging data, patient generated data, sensor data, and other forms of difficult to process data. There is now an even greater need for such environments to pay greater attention to data and information quality. ""Big data very often means 'dirty data' and the fraction of data inaccuracies increases with data volume growth."" Human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed. While extensive information in healthcare is now electronic, it fits under the big data umbrella as most is unstructured and difficult to use. The use of big data in healthcare has raised significant ethical challenges ranging from risks for individual rights, privacy and autonomy, to transparency and trust.Big data in health research is particularly promising in terms of exploratory biomedical research, as data-driven analysis can move forward more quickly than hypothesis-driven research. Then, trends seen in data analysis can be tested in traditional, hypothesis-driven follow up biological research and eventually clinical research.
A related application sub-area, that heavily relies on big data, within the healthcare field is that of computer-aided diagnosis in medicine.
 For instance, for epilepsy monitoring it is customary to create 5 to 10 GB of data daily. 
 Similarly, a single uncompressed image of breast tomosynthesis averages 450 MB of data. 

These are just few of the many examples where computer-aided diagnosis uses big data.  For this reason, big data has been recognized as one of the seven key challenges that computer-aided diagnosis systems need to overcome in order to reach the next level of performance.

Education
A McKinsey Global Institute study found a shortage of 1.5 million highly trained data professionals and managers and a number of universities including University of Tennessee and UC Berkeley, have created masters programs to meet this demand. Private boot camps have also developed programs to meet that demand, including free programs like The Data Incubator or paid programs like General Assembly. In the specific field of marketing, one of the problems stressed by Wedel and Kannan is that marketing has several sub domains (e.g., advertising, promotions,
product development, branding) that all use different types of data. Because one-size-fits-all analytical solutions are not desirable, business schools should prepare marketing managers to have wide knowledge on all the different techniques used in these subdomains to get a big picture and work effectively with analysts.

Media
To understand how the media uses big data, it is first necessary to provide some context into the mechanism used for media process. It has been suggested by Nick Couldry and Joseph Turow that practitioners in media and advertising approach big data as many actionable points of information about millions of individuals. The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations.  The ultimate aim is to serve or convey, a message or content that is (statistically speaking) in line with the consumer's mindset. For example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various data-mining activities.
Targeting of consumers (for advertising by marketers)
Data capture
Data journalism: publishers and journalists use big data tools to provide unique and innovative insights and infographics.Channel 4, the British public-service television broadcaster, is a leader in the field of big data and data analysis.

Insurance
Health insurance providers are collecting data on social ""determinants of health"" such as food and TV consumption, marital status, clothing size, and purchasing habits, from which they make predictions on health costs, in order to spot health issues in their clients. It is controversial whether these predictions are currently being used for pricing.

Internet of Things (IoT)
Big data and the IoT work in conjunction. Data extracted from IoT devices provides a mapping of device inter-connectivity. Such mappings have been used by the media industry, companies, and governments to more accurately target their audience and increase media efficiency. The IoT is also increasingly adopted as a means of gathering sensory data, and this sensory data has been used in medical, manufacturing and transportation contexts.
Kevin Ashton, the digital innovation expert who is credited with coining the term, defines the Internet of things in this quote: ""If we had computers that knew everything there was to know about things—using data they gathered without any help from us—we would be able to track and count everything, and greatly reduce waste, loss, and cost. We would know when things needed replacing, repairing, or recalling, and whether they were fresh or past their best.""

Information technology
Especially since 2015, big data has come to prominence within business operations as a tool to help employees work more efficiently and streamline the collection and distribution of information technology (IT). The use of big data to resolve IT and data collection issues within an enterprise is called IT operations analytics (ITOA). By applying big data principles into the concepts of machine intelligence and deep computing, IT departments can predict potential issues and move to provide solutions before the problems even happen. In this time, ITOA businesses were also beginning to play a major role in systems management by offering platforms that brought individual data silos together and generated insights from the whole of the system rather than from isolated pockets of data.

Case studies
Government
China
The Integrated Joint Operations Platform (IJOP, 一体化联合作战平台) is used by the government to monitor the population, particularly Uyghurs. Biometrics, including DNA samples, are gathered through a program of free physicals.
By 2020, China plans to give all its citizens a personal ""social credit"" score based on how they behave. The Social Credit System, now being piloted in a number of Chinese cities, is considered a form of mass surveillance which uses big data analysis technology.

India
Big data analysis was tried out for the BJP to win the 2014 Indian General Election.
The Indian government uses numerous techniques to ascertain how the Indian electorate is responding to government action, as well as ideas for policy augmentation.

Israel
Personalized diabetic treatments can be created through GlucoMe's big data solution.

United Kingdom
Examples of uses of big data in public services:

Data on prescription drugs: by connecting origin, location and the time of each prescription, a research unit was able to exemplify the considerable delay between the release of any given drug, and a UK-wide adaptation of the National Institute for Health and Care Excellence guidelines. This suggests that new or most up-to-date drugs take some time to filter through to the general patient.
Joining up data: a local authority blended data about services, such as road gritting rotas, with services for people at risk, such as ""meals on wheels."" The connection of data allowed the local authority to avoid any weather-related delay.

United States
In 2012, the Obama administration announced the Big Data Research and Development Initiative, to explore how big data could be used to address important problems faced by the government. The initiative is composed of 84 different big data programs spread across six departments.
Big data analysis played a large role in Barack Obama's successful 2012 re-election campaign.
The United States Federal Government owns five of the ten most powerful supercomputers in the world.
The Utah Data Center has been constructed by the United States National Security Agency. When finished, the facility will be able to handle a large amount of information collected by the NSA over the Internet. The exact amount of storage space is unknown, but more recent sources claim it will be on the order of a few exabytes. This has posed security concerns regarding the anonymity of the data collected.

Retail
Walmart handles more than 1 million customer transactions every hour, which are imported into databases estimated to contain more than 2.5 petabytes (2560 terabytes) of data—the equivalent of 167 times the information contained in all the books in the US Library of Congress.
Windermere Real Estate uses location information from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day.
FICO Card Detection System protects accounts worldwide.

Science
The Large Hadron Collider experiments represent about 150 million sensors delivering data 40 million times per second. There are nearly 600 million collisions per second. After filtering and refraining from recording more than 99.99995% of these streams, there are 1,000 collisions of interest per second.As a result, only working with less than 0.001% of the sensor stream data, the data flow from all four LHC experiments represents 25 petabytes annual rate before replication (as of 2012). This becomes nearly 200 petabytes after replication.
If all sensor data were recorded in LHC, the data flow would be extremely hard to work with. The data flow would exceed 150 million petabytes annual rate, or nearly 500 exabytes per day, before replication. To put the number in perspective, this is equivalent to 500 quintillion (5×1020) bytes per day, almost 200 times more than all the other sources combined in the world.
The Square Kilometre Array is a radio telescope built of thousands of antennas. It is expected to be operational by 2024. Collectively, these antennas are expected to gather 14 exabytes and store one petabyte per day. It is considered one of the most ambitious scientific projects ever undertaken.
When the Sloan Digital Sky Survey (SDSS) began to collect astronomical data in 2000, it amassed more in its first few weeks than all data collected in the history of astronomy previously. Continuing at a rate of about 200 GB per night, SDSS has amassed more than 140 terabytes of information. When the Large Synoptic Survey Telescope, successor to SDSS, comes online in 2020, its designers expect it to acquire that amount of data every five days.
Decoding the human genome originally took 10 years to process; now it can be achieved in less than a day. The DNA sequencers have divided the sequencing cost by 10,000 in the last ten years, which is 100 times cheaper than the reduction in cost predicted by Moore's law.
The NASA Center for Climate Simulation (NCCS) stores 32 petabytes of climate observations and simulations on the Discover supercomputing cluster.
Google's DNAStack compiles and organizes DNA samples of genetic data from around the world to identify diseases and other medical defects. These fast and exact calculations eliminate any ""friction points"", or human errors that could be made by one of the numerous science and biology experts working with the DNA. DNAStack, a part of Google Genomics, allows scientists to use the vast sample of resources from Google's search server to scale social experiments that would usually take years, instantly.
23andme's DNA database contains genetic information of over 1,000,000 people worldwide. The company explores selling the ""anonymous aggregated genetic data"" to other researchers and pharmaceutical companies for research purposes if patients give their consent. Ahmad Hariri, professor of psychology and neuroscience at Duke University who has been using 23andMe in his research since 2009 states that the most important aspect of the company's new service is that it makes genetic research accessible and relatively cheap for scientists. A study that identified 15 genome sites linked to depression in 23andMe's database lead to a surge in demands to access the repository with 23andMe fielding nearly 20 requests to access the depression data in the two weeks after publication of the paper.
Computational fluid dynamics (CFD) and hydrodynamic turbulence research generate massive data sets. The Johns Hopkins Turbulence Databases (JHTDB) contains over 350 terabytes of spatiotemporal fields from Direct Numerical simulations of various turbulent flows. Such data have been difficult to share using traditional methods such as downloading flat simulation output files. The data within JHTDB can be accessed using ""virtual sensors"" with various access modes ranging from direct web-browser queries, access through Matlab, Python, Fortran and C programs executing on clients' platforms, to cut out services to download raw data.  The data have been used in over  150 scientific publications.

Sports
Big data can be used to improve training and understanding competitors, using sport sensors. It is also possible to predict winners in a match using big data analytics.
Future performance of players could be predicted as well. Thus, players' value and salary is determined by data collected throughout the season.In Formula One races, race cars with hundreds of sensors generate terabytes of data. These sensors collect data points from tire pressure to fuel burn efficiency.
Based on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.

Technology
eBay.com uses two data warehouses at 7.5 petabytes and 40PB as well as a 40PB Hadoop cluster for search, consumer recommendations, and merchandising.
Amazon.com handles millions of back-end operations every day, as well as queries from more than half a million third-party sellers. The core technology that keeps Amazon running is Linux-based and as of 2005 they had the world's three largest Linux databases, with capacities of 7.8 TB, 18.5 TB, and 24.7 TB.
Facebook handles 50 billion photos from its user base. As of June 2017, Facebook reached 2 billion monthly active users.
Google was handling roughly 100 billion searches per month as of August 2012.

COVID-19
During the COVID-19 pandemic, big data was raised as a way to minimise the impact of the disease. Significant applications of big data included minimising the spread of the virus, case identification and development of medical treatment.Governments used big data to track infected people to minimise spread. Early adopters included China, Taiwan, South Korea, and Israel.

Research activities
Encrypted search and cluster formation in big data were demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at Tackling the challenges of Big Data by MIT Computer Science and Artificial Intelligence Laboratory and Amir Esmailpour at the UNH Research Group investigated the key features of big data as the formation of clusters and their interconnections. They focused on the security of big data and the orientation of the term towards the presence of different types of data in an encrypted form at cloud interface by providing the raw definitions and real-time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.In March 2012, The White House announced a national ""Big Data Initiative"" that consisted of six federal departments and agencies committing more than $200 million to big data research projects.The initiative included a National Science Foundation ""Expeditions in Computing"" grant of $10 million over five years to the AMPLab at the University of California, Berkeley. The AMPLab also received funds from DARPA, and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion to fighting cancer.The White House Big Data Initiative also included a commitment by the Department of Energy to provide $25 million in funding over five years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute, led by the Energy Department's Lawrence Berkeley National Laboratory. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department's supercomputers.
The U.S. state of Massachusetts announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions. The Massachusetts Institute of Technology hosts the Intel Science and Technology Center for Big Data in the MIT Computer Science and Artificial Intelligence Laboratory, combining government, corporate, and institutional funding and research efforts.The European Commission is funding the two-year-long Big Data Public Private Forum through their Seventh Framework Program to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for Horizon 2020, their next framework program.The British government announced in March 2014 the founding of the Alan Turing Institute, named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyze large data sets.At the University of Waterloo Stratford Campus Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.Computational social sciences – Anyone can use application programming interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences. Often these APIs are provided for free. Tobias Preis et al. used Google Trends data to demonstrate that Internet users from countries with a higher per capita gross domestic products (GDPs) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviors and real-world economic indicators. The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year (2011) to the volume of searches for the previous year (2009), which they call the ""future orientation index"". They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP. 
Tobias Preis and his colleagues Helen Susannah Moat and H. Eugene Stanley introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends. Their analysis of Google search volume for 98 terms of varying financial relevance, published in Scientific Reports, suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.Big data sets come with algorithmic challenges that previously did not exist. Hence, there is seen by some to be a need to fundamentally change the processing ways.The Workshops on Algorithms for Modern Massive Data Sets (MMDS) bring together computer scientists, statisticians, mathematicians, and data analysis practitioners to discuss algorithmic challenges of big data. Regarding big data, such concepts of magnitude are relative.  As it is stated ""If the past is of any guidance, then today’s big data most likely will not be considered as such in the near future.""

Sampling big data
A research question that is asked about big data sets is whether it is necessary to look at the full data to draw certain conclusions about the properties of the data or if is a sample is good enough. The name big data itself contains a term related to size and this is an important characteristic of big data. But sampling enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage, and controller data are available at short time intervals. To predict downtime it may not be necessary to look at all the data but a sample may be sufficient.  Big data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data.  With large sets of data points, marketers are able to create and use more customized segments of consumers for more strategic targeting.
There has been some work done in sampling algorithms for big data. A theoretical formulation for sampling Twitter data has been developed.

Critique
Critiques of the big data paradigm come in two flavors: those that question the implications of the approach itself, and those that question the way it is currently done. One approach to this criticism is the field of critical data studies.

Critiques of the big data paradigm
""A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data."" In their critique, Snijders, Matzat, and Reips point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at Chris Anderson's assertion that big data will spell the end of theory: focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts. Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, big data, no matter how comprehensive or well analyzed, must be complemented by ""big judgment"", according to an article in the Harvard Business Review.Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably ""informed by the world as it was in the past, or, at best, as it currently is"". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the system's dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory. As a response to this critique Alemany Oliver and Vayre suggest to use ""abductive reasoning as a first step in the research process in order to bring context to consumers' digital traces and make new theories emerge"".
Additionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models and complex systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms. Finally, the use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.
In health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis.
A new postulate is accepted now in biosciences: the information provided by the data in huge volumes (omics) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation. In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor. The search logic is reversed and the limits of induction (""Glory of Science and Philosophy scandal"", C. D. Broad, 1926) are to be considered.Privacy advocates are concerned about the threat to privacy represented by increasing storage and integration of personally identifiable information; expert panels have released various policy recommendations to conform practice to expectations of privacy. The misuse of big data in several cases by media, companies, and even the government has allowed for abolition of trust in almost every fundamental institution holding up society.Nayef Al-Rodhan argues that a new kind of social contract will be needed to protect individual liberties in the context of big data and giant corporations that own vast amounts of information, and that the use of big data should be monitored and better regulated at the national and international levels. Barocas and Nissenbaum argue that one way of protecting individual users is by being informed about the types of information being collected, with whom it is shared, under what constraints and for what purposes.

Critiques of the ""V"" model
The ""V"" model of big data is concerning as it centers around computational scalability and lacks in a loss around the perceptibility and understandability of information. This led to the framework of cognitive big data, which characterizes big data applications according to:
Data completeness: understanding of the non-obvious from data
Data correlation, causation, and predictability: causality as not essential requirement to achieve predictability
Explainability and interpretability: humans desire to understand and accept what they understand, where algorithms do not cope with this
Level of automated decision making: algorithms that support automated decision making and algorithmic self-learning

Critiques of novelty
Large data sets have been analyzed by computing machines for well over a century, including the US census analytics performed by IBM's punch-card machines which computed statistics including means and variances of populations across the whole continent.   In more recent decades, science experiments such as CERN have produced data on similar scales to current commercial ""big data"".  However, science experiments have tended to analyze their data using specialized custom-built high-performance computing (super-computing) clusters and grids, rather than clouds of cheap commodity computers as in the current commercial wave, implying a difference in both culture and technology stack.

Critiques of big data execution
Ulf-Dietrich Reips and Uwe Matzat wrote in 2014 that big data had become a ""fad"" in scientific research. Researcher danah boyd has raised concerns about the use of big data in science neglecting principles such as choosing a representative sample by being too concerned about handling the huge amounts of data. This approach may lead to results that have bias in one way or another. Integration across heterogeneous data resources—some that might be considered big data and others not—presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.
In the provocative article ""Critical Questions for Big Data"", the authors title big data a part of mythology: ""large data sets offer a higher form of intelligence and knowledge [...], with the aura of truth, objectivity, and accuracy"". Users of big data are often ""lost in the sheer volume of numbers"", and ""working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth"". Recent developments in BI domain, such as pro-active reporting especially target improvements in usability of big data, through automated filtering of non-useful data and correlations. Big structures are full of spurious correlations either because of non-causal coincidences (law of truly large numbers), solely nature of big randomness (Ramsey theory), or existence of non-included factors so the hope, of early experimenters to make large databases of numbers ""speak for themselves"" and revolutionize scientific method, is questioned.Big data analysis is often shallow compared to analysis of smaller data sets. In many big data projects, there is no large data analysis happening, but the challenge is the extract, transform, load part of data pre-processing.Big data is a buzzword and a ""vague term"", but at the same time an ""obsession"" with entrepreneurs, consultants, scientists, and the media. Big data showcases such as Google Flu Trends failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, Academy awards and election predictions solely based on Twitter were more often off than on target.
Big data often poses the same challenges as small data; adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. Google Translate—which is based on big data statistical analysis of text—does a good job at translating web pages. However, results from specialized domains may be dramatically skewed.
On the other hand, big data may also introduce new problems, such as the multiple comparisons problem: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant.
Ioannidis argued that ""most published research findings are false"" due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i.e. process a big amount of scientific data; although not with big data technology), the likelihood of a ""significant"" result being false grows fast – even more so, when only positive results are published.
Furthermore, big data analytics results are only as good as the model on which they are predicated.  In an example, big data took part in attempting to predict the results of the 2016 U.S. Presidential Election with varying degrees of success.

Critiques of big data policing and surveillance
Big data has been used in policing and surveillance by institutions like law enforcement and corporations. Due to the less visible nature of data-based surveillance as compared to traditional method of policing, objections to big data policing are less likely to arise. According to Sarah Brayne's Big Data Surveillance: The Case of Policing, big data policing can reproduce existing societal inequalities in three ways:

Placing suspected criminals under increased surveillance by using the justification of a mathematical and therefore unbiased algorithm
Increasing the scope and number of people that are subject to law enforcement tracking and exacerbating existing racial overrepresentation in the criminal justice system
Encouraging members of society to abandon interactions with institutions that would create a digital trace, thus creating obstacles to social inclusionIf these potential problems are not corrected or regulated, the effects of big data policing may continue to shape societal hierarchies. Conscientious usage of big data policing could prevent individual level biases from becoming institutional biases, Brayne also notes.

In popular culture
Books
Moneyball is a non-fiction book that explores how the Oakland Athletics used statistical analysis to outperform teams with larger budgets. In 2011 a film adaptation starring Brad Pitt was released.
Nineteen Eighty-Four is a dystopian novel by George Orwell. In the novel, the government collects information on citizens and uses the information to maintain a totalitarian rule.

Film
In Captain America: The Winter Soldier, H.Y.D.R.A (disguised as S.H.I.E.L.D) develops helicarriers that use data to determine and eliminate threats over the globe.
In The Dark Knight, Batman uses a sonar device that can spy on all of Gotham City. The data is gathered from the mobile phones of people within the city.

See also
References
Further reading
Peter Kinnaird; Inbal Talgam-Cohen, eds. (2012). ""Big Data"". ACM Crossroads student magazine. XRDS: Crossroads, The ACM Magazine for Students. Vol. 19 no. 1. Association for Computing Machinery. ISSN 1528-4980. OCLC 779657714.
Jure Leskovec; Anand Rajaraman; Jeffrey D. Ullman (2014). Mining of massive datasets. Cambridge University Press. ISBN 9781107077232. OCLC 888463433.
Viktor Mayer-Schönberger; Kenneth Cukier (2013). Big Data: A Revolution that Will Transform how We Live, Work, and Think. Houghton Mifflin Harcourt. ISBN 9781299903029. OCLC 828620988.
Press, Gil (9 May 2013). ""A Very Short History of Big Data"". forbes.com. Jersey City, NJ: Forbes Magazine. Retrieved 17 September 2016.
""Big Data: The Management Revolution"". hbr.org. Harvard Business Review. October 2012.
O'Neil, Cathy (2017). Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Broadway Books. ISBN 978-0553418835.

External links
 Media related to Big data at Wikimedia Commons
 The dictionary definition of big data at Wiktionary",https://en.wikipedia.org/wiki/Big_data,"['All articles containing potentially dated statements', 'All articles covered by WikiProject Wikify', 'All articles lacking reliable references', 'All articles needing references cleanup', 'All articles with unsourced statements', 'Articles containing potentially dated statements from 2005', 'Articles containing potentially dated statements from 2011', 'Articles containing potentially dated statements from 2012', 'Articles containing potentially dated statements from 2017', 'Articles containing potentially dated statements from August 2012', 'Articles containing potentially dated statements from June 2017', 'Articles covered by WikiProject Wikify from November 2019', 'Articles lacking reliable references from December 2018', 'Articles lacking reliable references from November 2018', 'Articles with short description', 'Articles with unsourced statements from April 2015', 'Articles with unsourced statements from January 2021', 'Articles with unsourced statements from September 2011', 'Big data', 'CS1 errors: URL', 'CS1 errors: missing periodical', 'Commons link from Wikidata', 'Data analysis', 'Data management', 'Databases', 'Distributed computing problems', 'Short description is different from Wikidata', 'Technology forecasting', 'Transaction processing', 'Use dmy dates from January 2020', 'Webarchive template wayback links', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NDL identifiers', 'Wikipedia references cleanup from November 2019']",Data Science
14,Boosting (machine learning),"In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): ""Can a set of weak learners create a single strong learner?"" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.
Robert Schapire's affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.When first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. ""Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm […] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner]."" Algorithms that achieve hypothesis boosting quickly became simply known as ""boosting"". Freund and Schapire's arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.

Boosting algorithms
While boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are weighted in a way that is related to the weak learners' accuracy.  After a weak learner is added, the data weights are readjusted, known as ""re-weighting"".  Misclassified input data gain a higher weight and examples that are classified correctly lose weight. Thus, future weak learners focus more on the examples that previous weak learners misclassified.

There are many boosting algorithms.  The original ones, proposed by Robert Schapire (a recursive majority gate formulation) and Yoav Freund (boost by majority), were not adaptive and could not take full advantage of the weak learners.  Schapire and Freund then developed AdaBoost, an adaptive boosting algorithm that won the prestigious Gödel Prize.
Only algorithms that are provable boosting algorithms in the probably approximately correct learning formulation can accurately be called boosting algorithms.  Other algorithms that are similar in spirit to boosting algorithms are sometimes called ""leveraging algorithms"", although they are also sometimes incorrectly called boosting algorithms.The main variation between many boosting algorithms is their method of weighting training data points and hypotheses.  AdaBoost is very popular and the most significant historically as it was the first algorithm that could adapt to the weak learners.  It is often the basis of introductory coverage of boosting in university machine learning courses.  There are many more recent algorithms such as LPBoost, TotalBoost, BrownBoost, xgboost, MadaBoost, LogitBoost, and others.  Many boosting algorithms fit into the AnyBoost framework, which shows that boosting performs gradient descent in a function space using a convex cost function.

Object categorization in computer vision
Given images containing various known objects in the world, a classifier can be learned from them to automatically classify the objects in future images.  Simple classifiers built based on some image feature of the object tend to be weak in categorization performance. Using boosting methods for object categorization is a way to unify the weak classifiers in a special way to boost the overall ability of categorization.

Problem of object categorization
Object categorization is a typical task of computer vision that involves determining whether or not an image contains some specific category of object.  The idea is closely related with recognition, identification, and detection.  Appearance based object categorization typically contains feature extraction, learning a classifier, and applying the classifier to new examples.  There are many ways to represent a category of objects, e.g. from shape analysis, bag of words models, or local descriptors such as SIFT, etc.  Examples of supervised classifiers are Naive Bayes classifiers, support vector machines, mixtures of Gaussians, and neural networks.  However, research has shown that object categories and their locations in images can be discovered in an unsupervised manner as well.

Status quo for object categorization
The recognition of object categories in images is a challenging problem in computer vision, especially when the number of categories is large.  This is due to high intra class variability and the need for generalization across variations of objects within the same category. Objects within one category may look quite different. Even the same object may appear unalike under different viewpoint, scale, and illumination. Background clutter and partial occlusion add difficulties to recognition as well.  Humans are able to recognize thousands of object types, whereas most of the existing object recognition systems are trained to recognize only a few, e.g. human faces, cars, simple objects, etc.  Research has been very active on dealing with more categories and enabling incremental additions of new categories, and although the general problem remains unsolved, several multi-category objects detectors (for up to hundreds or thousands of categories) have been developed.  One means is by feature sharing and boosting.

Boosting for binary categorization
AdaBoost can be used for face detection as an example of binary categorization. The two categories are faces versus background. The general algorithm is as follows:

Form a large set of simple features
Initialize weights for training images
For T rounds
Normalize the weights
For available features from the set, train a classifier using a single feature and evaluate the training error
Choose the classifier with the lowest error
Update the weights of the training images: increase if classified wrongly by this classifier, decrease if correctly
Form the final strong classifier as the linear combination of the T classifiers (coefficient larger if training error is small)After boosting, a classifier constructed from 200 features could yield a 95% detection rate under a 
  
    
      
        
          10
          
            −
            5
          
        
      
    
    {\displaystyle 10^{-5}}
   false positive rate.Another application of boosting for binary categorization is a system that detects pedestrians using patterns of motion and appearance. This work is the first to combine both motion information and appearance information as features to detect a walking person. It takes a similar approach to the Viola-Jones object detection framework.

Boosting for multi-class categorization
Compared with binary categorization, multi-class categorization looks for common features that can be shared across the categories at the same time.  They turn to be more generic edge like features. During learning, the detectors for each category can be trained jointly. Compared with training separately, it generalizes better, needs less training data, and requires fewer features to achieve the same performance.
The main flow of the algorithm is similar to the binary case. What is different is that a measure of the joint training error shall be defined in advance. During each iteration the algorithm chooses a classifier of a single feature (features that can be shared by more categories shall be encouraged). This can be done via converting multi-class classification into a binary one (a set of categories versus the rest), or by introducing a penalty error from the categories that do not have the feature of the classifier.In the paper ""Sharing visual features for multiclass and multiview object detection"", A. Torralba et al. used GentleBoost for boosting and showed that when training data is limited, learning via sharing features does a much better job than no sharing, given same boosting rounds. Also, for a given performance level, the total number of features required (and therefore the run time cost of the classifier) for the feature sharing detectors, is observed to scale approximately logarithmically with the number of class, i.e., slower than linear growth in the non-sharing case. Similar results are shown in the paper ""Incremental learning of object detectors using a visual shape alphabet"", yet the authors used AdaBoost for boosting.

Convex vs non-convex boosting algorithms
Boosting algorithms can be based on convex or non-convex optimization algorithms.  Convex algorithms, such as AdaBoost and LogitBoost, can be ""defeated"" by random  noise such that they can't learn basic and learnable combinations of weak hypotheses. This limitation was pointed out by Long & Servedio in 2008.  However, by 2009, multiple authors demonstrated that  boosting algorithms based on non-convex optimization, such as BrownBoost, can learn from noisy datasets and can specifically learn the underlying classifier of the Long–Servedio dataset.

See also
Implementations
Scikit-learn, an open source machine learning library for python
Orange, a free data mining software suite, module Orange.ensemble
Weka is a machine learning set of tools that offers variate implementations of boosting algorithms like AdaBoost and LogitBoost
R package GBM (Generalized Boosted Regression Models) implements extensions to Freund and Schapire's AdaBoost algorithm and Friedman's gradient boosting machine.
jboost; AdaBoost, LogitBoost, RobustBoost, Boostexter and alternating decision trees
R package adabag: Applies Multiclass AdaBoost.M1, AdaBoost-SAMME and Bagging
R package xgboost: An implementation of gradient boosting for linear and tree-based models.

Notes
References
Further reading
Yoav Freund and Robert E. Schapire (1997); A Decision-Theoretic Generalization of On-line Learning and an Application to Boosting, Journal of Computer and System Sciences, 55(1):119-139
Robert E. Schapire and Yoram Singer (1999); Improved Boosting Algorithms Using Confidence-Rated Predictors, Machine Learning, 37(3):297-336

External links
Robert E. Schapire (2003); The Boosting Approach to Machine Learning: An Overview, MSRI (Mathematical Sciences Research Institute) Workshop on Nonlinear Estimation and Classification
Zhou Zhi-Hua (2014) Boosting 25 years, CCL 2014 Keynote.
Zhou, Zhihua (2008). ""On the margin explanation of boosting algorithm"" (PDF). In: Proceedings of the 21st Annual Conference on Learning Theory (COLT'08): 479–490.
Zhou, Zhihua (2013). ""On the doubt about margin explanation of boosting"" (PDF). Artificial Intelligence. 203: 1–18. arXiv:1009.3613. doi:10.1016/j.artint.2013.07.002. S2CID 2828847.",https://en.wikipedia.org/wiki/Boosting_(machine_learning),"['All Wikipedia articles in need of updating', 'All articles with specifically marked weasel-worded phrases', 'All articles with unsourced statements', 'Articles with short description', 'Articles with specifically marked weasel-worded phrases from October 2018', 'Articles with unsourced statements from October 2018', 'Classification algorithms', 'Ensemble learning', 'Learning in computer vision', 'Object recognition and categorization', 'Short description matches Wikidata', 'Wikipedia articles in need of updating from October 2018', 'Wikipedia articles needing clarification from October 2018', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with multiple identifiers']",Data Science
15,Bootstrap aggregating,"Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.

Description of the technique
Given a standard training set 
  
    
      
        D
      
    
    {\displaystyle D}
   of size n, bagging generates m new training sets 
  
    
      
        
          D
          
            i
          
        
      
    
    {\displaystyle D_{i}}
  , each of size n′, by sampling from D uniformly and with replacement. By sampling with replacement, some observations may be repeated in each 
  
    
      
        
          D
          
            i
          
        
      
    
    {\displaystyle D_{i}}
  . If n′=n, then for large n the set 
  
    
      
        
          D
          
            i
          
        
      
    
    {\displaystyle D_{i}}
   is expected to have the fraction (1 - 1/e) (≈63.2%) of the unique examples of D, the rest being duplicates. This kind of sample is known as a bootstrap sample. Sampling with replacement ensures each bootstrap is independent from its peers, as it does not depend on previous chosen samples when sampling.   Then, m models  are fitted using the above m bootstrap samples and combined by averaging the output (for regression) or voting (for classification).

Bagging leads to ""improvements for unstable procedures"", which include, for example, artificial neural networks, classification and regression trees, and subset selection in linear regression. Bagging was shown to improve preimage learning. On the other hand, it can mildly degrade the performance of stable methods such as K-nearest neighbors.

Process of the Algorithm
Original dataset
The original dataset contains several entries of samples from s1 to s5. Each sample has 5 features (Gene 1 to Gene 5). All samples are labeled as Yes or No for a classification problem.

Creation of Bootstrapped datasets
Given the table above to classify a new sample, first a bootstrapped dataset must be created using the data from the original dataset. This Bootstrapped dataset is typically the size of the original dataset, or smaller. 
In this example, the size is 5 (s1 through s5). The Bootstrapped Dataset is created by randomly selecting samples from the original dataset. Repeat selections are allowed. Any samples that are not chosen for the bootstrapped dataset are placed in a separate dataset called the Out-of-Bag dataset.
See an example bootstrapped dataset below. It has 5 entries (same size as the original dataset). There are duplicated entries such as two s3 since the entries are selected randomly with replacement. 

This step will repeat to generate m bootstrapped datasets.

Creating of Decision Trees
A Decision tree is created for each Bootstrapped dataset using randomly selected column values to split the nodes.

Predicting using Multiple Decision Trees
When a new sample is added to the table, the bootstrapped dataset is used to determine the new entry's clasifier value.

The new sample is tested in the random forest created by each bootstrapped dataset and each tree produces a classifier value for the new sample. For Classification, a process called voting is used to determine the final result, where the result produced the most frequently by the random forest is the given result for the sample. For Regression, the sample is assigned the average classifier value produced by the trees.

After the sample is tested in the random forest, a classifier value is assigned to the sample and it is added to the table.

Algorithm (Classification)
For Classification, use a training set 
  
    
      
        D
      
    
    {\displaystyle D}
  , Inducer 
  
    
      
        I
      
    
    {\displaystyle I}
   and the number of bootstrap samples 
  
    
      
        m
      
    
    {\displaystyle m}
   as input. Generate a classifier 
  
    
      
        
          C
          
            ∗
          
        
      
    
    {\displaystyle C^{*}}
   as output
Create 
  
    
      
        m
      
    
    {\displaystyle m}
    new training sets  
  
    
      
        
          D
          
            i
          
        
      
    
    {\displaystyle D_{i}}
  , from 
  
    
      
        D
      
    
    {\displaystyle D}
   with replacement
Classifier 
  
    
      
        
          C
          
            i
          
        
      
    
    {\displaystyle C_{i}}
   is built from each set 
  
    
      
        
          D
          
            i
          
        
      
    
    {\displaystyle D_{i}}
   using 
  
    
      
        I
      
    
    {\displaystyle I}
   to determine the classification of set 
  
    
      
        
          D
          
            i
          
        
      
    
    {\displaystyle D_{i}}
  
Finally classifier 
  
    
      
        
          C
          
            ∗
          
        
      
    
    {\displaystyle C^{*}}
   is generated by using the previously created set of classifiers 
  
    
      
        
          C
          
            i
          
        
      
    
    {\displaystyle C_{i}}
   on the original data set 
  
    
      
        D
      
    
    {\displaystyle D}
  , the classification predicted most often by the sub-classifiers 
  
    
      
        
          C
          
            i
          
        
      
    
    {\displaystyle C_{i}}
   is the final classificationfor i = 1 to m {
    D' = bootstrap sample from D    (sample with replacement)
    Ci = I(D')
}
C*(x) = argmax    Σ 1               (most often predicted label y)
         y∈Y   i:Ci(x)=y

Example: Ozone data
To illustrate the basic principles of bagging, below is an analysis on the relationship between ozone and temperature (data from Rousseeuw and Leroy (1986), analysis done in R).
The relationship between temperature and ozone appears to be nonlinear in this data set, based on the scatter plot. To mathematically describe this relationship, LOESS smoothers (with bandwidth 0.5) are used. Rather than building a single smoother for the complete data set, 100 bootstrap samples were drawn. Each sample is composed of a random subset of the original data and maintains a semblance of the master set’s distribution and variability. For each bootstrap sample, a LOESS smoother was fit. Predictions from these 100 smoothers were then made across the range of the data. The black lines represent these initial predictions. The lines lack agreement in their predictions and tend to overfit their data points: evident by the wobbly flow of the lines.

By taking the average of 100 smoothers, each corresponding to a subset of the original data set, we arrive at one bagged predictor (red line). The red line's flow is stable and does not overly conform to any data point(s).

Advantages vs Disadvantages
Advantages:

Many weak learners aggregated typically outperform a single learner over the entire set, and has less overfit
Removes variance in high-variance low-bias data sets
Can be performed in parallel, as each separate bootstrap can be processed on its own before combinationDisadvantages:

In a data set with high bias, bagging will also carry high bias into its aggregate
Loss of interpretability of a model.
Can be computationally expensive depending on the data set

History
The concept of Bootstrap Aggregating is derived from the concept of Bootstrapping which was developed by Bradley Efron.
Bootstrap Aggregating was proposed by Leo Breiman who also coined the abbreviated term ""Bagging"" (Bootstrap aggregating). Breiman developed the concept of bagging in 1994 to improve classification by combining classifications of randomly generated training sets. He argued, “If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.”

See also
Boosting (meta-algorithm)
Bootstrapping (statistics)
Cross-validation (statistics)
Random forest
Random subspace method (attribute bagging)
Resampled efficient frontier
Predictive analysis: Classification and regression trees

References
Further reading
Breiman, Leo (1996). ""Bagging predictors"". Machine Learning. 24 (2): 123–140. CiteSeerX 10.1.1.32.9399. doi:10.1007/BF00058655. S2CID 47328136.
Alfaro, E., Gámez, M. and García, N. (2012). ""adabag: An R package for classification with AdaBoost.M1, AdaBoost-SAMME and Bagging"". 
Kotsiantis, Sotiris (2014). ""Bagging and boosting variants for handling classifications problems: a survey"". Knowledge Eng. Review. 29 (1): 78–100. doi:10.1017/S0269888913000313.
Boehmke, Bradley; Greenwell, Brandon (2019). ""Bagging"". Hands-On Machine Learning with R. Chapman & Hall. pp. 191–202. ISBN 978-1-138-49568-5.",https://en.wikipedia.org/wiki/Bootstrap_aggregating,"['CS1: long volume value', 'CS1 errors: missing periodical', 'Computational statistics', 'Ensemble learning', 'Machine learning algorithms']",Data Science
16,Ben Fry,"Benjamin Fry (born 1975) is an  American expert in data visualization. He is a principal of Fathom, a design and software consultancy in Boston, Massachusetts. He is also a co-developer of Processing, an open-source programming language and integrated development environment (IDE) built for the electronic arts and visual design communities with the purpose of teaching the basics of computer programming in a visual context. The Processing design environment developed together with Casey Reas won a Golden Nica from the Prix Ars Electronica in 2005.Fry received his Ph.D. in ""Computational Information Design"" from the Aesthetics and Computation Group at the MIT Media Lab, under the direction of John Maeda. During 2006–2007, Fry was the Nierenberg Chair of Design for the Carnegie Mellon School of Design.
His other interests include visualization of genetic data. His personal work in this area was featured at the Cooper-Hewitt Museum National Design Triennial in 2003.Fry's artwork has been featured in the 2006 Cooper-Hewitt Design Triennial, the Whitney Biennial in 2002, Museum of Modern Art in New York (2001, 2008), at Ars Electronica in Linz, Austria (2000, 2002, 2005) and in the films Minority Report and The Hulk. He is the winner of the 2011 National Design Award in category ""Interaction Design""

Books
2007: (with Casey Reas) Processing: A Programming Handbook for Visual Designers and Artists, MIT Press
2007: Visualizing Data, O'Reilly
2010: (with Casey Reas) Getting Started with Processing, O'Reilly
2015: (with Casey Reas and  Lauren McCarthy) Getting Started with p5.js, O'Reilly

See also
Timeline of programming languages
Processing programming language

References
External links
Official website
Ben Fry on GitHub",https://en.wikipedia.org/wiki/Ben_Fry,"['1975 births', 'All articles with unsourced statements', 'All stub articles', 'American computer scientists', 'American computer specialist stubs', 'American designers', 'American digital artists', 'Articles with hCards', 'Articles with unsourced statements from May 2012', 'Carnegie Mellon University faculty', 'Information graphic designers', 'Living people', 'MIT Media Lab people', 'Massachusetts Institute of Technology alumni', 'People from Massachusetts', 'Webarchive template wayback links', 'Wikipedia articles with DAAO identifiers', 'Wikipedia articles with DBLP identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with NLK identifiers', 'Wikipedia articles with NTA identifiers', 'Wikipedia articles with SUDOC identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers']",Data Science
17,CURE data clustering algorithm,"CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases. Compared with K-means clustering it is more robust to outliers and able to identify clusters having non-spherical shapes and size variances.

Drawbacks of traditional algorithms
The popular K-means clustering algorithm minimizes the sum of squared errors criterion:

  
    
      
        E
        =
        
          ∑
          
            i
            =
            1
          
          
            k
          
        
        
          ∑
          
            p
            ∈
            
              C
              
                i
              
            
          
        
        (
        p
        −
        
          m
          
            i
          
        
        
          )
          
            2
          
        
        ,
      
    
    {\displaystyle E=\sum _{i=1}^{k}\sum _{p\in C_{i}}(p-m_{i})^{2},}
  Given large differences in sizes or geometries of different clusters, the square error method could split the large clusters to minimize the square error, which is not always correct. Also, with hierarchic clustering algorithms these problems exist as none of the distance measures between clusters (
  
    
      
        
          d
          
            m
            i
            n
          
        
        ,
        
          d
          
            m
            e
            a
            n
          
        
      
    
    {\displaystyle d_{min},d_{mean}}
  ) tend to work with different cluster shapes.  Also the running time is high when n is large.
The problem with the BIRCH algorithm is that once the clusters are generated after step 3, it uses centroids of the clusters and assigns each data point to the cluster with the closest centroid. Using only the centroid to redistribute the data has problems when clusters lack uniform sizes and shapes.

CURE clustering algorithm
To avoid the problems with non-uniform sized or shaped clusters, CURE employs a hierarchical clustering algorithm that adopts a middle ground between the centroid based and all point extremes. In CURE, a constant number c of well scattered points of a cluster are chosen and they are shrunk towards the centroid of the cluster by a fraction α. The scattered points after shrinking are used as representatives of the cluster. The clusters with the closest pair of representatives are the clusters that are merged at each step of CURE's hierarchical clustering algorithm. This enables CURE to correctly identify the clusters and makes it less sensitive to outliers.
Running time is O(n2 log n), making it rather expensive, and space complexity is O(n).
The algorithm cannot be directly applied to large databases because of the high runtime complexity. Enhancements address this requirement.

Random sampling :  random sampling supports large data sets. Generally the random sample fits in main memory. The random sampling involves a trade off between accuracy and efficiency.
Partitioning : The basic idea is to partition the sample space into p partitions. Each partition contains n/p elements. The first pass partially clusters each partition until the final number of clusters reduces to n/pq for some constant q ≥ 1. A second clustering pass on n/q partially clusters partitions. For the second pass only the representative points are stored since the merge procedure only requires representative points of previous clusters before computing the representative points for the merged cluster. Partitioning the input reduces the execution times.
Labeling data on disk : Given only representative points for k clusters, the remaining data points are also assigned to the clusters. For this a fraction of randomly selected representative points for each of the k clusters is chosen and data point is assigned to the cluster containing the representative point closest to it.

Pseudocode
CURE (no. of points,k)
Input : A set of points S
Output : k clusters

For every cluster u (each input point), in u.mean and u.rep store the mean of the points in the cluster and a set of c representative points of the cluster (initially c = 1 since each cluster has one data point). Also u.closest stores the cluster closest to u.
All the input points are inserted into a k-d tree T
Treat each input point as separate cluster, compute u.closest for each u and then insert each cluster into the heap Q. (clusters are arranged in increasing order of distances between u and u.closest).
While size (Q) > k
Remove the top element of Q (say u) and merge it with its closest cluster u.closest (say v) and compute the new representative points for the merged cluster w.
Remove u and v from T and Q.
For all the clusters x in Q, update x.closest and relocate x
insert w into Q
repeat

Availability
pyclustering open source library includes a Python and C++ implementation of CURE algorithm.

See also
k-means clustering
BFR algorithm

References
Guha, Sudipto; Rastogi, Rajeev; Shim, Kyuseok (1998). ""CURE: An Efficient Clustering Algorithm for Large Databases"" (PDF). Information Systems. 26 (1): 35–58. doi:10.1016/S0306-4379(01)00008-4.
Kogan, Jacob; Nicholas, Charles K.; Teboulle, M. (2006). Grouping multidimensional data: recent advances in clustering. Springer. ISBN 978-3-540-28348-5.
Theodoridis, Sergios; Koutroumbas, Konstantinos (2006). Pattern recognition. Academic Press. pp. 572–574. ISBN 978-0-12-369531-4.",https://en.wikipedia.org/wiki/CURE_algorithm,"['All articles with unsourced statements', 'Articles with example pseudocode', 'Articles with unsourced statements from May 2015', 'Articles with unsourced statements from May 2018', 'Cluster analysis algorithms']",Data Science
18,Canonical correlation,"In statistics, canonical-correlation analysis (CCA), also called canonical variates analysis, is a way of inferring information from cross-covariance matrices. If we have two vectors X = (X1, ..., Xn) and Y = (Y1, ..., Ym)  of random variables, and there are correlations among the variables, then canonical-correlation analysis will find linear combinations of X and Y which have maximum correlation with each other. T. R. Knapp notes that ""virtually all of the commonly encountered parametric tests of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables."" The method was first introduced by Harold Hotelling in 1936, although in the context of angles between flats the mathematical concept was published by Jordan in 1875.

Definition
Given two column vectors 
  
    
      
        X
        =
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        
          )
          ′
        
      
    
    {\displaystyle X=(x_{1},\dots ,x_{n})'}
   and 
  
    
      
        Y
        =
        (
        
          y
          
            1
          
        
        ,
        …
        ,
        
          y
          
            m
          
        
        
          )
          ′
        
      
    
    {\displaystyle Y=(y_{1},\dots ,y_{m})'}
   of random variables with finite second moments, one may define the cross-covariance 
  
    
      
        
          Σ
          
            X
            Y
          
        
        =
        cov
        ⁡
        (
        X
        ,
        Y
        )
      
    
    {\displaystyle \Sigma _{XY}=\operatorname {cov} (X,Y)}
   to be the 
  
    
      
        n
        ×
        m
      
    
    {\displaystyle n\times m}
   matrix whose 
  
    
      
        (
        i
        ,
        j
        )
      
    
    {\displaystyle (i,j)}
   entry is the covariance 
  
    
      
        cov
        ⁡
        (
        
          x
          
            i
          
        
        ,
        
          y
          
            j
          
        
        )
      
    
    {\displaystyle \operatorname {cov} (x_{i},y_{j})}
  . In practice, we would estimate the covariance matrix based on sampled data from 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   (i.e. from a pair of data matrices).
Canonical-correlation analysis seeks vectors 
  
    
      
        a
      
    
    {\displaystyle a}
    (
  
    
      
        a
        ∈
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle a\in \mathbb {R} ^{n}}
  ) and 
  
    
      
        b
      
    
    {\displaystyle b}
   (
  
    
      
        b
        ∈
        
          
            R
          
          
            m
          
        
      
    
    {\displaystyle b\in \mathbb {R} ^{m}}
  ) such that the random variables 
  
    
      
        
          a
          
            T
          
        
        X
      
    
    {\displaystyle a^{T}X}
   and 
  
    
      
        
          b
          
            T
          
        
        Y
      
    
    {\displaystyle b^{T}Y}
   maximize the correlation 
  
    
      
        ρ
        =
        corr
        ⁡
        (
        
          a
          
            T
          
        
        X
        ,
        
          b
          
            T
          
        
        Y
        )
      
    
    {\displaystyle \rho =\operatorname {corr} (a^{T}X,b^{T}Y)}
  . The random variables 
  
    
      
        U
        =
        
          a
          
            T
          
        
        X
      
    
    {\displaystyle U=a^{T}X}
   and 
  
    
      
        V
        =
        
          b
          
            T
          
        
        Y
      
    
    {\displaystyle V=b^{T}Y}
   are the first pair of canonical variables. Then one seeks vectors maximizing the same correlation subject to the constraint that they are to be uncorrelated with the first pair of canonical variables; this gives the second pair of canonical variables. This procedure may be continued up to 
  
    
      
        min
        {
        m
        ,
        n
        }
      
    
    {\displaystyle \min\{m,n\}}
   times.

Computation
Derivation
Let 
  
    
      
        
          Σ
          
            U
            V
          
        
      
    
    {\displaystyle \Sigma _{UV}}
   be the cross-covariance matrix for any random variables 
  
    
      
        U
      
    
    {\displaystyle U}
   and 
  
    
      
        V
      
    
    {\displaystyle V}
  . The parameter to maximize is

  
    
      
        ρ
        =
        
          
            
              
                a
                
                  T
                
              
              
                Σ
                
                  X
                  Y
                
              
              b
            
            
              
                
                  
                    a
                    
                      T
                    
                  
                  
                    Σ
                    
                      X
                      X
                    
                  
                  a
                
              
              
                
                  
                    b
                    
                      T
                    
                  
                  
                    Σ
                    
                      Y
                      Y
                    
                  
                  b
                
              
            
          
        
        .
      
    
    {\displaystyle \rho ={\frac {a^{T}\Sigma _{XY}b}{{\sqrt {a^{T}\Sigma _{XX}a}}{\sqrt {b^{T}\Sigma _{YY}b}}}}.}
  The first step is to define a change of basis and define

  
    
      
        c
        =
        
          Σ
          
            X
            X
          
          
            1
            
              /
            
            2
          
        
        a
        ,
      
    
    {\displaystyle c=\Sigma _{XX}^{1/2}a,}
  
  
    
      
        d
        =
        
          Σ
          
            Y
            Y
          
          
            1
            
              /
            
            2
          
        
        b
        .
      
    
    {\displaystyle d=\Sigma _{YY}^{1/2}b.}
  And thus we have

  
    
      
        ρ
        =
        
          
            
              
                c
                
                  T
                
              
              
                Σ
                
                  X
                  X
                
                
                  −
                  1
                  
                    /
                  
                  2
                
              
              
                Σ
                
                  X
                  Y
                
              
              
                Σ
                
                  Y
                  Y
                
                
                  −
                  1
                  
                    /
                  
                  2
                
              
              d
            
            
              
                
                  
                    c
                    
                      T
                    
                  
                  c
                
              
              
                
                  
                    d
                    
                      T
                    
                  
                  d
                
              
            
          
        
        .
      
    
    {\displaystyle \rho ={\frac {c^{T}\Sigma _{XX}^{-1/2}\Sigma _{XY}\Sigma _{YY}^{-1/2}d}{{\sqrt {c^{T}c}}{\sqrt {d^{T}d}}}}.}
  By the Cauchy–Schwarz inequality, we have

  
    
      
        
          (
          
            
              c
              
                T
              
            
            
              Σ
              
                X
                X
              
              
                −
                1
                
                  /
                
                2
              
            
            
              Σ
              
                X
                Y
              
            
            
              Σ
              
                Y
                Y
              
              
                −
                1
                
                  /
                
                2
              
            
          
          )
        
        (
        d
        )
        ≤
        
          
            (
            
              
                c
                
                  T
                
              
              
                Σ
                
                  X
                  X
                
                
                  −
                  1
                  
                    /
                  
                  2
                
              
              
                Σ
                
                  X
                  Y
                
              
              
                Σ
                
                  Y
                  Y
                
                
                  −
                  1
                  
                    /
                  
                  2
                
              
              
                Σ
                
                  Y
                  Y
                
                
                  −
                  1
                  
                    /
                  
                  2
                
              
              
                Σ
                
                  Y
                  X
                
              
              
                Σ
                
                  X
                  X
                
                
                  −
                  1
                  
                    /
                  
                  2
                
              
              c
            
            )
          
          
            1
            
              /
            
            2
          
        
        
          
            (
            
              
                d
                
                  T
                
              
              d
            
            )
          
          
            1
            
              /
            
            2
          
        
        ,
      
    
    {\displaystyle \left(c^{T}\Sigma _{XX}^{-1/2}\Sigma _{XY}\Sigma _{YY}^{-1/2}\right)(d)\leq \left(c^{T}\Sigma _{XX}^{-1/2}\Sigma _{XY}\Sigma _{YY}^{-1/2}\Sigma _{YY}^{-1/2}\Sigma _{YX}\Sigma _{XX}^{-1/2}c\right)^{1/2}\left(d^{T}d\right)^{1/2},}
  
  
    
      
        ρ
        ≤
        
          
            
              
                (
                
                  
                    c
                    
                      T
                    
                  
                  
                    Σ
                    
                      X
                      X
                    
                    
                      −
                      1
                      
                        /
                      
                      2
                    
                  
                  
                    Σ
                    
                      X
                      Y
                    
                  
                  
                    Σ
                    
                      Y
                      Y
                    
                    
                      −
                      1
                    
                  
                  
                    Σ
                    
                      Y
                      X
                    
                  
                  
                    Σ
                    
                      X
                      X
                    
                    
                      −
                      1
                      
                        /
                      
                      2
                    
                  
                  c
                
                )
              
              
                1
                
                  /
                
                2
              
            
            
              
                (
                
                  
                    c
                    
                      T
                    
                  
                  c
                
                )
              
              
                1
                
                  /
                
                2
              
            
          
        
        .
      
    
    {\displaystyle \rho \leq {\frac {\left(c^{T}\Sigma _{XX}^{-1/2}\Sigma _{XY}\Sigma _{YY}^{-1}\Sigma _{YX}\Sigma _{XX}^{-1/2}c\right)^{1/2}}{\left(c^{T}c\right)^{1/2}}}.}
  There is equality if the vectors 
  
    
      
        d
      
    
    {\displaystyle d}
   and 
  
    
      
        
          Σ
          
            Y
            Y
          
          
            −
            1
            
              /
            
            2
          
        
        
          Σ
          
            Y
            X
          
        
        
          Σ
          
            X
            X
          
          
            −
            1
            
              /
            
            2
          
        
        c
      
    
    {\displaystyle \Sigma _{YY}^{-1/2}\Sigma _{YX}\Sigma _{XX}^{-1/2}c}
   are collinear. In addition, the maximum of correlation is attained if 
  
    
      
        c
      
    
    {\displaystyle c}
   is the eigenvector with the maximum eigenvalue for the matrix 
  
    
      
        
          Σ
          
            X
            X
          
          
            −
            1
            
              /
            
            2
          
        
        
          Σ
          
            X
            Y
          
        
        
          Σ
          
            Y
            Y
          
          
            −
            1
          
        
        
          Σ
          
            Y
            X
          
        
        
          Σ
          
            X
            X
          
          
            −
            1
            
              /
            
            2
          
        
      
    
    {\displaystyle \Sigma _{XX}^{-1/2}\Sigma _{XY}\Sigma _{YY}^{-1}\Sigma _{YX}\Sigma _{XX}^{-1/2}}
   (see Rayleigh quotient). The subsequent pairs are found by using eigenvalues of decreasing magnitudes. Orthogonality is guaranteed by the symmetry of the correlation matrices.
Another way of viewing this computation is that 
  
    
      
        c
      
    
    {\displaystyle c}
   and 
  
    
      
        d
      
    
    {\displaystyle d}
   are the left and right singular vectors of the correlation matrix of X and Y corresponding to the highest singular value.

Solution
The solution is therefore:

  
    
      
        c
      
    
    {\displaystyle c}
   is an eigenvector of 
  
    
      
        
          Σ
          
            X
            X
          
          
            −
            1
            
              /
            
            2
          
        
        
          Σ
          
            X
            Y
          
        
        
          Σ
          
            Y
            Y
          
          
            −
            1
          
        
        
          Σ
          
            Y
            X
          
        
        
          Σ
          
            X
            X
          
          
            −
            1
            
              /
            
            2
          
        
      
    
    {\displaystyle \Sigma _{XX}^{-1/2}\Sigma _{XY}\Sigma _{YY}^{-1}\Sigma _{YX}\Sigma _{XX}^{-1/2}}
  

  
    
      
        d
      
    
    {\displaystyle d}
   is proportional to 
  
    
      
        
          Σ
          
            Y
            Y
          
          
            −
            1
            
              /
            
            2
          
        
        
          Σ
          
            Y
            X
          
        
        
          Σ
          
            X
            X
          
          
            −
            1
            
              /
            
            2
          
        
        c
      
    
    {\displaystyle \Sigma _{YY}^{-1/2}\Sigma _{YX}\Sigma _{XX}^{-1/2}c}
  Reciprocally, there is also:

  
    
      
        d
      
    
    {\displaystyle d}
   is an eigenvector of 
  
    
      
        
          Σ
          
            Y
            Y
          
          
            −
            1
            
              /
            
            2
          
        
        
          Σ
          
            Y
            X
          
        
        
          Σ
          
            X
            X
          
          
            −
            1
          
        
        
          Σ
          
            X
            Y
          
        
        
          Σ
          
            Y
            Y
          
          
            −
            1
            
              /
            
            2
          
        
      
    
    {\displaystyle \Sigma _{YY}^{-1/2}\Sigma _{YX}\Sigma _{XX}^{-1}\Sigma _{XY}\Sigma _{YY}^{-1/2}}
  

  
    
      
        c
      
    
    {\displaystyle c}
   is proportional to 
  
    
      
        
          Σ
          
            X
            X
          
          
            −
            1
            
              /
            
            2
          
        
        
          Σ
          
            X
            Y
          
        
        
          Σ
          
            Y
            Y
          
          
            −
            1
            
              /
            
            2
          
        
        d
      
    
    {\displaystyle \Sigma _{XX}^{-1/2}\Sigma _{XY}\Sigma _{YY}^{-1/2}d}
  Reversing the change of coordinates, we have that

  
    
      
        a
      
    
    {\displaystyle a}
   is an eigenvector of 
  
    
      
        
          Σ
          
            X
            X
          
          
            −
            1
          
        
        
          Σ
          
            X
            Y
          
        
        
          Σ
          
            Y
            Y
          
          
            −
            1
          
        
        
          Σ
          
            Y
            X
          
        
      
    
    {\displaystyle \Sigma _{XX}^{-1}\Sigma _{XY}\Sigma _{YY}^{-1}\Sigma _{YX}}
  ,

  
    
      
        b
      
    
    {\displaystyle b}
   is proportional to 
  
    
      
        
          Σ
          
            Y
            Y
          
          
            −
            1
          
        
        
          Σ
          
            Y
            X
          
        
        a
        ;
      
    
    {\displaystyle \Sigma _{YY}^{-1}\Sigma _{YX}a;}
  

  
    
      
        b
      
    
    {\displaystyle b}
   is an eigenvector of 
  
    
      
        
          Σ
          
            Y
            Y
          
          
            −
            1
          
        
        
          Σ
          
            Y
            X
          
        
        
          Σ
          
            X
            X
          
          
            −
            1
          
        
        
          Σ
          
            X
            Y
          
        
        ,
      
    
    {\displaystyle \Sigma _{YY}^{-1}\Sigma _{YX}\Sigma _{XX}^{-1}\Sigma _{XY},}
  

  
    
      
        a
      
    
    {\displaystyle a}
   is proportional to 
  
    
      
        
          Σ
          
            X
            X
          
          
            −
            1
          
        
        
          Σ
          
            X
            Y
          
        
        b
      
    
    {\displaystyle \Sigma _{XX}^{-1}\Sigma _{XY}b}
  .The canonical variables are defined by:

  
    
      
        U
        =
        
          c
          ′
        
        
          Σ
          
            X
            X
          
          
            −
            1
            
              /
            
            2
          
        
        X
        =
        
          a
          ′
        
        X
      
    
    {\displaystyle U=c'\Sigma _{XX}^{-1/2}X=a'X}
  
  
    
      
        V
        =
        
          d
          ′
        
        
          Σ
          
            Y
            Y
          
          
            −
            1
            
              /
            
            2
          
        
        Y
        =
        
          b
          ′
        
        Y
      
    
    {\displaystyle V=d'\Sigma _{YY}^{-1/2}Y=b'Y}

Implementation
CCA can be computed using singular value decomposition on a correlation matrix. It is available as a function in
MATLAB as canoncorr (also in Octave)
R as the standard function cancor and several other packages, including CCA and vegan. CCP for statistical hypothesis testing in canonical correlation analysis.
SAS as proc cancorr
Python in the library scikit-learn, as Cross decomposition and in statsmodels, as CanCorr.
SPSS as macro CanCorr shipped with the main software
Julia (programming language) in the MultivariateStats.jl package.CCA computation using singular value decomposition on a correlation matrix is related to the cosine of the angles between flats. The cosine function is ill-conditioned for small angles, leading to very inaccurate computation of highly correlated principal vectors in finite precision computer arithmetic. To  fix this trouble, alternative algorithms are available in

SciPy as linear-algebra function subspace_angles
MATLAB as FileExchange function subspacea

Hypothesis testing
Each row can be tested for significance with the following method. Since the correlations are sorted, saying that row 
  
    
      
        i
      
    
    {\displaystyle i}
   is zero implies all further correlations are also zero.  If we have 
  
    
      
        p
      
    
    {\displaystyle p}
   independent observations in a sample and 
  
    
      
        
          
            
              
                ρ
                ^
              
            
          
          
            i
          
        
      
    
    {\displaystyle {\widehat {\rho }}_{i}}
   is the estimated correlation for 
  
    
      
        i
        =
        1
        ,
        …
        ,
        min
        {
        m
        ,
        n
        }
      
    
    {\displaystyle i=1,\dots ,\min\{m,n\}}
  . For the 
  
    
      
        i
      
    
    {\displaystyle i}
  th row, the test statistic is:

  
    
      
        
          χ
          
            2
          
        
        =
        −
        
          (
          
            p
            −
            1
            −
            
              
                1
                2
              
            
            (
            m
            +
            n
            +
            1
            )
          
          )
        
        ln
        ⁡
        
          ∏
          
            j
            =
            i
          
          
            min
            {
            m
            ,
            n
            }
          
        
        (
        1
        −
        
          
            
              
                ρ
                ^
              
            
          
          
            j
          
          
            2
          
        
        )
        ,
      
    
    {\displaystyle \chi ^{2}=-\left(p-1-{\frac {1}{2}}(m+n+1)\right)\ln \prod _{j=i}^{\min\{m,n\}}(1-{\widehat {\rho }}_{j}^{2}),}
  which is asymptotically distributed as a chi-squared with 
  
    
      
        (
        m
        −
        i
        +
        1
        )
        (
        n
        −
        i
        +
        1
        )
      
    
    {\displaystyle (m-i+1)(n-i+1)}
   degrees of freedom for large 
  
    
      
        p
      
    
    {\displaystyle p}
  .  Since all the correlations from 
  
    
      
        min
        {
        m
        ,
        n
        }
      
    
    {\displaystyle \min\{m,n\}}
   to 
  
    
      
        p
      
    
    {\displaystyle p}
   are logically zero (and estimated that way also) the product for the terms after this point is irrelevant.
Note that in the small sample size limit with 
  
    
      
        p
        <
        n
        +
        m
      
    
    {\displaystyle p<n+m}
   then we are guaranteed that the top 
  
    
      
        m
        +
        n
        −
        p
      
    
    {\displaystyle m+n-p}
   correlations will be identically 1 and hence the test is meaningless.

Practical uses
A typical use for canonical correlation in the experimental context is to take two sets of variables and see what is common among the two sets. For example, in psychological testing, one could take two well established multidimensional personality tests such as the Minnesota Multiphasic Personality Inventory (MMPI-2) and the NEO. By seeing how the MMPI-2 factors relate to the NEO factors, one could gain insight into what dimensions were common between the tests and how much variance was shared. For example, one might find that an extraversion or neuroticism dimension accounted for a substantial amount of shared variance between the two tests.
One can also use canonical-correlation analysis to produce a model equation which relates two sets of variables, for example a set of performance measures and a set of explanatory variables, or a set of outputs and set of inputs. Constraint restrictions can be imposed on such a model to ensure it reflects theoretical requirements or intuitively obvious conditions. This type of model is known as a maximum correlation model.Visualization of the results of canonical correlation is usually through bar plots of the coefficients of the two sets of variables for the pairs of canonical variates showing significant correlation. Some authors suggest that they are best visualized by plotting them as heliographs, a circular format with ray like bars, with each half representing the two sets of variables.

Examples
Let 
  
    
      
        X
        =
        
          x
          
            1
          
        
      
    
    {\displaystyle X=x_{1}}
   with zero expected value, i.e., 
  
    
      
        E
        ⁡
        (
        X
        )
        =
        0
      
    
    {\displaystyle \operatorname {E} (X)=0}
  . If 
  
    
      
        Y
        =
        X
      
    
    {\displaystyle Y=X}
  , i.e., 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   are perfectly correlated, then, e.g., 
  
    
      
        a
        =
        1
      
    
    {\displaystyle a=1}
   and 
  
    
      
        b
        =
        1
      
    
    {\displaystyle b=1}
  , so that the first (and only in this example) pair of canonical variables is 
  
    
      
        U
        =
        X
      
    
    {\displaystyle U=X}
   and 
  
    
      
        V
        =
        Y
        =
        X
      
    
    {\displaystyle V=Y=X}
  . If 
  
    
      
        Y
        =
        −
        X
      
    
    {\displaystyle Y=-X}
  , i.e., 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   are perfectly anticorrelated, then, e.g., 
  
    
      
        a
        =
        1
      
    
    {\displaystyle a=1}
   and 
  
    
      
        b
        =
        −
        1
      
    
    {\displaystyle b=-1}
  , so that the first (and only in this example) pair of canonical variables is 
  
    
      
        U
        =
        X
      
    
    {\displaystyle U=X}
   and 
  
    
      
        V
        =
        −
        Y
        =
        X
      
    
    {\displaystyle V=-Y=X}
  . We notice that in both cases 
  
    
      
        U
        =
        V
      
    
    {\displaystyle U=V}
  , which illustrates that the canonical-correlation analysis treats correlated and anticorrelated variables similarly.

Connection to principal angles
Assuming that 
  
    
      
        X
        =
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        
          )
          ′
        
      
    
    {\displaystyle X=(x_{1},\dots ,x_{n})'}
   and 
  
    
      
        Y
        =
        (
        
          y
          
            1
          
        
        ,
        …
        ,
        
          y
          
            m
          
        
        
          )
          ′
        
      
    
    {\displaystyle Y=(y_{1},\dots ,y_{m})'}
   have zero expected values, i.e., 
  
    
      
        E
        ⁡
        (
        X
        )
        =
        E
        ⁡
        (
        Y
        )
        =
        0
      
    
    {\displaystyle \operatorname {E} (X)=\operatorname {E} (Y)=0}
  , their covariance  matrices 
  
    
      
        
          Σ
          
            X
            X
          
        
        =
        Cov
        ⁡
        (
        X
        ,
        X
        )
        =
        E
        ⁡
        [
        X
        
          X
          ′
        
        ]
      
    
    {\displaystyle \Sigma _{XX}=\operatorname {Cov} (X,X)=\operatorname {E} [XX']}
   and 
  
    
      
        
          Σ
          
            Y
            Y
          
        
        =
        Cov
        ⁡
        (
        Y
        ,
        Y
        )
        =
        E
        ⁡
        [
        Y
        
          Y
          ′
        
        ]
      
    
    {\displaystyle \Sigma _{YY}=\operatorname {Cov} (Y,Y)=\operatorname {E} [YY']}
   can be viewed as Gram matrices in an inner product for the entries of  
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
  , correspondingly. In this interpretation, the random variables, entries 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   of  
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        
          y
          
            j
          
        
      
    
    {\displaystyle y_{j}}
   of 
  
    
      
        Y
      
    
    {\displaystyle Y}
   are treated as elements of a vector space with an inner product given by the covariance 
  
    
      
        cov
        ⁡
        (
        
          x
          
            i
          
        
        ,
        
          y
          
            j
          
        
        )
      
    
    {\displaystyle \operatorname {cov} (x_{i},y_{j})}
  ; see Covariance#Relationship to inner products.
The definition of the canonical variables 
  
    
      
        U
      
    
    {\displaystyle U}
   and 
  
    
      
        V
      
    
    {\displaystyle V}
   is then equivalent to the definition of principal vectors for the pair of subspaces spanned by the entries of  
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   with respect to this  inner product. The canonical correlations 
  
    
      
        corr
        ⁡
        (
        U
        ,
        V
        )
      
    
    {\displaystyle \operatorname {corr} (U,V)}
   is equal to the cosine of principal angles.

Whitening and probabilistic canonical correlation analysis
CCA can also be viewed as a special whitening transformation where the random vectors 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   are simultaneously transformed in such a way that the cross-correlation between the whitened vectors 
  
    
      
        
          X
          
            C
            C
            A
          
        
      
    
    {\displaystyle X^{CCA}}
   and 
  
    
      
        
          Y
          
            C
            C
            A
          
        
      
    
    {\displaystyle Y^{CCA}}
   is diagonal.
The canonical correlations are then interpreted as regression coefficients linking 
  
    
      
        
          X
          
            C
            C
            A
          
        
      
    
    {\displaystyle X^{CCA}}
   and 
  
    
      
        
          Y
          
            C
            C
            A
          
        
      
    
    {\displaystyle Y^{CCA}}
   and may also be negative.  The regression view of CCA also provides a way to construct a latent variable probabilistic generative model for CCA, with uncorrelated hidden variables representing shared and non-shared variability.

See also
Generalized canonical correlation
Multilinear subspace learning
RV coefficient
Angles between flats
Principal component analysis
Linear discriminant analysis
Regularized canonical correlation analysis
Singular-value decomposition
Partial least squares regression

References
External links
Discriminant Correlation Analysis (DCA) (MATLAB)
Hardoon, D. R.; Szedmak, S.; Shawe-Taylor, J. (2004). ""Canonical Correlation Analysis: An Overview with Application to Learning Methods"". Neural Computation. 16 (12): 2639–2664. CiteSeerX 10.1.1.14.6452. doi:10.1162/0899766042321814. PMID 15516276.
A note on the ordinal canonical-correlation analysis of two sets of ranking scores (Also provides a FORTRAN program)- in Journal of Quantitative Economics 7(2), 2009, pp. 173–199
Representation-Constrained Canonical Correlation Analysis: A Hybridization of Canonical Correlation and Principal Component Analyses (Also provides a FORTRAN program)- in Journal of Applied Economic Sciences 4(1), 2009, pp. 115–124",https://en.wikipedia.org/wiki/Canonical_correlation,['Covariance and correlation'],Data Science
19,Cognitive computing,"Cognitive computing (CC) refers to technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing.  These platforms encompass machine learning, reasoning, natural language processing, speech recognition and vision (object recognition), human–computer interaction, dialog and narrative generation, among other technologies.

Definition
At present, there is no widely agreed upon definition for cognitive computing in either academia or industry.In general, the term cognitive computing has been used to refer to new hardware and/or software that mimics the functioning of the human brain (2004) and helps to improve human decision-making. In this sense, CC is a new type of computing with the goal of more accurate models of how the human brain/mind senses, reasons, and responds to stimulus. CC applications link data analysis and adaptive page displays (AUI) to adjust content for a particular type of audience. As such, CC hardware and applications strive to be more affective and more influential by design.
Some features that cognitive systems may express are:

Adaptive
They may learn as information changes, and as goals and requirements evolve. They may resolve ambiguity and tolerate unpredictability. They may be engineered to feed on dynamic data in real time, or near real time.
Interactive
They may interact easily with users so that those users can define their needs comfortably. They may also interact with other processors, devices, and cloud services, as well as with people.
Iterative and stateful
They may aid in defining a problem by asking questions or finding additional source input if a problem statement is ambiguous or incomplete. They may ""remember"" previous interactions in a process and return information that is suitable for the specific application at that point in time.
Contextual
They may understand, identify, and extract contextual elements such as meaning, syntax, time, location, appropriate domain, regulations, user’s profile, process, task and goal. They may draw on multiple sources of information, including both structured and unstructured digital information, as well as sensory inputs (visual, gestural, auditory, or sensor-provided).

Use cases
Speech recognition
Sentiment analysis
Face detection
Risk assessment
Fraud detection
Behavioral recommendations

Cognitive analytics
Cognitive computing-branded technology platforms typically specialize in the processing and analysis of large, unstructured datasets.Word processing documents, emails, videos, images, audio files, presentations, webpages, social media and many other data formats often need to be manually tagged with metadata before they can be fed to a computer for analysis and insight generation. The principal benefit of utilizing cognitive analytics over traditional big data analytics is that such datasets do not need to be pre-tagged.
Other characteristics of a cognitive analytics system include:

Adaptability: cognitive analytics systems can use machine learning to adapt to different contexts with minimal human supervision
Natural language interaction: cognitive analytics systems can be equipped with a chatbot or search assistant that understands queries, explains data insights and interacts with humans in natural language.

Applications
Education
Even if Cognitive Computing can not take the place of teachers, it can still be a heavy driving force in the education of students. Cognitive Computing being used in the classroom is applied by essentially having an assistant that is personalized for each individual student. This cognitive assistant can relieve the stress that teachers face while teaching students, while also enhancing the student’s learning experience over all. Teachers may not be able to pay each and every student individual attention, this being the place that cognitive computers fill the gap. Some students may need a little more help with a particular subject. For many students, Human interaction between student and teacher can cause anxiety and can be uncomfortable. With the help of Cognitive Computer tutors, students will not have to face their uneasiness and can gain the confidence to learn and do well in the classroom. While a student is in class with their personalized assistant, this assistant can develop various techniques, like creating lesson plans, to tailor and aid the student and their needs.
Healthcare
Numerous tech companies are in the process of developing technology that involves Cognitive Computing that can be used in the medical field. The ability to classify and identify is one of the main goals of these cognitive devices. This trait can be very helpful in the study of identifying carcinogens. This cognitive system that can detect would be able to assist the examiner in interpreting countless numbers of documents in a lesser amount of time than if they did not use Cognitive Computer technology. This technology can also evaluate information about the patient, looking through every medical record in depth, searching for indications that can be the source of their problems.

Industry work
Cognitive Computing in conjunction with big data and algorithms that comprehend customer needs, can be a major advantage in economic decision making.
The powers of Cognitive Computing and AI hold the potential to affect almost every task that humans are capable of performing. This can negatively affect employment for humans, as there would be no such need for human labor anymore. It would also increase the inequality of wealth; the people at the head of the Cognitive Computing industry would grow significantly richer, while workers without ongoing, reliable employment would become less well off.The more industries start to utilize Cognitive Computing, the more difficult it will be for humans to compete. Increased use of the technology will also increase the amount of work that AI-driven robots and machines can perform. Only extraordinarily talented, capable and motivated humans would be able to keep up with the machines. The influence of competitive individuals in conjunction with AI/CC with has the potential to change the course of humankind.

See also
Affective computing
Analytics
Artificial neural network
Brain computer interface
Cognitive computer
Cognitive reasoning
Enterprise cognitive system
Semantic Web
Social neuroscience
Synthetic intelligence
Usability
Neuromorphic engineering
AI accelerator

References
Further reading
Russell, John (February 15, 2016). ""Mapping Out a New Role for Cognitive Computing in Science"". HPCwire. Retrieved April 21, 2016.",https://en.wikipedia.org/wiki/Cognitive_computing,"['All Wikipedia articles written in American English', 'All articles needing additional references', 'Articles needing additional references from May 2019', 'Articles with short description', 'Artificial intelligence', 'CS1 errors: missing periodical', 'Cognitive science', 'Short description matches Wikidata', 'Use American English from January 2019', 'Use mdy dates from January 2019', 'Webarchive template wayback links', 'Wikipedia articles containing buzzwords from January 2019', 'Wikipedia articles needing page number citations from August 2020']",Data Science
20,Cluster analysis,"Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.
Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their understanding of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances between cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including parameters such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.
Besides the term clustering, there are a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek βότρυς ""grape""), typological analysis, and community detection. The subtle differences are often in the use of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest.
Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Joseph Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology.

Definition
The notion of a ""cluster"" cannot be precisely defined, which is one of the reasons why there are so many clustering algorithms. There is a common denominator: a group of data objects. However, different researchers employ different cluster models, and for each of these cluster models again different algorithms can be given. The notion of a cluster, as found by different algorithms, varies significantly in its properties. Understanding these ""cluster models"" is key to understanding the differences between the various algorithms. Typical cluster models include:

Connectivity models: for example, hierarchical clustering builds models based on distance connectivity.
Centroid models: for example, the k-means algorithm represents each cluster by a single mean vector.
Distribution models: clusters are modeled using statistical distributions, such as multivariate normal distributions used by the expectation-maximization algorithm.
Density models: for example, DBSCAN and OPTICS defines clusters as connected dense regions in the data space.
Subspace models: in biclustering (also known as co-clustering or two-mode-clustering), clusters are modeled with both cluster members and relevant attributes.
Group models: some algorithms do not provide a refined model for their results and just provide the grouping information.
Graph-based models: a clique, that is, a subset of nodes in a graph such that every two nodes in the subset are connected by an edge can be considered as a prototypical form of cluster. Relaxations of the complete connectivity requirement (a fraction of the edges can be missing) are known as quasi-cliques, as in the HCS clustering algorithm.
Signed graph models: Every path in a signed graph has a sign from the product of the signs on the edges. Under the assumptions of balance theory, edges may change sign and result in a bifurcated graph. The weaker ""clusterability axiom"" (no cycle has exactly one negative edge) yields results with more than two clusters, or subgraphs with only positive edges.
Neural models: the most well known unsupervised neural network is the self-organizing map and these models can usually be characterized as similar to one or more of the above models, and including subspace models when neural networks implement a form of Principal Component Analysis or Independent Component Analysis.A ""clustering"" is essentially a set of such clusters, usually containing all objects in the data set. Additionally, it may specify the relationship of the clusters to each other, for example, a hierarchy of clusters embedded in each other. Clusterings can be roughly distinguished as:

Hard clustering: each object belongs to a cluster or not
Soft clustering (also: fuzzy clustering): each object belongs to each cluster to a certain degree (for example, a likelihood of belonging to the cluster)There are also finer distinctions possible, for example:

Strict partitioning clustering: each object belongs to exactly one cluster
Strict partitioning clustering with outliers: objects can also belong to no cluster, and are considered outliers
Overlapping clustering (also: alternative clustering, multi-view clustering): objects may belong to more than one cluster; usually involving hard clusters
Hierarchical clustering: objects that belong to a child cluster also belong to the parent cluster
Subspace clustering: while an overlapping clustering, within a uniquely defined subspace, clusters are not expected to overlap

Algorithms
As listed above, clustering algorithms can be categorized based on their cluster model. The following overview will only list the most prominent examples of clustering algorithms, as there are possibly over 100 published clustering algorithms. Not all provide models for their clusters and can thus not easily be categorized. An overview of algorithms explained in Wikipedia can be found in the list of statistics algorithms.
There is no objectively ""correct"" clustering algorithm, but as it was noted, ""clustering is in the eye of the beholder."" The most appropriate clustering algorithm for a particular problem often needs to be chosen experimentally, unless there is a mathematical reason to prefer one cluster model over another. An algorithm that is designed for one kind of model will generally fail on a data set that contains a radically different kind of model. For example, k-means cannot find non-convex clusters.

Connectivity-based clustering (hierarchical clustering)
Connectivity-based clustering, also known as hierarchical clustering, is based on the core idea of objects being more related to nearby objects than to objects farther away. These algorithms connect ""objects"" to form ""clusters"" based on their distance. A cluster can be described largely by the maximum distance needed to connect parts of the cluster. At different distances, different clusters will form, which can be represented using a dendrogram, which explains where the common name ""hierarchical clustering"" comes from: these algorithms do not provide a single partitioning of the data set, but instead provide an extensive hierarchy of clusters that merge with each other at certain distances. In a dendrogram, the y-axis marks the distance at which the clusters merge, while the objects are placed along the x-axis such that the clusters don't mix.
Connectivity-based clustering is a whole family of methods that differ by the way distances are computed. Apart from the usual choice of distance functions, the user also needs to decide on the linkage criterion (since a cluster consists of multiple objects, there are multiple candidates to compute the distance) to use. Popular choices are known as single-linkage clustering (the minimum of object distances), complete linkage clustering (the maximum of object distances), and UPGMA or WPGMA (""Unweighted or Weighted Pair Group Method with Arithmetic Mean"", also known as average linkage clustering). Furthermore, hierarchical clustering can be agglomerative (starting with single elements and aggregating them into clusters) or divisive (starting with the complete data set and dividing it into partitions).
These methods will not produce a unique partitioning of the data set, but a hierarchy from which the user still needs to choose appropriate clusters. They are not very robust towards outliers, which will either show up as additional clusters or even cause other clusters to merge (known as ""chaining phenomenon"", in particular with single-linkage clustering). In the general case, the complexity is 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{3})}
   for agglomerative clustering and 
  
    
      
        
          
            O
          
        
        (
        
          2
          
            n
            −
            1
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(2^{n-1})}
   for divisive clustering, which makes them too slow for large data sets. For some special cases, optimal efficient methods (of complexity 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{2})}
  ) are known: SLINK for single-linkage and CLINK for complete-linkage clustering. In the data mining community these methods are recognized as a theoretical foundation of cluster analysis, but often considered obsolete. They did however provide inspiration for many later methods such as density based clustering.

Linkage clustering examples

Centroid-based clustering
In centroid-based clustering, clusters are represented by a central vector, which may not necessarily be a member of the data set. When the number of clusters is fixed to k, k-means clustering gives a formal definition as an optimization problem: find the k cluster centers and assign the objects to the nearest cluster center, such that the squared distances from the cluster are minimized.
The optimization problem itself is known to be NP-hard, and thus the common approach is to search only for approximate solutions. A particularly well known approximate method is Lloyd's algorithm, often just referred to as ""k-means algorithm"" (although another algorithm introduced this name). It does however only find a local optimum, and is commonly run multiple times with different random initializations. Variations of k-means often include such optimizations as choosing the best of multiple runs, but also restricting the centroids to members of the data set (k-medoids), choosing medians (k-medians clustering), choosing the initial centers less randomly (k-means++) or allowing a fuzzy cluster assignment (fuzzy c-means).
Most k-means-type algorithms require the number of clusters – k – to be specified in advance, which is considered to be one of the biggest drawbacks of these algorithms. Furthermore, the algorithms prefer clusters of approximately similar size, as they will always assign an object to the nearest centroid. This often leads to incorrectly cut borders of clusters (which is not surprising since the algorithm optimizes cluster centers, not cluster borders).
K-means has a number of interesting theoretical properties. First, it partitions the data space into a structure known as a Voronoi diagram. Second, it is conceptually close to nearest neighbor classification, and as such is popular in machine learning. Third, it can be seen as a variation of model based clustering, and Lloyd's algorithm as a variation of the Expectation-maximization algorithm for this model discussed below.

k-means clustering examples
		
		
Centroid-based clustering problems such as k-means and k-medoids are special cases of the uncapacitated, metric facility location problem, a canonical problem in the operations research and computational geometry communities. In a basic facility location problem (of which there are numerous variants that model more elaborate settings), the task is to find the best warehouse locations to optimally service a given set of consumers. One may view ""warehouses"" as cluster centroids and ""consumer locations"" as the data to be clustered. This makes it possible to apply the well-developed algorithmic solutions from the facility location literature to the presently considered centroid-based clustering problem.

Distribution-based clustering
The clustering model most closely related to statistics is based on distribution models. Clusters can then easily be defined as objects belonging most likely to the same distribution. A convenient property of this approach is that this closely resembles the way artificial data sets are generated: by sampling random objects from a distribution.
While the theoretical foundation of these methods is excellent, they suffer from one key problem known as overfitting, unless constraints are put on the model complexity. A more complex model will usually be able to explain the data better, which makes choosing the appropriate model complexity inherently difficult.
One prominent method is known as Gaussian mixture models (using the expectation-maximization algorithm). Here, the data set is usually modeled with a fixed (to avoid overfitting) number of Gaussian distributions that are initialized randomly and whose parameters are iteratively optimized to better fit the data set. This will converge to a local optimum, so multiple runs may produce different results. In order to obtain a hard clustering, objects are often then assigned to the Gaussian distribution they most likely belong to; for soft clusterings, this is not necessary.
Distribution-based clustering produces complex models for clusters that can capture correlation and dependence between attributes. However, these algorithms put an extra burden on the user: for many real data sets, there may be no concisely defined mathematical model (e.g. assuming Gaussian distributions is a rather strong assumption on the data).

Gaussian mixture model clustering examples

Density-based clustering
In density-based clustering, clusters are defined as areas of higher density than the remainder of the data set. Objects in sparse areas - that are required to separate clusters - are usually considered to be noise and border points.
The most popular density based clustering method is DBSCAN. In contrast to many newer methods, it features a well-defined cluster model called ""density-reachability"". Similar to linkage based clustering, it is based on connecting points within certain distance thresholds. However, it only connects points that satisfy a density criterion, in the original variant defined as a minimum number of other objects within this radius. A cluster consists of all density-connected objects (which can form a cluster of an arbitrary shape, in contrast to many other methods) plus all objects that are within these objects' range. Another interesting property of DBSCAN is that its complexity is fairly low – it requires a linear number of range queries on the database – and that it will discover essentially the same results (it is deterministic for core and noise points, but not for border points) in each run, therefore there is no need to run it multiple times. OPTICS is a generalization of DBSCAN that removes the need to choose an appropriate value for the range parameter 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  , and produces a hierarchical result related to that of linkage clustering. DeLi-Clu, Density-Link-Clustering combines ideas from single-linkage clustering and OPTICS, eliminating the 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   parameter entirely and offering performance improvements over OPTICS by using an R-tree index.
The key drawback of DBSCAN and OPTICS is that they expect some kind of density drop to detect cluster borders. On data sets with, for example, overlapping Gaussian distributions – a common use case in artificial data – the cluster borders produced by these algorithms will often look arbitrary, because the cluster density decreases continuously. On a data set consisting of mixtures of Gaussians, these algorithms are nearly always outperformed by methods such as EM clustering that are able to precisely model this kind of data.
Mean-shift is a clustering approach where each object is moved to the densest area in its vicinity, based on kernel density estimation. Eventually, objects converge to local maxima of density. Similar to k-means clustering, these ""density attractors"" can serve as representatives for the data set, but mean-shift can detect arbitrary-shaped clusters similar to DBSCAN. Due to the expensive iterative procedure and density estimation, mean-shift is usually slower than DBSCAN or k-Means. Besides that, the applicability of the mean-shift algorithm to multidimensional data is hindered by the unsmooth behaviour of the kernel density estimate, which results in over-fragmentation of cluster tails.
Density-based clustering examples

Grid-based clustering
The grid-based technique is used for a multi-dimensional data set. In this technique, we create a grid structure, and the comparison is performed on grids (also known as cells). The grid-based technique is fast and has low computational complexity. There are two types of grid-based clustering methods: STING and CLIQUE. Steps involved in grid-based clustering algorithm are:

Divide data space into a finite number of cells.
Randomly select a cell ‘c’, where c should not be traversed beforehand.
Calculate the density of ‘c’
If the density of ‘c’ greater than threshold density
Mark cell ‘c’ as a new cluster
Calculate the density of all the neighbors of ‘c’
If the density of a neighboring cell is greater than threshold density then, add the cell in the cluster and repeat steps 4.2 and 4.3 till there is no neighbor with a density greater than threshold density.
Repeat steps 2,3 and 4 till all the cells are traversed.
Stop.

Recent developments
In recent years, considerable effort has been put into improving the performance of existing algorithms. Among them are CLARANS, and BIRCH. With the recent need to process larger and larger data sets (also known as big data), the willingness to trade semantic meaning of the generated clusters for performance has been increasing. This led to the development of pre-clustering methods such as canopy clustering, which can process huge data sets efficiently, but the resulting ""clusters"" are merely a rough pre-partitioning of the data set to then analyze the partitions with existing slower methods such as k-means clustering.
For high-dimensional data, many of the existing methods fail due to the curse of dimensionality, which renders particular distance functions problematic in high-dimensional spaces. This led to new clustering algorithms for high-dimensional data that focus on subspace clustering (where only some attributes are used, and cluster models include the relevant attributes for the cluster) and correlation clustering that also looks for arbitrary rotated (""correlated"") subspace clusters that can be modeled by giving a correlation of their attributes. Examples for such clustering algorithms are CLIQUE and SUBCLU.Ideas from density-based clustering methods (in particular the DBSCAN/OPTICS family of algorithms) have been adapted to subspace clustering (HiSC, hierarchical subspace clustering and DiSH) and correlation clustering (HiCO, hierarchical correlation clustering, 4C using ""correlation connectivity"" and ERiC exploring hierarchical density-based correlation clusters).
Several different clustering systems based on mutual information have been proposed. One is Marina Meilă's variation of information metric; another provides hierarchical clustering. Using genetic algorithms, a wide range of different fit-functions can be optimized, including mutual information. Also belief propagation, a recent development in computer science and statistical physics, has led to the creation of new types of clustering algorithms.

Evaluation and assessment
Evaluation (or ""validation"") of clustering results is as difficult as the clustering itself. Popular approaches involve ""internal"" evaluation, where the clustering is summarized to a single quality score, ""external"" evaluation, where the clustering is compared to an existing ""ground truth"" classification, ""manual"" evaluation by a human expert, and ""indirect"" evaluation by evaluating the utility of the clustering in its intended application.Internal evaluation measures suffer from the problem that they represent functions that themselves can be seen as a clustering objective. For example, one could cluster the data set by the Silhouette coefficient; except that there is no known efficient algorithm for this. By using such an internal measure for evaluation, one rather compares the similarity of the optimization problems, and not necessarily how useful the clustering is.
External evaluation has similar problems: if we have such ""ground truth"" labels, then we would not need to cluster; and in practical applications we usually do not have such labels. On the other hand, the labels only reflect one possible partitioning of the data set, which does not imply that there does not exist a different, and maybe even better, clustering.
Neither of these approaches can therefore ultimately judge the actual quality of a clustering, but this needs human evaluation, which is highly subjective. Nevertheless, such statistics can be quite informative in identifying bad clusterings, but one should not dismiss subjective human evaluation.

Internal evaluation
When a clustering result is evaluated based on the data that was clustered itself, this is called internal evaluation. These methods usually assign the best score to the algorithm that produces clusters with high similarity within a cluster and low similarity between clusters. One drawback of using internal criteria in cluster evaluation is that high scores on an internal measure do not necessarily result in effective information retrieval applications. Additionally, this evaluation is biased towards algorithms that use the same cluster model. For example, k-means clustering naturally optimizes object distances, and a distance-based internal criterion will likely overrate the resulting clustering.
Therefore, the internal evaluation measures are best suited to get some insight into situations where one algorithm performs better than another, but this shall not imply that one algorithm produces more valid results than another. Validity as measured by such an index depends on the claim that this kind of structure exists in the data set. An algorithm designed for some kind of models has no chance if the data set contains a radically different set of models, or if the evaluation measures a radically different criterion. For example, k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means, nor of an evaluation criterion that assumes convexity, is sound.
More than a dozen of internal evaluation measures exist, usually based on the intuition that items in the same cluster should be more similar than items in different clusters. For example, the following methods can be used to assess the quality of clustering algorithms based on internal criterion:

Davies–Bouldin indexThe Davies–Bouldin index can be calculated by the following formula:

  
    
      
        D
        B
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          max
          
            j
            ≠
            i
          
        
        
          (
          
            
              
                
                  σ
                  
                    i
                  
                
                +
                
                  σ
                  
                    j
                  
                
              
              
                d
                (
                
                  c
                  
                    i
                  
                
                ,
                
                  c
                  
                    j
                  
                
                )
              
            
          
          )
        
      
    
    {\displaystyle DB={\frac {1}{n}}\sum _{i=1}^{n}\max _{j\neq i}\left({\frac {\sigma _{i}+\sigma _{j}}{d(c_{i},c_{j})}}\right)}
  
where n is the number of clusters, 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
   is the centroid of cluster 
  
    
      
        i
      
    
    {\displaystyle i}
  , 
  
    
      
        
          σ
          
            i
          
        
      
    
    {\displaystyle \sigma _{i}}
   is the average distance of all elements in cluster 
  
    
      
        i
      
    
    {\displaystyle i}
   to centroid 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
  , and 
  
    
      
        d
        (
        
          c
          
            i
          
        
        ,
        
          c
          
            j
          
        
        )
      
    
    {\displaystyle d(c_{i},c_{j})}
   is the distance between centroids 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
   and 
  
    
      
        
          c
          
            j
          
        
      
    
    {\displaystyle c_{j}}
  . Since algorithms that produce clusters with low intra-cluster distances (high intra-cluster similarity) and high inter-cluster distances (low inter-cluster similarity) will have a low Davies–Bouldin index, the clustering algorithm that produces a collection of clusters with the smallest Davies–Bouldin index is considered the best algorithm based on this criterion.Dunn indexThe Dunn index aims to identify dense and well-separated clusters. It is defined as the ratio between the minimal inter-cluster distance to maximal intra-cluster distance. For each cluster partition, the Dunn index can be calculated by the following formula:
  
    
      
        D
        =
        
          
            
              
                min
                
                  1
                  ≤
                  i
                  <
                  j
                  ≤
                  n
                
              
              d
              (
              i
              ,
              j
              )
            
            
              
                max
                
                  1
                  ≤
                  k
                  ≤
                  n
                
              
              
                d
                
                  ′
                
              
              (
              k
              )
            
          
        
        
        ,
      
    
    {\displaystyle D={\frac {\min _{1\leq i<j\leq n}d(i,j)}{\max _{1\leq k\leq n}d^{\prime }(k)}}\,,}
  
where d(i,j) represents the distance between clusters i and j, and d '(k) measures the intra-cluster distance of cluster k. The inter-cluster distance d(i,j) between two clusters may be any number of distance measures, such as the distance between the centroids of the clusters. Similarly, the intra-cluster distance d '(k) may be measured in a variety ways, such as the maximal distance between any pair of elements in cluster k. Since internal criterion seek clusters with high intra-cluster similarity and low inter-cluster similarity, algorithms that produce clusters with high Dunn index are more desirable.Silhouette coefficientThe silhouette coefficient contrasts the average distance to elements in the same cluster with the average distance to elements in other clusters. Objects with a high silhouette value are considered well clustered, objects with a low value may be outliers. This index works well with k-means clustering, and is also used to determine the optimal number of clusters.

External evaluation
In external evaluation, clustering results are evaluated based on data that was not used for clustering, such as known class labels and external benchmarks. Such benchmarks consist of a set of pre-classified items, and these sets are often created by (expert) humans. Thus, the benchmark sets can be thought of as a gold standard for evaluation. These types of evaluation methods measure how close the clustering is to the predetermined benchmark classes. However, it has recently been discussed whether this is adequate for real data, or only on synthetic data sets with a factual ground truth, since classes can contain internal structure, the attributes present may not allow separation of clusters or the classes may contain anomalies. Additionally, from a knowledge discovery point of view, the reproduction of known knowledge may not necessarily be the intended result. In the special scenario of constrained clustering, where meta information (such as class labels) is used already in the clustering process, the hold-out of information for evaluation purposes is non-trivial.A number of measures are adapted from variants used to evaluate classification tasks. In place of counting the number of times a class was correctly assigned to a single data point (known as true positives), such pair counting metrics assess whether each pair of data points that is truly in the same cluster is predicted to be in the same cluster.As with internal evaluation, several external evaluation measures exist, for example:

Purity: Purity is a measure of the extent to which clusters contain a single class. Its calculation can be thought of as follows: For each cluster, count the number of data points from the most common class in said cluster. Now take the sum over all clusters and divide by the total number of data points. Formally, given some set of clusters 
  
    
      
        M
      
    
    {\displaystyle M}
   and some set of classes 
  
    
      
        D
      
    
    {\displaystyle D}
  , both partitioning 
  
    
      
        N
      
    
    {\displaystyle N}
   data points, purity can be defined as:
  
    
      
        
          
            1
            N
          
        
        
          ∑
          
            m
            ∈
            M
          
        
        
          max
          
            d
            ∈
            D
          
        
        
          
            |
          
          m
          ∩
          d
          
            |
          
        
      
    
    {\displaystyle {\frac {1}{N}}\sum _{m\in M}\max _{d\in D}{|m\cap d|}}
  
This measure doesn't penalize having many clusters, and more clusters will make it easier to produce a high purity. A purity score of 1 is always possible by putting each data point in its own cluster. Also, purity doesn't work well for imbalanced data, where even poorly performing clustering algorithms will give a high purity value. For example, if a size 1000 dataset consists of two classes, one containing 999 points and the other containing 1 point, then every possible partition will have a purity of at least 99.9%.Rand indexThe Rand index computes how similar the clusters (returned by the clustering algorithm) are to the benchmark classifications. It can be computed using the following formula:

  
    
      
        R
        I
        =
        
          
            
              T
              P
              +
              T
              N
            
            
              T
              P
              +
              F
              P
              +
              F
              N
              +
              T
              N
            
          
        
      
    
    {\displaystyle RI={\frac {TP+TN}{TP+FP+FN+TN}}}
  
where  
  
    
      
        T
        P
      
    
    {\displaystyle TP}
   is the number of true positives, 
  
    
      
        T
        N
      
    
    {\displaystyle TN}
   is the number of true negatives, 
  
    
      
        F
        P
      
    
    {\displaystyle FP}
   is the number of false positives, and 
  
    
      
        F
        N
      
    
    {\displaystyle FN}
   is the number of false negatives. The instances being counted here are the number of correct pairwise assignments. That is, 
  
    
      
        T
        P
      
    
    {\displaystyle TP}
   is the number of pairs of points that are clustered together in the predicted partition and in the ground truth partition, 
  
    
      
        F
        P
      
    
    {\displaystyle FP}
   is the number of pairs of points that are clustered together in the predicted partition but not in the ground truth partition etc. If the dataset is of size N, then 
  
    
      
        T
        P
        +
        T
        N
        +
        F
        P
        +
        F
        N
        =
        
          
            
              (
            
            
              N
              2
            
            
              )
            
          
        
      
    
    {\displaystyle TP+TN+FP+FN={\binom {N}{2}}}
  .One issue with the Rand index is that false positives and false negatives are equally weighted. This may be an undesirable characteristic for some clustering applications. The F-measure addresses this concern, as does the chance-corrected adjusted Rand index.

F-measureThe F-measure can be used to balance the contribution of false negatives by weighting recall through a parameter 
  
    
      
        β
        ≥
        0
      
    
    {\displaystyle \beta \geq 0}
  . Let precision and recall (both external evaluation measures in themselves) be defined as follows:

  
    
      
        P
        =
        
          
            
              T
              P
            
            
              T
              P
              +
              F
              P
            
          
        
      
    
    {\displaystyle P={\frac {TP}{TP+FP}}}
  

  
    
      
        R
        =
        
          
            
              T
              P
            
            
              T
              P
              +
              F
              N
            
          
        
      
    
    {\displaystyle R={\frac {TP}{TP+FN}}}
  
where 
  
    
      
        P
      
    
    {\displaystyle P}
   is the precision rate and 
  
    
      
        R
      
    
    {\displaystyle R}
   is the recall rate. We can calculate the F-measure by using the following formula:
  
    
      
        
          F
          
            β
          
        
        =
        
          
            
              (
              
                β
                
                  2
                
              
              +
              1
              )
              ⋅
              P
              ⋅
              R
            
            
              
                β
                
                  2
                
              
              ⋅
              P
              +
              R
            
          
        
      
    
    {\displaystyle F_{\beta }={\frac {(\beta ^{2}+1)\cdot P\cdot R}{\beta ^{2}\cdot P+R}}}
  
When 
  
    
      
        β
        =
        0
      
    
    {\displaystyle \beta =0}
  , 
  
    
      
        
          F
          
            0
          
        
        =
        P
      
    
    {\displaystyle F_{0}=P}
  . In other words, recall has no impact on the F-measure when 
  
    
      
        β
        =
        0
      
    
    {\displaystyle \beta =0}
  , and increasing 
  
    
      
        β
      
    
    {\displaystyle \beta }
   allocates an increasing amount of weight to recall in the final F-measure.
Also 
  
    
      
        T
        N
      
    
    {\displaystyle TN}
   is not taken into account and can vary from 0 upward without bound.Jaccard indexThe Jaccard index is used to quantify the similarity between two datasets. The Jaccard index takes on a value between 0 and 1. An index of 1 means that the two dataset are identical, and an index of 0 indicates that the datasets have no common elements. The Jaccard index is defined by the following formula:

  
    
      
        J
        (
        A
        ,
        B
        )
        =
        
          
            
              
                |
              
              A
              ∩
              B
              
                |
              
            
            
              
                |
              
              A
              ∪
              B
              
                |
              
            
          
        
        =
        
          
            
              T
              P
            
            
              T
              P
              +
              F
              P
              +
              F
              N
            
          
        
      
    
    {\displaystyle J(A,B)={\frac {|A\cap B|}{|A\cup B|}}={\frac {TP}{TP+FP+FN}}}
  
This is simply the number of unique elements common to both sets divided by the total number of unique elements in both sets.
Also 
  
    
      
        T
        N
      
    
    {\displaystyle TN}
   is not taken into account and can vary from 0 upward without bound.Dice indexThe Dice symmetric measure doubles the weight on 
  
    
      
        T
        P
      
    
    {\displaystyle TP}
   while still ignoring 
  
    
      
        T
        N
      
    
    {\displaystyle TN}
  :

  
    
      
        D
        S
        C
        =
        
          
            
              2
              T
              P
            
            
              2
              T
              P
              +
              F
              P
              +
              F
              N
            
          
        
      
    
    {\displaystyle DSC={\frac {2TP}{2TP+FP+FN}}}
  Fowlkes–Mallows indexThe Fowlkes–Mallows index computes the similarity between the clusters returned by the clustering algorithm and the benchmark classifications. The higher the value of the Fowlkes–Mallows index the more similar the clusters and the benchmark classifications are. It can be computed using the following formula:

  
    
      
        F
        M
        =
        
          
            
              
                
                  T
                  P
                
                
                  T
                  P
                  +
                  F
                  P
                
              
            
            ⋅
            
              
                
                  T
                  P
                
                
                  T
                  P
                  +
                  F
                  N
                
              
            
          
        
      
    
    {\displaystyle FM={\sqrt {{\frac {TP}{TP+FP}}\cdot {\frac {TP}{TP+FN}}}}}
  
where  
  
    
      
        T
        P
      
    
    {\displaystyle TP}
   is the number of true positives, 
  
    
      
        F
        P
      
    
    {\displaystyle FP}
   is the number of false positives, and 
  
    
      
        F
        N
      
    
    {\displaystyle FN}
   is the number of false negatives. The 
  
    
      
        F
        M
      
    
    {\displaystyle FM}
   index is the geometric mean of the precision and recall 
  
    
      
        P
      
    
    {\displaystyle P}
   and 
  
    
      
        R
      
    
    {\displaystyle R}
  , and is thus also known as the G-measure, while the F-measure is their harmonic mean. Moreover, precision and recall are also known as Wallace's indices 
  
    
      
        
          B
          
            I
          
        
      
    
    {\displaystyle B^{I}}
   and 
  
    
      
        
          B
          
            I
            I
          
        
      
    
    {\displaystyle B^{II}}
  . Chance normalized versions of recall, precision and G-measure correspond to Informedness, Markedness and Matthews Correlation and relate strongly to Kappa.The mutual information is an information theoretic measure of how much information is shared between a clustering and a ground-truth classification that can detect a non-linear similarity between two clusterings. Normalized mutual information is a family of corrected-for-chance variants of this that has a reduced bias for varying cluster numbers.
Confusion matrixA confusion matrix can be used to quickly visualize the results of a classification (or clustering) algorithm. It shows how different a cluster is from the gold standard cluster.

Cluster tendency
To measure cluster tendency is to measure to what degree clusters exist in the data to be clustered, and may be performed as an initial test, before attempting clustering. One way to do this is to compare the data against random data. On average, random data should not have clusters.

Hopkins statisticThere are multiple formulations of the Hopkins statistic. A typical one is as follows. Let 
  
    
      
        X
      
    
    {\displaystyle X}
   be the set of 
  
    
      
        n
      
    
    {\displaystyle n}
   data points in 
  
    
      
        d
      
    
    {\displaystyle d}
   dimensional space. Consider a random sample (without replacement) of 
  
    
      
        m
        ≪
        n
      
    
    {\displaystyle m\ll n}
   data points with members 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  . Also generate a set 
  
    
      
        Y
      
    
    {\displaystyle Y}
   of 
  
    
      
        m
      
    
    {\displaystyle m}
   uniformly randomly distributed data points. Now define two distance measures, 
  
    
      
        
          u
          
            i
          
        
      
    
    {\displaystyle u_{i}}
   to be the distance of 
  
    
      
        
          y
          
            i
          
        
        ∈
        Y
      
    
    {\displaystyle y_{i}\in Y}
   from its nearest neighbor in X and 
  
    
      
        
          w
          
            i
          
        
      
    
    {\displaystyle w_{i}}
   to be the distance of 
  
    
      
        
          x
          
            i
          
        
        ∈
        X
      
    
    {\displaystyle x_{i}\in X}
   from its nearest neighbor in X. We then define the Hopkins statistic as:

  
    
      
        H
        =
        
          
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  m
                
              
              
                
                  u
                  
                    i
                  
                  
                    d
                  
                
              
            
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  m
                
              
              
                
                  u
                  
                    i
                  
                  
                    d
                  
                
              
              +
              
                ∑
                
                  i
                  =
                  1
                
                
                  m
                
              
              
                
                  w
                  
                    i
                  
                  
                    d
                  
                
              
            
          
        
        
        ,
      
    
    {\displaystyle H={\frac {\sum _{i=1}^{m}{u_{i}^{d}}}{\sum _{i=1}^{m}{u_{i}^{d}}+\sum _{i=1}^{m}{w_{i}^{d}}}}\,,}
  
With this definition, uniform random data should tend to have values near to 0.5, and clustered data should tend to have values nearer to 1.
However, data containing just a single Gaussian will also score close to 1, as this statistic measures deviation from a uniform distribution, not multimodality, making this statistic largely useless in application (as real data never is remotely uniform).

Applications
Biology, computational biology and bioinformatics
Plant and animal ecology
Cluster analysis is used to describe and to make spatial and temporal comparisons of communities (assemblages) of organisms in heterogeneous environments. It is also used in plant systematics to generate artificial phylogenies or clusters of organisms (individuals) at the species, genus or higher level that share a number of attributes.
Transcriptomics
Clustering is used to build groups of genes with related expression patterns (also known as coexpressed genes) as in HCS clustering algorithm. Often such groups contain functionally related proteins, such as enzymes for a specific pathway, or genes that are co-regulated. High throughput experiments using expressed sequence tags (ESTs) or DNA microarrays can be a powerful tool for genome annotation—a general aspect of genomics.
Sequence analysis
Sequence clustering is used to group homologous sequences into gene families. This is a very important concept in bioinformatics, and evolutionary biology in general. See evolution by gene duplication.
High-throughput genotyping platforms
Clustering algorithms are used to automatically assign genotypes.
Human genetic clustering
The similarity of genetic data is used in clustering to infer population structures.

Medicine
Medical imaging
On PET scans, cluster analysis can be used to differentiate between different types of tissue in a three-dimensional image for many different purposes.
Analysis of antimicrobial activity
Cluster analysis can be used to analyse patterns of antibiotic resistance, to classify antimicrobial compounds according to their mechanism of action, to classify antibiotics according to their antibacterial activity.
IMRT segmentation
Clustering can be used to divide a fluence map into distinct regions for conversion into deliverable fields in MLC-based Radiation Therapy.

Business and marketing
Market research
Cluster analysis is widely used in market research when working with multivariate data from surveys and test panels. Market researchers use cluster analysis to partition the general population of consumers into market segments and to better understand the relationships between different groups of consumers/potential customers, and for use in market segmentation, product positioning, new product development and selecting test markets.
Grouping of shopping items
Clustering can be used to group all the shopping items available on the web into a set of unique products. For example, all the items on eBay can be grouped into unique products (eBay does not have the concept of a SKU).

World wide web
Social network analysis
In the study of social networks, clustering may be used to recognize communities within large groups of people.
Search result grouping
In the process of intelligent grouping of the files and websites, clustering may be used to create a more relevant set of search results compared to normal search engines like Google. There are currently a number of web-based clustering tools such as Clusty. It also may be used to return a more comprehensive set of results in cases where a search term could refer to vastly different things. Each distinct use of the term corresponds to a unique cluster of results, allowing a ranking algorithm to return comprehensive results by picking the top result from each cluster.
Slippy map optimization
Flickr's map of photos and other map sites use clustering to reduce the number of markers on a map. This makes it both faster and reduces the amount of visual clutter.

Computer science
Software evolution
Clustering is useful in software evolution as it helps to reduce legacy properties in code by reforming functionality that has become dispersed. It is a form of restructuring and hence is a way of direct preventative maintenance.
Image segmentation
Clustering can be used to divide a digital image into distinct regions for border detection or object recognition.
Evolutionary algorithms
Clustering may be used to identify different niches within the population of an evolutionary algorithm so that reproductive opportunity can be distributed more evenly amongst the evolving species or subspecies.
Recommender systems
Recommender systems are designed to recommend new items based on a user's tastes.  They sometimes use clustering algorithms to predict a user's preferences based on the preferences of other users in the user's cluster.
Markov chain Monte Carlo methods
Clustering is often utilized to locate and characterize extrema in the target distribution.
Anomaly detection
Anomalies/outliers are typically – be it explicitly or implicitly – defined with respect to clustering structure in data.
Natural language processing
Clustering can be used to resolve lexical ambiguity.

Social science
Crime analysis
Cluster analysis can be used to identify areas where there are greater incidences of particular types of crime. By identifying these distinct areas or ""hot spots"" where a similar crime has happened over a period of time, it is possible to manage law enforcement resources more effectively.
Educational data mining
Cluster analysis is for example used to identify groups of schools or students with similar properties.
Typologies
From poll data, projects such as those undertaken by the Pew Research Center use cluster analysis to discern typologies of opinions, habits, and demographics that may be useful in politics and marketing.

Others
Field robotics
Clustering algorithms are used for robotic situational awareness to track objects and detect outliers in sensor data.Mathematical chemistry
To find structural similarity, etc., for example, 3000 chemical compounds were clustered in the space of 90 topological indices.Climatology
To find weather regimes or preferred sea level pressure atmospheric patterns.Finance
Cluster analysis has been used to cluster stocks into sectors.Petroleum geology
Cluster analysis is used to reconstruct missing bottom hole core data or missing log curves in order to evaluate reservoir properties.Geochemistry
The clustering of chemical properties in different sample locations.

See also
Specialized types of cluster analysis
Automatic clustering algorithms
Balanced clustering
Clustering high-dimensional data
Conceptual clustering
Consensus clustering
Constrained clustering
Community detection
Data stream clustering
HCS clustering
Sequence clustering
Spectral clustering

Techniques used in cluster analysis
Artificial neural network (ANN)
Nearest neighbor search
Neighbourhood components analysis
Latent class analysis
Affinity propagation

Data projection and preprocessing
Dimension reduction
Principal component analysis
Multidimensional scaling

Other
Cluster-weighted modeling
Curse of dimensionality
Determining the number of clusters in a data set
Parallel coordinates
Structured data analysis


== References ==",https://en.wikipedia.org/wiki/Cluster_analysis,"['All articles needing additional references', 'All articles with unsourced statements', 'Articles needing additional references from November 2016', 'Articles with long short description', 'Articles with short description', 'Articles with unsourced statements from July 2018', 'Articles with unsourced statements from March 2016', 'Articles with unsourced statements from March 2021', 'Articles with unsourced statements from May 2018', 'CS1: long volume value', 'CS1 errors: missing periodical', 'CS1 maint: DOI inactive as of January 2021', 'CS1 maint: multiple names: authors list', 'Cluster analysis', 'Commons category link is on Wikidata', 'Data mining', 'Geostatistics', 'Short description is different from Wikidata', 'Webarchive template wayback links']",Data Science
21,Comet NEOWISE,"C/2020 F3 (NEOWISE) or Comet NEOWISE is a long period comet with a near-parabolic orbit discovered on March 27, 2020, by astronomers during the NEOWISE mission of the Wide-field Infrared Survey Explorer (WISE) space telescope. At that time, it was an 18th-magnitude object, located 2 AU (300 million km; 190 million mi) away from the Sun and 1.7 AU (250 million km; 160 million mi) away from Earth.NEOWISE is known for being the brightest comet in the northern hemisphere since Comet Hale–Bopp in 1997. It was widely photographed by professional and amateur observers and was even spotted by people living near city centers and areas with light pollution. While it was too close to the Sun to be observed at perihelion, it emerged from perihelion around magnitude 0.5 to 1, making it bright enough to be visible to the naked eye. Under dark skies, it could be seen with the naked eye and remained visible to the naked eye throughout July 2020. By July 30, the comet was about magnitude 5, when binoculars were required near urban areas to locate the comet. 
For observers in the northern hemisphere, the comet could be seen on the northwestern horizon, below the Plough or Big Dipper. North of 45 degrees north, the comet was visible all night in mid-July 2020. On July 30, Comet NEOWISE entered the constellation of Coma Berenices, below the bright star Arcturus.

History and observations
The object was discovered by a team using the WISE space telescope under the NEOWISE program on March 27, 2020. It was classified as a comet on March 31 and named after NEOWISE on April 1. It has the systematic designation C/2020 F3, indicating a non-periodic comet which was the third discovered in the second half of March 2020.
Comet NEOWISE made its closest approach to the Sun (perihelion) on July 3, 2020, at a distance of 0.29 AU (43 million km; 27 million mi). This passage through the planetary region increases the comet's orbital period from about 4400 years to about 6700 years. Its closest approach to Earth occurred on July 23, 2020, 01:09 UT, at a distance of 0.69 AU (103 million km; 64 million mi) while located in the constellation of Ursa Major.In early July, the comet could be seen in the morning sky just above the north-eastern horizon and below Capella. Seen from Earth, the comet was less than 20 degrees from the Sun between June 11 and July 9, 2020. By June 10, 2020, as the comet was being lost to the glare of the Sun, it was apparent magnitude 7, when it was 0.7 AU (100 million km; 65 million mi) away from Sun and 1.6 AU (240 million km; 150 million mi) away from Earth. When the comet entered the field of view of the SOHO spacecraft's LASCO C3 instrument on June 22, 2020, the comet had brightened to about magnitude 3, when it was 0.4 AU (60 million km; 37 million mi) away from the Sun and 1.4 AU (210 million km; 130 million mi) away from Earth.By early July, Comet NEOWISE had brightened to magnitude 1, far exceeding the brightness attained by previous comets, C/2020 F8 (SWAN), and C/2019 Y4 (ATLAS). By July, it also had developed a second tail. The first tail is blue and made of gas and ions stretching almost 70° from its nucleus. There is also a red separation in the tail caused by high amounts of sodium which is nearly stretched 1°. The second twin tail is a golden color and is made of dust stretched almost 50°, like the tail of Comet Hale–Bopp. This combination resembles comet C/2011 L4 (PANSTARRS). The comet is brighter than C/2011 L4 (PANSTARRS), but not as bright as Hale–Bopp was in 1997. According to the British Astronomical Association, the comet brightened from a magnitude of about 8 at the beginning of June to −2 in early July. This would make it brighter than Hale–Bopp. However, as it was very near to the Sun, it was reported as 0 or +1 magnitude and remained that bright for only a few days. After perihelion, the comet began to fade, dropping to magnitude 2. Its nucleus activity subdued after mid-July, and its green coma was clearly visible after that.
On July 13, 2020, a sodium tail was confirmed by the Planetary Science Institute's Input/Output facility. Sodium tails have only been observed in very bright comets such as Hale–Bopp and C/2012 S1 (ISON).
From the infrared signature, the diameter of the comet nucleus is estimated to be approximately 5 km (3 mi). The nucleus is similar in size to Comet Hyakutake and many short-period comets such as 2P/Encke, 7P/Pons-Winnecke, 8P/Tuttle, 14P/Wolf, and 19P/Borrelly. By July 5, NASA's Parker Solar Probe had captured an image of the comet, from which astronomers also estimated the diameter of the comet nucleus at approximately 5 km (3 mi). Later in July 2020, other observations were also reported, including those related to coma morphology and spectrographic emissions. On 31 July 2020, strong detection of OH 18-cm emission was observed in radio spectroscopic studies at the Arecibo Observatory. On August 14, 2020, the rotation period of the comet was reported to be ""7.58 +/- 0.03 hr"".A number of authors have suggested considering the comet a great comet. Others have argued that it lacked the brightness and visible tail to qualify.

Trajectory
Comet NEOWISE retrograde orbit crossed to the north of the plane of the ecliptic, to which it is inclined at approximately 129 degrees, on June 29, 2020, 01:47 UT.  It made its closest approach to the Sun (perihelion) on July 3, 2020, at a distance of 0.29 AU (43 million km; 27 million mi). This passage increases the comet's orbital period from about 4400 years to about 6700 years. On July 18 the comet peaked at a northern declination of +48 and was circumpolar down to latitude 42N. Its closest approach to Earth occurred on July 23, 2020, 01:09 UT, at a distance of 0.69 AU (103 million km; 64 million mi) while located in the constellation of Ursa Major.

Gallery
In chronological order:

References
External links
C/2020 F3 (NEOWISE) – Comet Watch
C/2020 F3 (NEOWISE) – AiM-Project-Group
C/2020 F3 (NEOWISE) – Ernesto Guido & Adriano Valvasori
Comet C/2020 F3 (NEOWISE) Information & Planetarium – TheSkyLive
C/2020 F3 (NEOWISE) on YouTube – ISS view (video; 7:00; July 7, 2020)
C/2020 F3 (NEOWISE) on YouTube – Tom Polakis; 300w rms/20mins (video; 0:10; July 7, 2020)
C/2020 F3 (NEOWISE) – Seiichi Yoshida
C/2020 F3 (NEOWISE) photo gallery - Mathew Browne
Comet NEOWISE at the JPL Small-Body Database
Close approach · Discovery · Ephemeris · Orbit diagram · Orbital elements · Physical parameters",https://en.wikipedia.org/wiki/Comet_NEOWISE,"['All Wikipedia articles written in American English', 'All articles to be expanded', 'Articles containing video clips', 'Articles to be expanded from July 2020', 'Articles using small message boxes', 'Articles with short description', 'Astronomical objects discovered in 2020', 'CS1: long volume value', 'Cometary object articles', 'Comets in 2020', 'Commons category link is on Wikidata', 'Discoveries by NEOWISE', 'JPL Small-Body Database ID same as Wikidata', 'Short description is different from Wikidata', 'Use American English from July 2020', 'Use mdy dates from July 2020', 'Webarchive template wayback links']",Data Science
22,C.F. Jeff Wu,"Chien-Fu Jeff Wu (born 1949) is the Coca-Cola Chair in Engineering Statistics and Professor in the H. Milton Stewart School of Industrial and Systems Engineering at the Georgia Institute of Technology.  He is known for his work on the convergence of the EM algorithm, resampling methods such as the bootstrap and jackknife, and industrial statistics, including design of experiments, and robust parameter design (Taguchi methods).
Born in Taiwan, Wu earned a B.Sc. in Mathematics from National Taiwan University in 1971, and a Ph.D. in Statistics from University of California, Berkeley in 1976.  He has been a faculty member at the University of Wisconsin, Madison (1977-1988), the University of Waterloo (1988-1993; GM-NSERC chair in quality and productivity), the University of Michigan (1995-2003; chair of Department of Statistics 1995-98; H.C. Carver professor of statistics, 1997-2003) and currently the Georgia Institute of Technology.  He has supervised 50 Ph.D. students and published around 185 peer-reviewed articles and two books. He has received several awards, including the COPSS Presidents' Award in 1987,
the Shewhart Medal in 2008,
the COPSS R. A. Fisher Lectureship in 2011,
and the Deming Lecturer Award in 2012.  He gave the inaugural Akaike Memorial Lecture in 2016. He has been elected as a fellow of the American Statistical Association, the Institute of Mathematical Statistics, the American Society for Quality and the Institute for Operations Research and the Management Sciences.  In 2000 he was elected as a member of Academia Sinica. In 2004, he was elected as a member of the National Academy of Engineering.  He received the Shewhart Medal of the American Society for Quality and an honorary degree from the University of Waterloo in 2008. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, he used the term Data Science for the first time as an alternative name for statistics.  Later, in November 1997, he gave the inaugural lecture entitled ""Statistics = Data Science?"" for his appointment to the H. C. Carver Professorship at the University of Michigan.
He popularized the term ""data science"" and advocated that statistics be renamed data science and statisticians data scientists.
He also presented his lecture entitled ""Statistics = Data Science?"" as the first of his 1998 P.C. Mahalanobis Memorial Lectures. These lectures honor Prasanta Chandra Mahalanobis, an Indian scientist and statistician and founder of the Indian Statistical Institute.
In Mile, Yunnan, China, a conference was held in July 2014 celebrating Professor Wu's 65th birthday. In 2014 he gave the Bradley Lecture at the University of Georgia. In 2016 he was the inaugural recipient of the Akaike Memorial Lecture Award.  In 2017 Jeff Wu received the George Box Medal from ENBIS. In 2020, Jeff Wu received Georgia Institute of Technology’s highest award given to a faculty member: the Class of 1934 Distinguished Professor Award.  In the same year, he also got the Sigma Xi’s Monie A. Ferst Award which, since 1977, has honored science and engineering teachers who have inspired their students to significant research achievements.  In 2020 he delivered the CANSSI/Fields Distinguished Lectures Series in Statistical Sciences.

References
External links
Interview with Jeff Wu, ICSA Bulletin (pages 36–45)
Convergence of the EM algorithm
Personal page at Georgia Tech
Recognition of Jeff Wu by Academia Sinica",https://en.wikipedia.org/wiki/C._F._Jeff_Wu,"['1949 births', 'American people of Taiwanese descent', 'American statisticians', 'Articles with hCards', 'CS1 maint: date and year', 'Fellows of the American Statistical Association', 'Fellows of the Institute for Operations Research and the Management Sciences', 'Georgia Tech faculty', 'Living people', 'Members of Academia Sinica', 'Members of the United States National Academy of Engineering', 'University of Michigan faculty', 'University of Waterloo faculty', 'University of Wisconsin–Madison faculty', 'Webarchive template wayback links', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with DBLP identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MGP identifiers', 'Wikipedia articles with NKC identifiers', 'Wikipedia articles with NTA identifiers', 'Wikipedia articles with SUDOC identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers']",Data Science
23,Committee on Data for Science and Technology,"The CODATA is the Committee on Data of the International Science Council and was established as ICSU Committee on Data for Science and Technology in 1966.CODATA exists to promote global collaboration to advance open science and to improve the availability and usability of data for all areas of research. CODATA supports the principle that data produced by research and susceptible to be used for research should be as open as possible and as closed as necessary. CODATA works also to advance the interoperability and the usability of such data: research data should be FAIR (Findable, Accessible, Interoperable and Reusable). By promoting the policy, technological and cultural changes that are essential to promote open science, CODATA helps advance ISC's vision and mission of advancing science as a global public good.
The CODATA Strategic Plan 2015 and Prospectus of Strategy and Achievement 2016 identify three priority areas:

promoting principles, policies and practices for open data and open science;
advancing the frontiers of data science;
building capacity for open science by improving data skills and the functions of national science systems needed to support open data.CODATA achieves these objectives through a number of standing committees and strategic executive led initiatives, and through its task groups and working groups.

Publications and conferences
CODATA supports the Data Science Journal and collaborates on major data conferences like SciDataCon and International Data Week.In October 2020 CODATA is co-organising an International FAIR Symposium together with the GO FAIR initiative to provide a forum for advancing international and cross-domain convergence around FAIR. The event will bring together a global data community with an interest in combining data across domains for a host of research issues – including major global challenges, such as those relating to the Sustainable Development Goals. Outcomes will directly link to the CODATA Decadal Programme Data for the Planet: making data work for cross-domain grand challenges and to the developments of GO FAIR community towards the Internet of FAIR data and services.

Task Group on Fundamental Physical Constants
One of the CODATA strategic Initiatives and Task Groups concentrates on Fundamental Physical Constants. Established in 1969, its purpose is to periodically provide the international scientific and technological communities with an internationally accepted set of values of the fundamental physical constants and closely related conversion factors for use worldwide.
The first such CODATA set was published in 1973.  Later versions are named based on the year of the data incorporated; the 1986 CODATA (published April 1987) used data up to 1 January 1986.  All subsequent releases use data up to the end of the stated year, and are necessarily published a year or two later: 1998 (April 2000), 2002 (January 2005), 2006 (June 2008) and the sixth in 2010 (November 2012). The latest version is Version 7.0 called ""2014 CODATA"" published on 25 June 2015.The CODATA recommended values of fundamental physical constants are published at the NIST Reference on Constants, Units, and Uncertainty.

Schedule
Since 1998, the task group has produced a new version every four years, incorporating results published up to the end of the specified year.
In order to support the redefinition of the SI base units, adopted at the 26th General Conference on Weights and Measures on 16 November 2018, CODATA made a special release that was published in October 2017.
It incorporates all data up to 1 July 2017, and determines the final numerical values of h, e, k, and NA that are to be used for the new SI definitions.
The last regular version, with a closing date of 31 December 2018, was used to produce the new 2018 CODATA values that were made available by the time the revised SI came into force on 20 May 2019. This was necessary because the redefinitions have a significant (mostly beneficial) effect on the uncertainties and correlation coefficients reported by CODATA.

See also
Commission on Isotopic Abundances and Atomic Weights (CIAAW)
Particle Data Group

References
Further reading
Cox, J. D.; Wagman, Donald D.; Medvedev, Vadim Andreevich (1989). CODATA Key values for thermodynamics. Hemisphere Publishing Corporation. ISBN 0-89116-758-7.
International Bureau of Weights and Measures (2019-05-20), SI Brochure: The International System of Units (SI) (PDF) (9th ed.), ISBN 978-92-822-2272-0
International Bureau of Weights and Measures (BIPM) (2017-08-10). ""Input data for the special CODATA-2017 adjustment"". Metrologia (Updated ed.). Retrieved 2017-08-14.

External links
Official website
International Science Council.
The NIST references on constants, units, and uncertainty",https://en.wikipedia.org/wiki/Committee_on_Data_for_Science_and_Technology,"['All articles lacking reliable references', 'All articles to be expanded', 'Articles lacking reliable references from December 2016', 'Articles needing translation from German Wikipedia', 'Articles to be expanded from September 2016', 'Articles with multiple maintenance issues', 'International standards', 'Official website different in Wikidata and Wikipedia', 'Standards organizations in France', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers']",Data Science
24,Communication,"Communication (from Latin communicare, meaning ""to share"") is the act of developing meaning among entities or groups through the use of sufficiently mutually understood signs, symbols, and semiotic conventions.
The main elements inherent to communication have been described as:

The formation of communicative motivation or reason.
Message composition (further internal or technical elaboration on what exactly to express).
Message encoding (for example, into digital data, written text, speech, pictures, gestures and so on).
Transmission of the encoded message as a sequence of signals using a specific channel or medium.
Noise sources such as natural forces and in some cases human activity (both intentional and accidental) begin influencing the quality of signals propagating from the sender to one or more receivers.
Reception of signals and reassembling of the encoded message from a sequence of received signals.
Decoding of the reassembled encoded message.
Interpretation and making sense of the presumed original message.These elements are understood to be broadly overlapping and recursive activities rather than steps in a sequence. For example, communicative actions can commence before a communicator formulates a conscious attempt to do so, as in the case of phatics; likewise, communicators modify their intentions and formulations of a message in response to real-time feedback (e.g., a change in facial expression). Practices of decoding and interpretation are culturally enacted, not just by individuals (genre conventions, for instance, trigger anticipatory expectations for how a message is to be received), and receivers of any message operationalize their own frames of reference in interpretation.The scientific study of communication can be divided into:

Information theory which studies the quantification, storage, and communication of information in general;
Communication studies which concerns human communication;
Biosemiotics which examines communication in and between living organisms in general.
Biocommunication which exemplifies sign-mediated interactions in and between organisms of all domains of life, including viruses.The channel of communication can be visual, auditory, tactile/haptic (e.g. Braille or other physical means), olfactory, electromagnetic, or biochemical. Human communication is unique for its extensive use of abstract language. Development of civilization has been closely linked with progress in telecommunication.

Non-verbal communication
Nonverbal communication explains the processes of conveying a type of information in a form of non-linguistic representations. Examples of nonverbal communication include haptic communication, chronemic communication, gestures, body language, facial expressions, eye contact etc. Nonverbal communication also relates to the intent of a message. Examples of intent are voluntary, intentional movements like shaking a hand or winking, as well as involuntary, such as sweating. Speech also contains nonverbal elements known as paralanguage, e.g. rhythm, intonation, tempo, and stress. It affects communication most at the subconscious level and establishes trust. Likewise, written texts include nonverbal elements such as handwriting style, the spatial arrangement of words and the use of emoticons to convey emotion.
Nonverbal communication demonstrates one of Paul Watzlawick's laws: you cannot not communicate. Once proximity has formed awareness, living creatures begin interpreting any signals received. Some of the functions of nonverbal communication in humans are to complement and illustrate, to reinforce and emphasize, to replace and substitute, to control and regulate, and to contradict the denotative message.
Nonverbal cues are heavily relied on to express communication and to interpret others' communication and can replace or substitute verbal messages. However, non-verbal communication is ambiguous. When verbal messages contradict non-verbal messages, observation of non-verbal behaviour is relied on to judge another's attitudes and feelings, rather than assuming the truth of the verbal message alone.
There are several reasons as to why non-verbal communication plays a vital role in communication:
""Non-verbal communication is omnipresent.""  They are included in every single communication act. To have total communication, all non-verbal channels such as the body, face, voice, appearance, touch, distance, timing, and other environmental forces must be engaged during face-to-face interaction. Written communication can also have non-verbal attributes. E-mails, web chats, and the social media have options to change text font colours, stationery, add emoticons, capitalization, and pictures in order to capture non-verbal cues into a verbal medium.""Non-verbal behaviours are multifunctional.""  Many different non-verbal channels are engaged at the same time in communication acts and allow the chance for simultaneous messages to be sent and received.
""Non-verbal behaviours may form a universal language system.""  Smiling, crying, pointing, caressing, and glaring are non-verbal behaviours that are used and understood by people regardless of nationality. Such non-verbal signals allow the most basic form of communication when verbal communication is not effective due to language barriers.

Verbal communication
Verbal communication is the spoken or written conveyance of a message. Human language can be defined as a system of symbols (sometimes known as lexemes) and the grammars (rules) by which the symbols are manipulated. The word ""language"" also refers to common properties of languages. Language learning normally occurs most intensively during human childhood. Most of the large number of human languages use patterns of sound or gesture for symbols which enable communication with others around them. Languages tend to share certain properties, although there are exceptions. There is no defined line between a language and a dialect. Constructed languages such as Esperanto, programming languages, and various mathematical formalisms are not necessarily restricted to the properties shared by human languages.
As previously mentioned, language can be characterized as symbolic. Charles Ogden and I.A Richards developed The Triangle of Meaning model to explain the symbol (the relationship between a word), the referent (the thing it describes), and the meaning (the thought associated with the word and the thing).
The properties of language are governed by rules. Language follows phonological rules (sounds that appear in a language), syntactic rules (arrangement of words and punctuation in a sentence), semantic rules (the agreed upon meaning of words), and pragmatic rules (meaning derived upon context).
The meanings that are attached to words can be literal, or otherwise known as denotative; relating to the topic being discussed, or, the meanings take context and relationships into account, otherwise known as connotative; relating to the feelings, history, and power dynamics of the communicators.Contrary to popular belief, signed languages of the world (e.g., American Sign Language) are considered to be verbal communication because their sign vocabulary, grammar, and other linguistic structures abide by all the necessary classifications as spoken languages. There are however, nonverbal elements to signed languages, such as the speed, intensity, and size of signs that are made. A signer might sign ""yes"" in response to a question, or they might sign a sarcastic-large slow yes to convey a different nonverbal meaning. The sign yes is the verbal message while the other movements add nonverbal meaning to the message.

Written communication and its historical development
Over time the forms of and ideas about communication have evolved through the continuing progression of technology. Advances include communications psychology and media psychology, an emerging field of study.
The progression of written communication can be divided into three ""information communication revolutions"":
Written communication first emerged through the use of pictographs. The pictograms were made in stone, hence written communication was not yet mobile. Pictograms began to develop standardized and simplified forms.
The next step occurred when writing began to appear on paper, papyrus, clay, wax, and other media with commonly shared writing systems, leading to adaptable alphabets. Communication became mobile.
The final stage is characterized by the transfer of information through controlled waves of electromagnetic radiation (i.e., radio, microwave, infrared) and other electronic signals.Communication is thus a process by which meaning is assigned and conveyed in an attempt to create shared understanding. Gregory Bateson called it ""the replication of tautologies in the universe. This process, which requires a vast repertoire of skills in interpersonal processing, listening, observing, speaking, questioning, analyzing, gestures, and evaluating enables collaboration and cooperation.

Business/Corporate
Business communication is used for a wide variety of activities including, but not limited to: strategic communications planning, media relations, internal communications, public relations (which can include social media, broadcast and written communications, and more), brand management, reputation management, speech-writing, customer-client relations, and internal/employee communications.
Companies with limited resources may choose to engage in only a few of these activities, while larger organizations may employ a full spectrum of communications. Since it is relatively difficult to develop such a broad range of skills, communications professionals often specialize in one or two of these areas but usually have at least a working knowledge of most of them. By far, the most important qualifications communications professionals must possess are excellent writing ability, good 'people' skills, and the capacity to think critically and strategically.
Business communication could also refer to the style of communication within a given corporate entity, i.e. email conversation styles, or internal communication styles.

Political
Communication is one of the most relevant tools in political strategies, including persuasion and propaganda. In mass media research and online media research, the effort of the strategist is that of getting a precise decoding, avoiding ""message reactance"", that is, message refusal. The reaction to a message is referred also in terms of approach to a message, as follows:

In ""radical reading"" the audience rejects the meanings, values, and viewpoints built into the text by its makers. Effect: message refusal.
In ""dominant reading"", the audience accepts the meanings, values, and viewpoints built into the text by its makers. Effect: message acceptance.
In ""subordinate reading"" the audience accepts, by and large, the meanings, values, and worldview built into the text by its makers. Effect: obey to the message.Holistic approaches are used by communication campaign leaders and communication strategists in order to examine all the options, ""actors"" and channels that can generate change in the semiotic landscape, that is, change in perceptions, change in credibility, change in the ""memetic background"", change in the image of movements, of candidates, players and managers as perceived by key influencers that can have a role in generating the desired ""end-state"".
The modern political communication field is highly influenced by the framework and practices of ""information operations"" doctrines that derive their nature from strategic and military studies. According to this view, what is really relevant is the concept of acting on the Information  Environment. The information environment is the aggregate of individuals, organizations, and systems that collect, process, disseminate, or act on information. This environment consists of three interrelated dimensions, which continuously interact with individuals, organizations, and systems. These dimensions are known as physical, informational, and cognitive.

Family
Family communication is the study of the communication perspective in a broadly defined family, with intimacy and trusting relationship. The main goal of family communication is to understand the interactions of family and the pattern of behaviors of family members in different circumstances. Open and honest communication creates an atmosphere that allows family members to express their differences as well as love and admiration for one another. It also helps to understand the feelings of one another.
Family communication study looks at topics such as family rules, family roles or family dialectics and how those factors could affect the communication between family members. Researchers develop theories to understand communication behaviors. Family communication study also digs deep into certain time periods of family life such as marriage, parenthood or divorce and how communication stands in those situations. It is important for family members to understand communication as a trusted way which leads to a well constructed family.

Interpersonal
In simple terms, interpersonal communication is the communication between one person and another (or others). It is often referred to as face-to-face communication between two (or more) people. Both verbal and nonverbal communication, or body language, play a part in how one person understands another, and attribute to one's own soft skills. In verbal interpersonal communication there are two types of messages being sent: a content message and a relational message. Content messages are messages about the topic at hand and relational messages are messages about the relationship itself. This means that relational messages come across in how one says something and it demonstrates a person's feelings, whether positive or negative, towards the individual they are talking to, indicating not only how they feel about the topic at hand, but also how they feel about their relationship with the other individual.There are many different aspects of interpersonal communication including:

Audiovisual Perception of Communication Problems. The concept follows the idea that our words change what form they take based on the stress level or urgency of the situation. It also explores the concept that stuttering during speech shows the audience that there is a problem or that the situation is more stressful.
The Attachment Theory. This is the combined work of John Bowlby and Mary Ainsworth (Ainsworth & Bowlby, 1991) This theory follows the relationships that builds between a mother and child, and the impact it has on their relationships with others.
Emotional Intelligence and Triggers. Emotional Intelligence focuses on the ability to monitor ones own emotions as well as those of others. Emotional Triggers focus on events or people that tend to set off intense, emotional reactions within individuals.
Attribution Theory. This is the study of how individuals explain what causes different events and behaviors.
The Power of Words (Verbal communications). Verbal communication focuses heavily on the power of words, and how those words are said. It takes into consideration tone, volume, and choice of words.
Nonverbal Communication. It focuses heavily on the setting that the words are conveyed in, as well as the physical tone of the words.
Ethics in Personal Relations. It is about a space of mutual responsibility between two individuals, it's about giving and receiving in a relationship. This theory is explored by Dawn J. Lipthrott in the article What IS Relationship? What is Ethical Partnership?
Deception in Communication. This concept goes into that everyone lies, and how this can impact relationships. This theory is explored by James Hearn in his article Interpersonal Deception Theory: Ten Lessons for Negotiators
Conflict in Couples. This focuses on the impact that social media has on relationships, as well as how to communicate through conflict. This theory is explored by Amanda Lenhart and Maeve Duggan in their paper Couples, the Internet, and Social Media

Barriers to effectiveness
Barriers to effective communication can retard or distort the message or intention of the message being conveyed. This may result in failure of the communication process or cause an effect that is undesirable. These include filtering, selective perception, information overload, emotions, language, silence, communication apprehension, gender differences and political correctness.This also includes a lack of expressing ""knowledge-appropriate"" communication, which occurs when a person uses ambiguous or complex legal words, medical jargon, or descriptions of a situation or environment that is not understood by the recipient.

Physical barriers – Physical barriers are often due to the nature of the environment. An example of this is the natural barrier which exists when workers are located in different buildings or on different sites. Likewise, poor or outdated equipment, particularly the failure of management to introduce new technology, may also cause problems. Staff shortages are another factor which frequently causes communication difficulties for an organization.
System design – System design faults refer to problems with the structures or systems in place in an organization. Examples might include an organizational structure which is unclear and therefore makes it confusing to know whom to communicate with. Other examples could be inefficient or inappropriate information systems, a lack of supervision or training, and a lack of clarity in roles and responsibilities which can lead to staff being uncertain about what is expected of them.
Attitudinal barriers– Attitudinal barriers come about as a result of problems with staff in an organization. These may be brought about, for example, by such factors as poor management, lack of consultation with employees, personality conflicts which can result in people delaying or refusing to communicate, the personal attitudes of individual employees which may be due to lack of motivation or dissatisfaction at work, brought about by insufficient training to enable them to carry out particular tasks, or simply resistance to change due to entrenched attitudes and ideas.
Ambiguity of words/phrases – Words sounding the same but having different meaning can convey a different meaning altogether. Hence the communicator must ensure that the receiver receives the same meaning. It is better if such words are avoided by using alternatives whenever possible.
Individual linguistic ability – The use of jargon, difficult or inappropriate words in communication can prevent the recipients from understanding the message. Poorly explained or misunderstood messages can also result in confusion. However, research in communication has shown that confusion can lend legitimacy to research when persuasion fails.
Physiological barriers – These may result from individuals' personal discomfort, caused—for example—by ill health, poor eyesight or hearing difficulties.
Bypassing – This happens when the communicators (the sender and the receiver) do not attach the same symbolic meanings to their words. It is when the sender is expressing a thought or a word but the receiver gives it a different meaning. For example- ASAP, Rest room
Technological multi-tasking and absorbency – With a rapid increase in technologically-driven communication in the past several decades, individuals are increasingly faced with condensed communication in the form of e-mail, text, and social updates. This has, in turn, led to a notable change in the way younger generations communicate and perceive their own self-efficacy to communicate and connect with others. With the ever-constant presence of another ""world"" in one's pocket, individuals are multi-tasking both physically and cognitively as constant reminders of something else happening somewhere else bombard them. Though perhaps too new an advancement to yet see long-term effects, this is a notion currently explored by such figures as Sherry Turkle.
Fear of being criticized – This is a major factor that prevents good communication. If we exercise simple practices to improve our communication skill, we can become effective communicators. For example, read an article from the newspaper or collect some news from the television and present it in front of the mirror. This will not only boost your confidence but also improve your language and vocabulary.
Gender barriers – Most communicators whether aware or not, often have a set agenda. This is very notable among the different genders. For example, many women are found to be more critical when addressing conflict. It's also been noted that men are more likely than women to withdraw from conflict.

Cultural aspects
Cultural differences exist within countries (tribal/regional differences, dialects and so on), between religious groups and in organisations or at an organisational level – where companies, teams and units may have different expectations, norms and idiolects. Families and family groups may also experience the effect of cultural barriers to communication within and between different family members or groups. For example: words, colours and symbols have different meanings in different cultures. In most parts of the world, nodding your head means agreement, shaking your head means ""no"", but this is not true everywhere.Communication to a great extent is influenced by culture and cultural variables. Understanding cultural aspects of communication refers to having knowledge of different cultures in order to communicate effectively with cross culture people. Cultural aspects of communication are of great relevance in today's world which is now a global village, thanks to globalisation. Cultural aspects of communication are the cultural differences which influence communication across borders.

Verbal communication refers to a form of communication which uses spoken and written words for expressing and transferring views and ideas. Language is the most important tool of verbal communication. Countries have different languages. A knowledge of languages of different countries can improve cross-cultural understanding.
Non-verbal communication is a very wide concept and it includes all the other forms of communication which do not use written or spoken words. Non verbal communication takes the following forms:
Paralinguistics are the elements other than language where the voice is involved in communication and includes tones, pitch, vocal cues etc. It also includes sounds from throat and all these are greatly influenced by cultural differences across borders.
Proxemics deals with the concept of the space element in communication. Proxemics explains four zones of spaces, namely intimate, personal, social and public. This concept differs from culture to culture as the permissible space varies in different countries.
Artifactics studies the non verbal signals or communication which emerges from personal accessories such as the dress or fashion accessories worn and it varies with culture as people of different countries follow different dress codes.
Chronemics deals with the time aspects of communication and also includes the importance given to time. Some issues explaining this concept are pauses, silences and response lag during an interaction. This aspect of communication is also influenced by cultural differences as it is well known that there is a great difference in the value given by different cultures to time.
Kinesics mainly deals with body language such as postures, gestures, head nods, leg movements, etc. In different countries, the same gestures and postures are used to convey different messages. Sometimes even a particular kinesic indicating something good in a country may have a negative meaning in another culture.So in order to have an effective communication across the world it is desirable to have a knowledge of cultural variables effecting communication.
According to Michael Walsh and Ghil'ad Zuckermann, Western conversational interaction is typically ""dyadic"", between two particular people, where eye contact is important and the speaker controls the interaction; and ""contained"" in a relatively short, defined time frame. However, traditional Aboriginal conversational interaction is ""communal"", broadcast to many people, eye contact is not important, the listener controls the interaction; and ""continuous"", spread over a longer, indefinite time frame.

Nonhuman
Every information exchange between living organisms — i.e. transmission of signals that involve a living sender and receiver can be considered a form of communication; and even primitive creatures such as corals are competent to communicate. Nonhuman communication also include cell signaling, cellular communication, and chemical transmissions between primitive organisms like bacteria and within the plant and fungal kingdoms.

Animals
The broad field of animal communication encompasses most of the issues in ethology. Animal communication can be defined as any behavior of one animal that affects the current or future behavior of another animal. The study of animal communication, called zoo semiotics (distinguishable from anthroposemiotics, the study of human communication) has played an important part in the development of ethology, sociobiology, and the study of animal cognition. Animal communication, and indeed the understanding of the animal world in general, is a rapidly growing field, and even in the 21st century so far, a great share of prior understanding related to diverse fields such as personal symbolic name use, animal emotions, animal culture and learning, and even sexual conduct, long thought to be well understood, has been revolutionized.

Plants and fungi
Communication is observed within the plant organism, i.e. within plant cells and between plant cells, between plants of the same or related species, and between plants and non-plant organisms, especially in the root zone. Plant roots communicate with rhizome bacteria, fungi, and insects within the soil. Recent research has shown that most of the microorganism plant communication processes are neuron-like. Plants also communicate via volatiles when exposed to herbivory attack behavior, thus warning neighboring plants. In parallel they produce other volatiles to attract parasites which attack these herbivores.
Fungi communicate to coordinate and organize their growth and development such as the formation of Marcelia and fruiting bodies. Fungi communicate with their own and related species as well as with non fungal organisms in a great variety of symbiotic interactions, especially with bacteria, unicellular eukaryote, plants and insects through biochemicals of biotic origin. The biochemicals trigger the fungal organism to react in a specific manner, while if the same chemical molecules are not part of biotic messages, they do not trigger the fungal organism to react. This implies that fungal organisms can differentiate between molecules taking part in biotic messages and similar molecules being irrelevant in the situation. So far five different primary signalling molecules are known to coordinate different behavioral patterns such as filamentation, mating, growth, and pathogenicity. Behavioral coordination and production of signaling substances is achieved through interpretation processes that enables the organism to differ between self or non-self, a biotic indicator, biotic message from similar, related, or non-related species, and even filter out ""noise"", i.e. similar molecules without biotic content.

Bacteria quorum sensing
Communication is not a tool used only by humans, plants and animals, but it is also used by microorganisms like bacteria. The process is called quorum sensing. Through quorum sensing, bacteria can sense the density of cells, and regulate gene expression accordingly. This can be seen in both gram positive and gram negative bacteria.
This was first observed by Fuqua et al. in marine microorganisms like V. harveyi and V. fischeri.

Models
The first major model for communication was introduced by Claude Shannon and Warren Weaver for Bell Laboratories in 1949 The original model was designed to mirror the functioning of radio and telephone technologies. Their initial model consisted of three primary parts: sender, channel, and receiver. The sender was the part of a telephone a person spoke into, the channel was the telephone itself, and the receiver was the part of the phone where one could hear the other person. Shannon and Weaver also recognized that often there is static that interferes with one listening to a telephone conversation, which they deemed noise.
In a simple model, often referred to as the transmission model or standard view of communication, information or content (e.g. a message in natural language) is sent in some form (as spoken language) from an emitter (emisor in the picture)/sender/encoder to a destination/receiver/decoder. This common conception of communication simply views communication as a means of sending and receiving information. The strengths of this model are simplicity, generality, and quantifiability. Claude Shannon and Warren Weaver structured this model based on the following elements:

An information source, which produces a message.
A transmitter, which encodes the message into signals.
A channel, to which signals are adapted for transmission.
A noise source, which distorts the signal while it propagates through the channel.
A receiver, which 'decodes' (reconstructs) the message from the signal.
A destination, where the message arrives.Shannon and Weaver argued that there were three levels of problems for communication within this theory.

The technical problem: how accurately can the message be transmitted?
The semantic problem: how precisely is the meaning conveyed?
The effectiveness problem: how effectively does the received meaning affect behavior?Daniel Chandler critiques the transmission model by stating:

It assumes communicators are isolated individuals.
No allowance for differing purposes.
No allowance for differing interpretations.
No allowance for unequal power relations.
No allowance for situational contexts.In 1960, David Berlo expanded on Shannon and Weaver's (1949) linear model of communication and created the SMCR Model of Communication. The Sender-Message-Channel-Receiver Model of communication separated the model into clear parts and has been expanded upon by other scholars.
Communication is usually described along a few major dimensions: message (what type of things are communicated), source/emisor/sender/encoder (from whom), form (in which form), channel (through which medium), destination/receiver/target/decoder (to whom). Wilbur Schram (1954) also indicated that we should also examine the impact that a message has (both desired and undesired) on the target of the message. Between parties, communication includes acts that confer knowledge and experiences, give advice and commands, and ask questions. These acts may take many forms, in one of the various manners of communication. The form depends on the abilities of the group communicating. Together, communication content and form make messages that are sent towards a destination. The target can be oneself, another person or being, another entity (such as a corporation or group of beings).
Communication can be seen as processes of information transmission with three levels of semiotic rules:

Pragmatic (concerned with the relations between signs/expressions and their users).
Semantic (study of relationships between signs and symbols and what they represent).
Syntactic (formal properties of signs and symbols).Therefore, communication is social interaction where at least two interacting agents share a common set of signs and a common set of semiotic rules. This commonly held rule in some sense ignores autocommunication, including intrapersonal communication via diaries or self-talk, both secondary phenomena that followed the primary acquisition of communicative competences within social interactions.
In light of these weaknesses, Barnlund (2008) proposed a transactional model of communication. The basic premise of the transactional model of communication is that individuals are simultaneously engaging in the sending and receiving of messages.
In a slightly more complex form a sender and a receiver are linked reciprocally. This second attitude of communication, referred to as the constitutive model or constructionist view, focuses on how an individual communicates as the determining factor of the way the message will be interpreted. Communication is viewed as a conduit; a passage in which information travels from one individual to another and this information becomes separate from the communication itself. A particular instance of communication is called a speech act. The sender's personal filters and the receiver's personal filters may vary depending upon different regional traditions, cultures, or gender; which may alter the intended meaning of message contents. In the presence of ""communication noise"" on the transmission channel (air, in this case), reception and decoding of content may be faulty, and thus the speech act may not achieve the desired effect. One problem with this encode-transmit-receive-decode model is that the processes of encoding and decoding imply that the sender and receiver each possess something that functions as a codebook, and that these two code books are, at the very least, similar if not identical. Although something like code books is implied by the model, they are nowhere represented in the model, which creates many conceptual difficulties.
Theories of coregulation describe communication as a creative and dynamic continuous process, rather than a discrete exchange of information. Canadian media scholar Harold Innis had the theory that people use different types of media to communicate and which one they choose to use will offer different possibilities for the shape and durability of society. His famous example of this is using ancient Egypt and looking at the ways they built themselves out of media with very different properties stone and papyrus. Papyrus is what he called 'Space Binding'. it made possible the transmission of written orders across space, empires and enables the waging of distant military campaigns and colonial administration. The other is stone and 'Time Binding', through the construction of temples and the pyramids can sustain their authority generation to generation, through this media they can change and shape communication in their society.

Noise
In any communication model, noise is interference with the decoding of messages sent over the channel by an encoder. There are many examples of noise:

Environmental noise. Noise that physically disrupts communication, such as standing next to loud speakers at a party, or the noise from a construction site next to a classroom making it difficult to hear the professor.
Physiological-impairment noise. Physical maladies that prevent effective communication, such as actual deafness or blindness preventing messages from being received as they were intended.
Semantic noise. Different interpretations of the meanings of certain words. For example, the word ""weed"" can be interpreted as an undesirable plant in a yard, or as a euphemism for marijuana.
Syntactical noise. Mistakes in grammar can disrupt communication, such as abrupt changes in verb tense during a sentence.
Organizational noise. Poorly structured communication can prevent the receiver from accurate interpretation. For example, unclear and badly stated directions can make the receiver even more lost.
Cultural noise. Stereotypical assumptions can cause misunderstandings, such as unintentionally offending a non-Christian person by wishing them a ""Merry Christmas"".
Psychological noise. Certain attitudes can also make communication difficult. For instance, great anger or sadness may cause someone to lose focus on the present moment. Disorders such as autism may also severely hamper effective communication.To face communication noise, redundancy and acknowledgement must often be used. Acknowledgements are messages from the addressee informing the originator that his/her communication has been received and is understood. Message repetition and feedback about message received are necessary in the presence of noise to reduce the probability of misunderstanding.
The act of disambiguation regards the attempt of reducing noise and wrong interpretations, when the semantic value or meaning of a sign can be subject to noise, or in presence of multiple meanings, which makes the sense-making difficult. Disambiguation attempts to decrease the likelihood of misunderstanding. This is also a fundamental skill in communication processes activated by counselors, psychotherapists, interpreters, and in coaching sessions based on colloquium. In Information Technology, the disambiguation process and the automatic disambiguation of meanings of words and sentences has also been an interest and concern since the earliest days of computer treatment of language.

As academic discipline
The academic discipline that deals with processes of human communication is communication studies. The discipline encompasses a range of topics, from face-to-face conversation to mass media outlets such as television broadcasting. Communication studies also examines how messages are interpreted through the political, cultural, economic, semiotic, hermeneutic, and social dimensions of their contexts. Statistics, as a quantitative approach to communication science, has also been incorporated into research on communication science in order to help substantiate claims.

See also
References
Further reading
Innis, Harold; Innis, Mary Q. (1975) [1950]. Empire and Communications. Foreword by Marshall McLuhan (Revised ed.). Toronto: University of Toronto Press. ISBN 978-0-8020-6119-5. OCLC 19403451.

External links
 Quotations related to Communication at Wikiquote
 Media related to Communication at Wikimedia Commons",https://en.wikipedia.org/wiki/Communication,"['All articles lacking reliable references', 'All articles with unsourced statements', 'Articles lacking reliable references from August 2020', 'Articles with long short description', 'Articles with short description', 'Articles with unsourced statements from December 2013', 'CS1 errors: requires URL', 'CS1 maint: archived copy as title', 'Commons category link is on Wikidata', 'Communication', 'Pages using authority control with parameters', 'Plant cognition', 'Scoutcraft', 'Short description matches Wikidata', 'Webarchive template wayback links', 'Wikipedia articles needing page number citations from August 2017', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NARA identifiers', 'Wikipedia articles with NDL identifiers', 'Wikipedia pending changes protected pages']",Data Science
25,Computational learning theory,"In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.

Overview
Theoretical results in machine learning mainly deal with a type of inductive learning called supervised learning.  In supervised learning, an algorithm is given samples that are labeled in some useful way.  For example, the samples might be descriptions of mushrooms, and the labels could be whether or not the mushrooms are edible.  The algorithm takes these previously labeled samples and uses them to induce a classifier.  This classifier is a function that assigns labels to samples, including samples that have not been seen previously by the algorithm.  The goal of the supervised learning algorithm is to optimize some measure of performance such as minimizing the number of mistakes made on new samples.
In addition to performance bounds, computational learning theory studies the time complexity and feasibility of learning. In
computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time
complexity results:

Positive results – Showing that a certain class of functions is learnable in polynomial time.
Negative results – Showing that certain classes cannot be learned in polynomial time.Negative results often rely on commonly believed, but yet unproven assumptions, such as:

Computational complexity – P ≠ NP (the P versus NP problem);
Cryptographic – One-way functions exist.There are several different approaches to computational learning theory based on making different assumptions about the
inference principles used to generalize from limited data. This includes different definitions of probability (see frequency probability, Bayesian probability) and different assumptions on the generation of samples. The different approaches include:
Exact learning, proposed by Dana Angluin;
Probably approximately correct learning (PAC learning), proposed by Leslie Valiant;
VC theory, proposed by Vladimir Vapnik and Alexey Chervonenkis;
Bayesian inference;
Algorithmic learning theory, from the work of E. Mark Gold;
Online machine learning, from the work of Nick Littlestone.While its primary goal is to understand learning abstractly, computational learning theory has led to the development of practical algorithms. For example, PAC theory inspired boosting, VC theory led to support vector machines, and Bayesian inference led to belief networks.

See also
Grammar induction
Information theory
Stability (learning theory)
Error Tolerance (PAC learning)

References
Surveys
Angluin, D. 1992. Computational learning theory: Survey and selected bibliography. In Proceedings of the Twenty-Fourth Annual ACM Symposium on Theory of Computing (May 1992), pages 351–369. http://portal.acm.org/citation.cfm?id=129712.129746
D. Haussler. Probably approximately correct learning. In AAAI-90 Proceedings of the Eight National Conference on Artificial Intelligence, Boston, MA, pages 1101–1108. American Association for Artificial Intelligence, 1990. http://citeseer.ist.psu.edu/haussler90probably.html

VC dimension
V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and Its Applications, 16(2):264–280, 1971.

Feature selection
A. Dhagat and L. Hellerstein, ""PAC learning with irrelevant attributes"", in 'Proceedings of the IEEE Symp. on Foundation of Computer Science', 1994. http://citeseer.ist.psu.edu/dhagat94pac.html

Inductive inference
Gold, E. Mark (1967). ""Language identification in the limit"" (PDF). Information and Control. 10 (5): 447–474. doi:10.1016/S0019-9958(67)91165-5.

Optimal O notation learning
Oded Goldreich, Dana Ron. On universal learning algorithms. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.2224

Negative results
M. Kearns and Leslie Valiant. 1989. Cryptographic limitations on learning boolean formulae and finite automata. In Proceedings of the 21st Annual ACM Symposium on Theory of Computing, pages 433–444, New York. ACM. http://citeseer.ist.psu.edu/kearns89cryptographic.html

Boosting (machine learning)
Robert E. Schapire. The strength of weak learnability. Machine Learning, 5(2):197–227, 1990 http://citeseer.ist.psu.edu/schapire90strength.html

Occam Learning
Blumer, A.; Ehrenfeucht, A.; Haussler, D.; Warmuth, M. K. Occam's razor Inf.Proc.Lett. 24, 377–380, 1987.
Blumer, A.; Ehrenfeucht, A.; Haussler, D.; Warmuth, M. K. Learnability and the Vapnik-Chervonenkis dimension. Journal of the ACM, 36(4):929–865, 1989.

Probably approximately correct learning
L. Valiant. A Theory of the Learnable. Communications of the ACM, 27(11):1134–1142, 1984.

Error tolerance
Michael Kearns and Ming Li. Learning in the presence of malicious errors. SIAM Journal on Computing, 22(4):807–837, August 1993. http://citeseer.ist.psu.edu/kearns93learning.html
Kearns, M. (1993). Efficient noise-tolerant learning from statistical queries. In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, pages 392–401. http://citeseer.ist.psu.edu/kearns93efficient.html

Equivalence
D.Haussler, M.Kearns, N.Littlestone and M. Warmuth, Equivalence of models for polynomial learnability, Proc. 1st ACM Workshop on Computational Learning Theory, (1988) 42-55.
Pitt, L.; Warmuth, M. K. (1990). ""Prediction-Preserving Reducibility"". Journal of Computer and System Sciences. 41 (3): 430–467. doi:10.1016/0022-0000(90)90028-J.A description of some of these publications is given at important publications in machine learning.

Distribution Learning Theory
External links
Basics of Bayesian inference",https://en.wikipedia.org/wiki/Computational_learning_theory,"['All articles needing additional references', 'All articles with unsourced statements', 'Articles needing additional references from November 2018', 'Articles with short description', 'Articles with unsourced statements from October 2017', 'Computational fields of study', 'Computational learning theory', 'Machine learning', 'Short description matches Wikidata', 'Theoretical computer science']",Data Science
26,Computational science,"Computational science, also known as scientific computing or scientific computation (SC), is a rapidly growing field that uses advanced computing capabilities to understand and solve complex problems. It is an area of science which spans many disciplines, but at its core, it involves the development of models and simulations to understand natural systems.

Algorithms (numerical and non-numerical): mathematical models, computational models, and computer simulations developed to solve science (e.g., biological, physical, and social), engineering, and humanities problems
Computer hardware that develops and optimizes the advanced system hardware, firmware, networking, and data management components needed to solve computationally demanding problems
The computing infrastructure that supports both the science and engineering problem solving and the developmental computer and information scienceIn practical use, it is typically the application of computer simulation and other forms of computation from numerical analysis and theoretical computer science to solve problems in various scientific disciplines. The field is different from theory and laboratory experiment which are the traditional forms of science and engineering. The scientific computing approach is to gain understanding, mainly through the analysis of mathematical models implemented on computers. Scientists and engineers develop computer programs, application software, that model systems being studied and run these programs with various sets of input parameters. The essence of computational science is the application of numerical algorithms and/or computational mathematics. In some cases, these models require massive amounts of calculations (usually floating-point) and are often executed on supercomputers or distributed computing platforms.

The computational scientist
The term computational scientist is used to describe someone skilled in scientific computing. This person is usually a scientist, an engineer or an applied mathematician who applies high-performance computing in different ways to advance the state-of-the-art in their respective applied disciplines in physics, chemistry or engineering.
Computational science is now commonly considered a third mode of science, complementing and adding to experimentation/observation and theory (see image on the right). Here, we define a system as a potential source of data, an experiment as a process of extracting data from a system by exerting it through its inputs and a model (M) for a system (S) and an experiment (E) as anything to which E can be applied in order to answer questions about S.  A computational scientist should be capable of:

recognizing complex problems
adequately conceptualising the system containing these problems
designing a framework of algorithms suitable for studying this system: the simulation
choosing a suitable computing infrastructure (parallel computing/grid computing/supercomputers)
hereby, maximising the computational power of the simulation
assessing to what level the output of the simulation resembles the systems: the model is validated
adjusting the conceptualisation of the system accordingly
repeating cycle until a suitable level of validation is obtained: the computational scientists trusts that the simulation generates adequately realistic results for the system, under the studied conditionsIn fact, substantial effort in computational sciences has been devoted to the development of algorithms, the efficient implementation in programming languages, and validation of computational results. A collection of problems and solutions in computational science can be found in Steeb, Hardy, Hardy and Stoop (2004).Philosophers of science addressed the question to what degree computational science qualifies as science, among them Humphreys and Gelfert. They address the general question of epistemology: how do we gain insight from such computational science approaches. Tolk uses these insights to show the epistemological constraints of computer-based simulation research. As computational science uses mathematical models representing the underlying theory in executable form, in essence, they apply modeling (theory building) and simulation (implementation and execution). While simulation and computational science are our most sophisticated way to express our knowledge and understanding, they also come with all constraints and limits already known for computational solutions.

Applications of computational science
Problem domains for computational science/scientific computing include:

Predictive computational science
Predictive computational science is a scientific discipline concerned with the formulation, calibration, numerical solution and validation of mathematical models designed to predict specific aspects of physical events, given initial and boundary conditions and a set of characterizing parameters and associated uncertainties. In typical cases, the predictive statement is formulated in terms of probabilities.  For example, given a mechanical component and a periodic loading condition, “the probability is (say) 90% that the number of cycles at failure (Nf) will be in the interval N1<Nf<N2”.

Urban complex systems
In 2015, over half the world's population live in cities. By the middle of the 21st century, it is estimated that 75% of the world's population will be urban. This urban growth is focused in the urban populations of developing countries where city dwellers will more than double, increasing from 2.5 billion in 2009 to almost 5.2 billion in 2050. Cities are massive complex systems created by humans, made up of humans and governed by humans. Trying to predict, understand and somehow shape the development of cities in the future requires complex thinking, and requires computational models and simulations to help mitigate challenges and possible disasters. The focus of research in urban complex systems is, through modeling and simulation, to build a greater understanding of city dynamics and help prepare for the coming urbanisation.

Computational finance
In today's financial markets huge volumes of interdependent assets are traded by a large number of interacting market participants in different locations and time zones. Their behavior is of unprecedented complexity and the characterization and measurement of the risk inherent to these highly diverse set of instruments is typically based on complicated mathematical and computational models. Solving these models exactly in closed form, even at a single instrument level, is typically not possible, and therefore we have to look for efficient numerical algorithms. This has become even more urgent and complex recently, as the credit crisis has clearly demonstrated the role of cascading effects going from single instruments through portfolios of single institutions to even the interconnected trading network. Understanding this requires a multi-scale and holistic approach where interdependent risk factors such as market, credit and liquidity risk are modelled simultaneously and at different interconnected scales.

Computational biology
Exciting new developments in biotechnology are now revolutionizing biology and biomedical research. Examples of these techniques are high-throughput sequencing, high-throughput quantitative PCR, intra-cellular imaging, in-situ hybridization of gene expression, three-dimensional imaging techniques like Light Sheet Fluorescence Microscopy and Optical Projection, (micro)-Computer Tomography. Given the massive amounts of complicated data that is generated by these techniques, their meaningful interpretation, and even their storage, form major challenges calling for new approaches. Going beyond current bioinformatics approaches, computational biology needs to develop new methods to discover meaningful patterns in these large data sets. Model-based reconstruction of gene networks can be used to organize the gene expression data in a systematic way and to guide future data collection. A major challenge here is to understand how gene regulation is controlling fundamental biological processes like biomineralisation and embryogenesis. The sub-processes like gene regulation, organic molecules interacting with the mineral deposition process, cellular processes, physiology and other processes at the tissue and environmental levels are linked. Rather than being directed by a central control mechanism, biomineralisation and embryogenesis can be viewed as an emergent behavior resulting from a complex system in which several sub-processes on very different temporal and spatial scales (ranging from nanometer and nanoseconds to meters and years) are connected into a multi-scale system. One of the few available options to understand such systems is by developing a multi-scale model of the system.

Complex systems theory
Using information theory, non-equilibrium dynamics and explicit simulations computational systems theory tries to uncover the true nature of complex adaptive systems.

Computational science in engineering
Computational science and engineering (CSE) is a relatively new discipline that deals with the development and application of computational models and simulations, often coupled with high-performance computing, to solve complex physical problems arising in engineering analysis and design (computational engineering) as well as natural phenomena (computational science). CSE has been described as the ""third mode of discovery"" (next to theory and experimentation). In many fields, computer simulation is integral and therefore essential to business and research. Computer simulation provides the capability to enter fields that are either inaccessible to traditional experimentation or where carrying out traditional empirical inquiries is prohibitively expensive. CSE should neither be confused with pure computer science, nor with computer engineering, although a wide domain in the former is used in CSE (e.g., certain algorithms, data structures, parallel programming, high performance computing) and some problems in the latter can be modeled and solved with CSE methods (as an application area).

Methods and algorithms
Algorithms and mathematical methods used in computational science are varied. Commonly applied methods include:

Both historically and today, Fortran remains popular for most applications of scientific computing. Other programming languages and computer algebra systems commonly used for the more mathematical aspects of scientific computing applications include GNU Octave, Haskell, Julia, Maple, Mathematica, MATLAB, Python (with third-party SciPy library), Perl (with third-party PDL library), R, Scilab, and TK Solver. The more computationally intensive aspects of scientific computing will often use some variation of C or Fortran and optimized algebra libraries such as BLAS or LAPACK. In addition, parallel computing is heavily used in scientific computing to achieve solutions of large problems in a reasonable amount of time. In this framework, the problem is either divided over many cores on a single CPU node (such as with OpenMP), divided over many CPU nodes networked together (such as with MPI), or is run on one or more GPUs (typically using either CUDA or OpenCL).
Computational science application programs often model real-world changing conditions, such as weather, airflow around a plane, automobile body distortions in a crash, the motion of stars in a galaxy, an explosive device, etc. Such programs might create a 'logical mesh' in computer memory where each item corresponds to an area in space and contains information about that space relevant to the model. For example, in weather models, each item might be a square kilometer; with land elevation, current wind direction, humidity, temperature, pressure, etc. The program would calculate the likely next state based on the current state, in simulated time steps, solving differential equations that describe how the system operates; and then repeat the process to calculate the next state.

Conferences and journals
In the year 2001, the International Conference on Computational Science (ICCS) was first organised. Since then it has been organised yearly. ICCS is an A-rank conference in CORE classification.
The international Journal of Computational Science published its first issue in May 2010. A new initiative was launched in 2012, the Journal of Open Research Software.
In 2015, ReScience C dedicated to the replication of computational results has been started on GitHub.

Education
At some institutions, a specialization in scientific computation can be earned as a ""minor"" within another program (which may be at varying levels). However, there are increasingly many bachelor's, master's and doctoral programs in computational science. The joint degree programme master program computational science at the University of Amsterdam and the Vrije Universiteit in computational science was first offered in 2004. In this programme, students:

learn to build computational models from real-life observations;
develop skills in turning these models into computational structures and in performing large-scale simulations;
learn theory that will give a firm basis for the analysis of complex systems;
learn to analyse the results of simulations in a virtual laboratory using advanced numerical algorithms.George Mason University was one of the early pioneers first offering a multidisciplinary doctorate Ph.D program in Computational Sciences and Informatics in 1992 that focused on a number of specialty areas including bioinformatics, computational chemistry, earth systems and global changes, computational mathematics, computational physics, space sciences, and computational statistics
School of Computational and Integrative Sciences, Jawaharlal Nehru University (erstwhile School of Information Technology) also offers a vibrant master's science program for computational science with two specialities namely- Computational Biology and Complex Systems.

Related fields
See also
Computer simulations in science
Computational science and engineering
Comparison of computer algebra systems
Differentiable programming
List of molecular modeling software
List of numerical analysis software
List of statistical packages
Timeline of scientific computing
Simulated reality
Extensions for Scientific Computation (XSC)

References
Additional sources
E. Gallopoulos and A. Sameh, ""CSE: Content and Product"". IEEE Computational Science and Engineering Magazine, 4(2):39–43 (1997)
G. Hager and G. Wellein, Introduction to High Performance Computing for Scientists and Engineers, Chapman and Hall (2010)
A.K. Hartmann, Practical Guide to Computer Simulations, World Scientific (2009)
Journal Computational Methods in Science and Technology (open access), Polish Academy of Sciences
Journal Computational Science and Discovery, Institute of Physics
R.H. Landau, C.C. Bordeianu, and M. Jose Paez, A Survey of Computational Physics: Introductory Computational Science, Princeton University Press (2008)

External links
John von Neumann-Institut for Computing (NIC) at Juelich (Germany)
The National Center for Computational Science at Oak Ridge National Laboratory
Center for Simulation and Modeling at George Mason University
Educational Materials for Undergraduate Computational Studies
Computational Science at the National Laboratories
Bachelor in Computational Science, University of Medellin, Colombia, South America
Simulation Optimization Systems (SOS) Research Laboratory, McMaster University, Hamilton, ON
Computational Sciences and Informatics, Ph.D Program, George Mason University",https://en.wikipedia.org/wiki/Computational_science,"['All articles with unsourced statements', 'Applied mathematics', 'Articles with short description', 'Articles with unsourced statements from December 2008', 'Commons category link from Wikidata', 'Computational fields of study', 'Computational science', 'Computer science', 'Short description is different from Wikidata']",Data Science
27,Conditional random field,"Conditional random fields (CRFs) are a class of statistical modeling method often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering ""neighboring"" samples, a CRF can take context into account. To do so, the prediction is modeled as a graphical model, which implements dependencies between the predictions. What kind of graph is used depends on the application. For example, in natural language processing, linear chain CRFs are popular, which implement sequential dependencies in the predictions. In image processing the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.
Other examples where CRFs are used are: labeling or parsing of sequential data for natural language processing or biological sequences, POS tagging, shallow parsing, named entity recognition, gene finding, peptide critical functional region finding, and object recognition and image segmentation in computer vision.

Description
CRFs are a type of discriminative undirected probabilistic graphical model.
Lafferty, McCallum and Pereira define a CRF on observations 
  
    
      
        
          X
        
      
    
    {\displaystyle {\boldsymbol {X}}}
   and random variables 
  
    
      
        
          Y
        
      
    
    {\displaystyle {\boldsymbol {Y}}}
   as follows:

Let 
  
    
      
        G
        =
        (
        V
        ,
        E
        )
      
    
    {\displaystyle G=(V,E)}
   be a graph such that

  
    
      
        
          Y
        
        =
        (
        
          
            Y
          
          
            v
          
        
        
          )
          
            v
            ∈
            V
          
        
      
    
    {\displaystyle {\boldsymbol {Y}}=({\boldsymbol {Y}}_{v})_{v\in V}}
  ,
 so that 
  
    
      
        
          Y
        
      
    
    {\displaystyle {\boldsymbol {Y}}}
   is indexed by the vertices of 
  
    
      
        G
      
    
    {\displaystyle G}
  . 
Then 
  
    
      
        (
        
          X
        
        ,
        
          Y
        
        )
      
    
    {\displaystyle ({\boldsymbol {X}},{\boldsymbol {Y}})}
   is a conditional random field when the random variables 
  
    
      
        
          
            Y
          
          
            v
          
        
      
    
    {\displaystyle {\boldsymbol {Y}}_{v}}
  , conditioned on 
  
    
      
        
          X
        
      
    
    {\displaystyle {\boldsymbol {X}}}
  , obey the Markov property with
respect to the graph: 
  
    
      
        p
        (
        
          
            Y
          
          
            v
          
        
        
          |
        
        
          X
        
        ,
        
          
            Y
          
          
            w
          
        
        ,
        w
        ≠
        v
        )
        =
        p
        (
        
          
            Y
          
          
            v
          
        
        
          |
        
        
          X
        
        ,
        
          
            Y
          
          
            w
          
        
        ,
        w
        ∼
        v
        )
      
    
    {\displaystyle p({\boldsymbol {Y}}_{v}|{\boldsymbol {X}},{\boldsymbol {Y}}_{w},w\neq v)=p({\boldsymbol {Y}}_{v}|{\boldsymbol {X}},{\boldsymbol {Y}}_{w},w\sim v)}
  , where 
  
    
      
        
          
            w
          
        
        ∼
        v
      
    
    {\displaystyle {\mathit {w}}\sim v}
   means
that 
  
    
      
        w
      
    
    {\displaystyle w}
   and 
  
    
      
        v
      
    
    {\displaystyle v}
   are neighbors in 
  
    
      
        G
      
    
    {\displaystyle G}
  .

What this means is that a CRF is an undirected graphical model whose nodes can be divided into exactly two disjoint sets 
  
    
      
        
          X
        
      
    
    {\displaystyle {\boldsymbol {X}}}
   and 
  
    
      
        
          Y
        
      
    
    {\displaystyle {\boldsymbol {Y}}}
  , the observed and output variables, respectively; the conditional distribution 
  
    
      
        p
        (
        
          Y
        
        
          |
        
        
          X
        
        )
      
    
    {\displaystyle p({\boldsymbol {Y}}|{\boldsymbol {X}})}
   is then modeled.

Inference
For general graphs, the problem of exact inference in CRFs is intractable. The inference problem for a CRF is basically the same as for an MRF and the same arguments hold.
However, there exist special cases for which exact inference is feasible:

If the graph is a chain or a tree, message passing algorithms yield exact solutions. The algorithms used in these cases are analogous to the forward-backward and Viterbi algorithm for the case of HMMs.
If the CRF only contains pair-wise potentials and the energy is submodular, combinatorial min cut/max flow algorithms yield exact solutions.If exact inference is impossible, several algorithms can be used to obtain approximate solutions. These include:

Loopy belief propagation
Alpha expansion
Mean field inference
Linear programming relaxations

Parameter Learning
Learning the parameters 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   is usually done by maximum likelihood learning for 
  
    
      
        p
        (
        
          Y
          
            i
          
        
        
          |
        
        
          X
          
            i
          
        
        ;
        θ
        )
      
    
    {\displaystyle p(Y_{i}|X_{i};\theta )}
  . If all nodes have exponential family distributions and all nodes are observed during training, this optimization is convex. It can be solved for example using gradient descent algorithms, or Quasi-Newton methods such as the L-BFGS algorithm. On the other hand, if some variables are unobserved, the inference problem has to be solved for these variables. Exact inference is intractable in general graphs, so approximations have to be used.

Examples
In sequence modeling, the graph of interest is usually a chain graph. An input sequence of observed variables 
  
    
      
        X
      
    
    {\displaystyle X}
   represents a sequence of observations and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   represents a hidden (or unknown) state variable that needs to be inferred given the observations. The 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
   are structured to form a chain, with an edge between each 
  
    
      
        
          Y
          
            i
            −
            1
          
        
      
    
    {\displaystyle Y_{i-1}}
   and 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
  . As well as having a simple interpretation of the 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
   as ""labels"" for each element in the input sequence, this layout admits efficient algorithms for:

model training, learning the conditional distributions between the 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
   and feature functions from some corpus of training data.
decoding, determining the probability of a given label sequence 
  
    
      
        Y
      
    
    {\displaystyle Y}
   given 
  
    
      
        X
      
    
    {\displaystyle X}
  .
inference, determining the most likely label sequence 
  
    
      
        Y
      
    
    {\displaystyle Y}
   given 
  
    
      
        X
      
    
    {\displaystyle X}
  .The conditional dependency of each 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
   on 
  
    
      
        X
      
    
    {\displaystyle X}
   is defined through a fixed set of feature functions of the form 
  
    
      
        f
        (
        i
        ,
        
          Y
          
            i
            −
            1
          
        
        ,
        
          Y
          
            i
          
        
        ,
        X
        )
      
    
    {\displaystyle f(i,Y_{i-1},Y_{i},X)}
  , which can be thought of as measurements on the input sequence that partially determine the likelihood of each possible value for 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
  . The model assigns each feature a numerical weight and combines them to determine the probability of a certain value for 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
  .
Linear-chain CRFs have many of the same applications as conceptually simpler hidden Markov models (HMMs), but relax certain assumptions about the input and output sequence distributions. An HMM can loosely be understood as a CRF with very specific feature functions that use constant probabilities to model state transitions and emissions. Conversely, a CRF can loosely be understood as a generalization of an HMM that makes the constant transition probabilities into arbitrary functions that vary across the positions in the sequence of hidden states, depending on the input sequence.
Notably, in contrast to HMMs, CRFs can contain any number of feature functions, the feature functions can inspect the entire input sequence 
  
    
      
        X
      
    
    {\displaystyle X}
   at any point during inference, and the range of the feature functions need not have a probabilistic interpretation.

Variants
Higher-order CRFs and semi-Markov CRFs
CRFs can be extended into higher order models by making each 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
   dependent on a fixed number 
  
    
      
        k
      
    
    {\displaystyle k}
   of previous variables 
  
    
      
        
          Y
          
            i
            −
            k
          
        
        ,
        .
        .
        .
        ,
        
          Y
          
            i
            −
            1
          
        
      
    
    {\displaystyle Y_{i-k},...,Y_{i-1}}
  . In conventional formulations of higher order CRFs, training and inference are only practical for small values of 
  
    
      
        k
      
    
    {\displaystyle k}
   (such as k ≤ 5), since their computational cost increases exponentially with 
  
    
      
        k
      
    
    {\displaystyle k}
  .
However, another recent advance has managed to ameliorate these issues by leveraging concepts and tools from the field of Bayesian nonparametrics. Specifically, the CRF-infinity approach constitutes a CRF-type model that is capable of learning infinitely-long temporal dynamics in a scalable fashion. This is effected by introducing a novel potential function for CRFs that is based on the Sequence Memoizer (SM), a nonparametric Bayesian model for learning infinitely-long dynamics in sequential observations. To render such a model computationally tractable, CRF-infinity employs a mean-field approximation of the postulated novel potential functions (which are driven by an SM). This allows for devising efficient approximate training and inference algorithms for the model, without undermining its capability to capture and model temporal dependencies of arbitrary length.
There exists another generalization of CRFs, the semi-Markov conditional random field (semi-CRF), which models variable-length segmentations of the label sequence 
  
    
      
        Y
      
    
    {\displaystyle Y}
  . This provides much of the power of higher-order CRFs to model long-range dependencies of the 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
  , at a reasonable computational cost.
Finally, large-margin models for structured prediction, such as the structured Support Vector Machine can be seen as an alternative training procedure to CRFs.

Latent-dynamic conditional random field
Latent-dynamic conditional random fields (LDCRF) or discriminative probabilistic latent variable models (DPLVM) are a type of CRFs for sequence tagging tasks. They are latent variable models that are trained discriminatively.
In an LDCRF, like in any sequence tagging task, given a sequence of observations x = 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},\dots ,x_{n}}
  , the main problem the model must solve is how to assign a sequence of labels y = 
  
    
      
        
          y
          
            1
          
        
        ,
        …
        ,
        
          y
          
            n
          
        
      
    
    {\displaystyle y_{1},\dots ,y_{n}}
   from one finite set of labels Y. Instead of directly modeling P(y|x) as an ordinary linear-chain CRF would do, a set of latent variables h is ""inserted"" between x and y using the chain rule of probability:

  
    
      
        P
        (
        
          y
        
        
          |
        
        
          x
        
        )
        =
        
          ∑
          
            
              h
            
          
        
        P
        (
        
          y
        
        
          |
        
        
          h
        
        ,
        
          x
        
        )
        P
        (
        
          h
        
        
          |
        
        
          x
        
        )
      
    
    {\displaystyle P(\mathbf {y} |\mathbf {x} )=\sum _{\mathbf {h} }P(\mathbf {y} |\mathbf {h} ,\mathbf {x} )P(\mathbf {h} |\mathbf {x} )}
  This allows capturing latent structure between the observations and labels. While LDCRFs can be trained using quasi-Newton methods, a specialized version of the perceptron algorithm called the latent-variable perceptron has been developed for them as well, based on Collins' structured perceptron algorithm. These models find applications in computer vision, specifically gesture recognition from video streams and shallow parsing.

Software
This is a partial list of software that implement generic CRF tools.

RNNSharp CRFs based on recurrent neural networks (C#, .NET)
CRF-ADF Linear-chain CRFs with fast online ADF training (C#, .NET)
CRFSharp Linear-chain CRFs (C#, .NET)
GCO CRFs with submodular energy functions (C++, Matlab)
DGM General CRFs (C++)
GRMM General CRFs (Java)
factorie General CRFs (Scala)
CRFall General CRFs (Matlab)
Sarawagi's CRF Linear-chain CRFs (Java)
HCRF library Hidden-state CRFs (C++, Matlab)
Accord.NET Linear-chain CRF, HCRF and HMMs (C#, .NET)
Wapiti Fast linear-chain CRFs (C)
CRFSuite Fast restricted linear-chain CRFs (C)
CRF++ Linear-chain CRFs (C++)
FlexCRFs First-order and second-order Markov CRFs (C++)
crf-chain1 First-order, linear-chain CRFs (Haskell)
imageCRF CRF for segmenting images and image volumes (C++)
MALLET Linear-chain for sequence tagging (Java)
PyStruct Structured Learning in Python (Python)
Pycrfsuite A python binding for crfsuite (Python)
Figaro Probabilistic programming language capable of defining CRFs and other graphical models (Scala)
CRF Modeling and computational tools for CRFs and other undirected graphical models (R)
OpenGM Library for discrete factor graph models and distributive operations on these models (C++)
UPGMpp Library for building, training, and performing inference with Undirected Graphical Models (C++)
KEG_CRF Fast Linear CRFs (C++)This is a partial list of software that implement CRF related tools.

MedaCy Medical Named Entity Recognizer (Python)
Conrad CRF based gene predictor (Java)
Stanford NER Named Entity Recognizer (Java)
BANNER Named Entity Recognizer (Java)

See also
Hammersley–Clifford theorem
Graphical model
Markov random field
Maximum entropy Markov model (MEMM)

References
Further reading
McCallum, A.: Efficiently inducing features of conditional random fields. In: Proc. 19th Conference on Uncertainty in Artificial Intelligence. (2003)
Wallach, H.M.: Conditional random fields: An introduction. Technical report MS-CIS-04-21, University of Pennsylvania (2004)
Sutton, C., McCallum, A.: An Introduction to Conditional Random Fields for Relational Learning. In ""Introduction to Statistical Relational Learning"". Edited by Lise Getoor and Ben Taskar. MIT Press. (2006) Online PDF
Klinger, R., Tomanek, K.: Classical Probabilistic Models and Conditional Random Fields. Algorithm Engineering Report TR07-2-013, Department of Computer Science, Dortmund University of Technology, December 2007. ISSN 1864-4503. Online PDF",https://en.wikipedia.org/wiki/Conditional_random_field,"['All Wikipedia articles needing context', 'All articles that are too technical', 'All pages needing cleanup', 'Articles with multiple maintenance issues', 'CS1 maint: uses authors parameter', 'Graphical models', 'Machine learning', 'Webarchive template wayback links', 'Wikipedia articles needing context from January 2013', 'Wikipedia articles that are too technical from June 2012', 'Wikipedia introduction cleanup from January 2013']",Data Science
28,Computer science,"Computer science is the study of algorithmic processes, computational machines and computation itself. As a discipline, computer science spans a range of topics from theoretical studies of algorithms, computation and information to the practical issues of implementing computational systems in hardware and software.Its fields can be divided into theoretical and practical disciplines. For example, the theory of computation concerns abstract models of computation and general classes of problems that can be solved using them, while computer graphics or computational geometry emphasize more specific applications. Algorithms and data structures have been called the heart of computer science. Programming language theory considers approaches to the description of computational processes, while computer programming involves the use of them to create complex systems. Computer architecture describes construction of computer components and computer-operated equipment. Artificial intelligence aims to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. A digital computer is capable of simulating various information processes. The fundamental concern of computer science is determining what can and cannot be automated. Computer scientists usually focus on academic research. The Turing Award is generally recognized as the highest distinction in computer sciences.

History
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment. 
Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, for, among other reasons, documenting the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and ""in less than two years, he had sketched out many of the salient features of the modern computer"". ""A crucial step was the adoption of a punched card system derived from the Jacquard loom"" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published  the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as ""Babbage's dream come true"".
During the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and the university was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.

Etymology
Although first proposed in 1956, the term ""computer science"" appears in a 1959 article in Communications of the ACM,
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921, justifying the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.
In the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist. Three months later in the same journal, comptologist was suggested, followed next year by hypologist. The term computics has also been suggested. In Europe, terms derived from contracted translations of the expression ""automatic information"" (e.g. ""informazione automatica"" in Italian) or ""information and mathematics"" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics of the University of Edinburgh).  ""In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.""A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that ""computer science is no more about computers than astronomy is about telescopes."" The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been much cross-fertilization of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as philosophy, cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, and logic.
Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term ""Software Engineering"" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.

Philosophy
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning's working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the ""rationalist paradigm"" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the ""technocratic paradigm"" (which might be found in engineering approaches, most prominently in software engineering), and the ""scientific paradigm"" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.

Fields
Computer science is no more about computers than astronomy is about telescopes.

As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.CSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.

Theoretical computer science
Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.

Theory of computation
According to Peter Denning, the fundamental question underlying computer science is, ""What can be automated?"" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.

Information and coding theory
Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.

Data structures and algorithms
Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.

Programming language theory and formal methods
Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.

Computer systems and computational processes
Artificial intelligence
Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question ""Can computers think?"", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.

Computer architecture and organization
Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term “architecture” in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.

Concurrent, parallel and distributed computing
Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model. When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.

Computer networks
This branch of computer science aims to manage networks between computers worldwide.

Computer security and cryptography
Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users. Cryptography is the practice and study of hiding (encryption) and therefore deciphering (decryption) information. Modern cryptography is largely related to computer science, for many encryption and decryption algorithms are based on their computational complexity.

Databases and data mining
A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.

Computer graphics and visualization
Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.

Image and sound processing
Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier - whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.

Applied computer science
Computational science, finance and engineering
Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.

Social computing and human-computer interaction
Social computing is an area that is concerned with the intersection of social behavior and computational systems. Human-computer interaction research develops theories, principles, and guidelines for user interface designers.

Software engineering
Software engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it doesn't just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.

Discoveries
The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:
Gottfried Wilhelm Leibniz's, George Boole's, Alan Turing's, Claude Shannon's, and Samuel Morse's insight: there are only two objects that a computer has to deal with in order to represent ""anything"".All the information about any computable problem can be represented using only 0 and 1 (or any other bistable pair that can flip-flop between two easily distinguishable states, such as ""on/off"", ""magnetized/de-magnetized"", ""high-voltage/low-voltage"", etc.).
Alan Turing's insight: there are only five actions that a computer has to perform in order to do ""anything"".Every algorithm can be expressed in a language for a computer consisting of only five basic instructions:move left one location;
move right one location;
read symbol at current location;
print 0 at current location;
print 1 at current location.
Corrado Böhm and Giuseppe Jacopini's insight: there are only three ways of combining these actions (into more complex ones) that are needed in order for a computer to do ""anything"".Only three rules are needed to combine any set of basic instructions into more complex ones:
sequence: first do this, then do that;
 selection: IF such-and-such is the case, THEN do this, ELSE do that;
repetition: WHILE such-and-such is the case, DO this.
Note that the three rules of Boehm's and Jacopini's insight can be further simplified with the use of goto (which means it is more elementary than structured programming).

Programming paradigms
Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:

Functional programming, a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements.
Imperative programming, a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.
Object-oriented programming, a programming paradigm based on the concept of ""objects"", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated. Thus object-oriented computer programs are made out of objects that interact with one another.
Service-oriented programming, a programming paradigm that uses ""services"" as the unit of computer work, to design and implement integrated business applications and mission critical software programsMany languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.

Academia
Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.

Education
Computer Science, known by its near synonyms, Computing, Computer Studies, has been taught in UK schools since the days of batch processing, mark sensitive cards and paper tape but usually to a select few students. In 1981, the BBC produced a micro-computer and classroom network and Computer Studies became common for GCE O level students (11–16-year-old), and Computer Science to A level students. Its importance was recognised, and it became a compulsory part of the National Curriculum, for Key Stage 3 & 4. In September 2014 it became an entitlement for all pupils over the age of 4.In the US, with 14,000 school districts deciding the curriculum, provision was fractured. According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula, and several others are following.

See also
Notes
References
Further reading
External links
Computer science at Curlie
Scholarly Societies in Computer Science
What is Computer Science?
Best Papers Awards in Computer Science since 1996
Photographs of computer scientists by Bertrand Meyer
EECS.berkeley.edu

Bibliography and academic search engines
CiteSeerx (article): search engine, digital library and repository for scientific and academic papers with a focus on computer and information science.
DBLP Computer Science Bibliography (article): computer science bibliography website hosted at Universität Trier, in Germany.
The Collection of Computer Science Bibliographies (Collection of Computer Science Bibliographies)

Professional organizations
Association for Computing Machinery
IEEE Computer Society
Informatics Europe
AAAI
AAAS Computer Science

Misc
Computer Science—Stack Exchange: a community-run question-and-answer site for computer science
What is computer science
Is computer science science?
Computer Science (Software) Must be Considered as an Independent Discipline.",https://en.wikipedia.org/wiki/Computer_science,"['All articles with unsourced statements', 'Articles with Curlie links', 'Articles with short description', 'Articles with unsourced statements from October 2010', 'CS1 German-language sources (de)', 'Computer engineering', 'Computer science', 'Formal sciences', 'Pages using Sister project links with hidden wikidata', 'Pages using Sister project links with wikidata namespace mismatch', 'Short description is different from Wikidata', 'Use mdy dates from October 2017', 'Webarchive template wayback links', 'Wikipedia articles with BNE identifiers', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with HDS identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia indefinitely move-protected pages']",Data Science
29,Conference on Neural Information Processing Systems,"The Conference and Workshop on Neural Information Processing Systems (abbreviated as NeurIPS and formerly NIPS)  is a machine learning and computational neuroscience conference held every December.  The conference is currently a double-track meeting (single-track until 2015) that includes invited talks as well as oral and poster presentations of refereed papers, followed by parallel-track workshops that up to 2013 were held at ski resorts.

History
The NeurIPS meeting was first proposed in 1986 as NIPS at the annual invitation-only Snowbird Meeting on Neural Networks for Computing organized by The California Institute of Technology and Bell Laboratories.  NIPS was designed as a complementary open interdisciplinary meeting for researchers exploring biological and artificial Neural Networks.  Reflecting this multidisciplinary approach, NIPS began in 1987 with information theorist Ed Posner as the conference president and learning theorist Yaser Abu-Mostafa as program chairman. Research presented in the early NIPS meetings included a wide range of topics from efforts to solve purely engineering problems to the use of computer models as a tool for understanding biological nervous systems.  Since then, the biological and artificial systems research streams have diverged, and recent NeurIPS proceedings have been dominated by papers on machine learning, artificial intelligence and statistics.
From 1987 until 2000 NIPS was held in Denver, United States. Since then, the conference was held in Vancouver, Canada (2001–2010), Granada, Spain (2011), and Lake Tahoe, United States (2012–2013). In 2014 and 2015, the conference was held in Montreal, Canada, in Barcelona, Spain in 2016, in Long Beach, United States in 2017, in Montreal, Canada in 2018 and Vancouver, Canada in 2019.  Reflecting its origins at Snowbird, Utah, the meeting was accompanied by workshops organized at a nearby ski resort up until 2013, when it outgrew ski resorts.
The first NeurIPS Conference was sponsored by the IEEE. The following NeurIPS Conferences have been organized by the NeurIPS Foundation, established by Ed Posner. Terrence Sejnowski has been the president of the NeurIPS Foundation since Posner's death in 1993. The board of trustees consists of previous general chairs of the NeurIPS Conference.
The first proceedings was published in book form by the American Institute of Physics in 1987, and was entitled Neural Information Processing Systems, then the proceedings from the following conferences have been published by Morgan Kaufmann (1988–1993), MIT Press (1994–2004) and Curran Associates (2005–present) under the name Advances in Neural Information Processing Systems.
The conference was originally abbreviated as ""NIPS"". By 2018 commentators were criticizing the abbreviation as encouraging sexism due to its association with the word nipples, and as being a slur against Japanese. The board changed the abbreviation to ""NeurIPS"" in Novemember 2018.

Topics
Along with machine learning and neuroscience, other fields represented at NeurIPS include cognitive science, psychology, computer vision, statistical linguistics, and information theory. Over the years, NeurIPS became a premier conference on machine learning and although the 'Neural' in the NeurIPS acronym had become something of a historical relic, the resurgence of deep learning in neural networks since 2012, fueled by faster computers and big data, has led to achievements in speech recognition, object recognition in images, image captioning, language translation and world championship performance in the game of Go, based on neural architectures inspired by the hierarchy of areas in the visual cortex (ConvNet) and reinforcement learning inspired by the basal ganglia (Temporal difference learning).

Named lectures
In addition to invited talks and symposia, NeurIPS also organizes two named lectureships to recognize distinguished researchers. The NeurIPS Board introduced the Posner Lectureship in honor of NeurIPS founder Ed Posner; two Posner Lectures were given each year up to 2015. Past lecturers have included:

2018 – Joëlle Pineau
2017 – John Platt
2016 – Yann LeCun
2015 – Zoubin Ghahramani and Vladimir Vapnik
2014 – Michael Kearns and John Hopfield
2013 – Daphne Koller and Peter Dayan
2012 – Thomas Dietterich and Terry Sejnowski
2011 – Rich Sutton and Bernhard Scholkopf
2010 – Josh Tenenbaum and Michael I. JordanIn 2015, the NeurIPS Board introduced the Breiman Lectureship to highlight work in statistics relevant to conference topics. The lectureship was named for statistician Leo Breiman, who served on the NeurIPS Board from 1994 to 2005. Past lecturers have included:

2019 – Bin Yu
2018 – David Spiegelhalter
2017 – Yee Whye Teh
2016 – Susan Holmes
2015 – Robert Tibshirani

The NIPS experiment
In NIPS 2014, the program chairs duplicated 10% of all submissions and sent them through separate reviewers to evaluate randomness in the reviewing process. Several researchers interpreted the result. Regarding whether the decision in NIPS is completely random or not, John Langford writes: ""Clearly not—a purely random decision would have arbitrariness of ~78%. It is, however, quite notable that 60% is much closer to 78% than 0%."" He concludes that the result of the reviewing process is mostly arbitrary.

Editions
Past editions:

 1987–2000: Denver, Colorado, United States
 2001–2010: Vancouver, British Columbia, Canada
 2011: Granada, Spain, EU
 2012 & 2013: Stateline, Nevada, United States
 2014 & 2015: Montréal, Quebec, Canada
 2016: Barcelona, Spain, EU 
 2017: Long Beach, California, United States
 2018: Montréal, Quebec, Canada
 2019: Vancouver, British Columbia, CanadaFuture editions:

 2020: Vancouver, British Columbia, Canada
 2021: Sydney, New South Wales, Australia

See also
CIBB
COSYNE
ICLR
ICML

Notes
External links
2019 Conference
NeurIPS proceedings
NIPS 2011 video lectures
NIPS 2012 video lectures
Video Journal of Machine Learning Abstracts – Volume 3",https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems,"['Artificial intelligence conferences', 'Computational neuroscience', 'International conferences in Canada', 'Neuroscience conferences', 'Signal processing conferences']",Data Science
30,DBSCAN,"Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996.
It is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).
DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.In 2014, the algorithm was awarded the test of time award (an award given to algorithms which have received substantial attention in theory and practice) at the leading data mining conference, ACM SIGKDD. As of July 2020, the follow-up paper ""DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN"" appears in the list of the 8 most downloaded articles of the prestigious ACM Transactions on Database Systems (TODS) journal.

History
In 1972, Robert F. Ling published a closely related algorithm in ""The Theory and Construction of k-Clusters"" in The Computer Journal with an estimated runtime complexity of O(n³). DBSCAN has a worst-case of O(n²), and the database-oriented range-query formulation of DBSCAN allows for index acceleration. The algorithms slightly differ in their handling of border points.

Preliminary
Consider a set of points in some space to be clustered. Let ε be a parameter specifying the radius of a neighborhood with respect to some point. For the purpose of DBSCAN clustering, the points are classified as core points, (density-) reachable points and outliers, as follows:

A point p is a core point if at least minPts points are within distance ε of it (including p).
A point q is directly reachable from p if point q is within distance ε from core point p. Points are only said to be directly reachable from core points.
A point q is reachable from p if there is a path p1, ..., pn with p1 = p and pn = q, where each pi+1 is directly reachable from pi. Note that this implies that the initial point and all points on the path must be core points, with the possible exception of q.
All points not reachable from any other point are outliers or noise points.Now if p is a core point, then it forms a cluster together with all points (core or non-core) that are reachable from it. Each cluster contains at least one core point; non-core points can be part of a cluster, but they form its ""edge"", since they cannot be used to reach more points.

Reachability is not a symmetric relation: by definition, only core points can reach non-core points. The opposite is not true, so a non-core point may be reachable, but nothing can be reached from it. Therefore, a further notion of connectedness is needed to formally define the extent of the clusters found by DBSCAN. Two points p and q are density-connected if there is a point o such that both p and q are reachable from o. Density-connectedness is symmetric.
A cluster then satisfies two properties:

All points within the cluster are mutually density-connected.
If a point is density-reachable from some point of the cluster, it is part of the cluster as well.

Algorithm
Original query-based algorithm
DBSCAN requires two parameters: ε (eps) and the minimum number of points required to form a dense region (minPts). It starts with an arbitrary starting point that has not been visited. This point's ε-neighborhood is retrieved, and if it contains sufficiently many points, a cluster is started. Otherwise, the point is labeled as noise. Note that this point might later be found in a sufficiently sized ε-environment of a different point and hence be made part of a cluster.
If a point is found to be a dense part of a cluster, its ε-neighborhood is also part of that cluster. Hence, all points that are found within the ε-neighborhood are added, as is their own ε-neighborhood when they are also dense. This process continues until the density-connected cluster is completely found. Then, a new unvisited point is retrieved and processed, leading to the discovery of a further cluster or noise.
DBSCAN can be used with any distance function (as well as similarity functions or other predicates). The distance function (dist) can therefore be seen as an additional parameter.
The algorithm can be expressed in pseudocode as follows:
DBSCAN(DB, distFunc, eps, minPts) {
    C := 0                                                  /* Cluster counter */
    for each point P in database DB {
        if label(P) ≠ undefined then continue               /* Previously processed in inner loop */
        Neighbors N := RangeQuery(DB, distFunc, P, eps)     /* Find neighbors */
        if |N| < minPts then {                              /* Density check */
            label(P) := Noise                               /* Label as Noise */
            continue
        }
        C := C + 1                                          /* next cluster label */
        label(P) := C                                       /* Label initial point */
        SeedSet S := N \ {P}                                /* Neighbors to expand */
        for each point Q in S {                             /* Process every seed point Q */
            if label(Q) = Noise then label(Q) := C          /* Change Noise to border point */
            if label(Q) ≠ undefined then continue           /* Previously processed (e.g., border point) */
            label(Q) := C                                   /* Label neighbor */
            Neighbors N := RangeQuery(DB, distFunc, Q, eps) /* Find neighbors */
            if |N| ≥ minPts then {                          /* Density check (if Q is a core point) */
                S := S ∪ N                                  /* Add new neighbors to seed set */
            }
        }
    }
}

where RangeQuery can be implemented using a database index for better performance, or using a slow linear scan:

RangeQuery(DB, distFunc, Q, eps) {
    Neighbors N := empty list
    for each point P in database DB {                      /* Scan all points in the database */
        if distFunc(Q, P) ≤ eps then {                     /* Compute distance and check epsilon */
            N := N ∪ {P}                                   /* Add to result */
        }
    }
    return N
}

Abstract algorithm
The DBSCAN algorithm can be abstracted into the following steps:
Find the points in the ε (eps) neighborhood of every point, and identify the core points with more than minPts neighbors.
Find the connected components of core points on the neighbor graph, ignoring all non-core points.
Assign each non-core point to a nearby cluster if the cluster is an ε (eps) neighbor, otherwise assign it to noise.A naive implementation of this requires storing the neighborhoods in step 1, thus requiring substantial memory. The original DBSCAN algorithm does not require this by performing these steps for one point at a time.

Complexity
DBSCAN visits each point of the database, possibly multiple times (e.g., as candidates to different clusters). For practical considerations, however, the time complexity is mostly governed by the number of regionQuery invocations. DBSCAN executes exactly one such query for each point, and if an indexing structure is used that executes a neighborhood query in O(log n), an overall average runtime complexity of O(n log n) is obtained (if parameter ε is chosen in a meaningful way, i.e. such that on average only O(log n) points are returned). Without the use of an accelerating index structure, or on degenerated data (e.g. all points within a distance less than ε), the worst case run time complexity remains O(n²). The distance matrix of size (n²-n)/2 can be materialized to avoid distance recomputations, but this needs O(n²) memory, whereas a non-matrix based implementation of DBSCAN only needs O(n) memory.

Advantages
DBSCAN does not require one to specify the number of clusters in the data a priori, as opposed to k-means.
DBSCAN can find arbitrarily-shaped clusters. It can even find a cluster completely surrounded by (but not connected to) a different cluster. Due to the MinPts parameter, the so-called single-link effect (different clusters being connected by a thin line of points) is reduced.
DBSCAN has a notion of noise, and is robust to outliers.
DBSCAN requires just two parameters and is mostly insensitive to the ordering of the points in the database. (However, points sitting on the edge of two different clusters might swap cluster membership if the ordering of the points is changed, and the cluster assignment is unique only up to isomorphism.)
DBSCAN is designed for use with databases that can accelerate region queries, e.g. using an R* tree.
The parameters minPts and ε can be set by a domain expert, if the data is well understood.

Disadvantages
DBSCAN is not entirely deterministic: border points that are reachable from more than one cluster can be part of either cluster, depending on the order the data are processed. For most data sets and domains, this situation does not arise often and has little impact on the clustering result: both on core points and noise points, DBSCAN is deterministic. DBSCAN* is a variation that treats border points as noise, and this way achieves a fully deterministic result as well as a more consistent statistical interpretation of density-connected components.
The quality of DBSCAN depends on the distance measure used in the function regionQuery(P,ε). The most common distance metric used is Euclidean distance. Especially for high-dimensional data, this metric can be rendered almost useless due to the so-called ""Curse of dimensionality"", making it difficult to find an appropriate value for ε. This effect, however, is also present in any other algorithm based on Euclidean distance.
DBSCAN cannot cluster data sets well with large differences in densities, since the minPts-ε combination cannot then be chosen appropriately for all clusters.
If the data and scale are not well understood, choosing a meaningful distance threshold ε can be difficult.See the section below on extensions for algorithmic modifications to handle these issues.

Parameter estimation
Every data mining task has the problem of parameters. Every parameter influences the algorithm in specific ways. For DBSCAN, the parameters ε and minPts are needed. The parameters must be specified by the user. Ideally, the value of ε is given by the problem to solve (e.g. a physical distance), and minPts is then the desired minimum cluster size.
MinPts: As a rule of thumb, a minimum minPts can be derived from the number of dimensions D in the data set, as minPts ≥ D + 1. The low value of minPts = 1 does not make sense, as then every point on its own will already be a cluster. With minPts ≤ 2, the result will be the same as of hierarchical clustering with the single link metric, with the dendrogram cut at height ε. Therefore, minPts must be chosen at least 3. However, larger values are usually better for data sets with noise and will yield more significant clusters. As a rule of thumb, minPts = 2·dim can be used, but it may be necessary to choose larger values for very large data, for noisy data or for data that contains many duplicates.
ε: The value for ε can then be chosen by using a k-distance graph, plotting the distance to the k = minPts-1 nearest neighbor ordered from the largest to the smallest value. Good values of ε are where this plot shows an ""elbow"": if ε is chosen much too small, a large part of the data will not be clustered; whereas for a too high value of ε, clusters will merge and the majority of objects will be in the same cluster. In general, small values of ε are preferable, and as a rule of thumb only a small fraction of points should be within this distance of each other. Alternatively, an OPTICS plot can be used to choose ε, but then the OPTICS algorithm itself can be used to cluster the data.
Distance function: The choice of distance function is tightly coupled to the choice of ε, and has a major impact on the results. In general, it will be necessary to first identify a reasonable measure of similarity for the data set, before the parameter ε can be chosen. There is no estimation for this parameter, but the distance functions needs to be chosen appropriately for the data set. For example, on geographic data, the great-circle distance is often a good choice.OPTICS can be seen as a generalization of DBSCAN that replaces the ε parameter with a maximum value that mostly affects performance. MinPts then essentially becomes the minimum cluster size to find. While the algorithm is much easier to parameterize than DBSCAN, the results are a bit more difficult to use, as it will usually produce a hierarchical clustering instead of the simple data partitioning that DBSCAN produces.
Recently, one of the original authors of DBSCAN has revisited DBSCAN and OPTICS, and published a refined version of hierarchical DBSCAN (HDBSCAN*), which no longer has the notion of border points. Instead, only the core points form the cluster.

Relationship to spectral clustering
DBSCAN can be seen as special (efficient) variant of spectral clustering: Connected components correspond to optimal spectral clusters (no edges cut – spectral clustering tries to partition the data with a minimum cut); DBSCAN finds connected components on the (asymmetric) reachability graph. However, spectral clustering can be computationally intensive (up to 
  
    
      
        O
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle O(n^{3})}
   without approximation and further assumptions), and one has to choose the number of clusters 
  
    
      
        k
      
    
    {\displaystyle k}
   for both the number of eigenvectors to choose and the number of clusters to produce with k-means on the spectral embedding. Thus, for performance reasons, the original DBSCAN algorithm remains preferable to a spectral implementation, and this relationship is so far only of theoretical interest.

Extensions
Generalized DBSCAN (GDBSCAN) is a generalization by the same authors to arbitrary ""neighborhood"" and ""dense"" predicates. The ε and minPts parameters are removed from the original algorithm and moved to the predicates. For example, on polygon data, the ""neighborhood"" could be any intersecting polygon, whereas the density predicate uses the polygon areas instead of just the object count.
Various extensions to the DBSCAN algorithm have been proposed, including methods for parallelization, parameter estimation, and support for uncertain data. The basic idea has been extended to hierarchical clustering by the OPTICS algorithm. DBSCAN is also used as part of subspace clustering algorithms like PreDeCon and SUBCLU. HDBSCAN is a hierarchical version of DBSCAN which is also faster than OPTICS, from which a flat partition consisting of the most prominent clusters can be extracted from the hierarchy.

Availability
Different implementations of the same algorithm were found to exhibit enormous performance differences, with the fastest on a test data set finishing in 1.4 seconds, the slowest taking 13803 seconds. The differences can be attributed to implementation quality, language and compiler differences, and the use of indexes for acceleration.

Apache Commons Math contains a Java implementation of the algorithm running in quadratic time.
ELKI offers an implementation of DBSCAN as well as GDBSCAN and other variants. This implementation can use various index structures for sub-quadratic runtime and supports arbitrary distance functions and arbitrary data types, but it may be outperformed by low-level optimized (and specialized) implementations on small data sets.
mlpack includes an implementation of DBSCAN accelerated with dual-tree range search techniques.
PostGIS includes ST_ClusterDBSCAN – a 2D implementation of DBSCAN that uses R-tree index. Any geometry type is supported, e.g. Point, LineString, Polygon, etc.
R contains implementations of DBSCAN in the packages dbscan and fpc. Both packages support arbitrary distance functions via distance matrices. The package fpc does not have index support (and thus has quadratic runtime and memory complexity) and is rather slow due to the R interpreter. The package dbscan provides a fast C++ implementation using k-d trees (for Euclidean distance only) and also includes implementations of DBSCAN*, HDBSCAN*, OPTICS, OPTICSXi, and other related methods.
scikit-learn includes a Python implementation of DBSCAN for arbitrary Minkowski metrics, which can be accelerated using k-d trees and ball trees but which uses worst-case quadratic memory. A contribution to scikit-learn provides an implementation of the HDBSCAN* algorithm.
pyclustering library includes a Python and C++ implementation of DBSCAN for Euclidean distance only as well as OPTICS algorithm.
SPMF includes an implementation of the DBSCAN algorithm with k-d tree support for Euclidean distance only.
Weka contains (as an optional package in latest versions) a basic implementation of DBSCAN that runs in quadratic time and linear memory.

Notes


== References ==",https://en.wikipedia.org/wiki/DBSCAN,"['All accuracy disputes', 'All articles containing potentially dated statements', 'Articles containing potentially dated statements from July 2020', 'Articles with disputed statements from September 2019', 'Articles with short description', 'CS1 maint: archived copy as title', 'Cluster analysis algorithms', 'Short description is different from Wikidata']",Data Science
31,Convolutional neural network,"In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on the shared-weight architecture of the convolution kernels that shift over input features and provide translation equivariant responses.  Counter-intuitively, most convolutional neural networks are only equivariant, as opposed to invariant, to translation. They have applications in image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain-computer interfaces, and financial time series.CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The ""full connectivity"" of these networks make them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.)  CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble patterns of increasing complexity using smaller and simpler patterns embossed in their filters. Therefore, on a scale of connectivity and complexity, CNNs are on the lower extreme.
Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.
CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.

Definition
The name “convolutional neural network” indicates that the network employs a mathematical operation called convolution.
Convolutional networks are a specialized type of neural networks that use convolution in place of general matrix multiplication in at least one of their layers.

Architecture
A convolutional neural network consists of an input layer, hidden layers and an output layer. In any feed-forward neural network, any middle layers are called hidden because their inputs and outputs are masked by the activation function and final convolution. In a convolutional neural network, the hidden layers include layers that perform convolutions. Typically this includes a layer that does multiplication or other dot product, and its activation function is commonly ReLU. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers.

Convolutional layers
In a CNN, the input is a tensor with a shape: (number of inputs) x (input height) x (input width) x (input channels). After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape: (number of inputs) x (feature map height) x (feature map width) x (feature map channels). A convolutional layer within a CNN generally has the following attributes:

Convolutional filters/kernels defined by a width and height (hyper-parameters).
The number of input channels and output channels (hyper-parameters).  One layer's input channels must equal the number of output channels (also called depth) of its input.
Additional hyperparameters of the convolution operation, such as: padding, stride, and dilation.Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field. Although fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs such as high resolution images. It would require a very high number of neurons, even in a shallow architecture, due to the large input size of images, where each pixel is a relevant input feature. For instance, a fully connected layer for a (small) image of size 100 x 100 has 10,000 weights for each neuron in the second layer. Instead, convolution reduces the number of free parameters, allowing the network to be deeper. For example, regardless of image size, using a 5 x 5 tiling region, each with the same shared weights, requires only 25 learnable parameters. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during backpropagation in traditional neural networks.  Furthermore, convolutional neural networks are ideal for data with a grid-like topology (such as images) as spatial relations between separate features are taken into account during convolution and/or pooling.

Pooling layers
Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 x 2 are commonly used. Global pooling acts on all the neurons of the feature map. There are two common types of pooling in popular use:  max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map, while average pooling takes the average value.

Fully connected layers
Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multi-layer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.

Receptive field
In neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is the entire previous layer. Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes into account the value of a pixel, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers.

Weights
Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights.
The vector of weights and the bias are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.

History
CNN design follows vision processing in living organisms.

Receptive fields in the visual cortex
Work by Hubel and Wiesel in the 1950s and 1960s showed that cat and monkey visual cortexes contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field. Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. The cortex in each hemisphere represents the contralateral visual field.Their 1968 paper identified two basic visual cell types in the brain:
simple cells, whose output is maximized by straight edges having particular orientations within their receptive field
complex cells, which have larger receptive fields, whose output is insensitive to the exact position of the edges in the field.Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.

Neocognitron, origin of the CNN architecture
The ""neocognitron"" was introduced by Kunihiko Fukushima in 1980.
It was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers, and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.
In a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging, J. Weng et al. introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. Today, however, the CNN architecture is usually trained through backpropagation.
The neocognitron is the first CNN which requires units located at multiple network positions to have shared weights.
Convolutional neural networks were presented at the Neural Information Processing Workshop in 1987, automatically analyzing time-varying signals by replacing learned multiplication with convolution in time, and demonstrated for speech recognition.

Time delay neural networks
The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. and was the first convolutional network, as it achieved shift invariance. It did so by utilizing weight sharing in combination with Backpropagation training. Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.TDNNs are convolutional networks that share weights along the temporal dimension. They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant which performs a two dimensional convolution. Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both shifts in time and in frequency. This inspired translation invariance in image processing with CNNs. The tiling of neuron outputs can cover timed stages.TDNNs now achieve the best performance in far distance speech recognition.

Max pooling
In 1990 Yamaguchi et al. introduced the concept of max pooling, which is a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling in order to realize a speaker independent isolated word recognition system. In their system they used several TDNNs per word, one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.

Image recognition with CNNs trained by gradient descent
A system to recognize hand-written ZIP Code numbers involved convolutions in which the kernel coefficients had been laboriously hand designed.Yann LeCun et al. (1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types.
This approach became a foundation of modern computer vision.

LeNet-5
LeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1998, that classifies digits, was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.

Shift-invariant neural network
Similarly, a shift invariant neural network was proposed by W. Zhang et al. for image character recognition in 1988. The architecture and training algorithm were modified in 1991 and applied for medical image processing and automatic detection of breast cancer in mammograms.A different convolution-based design was proposed in 1988 for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.

Neural abstraction pyramid
The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.

GPU implementations
Although CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).
In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU. In 2005, another paper also emphasised the value of GPGPU for machine learning.The first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU. Subsequent work also used GPUs, initially for other types of neural networks (different from CNNs), especially unsupervised neural networks.In 2010, Dan Ciresan et al. at IDSIA showed that even deep standard neural networks with many layers can be quickly trained on GPU by supervised learning through the old method known as backpropagation. Their network outperformed previous machine learning methods on the MNIST handwritten digits benchmark. In 2011, they extended this GPU approach to CNNs, achieving an acceleration factor of 60, with impressive results. In 2011, they used such CNNs on GPU to win an image recognition contest where they achieved superhuman performance for the first time. Between May 15, 2011 and September 30, 2012, their CNNs won no less than four image competitions. In 2012, they also significantly improved on the best performance in the literature for multiple image databases, including the MNIST database, the NORB database, the HWDB1.0 dataset (Chinese characters) and the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images).Subsequently, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012. A very deep CNN with over 100 layers by Microsoft won the ImageNet 2015 contest.

Intel Xeon Phi implementations
Compared to the training of CNNs using GPUs, not much attention was given to the Intel Xeon Phi coprocessor.
A notable development is a parallelization method for training convolutional neural networks on the Intel Xeon Phi, named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS).
CHAOS exploits both the thread- and SIMD-level parallelism that is available on the Intel Xeon Phi.

Distinguishing features
In the past, traditional multilayer perceptron (MLP) models were used for image recognition. However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights, which is too high to feasibly process efficiently at scale with full connectivity.

For example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.
Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.
Convolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:

3D volumes of neurons. The layers of a CNN have neurons arranged in 3 dimensions: width, height and depth. Where each neuron inside a convolutional layer is connected to only a small region of the layer before it, called a receptive field. Distinct types of layers, both locally and completely connected, are stacked to form a CNN architecture.
Local connectivity: following the concept of receptive fields, CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers. The architecture thus ensures that the learned ""filters"" produce the strongest response to a spatially local input pattern. Stacking many such layers leads to non-linear filters that become increasingly global (i.e. responsive to a larger region of pixel space) so that the network first creates representations of small parts of the input, then from them assembles representations of larger areas.
Shared weights: In CNNs, each filter is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature within their specific response field. Replicating units in this way allows for the resulting activation map to be equivariant under shifts of the locations of input features in the visual field, i.e. they grant translational equivariance - given that the layer has a stride of one.
Pooling: In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value. In addition to reducing the sizes of feature maps, the pooling operation grants a degree of local translational invariance to the features contained therein, allowing the CNN to be more robust to variations in their positions.Together, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.

Building blocks
A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below.

Convolutional layer
The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the filter entries and the input, producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map.

Local connectivity
When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.
The extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.

Spatial arrangement
Three hyperparameters control the size of the output volume of the convolutional layer: the depth, stride and padding size.

The depth of the output volume controls the number of neurons in a layer that connect to the same region of the input volume. These neurons learn to activate for different features in the input. For example, if the first convolutional layer takes the raw image as input, then different neurons along the depth dimension may activate in the presence of various oriented edges, or blobs of color.
Stride controls how depth columns around the width and height are allocated. If the stride is 1, then we move the filters one pixel at a time. This leads to heavily overlapping receptive fields between the columns, and to large output volumes. For any integer 
  
    
      
        S
        >
        0
        ,
      
    
    {\textstyle S>0,}
   a stride S means that the filter is translated S units at a time per output. In practice, 
  
    
      
        S
        ≥
        3
      
    
    {\textstyle S\geq 3}
   is rare. A greater stride means smaller overlap of receptive fields and smaller spacial dimensions of the output volume.
Sometimes, it is convenient to pad the input with zeros (or other values, such as the average of the region) on the border of the input volume. The size of this padding is a third hyperparameter. Padding provides control of the output volume's spatial size. In particular, sometimes it is desirable to exactly preserve the spatial size of the input volume, this is commonly referred to as ""same"" padding.The spatial size of the output volume is a function of the input volume size 
  
    
      
        W
      
    
    {\displaystyle W}
  , the kernel field size 
  
    
      
        K
      
    
    {\displaystyle K}
   of the convolutional layer neurons, the stride 
  
    
      
        S
      
    
    {\displaystyle S}
  , and the amount of zero padding 
  
    
      
        P
      
    
    {\displaystyle P}
   on the border. The number of neurons that ""fit"" in a given volume is then:

If this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be 
  
    
      
        P
        =
        (
        K
        −
        1
        )
        
          /
        
        2
      
    
    {\textstyle P=(K-1)/2}
   when the stride is 
  
    
      
        S
        =
        1
      
    
    {\displaystyle S=1}
   ensures that the input volume and output volume will have the same size spatially. However, it is not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.

Parameter sharing
A parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a depth slice, the neurons in each depth slice are constrained to use the same weights and bias.
Since all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume. Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.Sometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a ""locally connected layer"".

Pooling layer
Another important concept of CNNs is pooling, which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling, where max pooling is the most common. It partitions the input image into a set of rectangles and, for each such sub-region, outputs the maximum.
Intuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting.  This is known as down-sampling It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as a ReLU layer) in a CNN architecture. While pooling layers contribute to local translation invariance, they do not provide global translation invariance in a CNN, unless a form of global pooling is used.  The pooling layer commonly operates independently on every depth, or slice, of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2×2, applied with a stride of 2, which subsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations:
In this case, every max operation is over 4 numbers. The depth dimension remains unchanged (this is true for other forms of pooling as well).
In addition to max pooling, pooling units can use other functions, such as average pooling or ℓ2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which generally performs better in practice.Due to the effects of fast spatial reduction of the size of the representation, there is a recent trend towards using smaller filters or discarding pooling layers altogether.

""Region of Interest"" pooling (also known as RoI pooling) is a variant of max pooling, in which output size is fixed and input rectangle is a parameter.Pooling is an important component of convolutional neural networks for object detection based on the Fast R-CNN architecture.

ReLU layer
ReLU is the abbreviation of rectified linear unit, which applies the non-saturating activation function 
  
    
      
        f
        (
        x
        )
        =
        max
        (
        0
        ,
        x
        )
      
    
    {\textstyle f(x)=\max(0,x)}
  . It effectively removes negative values from an activation map by setting them to zero. It introduces nonlinearities to the decision function and in the overall network without affecting the receptive fields of the convolution layers.
Other functions can also be used o increase nonlinearity, for example the saturating hyperbolic tangent 
  
    
      
        f
        (
        x
        )
        =
        tanh
        ⁡
        (
        x
        )
      
    
    {\displaystyle f(x)=\tanh(x)}
  , 
  
    
      
        f
        (
        x
        )
        =
        
          |
        
        tanh
        ⁡
        (
        x
        )
        
          |
        
      
    
    {\displaystyle f(x)=|\tanh(x)|}
  , and the sigmoid function 
  
    
      
        σ
        (
        x
        )
        =
        (
        1
        +
        
          e
          
            −
            x
          
        
        
          )
          
            −
            1
          
        
      
    
    {\textstyle \sigma (x)=(1+e^{-x})^{-1}}
  . ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.

Fully connected layer
After several convolutional and max pooling layers, the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).

Loss layer
The ""loss layer"", or ""loss function"", specifies how training penalizes the deviation between the predicted output of the network, and the true data labels (during supervised learning). Various loss functions can be used, depending on the specific task.
The Softmax loss function is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in 
  
    
      
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle [0,1]}
  . Euclidean loss is used for regressing to real-valued labels 
  
    
      
        (
        −
        ∞
        ,
        ∞
        )
      
    
    {\displaystyle (-\infty ,\infty )}
  .

Choosing hyperparameters
CNNs use more hyperparameters than a standard multilayer perceptron (MLP). While the usual rules for learning rates and regularization constants still apply, the following should be kept in mind when optimizing.

Number of filters
Since feature map size decreases with depth, layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.
The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.

Filter size
Common filter sizes found in the literature vary greatly, and are usually chosen based on the data set.
The challenge is to find the right level of granularity so as to create abstractions at the proper scale, given a particular data set, and without overfitting.

Pooling type and size
In modern CNNs, max pooling is typically used, and often of size 2×2, with a stride of 2.  This implies that the input is drastically downsampled, further improving the computational efficiency. 
Very large input volumes may warrant 4×4 pooling in the lower layers. However, choosing larger shapes will dramatically reduce the dimension of the signal, and may result in excess information loss. Often, non-overlapping pooling windows perform best.

Translation Equivariance
It is commonly assumed that CNNs are invariant to shifts of the input.  However, convolution or pooling layers within a CNN that do not have a stride greater than one are equivariant, as opposed to invariant, to translations of the input. Layers with a stride greater than one ignores the Nyquist-Shannon sampling theorem, and leads to aliasing of the input signal, which breaks the equivariance (also referred to as covariance) property. Furthermore, if a CNN makes use of fully connected layers, translation equivariance does not imply translation invariance, as the fully connected layers are not invariant to shifts of the input.  One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer. Additionally, several other partial solutions have been proposed, such as anti-aliasing, spatial transformer networks, data augmentation, subsampling combined with pooling, and capsule neural networks.

Regularization methods
Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.

Empirical
Dropout
Because a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is dropout. At each training stage, individual nodes are either ""dropped out"" of the net (ignored) with probability 
  
    
      
        1
        −
        p
      
    
    {\displaystyle 1-p}
   or kept with probability 
  
    
      
        p
      
    
    {\displaystyle p}
  , so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.
In the training stages, 
  
    
      
        p
      
    
    {\displaystyle p}
   is usually 0.5; for input nodes, it is typically much higher because information is directly lost when input nodes are ignored.
At testing time after training has finished, we would ideally like to find a sample average of all possible 
  
    
      
        
          2
          
            n
          
        
      
    
    {\displaystyle 2^{n}}
   dropped-out networks; unfortunately this is unfeasible for large values of 
  
    
      
        n
      
    
    {\displaystyle n}
  . However, we can find an approximation by using the full network with each node's output weighted by a factor of 
  
    
      
        p
      
    
    {\displaystyle p}
  , so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates 
  
    
      
        
          2
          
            n
          
        
      
    
    {\displaystyle 2^{n}}
   neural nets, and as such allows for model combination, at test time only a single network needs to be tested.
By avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even for deep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features that better generalize to new data.

DropConnect
DropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability 
  
    
      
        1
        −
        p
      
    
    {\displaystyle 1-p}
  . Each unit thus receives input from a random subset of units in the previous layer.DropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.

Stochastic pooling
A major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.
In stochastic pooling, the conventional deterministic pooling operations are replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation.
An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images, which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.

Artificial data
Because the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Because these networks are usually trained with all available data, one approach is to either generate new data from scratch (if possible) or perturb existing data to create new ones. For example, input images could be asymmetrically cropped by a few percent to create new examples with the same label as the original.

Explicit
Early stopping
One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.

Number of parameters
Another simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a ""zero norm"".

Weight decay
A simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter), thus increasing the penalty for large weight vectors.
L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.
L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called Elastic net regularization.

Max norm constraints
Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector 
  
    
      
        
          
            
              w
              →
            
          
        
      
    
    {\displaystyle {\vec {w}}}
   of every neuron to satisfy 
  
    
      
        ‖
        
          
            
              w
              →
            
          
        
        
          ‖
          
            2
          
        
        <
        c
      
    
    {\displaystyle \|{\vec {w}}\|_{2}<c}
  . Typical values of 
  
    
      
        c
      
    
    {\displaystyle c}
   are order of 3–4. Some papers report improvements when using this form of regularization.

Hierarchical coordinate frames
Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.An earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.Thus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (""pose vectors"") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.

Applications
Image recognition
CNNs are often used in image recognition systems. In 2012 an error rate of 0.23% on the MNIST database was reported. Another paper on using CNN for image classification reported that the learning process was ""surprisingly fast""; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database. Subsequently, a similar CNN called 
AlexNet won the ImageNet Large Scale Visual Recognition Challenge 2012.
When applied to facial recognition, CNNs achieved a large decrease in error rate. Another paper reported a 97.6% recognition rate on ""5,600 still images of more than 10 subjects"". CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014, a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.In 2015 a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.

Video analysis
Compared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space. Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream. Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies. Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis.

Natural language processing
CNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing, search query retrieval, sentence modeling, classification, prediction and other traditional NLP tasks.

Anomaly Detection
A CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.

Drug discovery
CNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based rational drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures, AtomNet discovers chemical features, such as aromaticity, sp3 carbons and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus and multiple sclerosis.

Health risk assessment and biomarkers of aging discovery
CNNs can be naturally tailored to analyze a sufficiently large collection of time series data representing one-week-long human physical activity streams augmented by the rich clinical data (including the death register, as provided by, e.g., the NHANES study). A simple CNN was combined with Cox-Gompertz proportional hazards model and used to produce a proof-of-concept example of digital biomarkers of aging in the form of all-causes-mortality predictor.

Checkers game
CNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checker using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%. It also earned a win against the program Chinook at its ""expert"" level of play.

Go
CNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play. Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.A couple of CNNs for choosing moves to try (""policy network"") and evaluating positions (""value network"") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.

Time series forecasting
Recurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better. Dilated convolutions might enable one-dimensional convolutional neural networks to effectively learn time series dependences. Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients. Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from. CNNs can also be applied to further tasks in time series analysis (e.g., time series classification or quantile forecasting).

Cultural Heritage and 3D-datasets
As archaeological findings like clay tablets with cuneiform writing are increasingly acquired using 3D scanners first benchmark datasets are becoming available like HeiCuBeDa providing almost 2.000 normalized 2D- and 3D-datasets prepared with the GigaMesh Software Framework. So curvature based measures are used in conjunction with Geometric Neural Networks (GNNs) e.g. for period classification of those clay tablets being among the oldest documents of human history.

Fine-tuning
For many applications, the training data is less available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known as transfer learning.  Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.

Human interpretable explanations
End-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars. With recent advances in visual salience, spatial and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.

Related architectures
Deep Q-networks
A deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning.Preliminary results were presented in 2014, with an accompanying paper in February 2015. The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.

Deep belief networks
Convolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR have been obtained using CDBNs.

Notable libraries
Caffe: A library for convolutional neural networks. Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++, and has Python and MATLAB wrappers.
Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka.
Dlib: A toolkit for making real world machine learning and data analysis applications in C++.
Microsoft Cognitive Toolkit: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java.
TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU, Google's proprietary tensor processing unit (TPU), and mobile devices.
Theano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation.
Torch: A scientific computing framework with wide support for machine learning algorithms, written in C and Lua. The main author is Ronan Collobert, and it is now used at Facebook AI Research and Twitter.

Notable APIs
Keras: A high level API written in Python for TensorFlow and Theano convolutional neural networks.

See also
Attention (machine learning)
Convolution
Deep learning
Natural-language processing
Neocognitron
Scale-invariant feature transform
Time delay neural network
Vision processing unit

Notes
References
External links
CS231n: Convolutional Neural Networks for Visual Recognition — Andrej Karpathy's Stanford computer science course on CNNs in computer vision
An Intuitive Explanation of Convolutional Neural Networks — A beginner level introduction to what Convolutional Neural Networks are and how they work
Convolutional Neural Networks for Image Classification — Literature Survey",https://en.wikipedia.org/wiki/Convolutional_neural_network,"['All articles needing additional references', 'All articles needing examples', 'All articles with specifically marked weasel-worded phrases', 'All articles with unsourced statements', 'Articles containing explicitly cited British English-language text', 'Articles needing additional references from June 2017', 'Articles needing additional references from June 2019', 'Articles needing examples from October 2017', 'Articles with short description', 'Articles with specifically marked weasel-worded phrases from December 2018', 'Articles with unsourced statements from June 2019', 'Articles with unsourced statements from November 2020', 'Articles with unsourced statements from October 2017', 'Artificial neural networks', 'CS1 German-language sources (de)', 'CS1 errors: missing periodical', 'CS1 maint: extra punctuation', 'CS1 maint: multiple names: authors list', 'Computational neuroscience', 'Computer vision', 'Machine learning', 'Short description is different from Wikidata', 'Webarchive template wayback links', 'Wikipedia articles needing clarification from December 2018']",Data Science
32,DJ Patil,"Dhanurjay ""DJ"" Patil (born August 3, 1974) is an American mathematician and computer scientist who served as the Chief Data Scientist of the United States Office of Science and Technology Policy. from 2015 to 2017. He is the Head of Technology for Devoted Health. 
He previously served as the Vice President of Product at RelateIQ, which was acquired by Salesforce.com, as Chief Product Officer of Color Labs, and as Head of Data Products and Chief Scientist of LinkedIn. His father, Suhas Patil, is a venture capitalist and the founder of Cirrus Logic.

Early life and education
Patil attended Kennedy Middle School, Monta Vista High School and De Anza College, all in Cupertino, California. While at Kennedy, he participated in a student exchange program, visiting Toyokawa, Cupertino's sister city in Japan along with nine other students in 1988. In high school, often got into trouble and graduating in 1992 near the bottom of his class due to ""his hijinks and poor SAT scores."" He followed his girlfriend to De Anza Community College, where he attended for one year. He earned a B.A. in mathematics from Earl Warren College at University of California, San Diego. He received a PhD from University of Maryland College Park, in applied mathematics, which is part of the University of Maryland College of Computer, Mathematical, and Natural Sciences. He also served as a faculty member at the University of Maryland, and used open datasets published by NOAA to improve numerical weather forecasting.

Career
Patil has held positions at LinkedIn, Greylock Partners, Skype, PayPal, and eBay.

2004
In 2004, Patil worked in the Advanced Systems and Concepts Office in the Department of Defense. He served as the project leader for the Threat Anticipation Project. His role was to anticipate threats connected to and surrounding terrorism, weapons of mass destruction and failed states (with an emphasis on human rights violations), employing social network analysis to help anticipate these threats. Patil described himself as part of “the second wave of people” who were to use data to detect signalled noise, a concern which grew following the 9/11 attacks.

2011
While a data-scientist-in-residence at Greylock Partners, Patil produced “Building Data Science Teams.” The book provides advice and strategies on creating data science teams in business and technology. It was published by O'Reilly Media.

2012
Patil wrote “Data Jujitsu--The art of turning data into product,” also while a data-scientist-in-residence at Greylock Partners. The book gives instructions as to solving data science problems and whether they are “worth solving at all. It was also published by O'Reilly Media, Inc.

2015–2017
On February 18, 2015, the White House announced Patil the first U.S. Chief Data Scientist (Deputy Chief Technology Officer for Data Policy and Chief Data Scientist). In addresses to the public, Patil states that ""The mission of the U.S. Chief Data Scientist, simply put, is to responsibly unleash the power of data to benefit all Americans"" and, in addition, that his team's priority to do so by making data.In a memorandum on February 20, 2015, entitled “Unleashing the Power of Data to Serve the American People”, Patil outlined his goals as Chief Data Scientist was in: 

Providing vision on how to provide a maximum social return on federal data.
Creating nationwide data policies that enable shared services and forward-leaning practices to advance our nation's leadership in the data age.
Working with agencies to establish best practices for data management and ensure the long-term sustainability of databases.
Recruiting and retaining the best minds in data science for public service to address these data science objectives and act as conduits among the government, academia, and industry.In his tenure, Patil helped launch the White House's Police Data Initiative as well as the White House's Data-Driven Justice Initiative, collecting data on police activities, and worked on the Precision Medicine Initiative, aiming to build the largest database on genetic information.

Law Enforcement
In 2015, President Obama initiated The President's Task Force on 21st Century Policing in response to Ferguson shootings in 2014. Included were the Police Data Initiative and the Data-Driven Justice Initiative.

Police Data Initiative
The Police Data Initiative aimed to build trust with the police and their communities by releasing data sets on “stops and searches, uses of force, officer-involved shootings, or other police actions.”Open data was also thought to increase accountability within the police. The initiative had gathered 126 jurisdictions across the country.The information would enable investigation into patterns of injustice as well as the ability to see if systems in place were wanted or working. Police departments struggled to release their data, primarily because it was not “collected well,” as Patil had mentioned in an interview. Disorganized datasets have prevented accurate assessments. Patil proposed that this is due to the absence of a dedicated technician whose purpose is to organize the data. Efforts were taken to improve coding abilities of police superintendents and homogenizing organization across the 18,000 jurisdictions on board.More than 170 datasets were released at the start of the initiative in 2015.

Data-Driven Justice Initiative
With open police data, Patil saw that data helped track law enforcement actions’ disproportionate interactions with minorities, the poor and those with mental health concerns. In a memorandum to the American People, Patil said, “if all the data from law enforcement were effectively captured, analyzed, and shared, imagine how the effective and analysis of data could advance proven reforms, increase efficiency, and prevent injustice.”. Efforts to decrease the injustice is considered the Data-Driven Justice Initiative (DDJ). 
DDJ aimed to help governments from the county to the state level to filter low-level offenders and those with mental illness from higher level offenders and the criminal system at large. Changing the approach to pre-trial incarceration would allow low-risk offenders to shorten jail time especially if they cannot afford bail.
At its start in 2015, more than 91 million people are covered by the DDJ.

Medicine
Precision Medicine Initiative
Patil had a special interest in using data to improve the medical field by way of the Precision Medicine Initiative (PMI). PMI aimed to build the largest database of genomic information to increase understanding of cancer treatments, chronic disease, and rare diseases 
The initiative was started as there is not a current dataset in which analyses and correlations can be made across gender and ethnic diversities. This would help with curing diseases but also understand the health of the general population. Patil had often stated that it would be helpful because one may very well be able to see what diseases are not actually rare, but very common. A key issue in PMI that Patil worked on was of trust.
Considering vulnerability and the issue of ethics that arises in pursuing an initiative PMI, Patil sought to develop workshops and “listening sessions” to hear specific concerns regarding the initiative. Based on the feedback, Patil devised the “privacy and trust principles.”
The first principle in its section entitled “Data Sharing, Access, and Use”, is that “data access, use, and sharing should be permitted for authorized purposes only.""Concerned with maintaining these principles grew for the future as Patil addressed concerns about the effect of American Healthcare Act (AHCA) on contributions to PMI. He feared that, with the AHCA, people will be scared to donate their genomic information because it will expose pre-existing conditions. While there are protections in place to secure data privacy such as the Genetic Information Nondiscrimination Act (GINA) instilled in 2008, Patil expressed a worry that the Act would not be heeded.

2017–present
Patil was a member of the National Infrastructure Advisory Council before resigning in August 2017.In October 2017, it was reported that Patil joined the executive team of Devoted Health. He is currently the Head of Technology.On November 14, 2017, Patil announced that he was joining Venrock as an adviser.

References
External links
DJ Patil on Twitter",https://en.wikipedia.org/wiki/DJ_Patil,"['1969 births', 'All Wikipedia articles written in American English', 'Articles with short description', 'Commons category link from Wikidata', 'Data scientists', 'De Anza College alumni', 'Living people', 'Obama administration personnel', 'Office of Science and Technology Policy officials', 'People from Cupertino, California', 'Short description is different from Wikidata', 'University of California, San Diego alumni', 'University of Maryland, College Park alumni', 'University of Maryland, College Park faculty', 'Use American English from November 2017', 'Use mdy dates from November 2017']",Data Science
33,Data Science,"Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains. Data science is related to data mining, machine learning and big data.
Data science is a ""concept to unify statistics, data analysis, informatics, and their related methods"" in order to ""understand and analyze actual phenomena"" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. Turing award winner Jim Gray imagined data science as a ""fourth paradigm"" of science (empirical, theoretical, computational and now data-driven) and asserted that ""everything about science is changing because of the impact of information technology"" and the data deluge.

Foundations
Data science is an interdisciplinary field focused on extracting knowledge from data sets, which are typically large (see big data), and applying the knowledge and actionable insights from data to solve problems in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, information visualization, data integration, graphic design, complex systems, communication and business. Statistician Nathan Yau, drawing on Ben Fry, also links data science to human-computer interaction: users should be able to intuitively control and explore data. In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.

Relationship to statistics
Many statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics. Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g. images) and emphasizes prediction and action. Andrew Gelman of Columbia University and data scientist Vincent Granville have described statistics as a nonessential part of data science.
Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing, and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data science program. He describes data science as an applied field growing out of traditional statistics. 
In summary, data science can be therefore described as an applied branch of statistics.

Etymology
Early usage
In 1962, John Tukey described a field he called “data analysis,” which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C.F. Jeff Wu used the term Data Science for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.The term “data science” has been traced back to 1974, when Peter Naur proposed it as an alternative name for computer science. In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic. However, the definition was still in flux. After the 1985 lecture in the Chinese Academy of Sciences in Beijing, in 1997 C.F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting, or limited to describing data. In 1998, Chikio Hayashi argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.During the 1990s, popular terms for the process of finding patterns in datasets (which were increasingly large) included “knowledge discovery” and “data mining”.

Modern usage
The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland. In a 2001 paper, he advocated an expansion of statistics beyond theory into technical areas; because this would significantly change the field, it warranted a new name. ""Data science"" became more widely used in the next few years: in 2002, the Committee on Data for Science and Technology launched Data Science Journal. In 2003, Columbia University launched The Journal of Data Science. In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.The professional title of “data scientist” has been attributed to DJ Patil and Jeff Hammerbacher in 2008. Though it was used by the National Science Board in their 2005 report, ""Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century,"" it referred broadly to any key role in managing a digital data collection.There is still no consensus on the definition of data science and it is considered by some to be a buzzword.

Impact
Big data is very quickly becoming a vital tool for businesses and companies of all sizes. The availability and interpretation of big data has altered the business models of old industries and enabled the creation of new ones. Data-driven businesses are worth $1.2 trillion collectively in 2020, an increase from $333 billion in the year 2015. Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations. As big data continues to have a major impact on the world, data science does as well due to the close relationship between the two.

Technologies and techniques
There are a variety of different technologies and techniques that are used for data science which depend on the application. More recently, full-featured, end-to-end platforms have been developed and heavily used for data science and machine learning.

Techniques

Linear Regression
Logistic Regression
Decision tree is used as prediction models for classification and data fitting. The decision tree structure can be used to generate rules able to classify or predict target/class/label variable based on the observation attributes.
Support Vector Machine (SVM)
Clustering is a technique used to group data together.
Dimensionality reduction is used to reduce the complexity of data computation so that it can be performed more quickly.
Machine learning is a technique used to perform tasks by inferencing patterns from data


== References ==",https://en.wikipedia.org/wiki/Data_science,"['Articles with short description', 'CS1 maint: others', 'Computational fields of study', 'Computer occupations', 'Data analysis', 'Information science', 'Short description matches Wikidata', 'Use dmy dates from December 2012']",Data Science
34,Data (computing),"Data  (treated as singular, plural, or as a mass noun) is any sequence of one or more symbols. Datum is a single symbol of data. Data requires interpretation to become information. Digital data is data that is represented using the binary number system of ones (1) and zeros (0), as opposed to analog representation.  In modern (post-1960) computer systems, all data is digital. 
Data exists in three states: data at rest, data in transit and data in use.  Data within a computer, in most cases, moves as parallel data. Data moving to or from a computer, in most cases, moves as serial data. Data sourced from an analog device, such as a temperature sensor, may be converted to digital using an analog-to-digital converter. Data representing quantities, characters, or symbols on which operations are performed by a computer are stored and recorded on magnetic, optical, electronic, or mechanical recording media, and transmitted in the form of digital electrical or optical signals. Data pass in and out of computers via peripheral devices.
Physical computer memory elements consist of an address and a byte/word of data storage. Digital data are often stored in relational databases, like tables or SQL databases, and can generally be represented as abstract key/value pairs. Data can be organized in many different types of data structures, including arrays, graphs, and objects. Data structures can store data of many different types, including numbers, strings and even other data structures.

Characteristics
Metadata helps translate data to information. Metadata is data about the data. Metadata may be implied, specified or given.  
Data relating to physical events or processes will have a temporal component. This temporal component may be implied. This is the case when a device such as a temperature logger receives data from a temperature sensor. When the temperature is received it is assumed that the data has a temporal reference of now. So the device records the date, time and temperature together.  When the data logger communicates temperatures, it must also report the date and time as metadata for each temperature reading.
Fundamentally, computers follow a sequence of instructions they are given in the form of data.  A set of instructions to perform a given task (or tasks) is called a ""program"".  In the nominal case, the program, as executed by the computer, will consist of binary machine code.  The elements of storage manipulated by the program, but not actually executed by the CPU, are also data.  Program instructions, and the data that the program manipulates, are both stored in exactly the same way. At its most essential, a single datum is a value stored at a specific location. Therefore, it is possible for computer programs to operate on other computer programs, by manipulating their programmatic data.
The line between program and data can become blurry.  An interpreter, for example, is a program.  The input data to an interpreter is itself a program, just not one expressed in native machine language.  In many cases, the interpreted program will be a human-readable text file, which is manipulated with a text editor program (more normally associated with plain text data). Metaprogramming similarly involves programs manipulating other programs as data.  Programs like compilers, linkers, debuggers, program updaters, virus scanners and such use other programs as their data.
To store data bytes in a file, they have to be serialized in a ""file format"". Typically, programs are stored in special file types, different from those used for other data.  Executable files contain programs; all other files are also data files.  However, executable files may also contain ""in-line"" data which is built into the program.  In particular, some executable files have a data segment, which nominally contains constants and initial values (both data).
For example: a user might first instruct the operating system to load a word processor program from one file, and then edit a document stored in another file with the word processor program.  In this example, the document would be considered data.  If the word processor also features a spell checker, then the dictionary (word list) for the spell checker would also be considered data.  The algorithms used by the spell checker to suggest corrections would be either machine code data or text in some interpretable programming language.
A program is data in the form of coded instructions to control the operation of a computer or other machine.In an alternate usage, binary files (which are not human-readable) are sometimes called ""data"" as distinguished from human-readable ""text"". The total amount of digital data in 2007 was estimated to be 281 billion gigabytes (= 281 exabytes).

Data keys and values, structures and persistence
Keys in data provide the context for values.  Regardless of the structure of data, there is always a key component present. Data keys in data and data-structures are essential for giving meaning to data values. Without a key that is directly or indirectly associated with a value, or collection of values in a structure, the values become meaningless and cease to be data. That is to say, there has to be at least a key component linked to a value component in order for it to be considered data. Data can be represented in computers in multiple ways, as per the following examples:

RAM
Random Access Memory holds data that the computer processor(s) has direct access to. A computer processor (CPU) may only manipulate data within itself (Processor register) or memory.  This is as opposed to data storage, where the processor(s) must move data between the storage device (disk, tape...) and memory. RAM is an array of one (1) or more block(s) of linear contiguous locations that a processor may read or write by providing an address for the read or write operation. The ""random"" part of RAM means that the processor may operate on any location in memory at any time in any order.  (Also see Memory management unit).  In RAM the smallest element of data is the ""Binary Bit"".  The capabilities and limitations of accessing RAM are processor specific. In general main memory or RAM is arranged as an array of ""sets of electronic on/off switches"" or locations beginning at address 0 (hexadecimal 0).   Each location can store usually 8, 16, 32 or 64 parallel bits depending on the processor (CPU) architecture. Therefore, any value stored in a byte in RAM has a matching location expressed as an offset from the first memory location in the memory array i.e. 0+n, where n is the offset into the array of memory locations.

Keys
Data keys need not be a direct hardware address in memory. Indirect, abstract and logical keys codes can be stored in association with  values to form a data structure.  Data structures have predetermined offsets (or links or paths) from the start of the structure, in which data values are stored. Therefore, the data key consists of the key to the structure plus the offset (or links or paths) into the structure. When such a structure is repeated, storing variations of [the data values and the data keys] within the same repeating structure, the result can be considered to resemble a table, in which each element of the repeating structure is considered to be a column and each repetition of the structure is considered as a row of the table.  In such an organization of data, the data key is usually a value in one (or a composite of the values in several) of the columns.

Organised recurring data structures
The tabular view of repeating data structures is only one of many possibilities.  Repeating data structures can be organised hierarchically, such that nodes are linked to each other in a cascade of parent-child relationships.  Values and potentially more complex data-structures are linked to the nodes. Thus the nodal hierarchy provides the key for addressing the data structures associated with the nodes. This representation can be thought of as an inverted tree. E.g. Modern computer operating system file-systems are a common example; and XML is another.

Sorted or ordered data
Data has some inherent features when it is sorted on a key. All the values for subsets of the key appear together. When passing sequentially through groups of the data with the same key, or a subset of the key changes, this is referred to in data processing circles as a break, or a control break. It particularly facilitates aggregation of data values on subsets of a key.

Peripheral storage
Until the advent of non-volatile computer memories like USB sticks, persistent data storage was traditionally achieved by writing the data to external block devices like magnetic tape and disk drives. These devices typically seek to a location on the magnetic media and then read or write blocks of data of a predetermined size. In this case, the seek location on the media, is the data key and the blocks are the data values. Early data file-systems, or disc operating systems used to reserve contiguous blocks on the disc drive for data files. In those systems, the files could be filled up, running out of data space before all the data had been written to them. Thus much unused data space was reserved unproductively to avoid incurring that situation. This was known as raw disk. Later file-systems introduced partitions. They reserved blocks of disc data space for partitions and used the allocated blocks more economically, by dynamically assigning blocks of a partition to a file as needed. To achieve this, the file-system had to keep track of which blocks were used or unused by data files in a catalog or file allocation table. Though this made better use of the disc data space, it resulted in fragmentation of files across the disc, and a concomitant performance overhead due to latency. Modern file systems reorganize fragmented files dynamically to optimize file access times. Further developments in file systems resulted in virtualization of disc drives i.e. where a logical drive can be defined as partitions from a number of physical drives.

Indexed data
Retrieving a small subset of data from a much larger set implies searching though the data sequentially.  This is uneconomical.  Indexes are a way to copy out keys and location addresses from data structures in files, tables and data sets, then organize them using inverted tree structures to reduce the time taken to retrieve a subset of the original data. In order to do this, the key of the subset of data to be retrieved must be known before retrieval begins. The most popular indexes are the B-tree and the dynamic hash key indexing methods.  Indexing is yet another costly overhead for filing and retrieving data. There are other ways of organizing indexes, e.g. sorting the keys  or correction of quantities (or even the key and the data together), and using a binary search on them.

Abstraction and indirection
Object orientation uses two basic concepts for understanding data and software: 1) The taxonomic rank-structure of program-code classes, which is an example of a hierarchical data structure; and 2) At run time, the creation of data key references to in-memory data-structures of objects that have been instantiated from a class library. It is only after instantiation that an executing object of a specified class exists. After an object's key reference is nullified, the data referred to by that object ceases to be data because the data key reference is null; and therefore the object also ceases to exist. The memory locations where the object's data was stored are then referred to as garbage and are reclassified as unused memory available for reuse.

Database data
The advent of databases introduced a further layer of abstraction for persistent data storage. Databases use meta data, and a structured query language protocol between client and server systems, communicating over a network, using a two phase commit logging system to ensure transactional completeness, when persisting data.

Parallel distributed data processing
Modern scalable / high performance data persistence technologies rely on massively parallel distributed data processing across many commodity computers on a high bandwidth network. An example of one is Apache Hadoop.  In such systems, the data is distributed across multiple computers and therefore any particular computer in the system must be represented in the key of the data, either directly, or indirectly.  This enables the differentiation between two identical sets of data, each being processed on a different computer at the same time.

See also


== References ==",https://en.wikipedia.org/wiki/Data_(computing),"['Articles with short description', 'Computer data', 'Short description matches Wikidata']",Data Science
35,Complex systems,"A complex system is a system composed of many components which may interact with each other. Examples of complex systems are Earth's global climate, organisms, the human brain, infrastructure such as power grid, transportation or communication systems, social and economic organizations (like cities), an ecosystem, a living cell, and ultimately the entire universe.
Complex systems are systems whose behavior is intrinsically difficult to model due to the dependencies, competitions, relationships, or other types of interactions between their parts or between a given system and its environment. Systems that are ""complex"" have distinct properties that arise from these relationships, such as nonlinearity, emergence, spontaneous order, adaptation, and feedback loops, among others. Because such systems appear in a wide variety of fields, the commonalities among them have become the topic of their independent area of research. In many cases, it is useful to represent such a system as a network where the nodes represent the components and links to their interactions.

Overview
The term complex systems often refers to the study of complex systems, which is an approach to science that investigates how relationships between a system's parts give rise to its collective behaviors and how the system interacts and forms relationships with its environment. The study of complex systems regards collective, or system-wide, behaviors as the fundamental object of study; for this reason, complex systems can be understood as an alternative paradigm to reductionism, which attempts to explain systems in terms of their constituent parts and the individual interactions between them.
As an interdisciplinary domain, complex systems draws contributions from many different fields, such as the study of self-organization from physics, that of spontaneous order from the social sciences, chaos from mathematics, adaptation from biology, and many others. Complex systems is therefore often used as a broad term encompassing a research approach to problems in many diverse disciplines, including statistical physics, information theory, nonlinear dynamics, anthropology, computer science, meteorology, sociology, economics, psychology, and biology.

Key concepts
Systems
Complex systems are chiefly concerned with the behaviors and properties of systems. A system, broadly defined, is a set of entities that, through their interactions, relationships, or dependencies, form a unified whole. It is always defined in terms of its boundary, which determines the entities that are or are not part of the system. Entities lying outside the system then become part of the system's environment.
A system can exhibit properties that produce behaviors which are distinct from the properties and behaviors of its parts; these system-wide or global properties and behaviors are characteristics of how the system interacts with or appears to its environment, or of how its parts behave (say, in response to external stimuli) by virtue of being within the system. The notion of behavior implies that the study of systems is also concerned with processes that take place over time (or, in mathematics, some other phase space parameterization). Because of their broad, interdisciplinary applicability, systems concepts play a central role in complex systems.
As a field of study, a complex system is a subset of systems theory. General systems theory focuses similarly on the collective behaviors of interacting entities, but it studies a much broader class of systems, including non-complex systems where traditional reductionist approaches may remain viable. Indeed, systems theory seeks to explore and describe all classes of systems, and the invention of categories that are useful to researchers across widely varying fields is one of the systems theory's main objectives.
As it relates to complex systems, systems theory contributes an emphasis on the way relationships and dependencies between a system's parts can determine system-wide properties. It also contributes to the interdisciplinary perspective of the study of complex systems: the notion that shared properties link systems across disciplines, justifying the pursuit of modeling approaches applicable to complex systems wherever they appear. Specific concepts important to complex systems, such as emergence, feedback loops, and adaptation, also originate in systems theory.

Complexity
""Systems exhibit complexity"" means that their behaviors cannot be easily inferred from their properties. Any modeling approach that ignores such difficulties or characterizes them as noise, then, will necessarily produce models that are neither accurate nor useful. As yet no fully general theory of complex systems has emerged for addressing these problems, so researchers must solve them in domain-specific contexts. Researchers in complex systems address these problems by viewing the chief task of modeling to be capturing, rather than reducing, the complexity of their respective systems of interest.
While no generally accepted exact definition of complexity exists yet, there are many archetypal examples of complexity. Systems can be complex if, for instance, they have chaotic behavior (behavior that exhibits extreme sensitivity to initial conditions), or if they have emergent properties (properties that are not apparent from their components in isolation but which result from the relationships and dependencies they form when placed together in a system), or if they are computationally intractable to model (if they depend on a number of parameters that grows too rapidly with respect to the size of the system).

Networks
The interacting components of a complex system form a network, which is a collection of discrete objects and relationships between them, usually depicted as a graph of vertices connected by edges. Networks can describe the relationships between individuals within an organization, between logic gates in a circuit, between genes in gene regulatory networks, or between any other set of related entities.
Networks often describe the sources of complexity in complex systems. Studying complex systems as networks, therefore, enables many useful applications of graph theory and network science. Some complex systems, for example, are also complex networks, which have properties such as phase transitions and power-law degree distributions that readily lend themselves to emergent or chaotic behavior. The fact that the number of edges in a complete graph grows quadratically in the number of vertices sheds additional light on the source of complexity in large networks: as a network grows, the number of relationships between entities quickly dwarfs the number of entities in the network.

Nonlinearity
Complex systems often have nonlinear behavior, meaning they may respond in different ways to the same input depending on their state or context. In mathematics and physics, nonlinearity describes systems in which a change in the size of the input does not produce a proportional change in the size of the output. For a given change in input, such systems may yield significantly greater than or less than proportional changes in output, or even no output at all, depending on the current state of the system or its parameter values.
Of particular interest to complex systems are nonlinear dynamical systems, which are systems of differential equations that have one or more nonlinear terms. Some nonlinear dynamical systems, such as the Lorenz system, can produce a mathematical phenomenon known as chaos. Chaos, as it applies to complex systems, refers to the sensitive dependence on initial conditions, or ""butterfly effect"", that a complex system can exhibit. In such a system, small changes to initial conditions can lead to dramatically different outcomes. Chaotic behavior can, therefore, be extremely hard to model numerically, because small rounding errors at an intermediate stage of computation can cause the model to generate completely inaccurate output. Furthermore, if a complex system returns to a state similar to one it held previously, it may behave completely differently in response to the same stimuli, so chaos also poses challenges for extrapolating from experience.

Emergence
Another common feature of complex systems is the presence of emergent behaviors and properties: these are traits of a system that are not apparent from its components in isolation but which result from the interactions, dependencies, or relationships they form when placed together in a system. Emergence broadly describes the appearance of such behaviors and properties, and has applications to systems studied in both the social and physical sciences. While emergence is often used to refer only to the appearance of unplanned organized behavior in a complex system, emergence can also refer to the breakdown of an organization; it describes any phenomena which are difficult or even impossible to predict from the smaller entities that make up the system.
One example of a complex system whose emergent properties have been studied extensively is cellular automata. In a cellular automaton, a grid of cells, each having one of the finitely many states, evolves according to a simple set of rules. These rules guide the ""interactions"" of each cell with its neighbors. Although the rules are only defined locally, they have been shown capable of producing globally interesting behavior, for example in Conway's Game of Life.

Spontaneous order and self-organization
When emergence describes the appearance of unplanned order, it is spontaneous order (in the social sciences) or self-organization (in physical sciences). Spontaneous order can be seen in herd behavior, whereby a group of individuals coordinates their actions without centralized planning. Self-organization can be seen in the global symmetry of certain crystals, for instance the apparent radial symmetry of snowflakes, which arises from purely local attractive and repulsive forces both between water molecules and their surrounding environment.

Adaptation
Complex adaptive systems are special cases of complex systems that are adaptive in that they have the capacity to change and learn from experience. Examples of complex adaptive systems include the stock market, social insect and ant colonies, the biosphere and the ecosystem, the brain and the immune system, the cell and the developing embryo, the cities, manufacturing businesses and any human social group-based endeavor in a cultural and social system such as political parties or communities.

Features
Complex systems may have the following features:
Cascading failures
Due to the strong coupling between components in complex systems, a failure in one or more components can lead to cascading failures which may have catastrophic consequences on the functioning of the system. Localized attack may lead to cascading failures and abrupt collapse in spatial networks.Complex systems may be open
Complex systems are usually open systems — that is, they exist in a thermodynamic gradient and dissipate energy. In other words, complex systems are frequently far from energetic equilibrium: but despite this flux, there may be pattern stability, see synergetics.Complex systems may exhibit critical transitions
Critical transitions are abrupt shifts in the state of ecosystems, the climate, financial systems or other complex systems that may occur when changing conditions pass a critical or bifurcation point. The 'direction of critical slowing down' in a system's state space may be indicative of a system's future state after such transitions when delayed negative feedbacks leading to oscillatory or other complex dynamics are weak.Complex systems may have a memory
Recovery from a critical transition may require more than a simple return to the conditions at which a transition occurred, a phenomenon called hysteresis. The history of a complex system may thus be important. Because complex systems are dynamical systems they change over time, and prior states may have an influence on present states. Interacting systems may have complex hysteresis of many transitions.Complex systems may be nested
The components of a complex system may themselves be complex systems. For example, an economy is made up of organisations, which are made up of people, which are made up of cells - all of which are complex systems. The arrangement of interactions within complex bipartite networks may be nested as well. More specifically, bipartite ecological and organisational networks of mutually beneficial interactions were found to have a nested structure. This structure promotes indirect facilitation and a system's capacity to persist under increasingly harsh circumstances as well as the potential for large-scale systemic regime shifts.Dynamic network of multiplicity
As well as coupling rules, the dynamic network of a complex system is important. Small-world or scale-free networks which have many local interactions and a smaller number of inter-area connections are often employed. Natural complex systems often exhibit such topologies. In the human cortex for example, we see dense local connectivity and a few very long axon projections between regions inside the cortex and to other brain regions.May produce emergent phenomena
Complex systems may exhibit behaviors that are emergent, which is to say that while the results may be sufficiently determined by the activity of the systems' basic constituents, they may have properties that can only be studied at a higher level.  For example, the termites in a mound have physiology, biochemistry and biological development that are at one level of analysis, but their social behavior and mound building is a property that emerges from the collection of termites and needs to be analyzed at a different level.Relationships are non-linear
In practical terms, this means a small perturbation may cause a large effect (see butterfly effect), a proportional effect, or even no effect at all. In linear systems, the effect is always directly proportional to cause. See nonlinearity.Relationships contain feedback loops
Both negative (damping) and positive (amplifying) feedback are always found in complex systems. The effects of an element's behavior are fed back in such a way that the element itself is altered.

History
Although arguably, humans have been studying complex systems for thousands of years, the modern scientific study of complex systems is relatively young in comparison to established fields of science such as physics and chemistry. The history of the scientific study of these systems follows several different research trends.
In the area of mathematics, arguably the largest contribution to the study of complex systems was the discovery of chaos in deterministic systems, a feature of certain dynamical systems that is strongly related to nonlinearity.  The study of neural networks was also integral in advancing the mathematics needed to study complex systems.
The notion of self-organizing systems is tied with work in nonequilibrium thermodynamics, including that pioneered by chemist and Nobel laureate Ilya Prigogine in his study of dissipative structures. Even older is the work by Hartree-Fock on the quantum chemistry equations and later calculations of the structure of molecules which can be regarded as one of the earliest examples of emergence and emergent wholes in science.
One complex system containing humans is the classical political economy of the Scottish Enlightenment, later developed by the Austrian school of economics, which argues that order in market systems is spontaneous (or emergent) in that it is the result of human action, but not the execution of any human design.Upon this, the Austrian school developed from the 19th to the early 20th century the economic calculation problem, along with the concept of dispersed knowledge, which were to fuel debates against the then-dominant Keynesian economics. This debate would notably lead economists, politicians, and other parties to explore the question of computational complexity.A pioneer in the field, and inspired by Karl Popper's and Warren Weaver's works, Nobel prize economist and philosopher Friedrich Hayek dedicated much of his work, from early to the late 20th century, to the study of complex phenomena, not constraining his work to human economies but venturing into other fields such as psychology, biology and cybernetics. Gregory Bateson played a key role in establishing the connection between anthropology and systems theory; he recognized that the interactive parts of cultures function much like ecosystems.
While the explicit study of complex systems dates at least to the 1970s, the first research institute focused on complex systems, the Santa Fe Institute, was founded in 1984. Early Santa Fe Institute participants included physics Nobel laureates Murray Gell-Mann and Philip Anderson, economics Nobel laureate Kenneth Arrow, and Manhattan Project scientists George Cowan and Herb Anderson. Today, there are over 50 institutes and research centers focusing on complex systems.

Applications
Complexity in practice
The traditional approach to dealing with complexity is to reduce or constrain it. Typically, this involves compartmentalization: dividing a large system into separate parts. Organizations, for instance, divide their work into departments that each deal with separate issues. Engineering systems are often designed using modular components. However, modular designs become susceptible to failure when issues arise that bridge the divisions.

Complexity management
As projects and acquisitions become increasingly complex, companies and governments are challenged to find effective ways to manage mega-acquisitions such as the Army Future Combat Systems.  Acquisitions such as the FCS rely on a web of interrelated parts which interact unpredictably.  As acquisitions become more network-centric and complex, businesses will be forced to find ways to manage complexity while governments will be challenged to provide effective governance to ensure flexibility and resiliency.

Complexity economics
Over the last decades, within the emerging field of complexity economics, new predictive tools have been developed to explain economic growth. Such is the case with the models built by the Santa Fe Institute in 1989 and the more recent economic complexity index (ECI), introduced by the MIT physicist Cesar A. Hidalgo and the Harvard economist Ricardo Hausmann. Based on the ECI, Hausmann, Hidalgo and their team of The Observatory of Economic Complexity have produced GDP forecasts for the year 2020.

Complexity and education
Focusing on issues of student persistence with their studies, Forsman, Moll and Linder explore the ""viability of using complexity science as a frame to extend methodological applications for physics education research"", finding that ""framing a social network analysis within a complexity science perspective offers a new and powerful applicability across a broad range of PER topics"".

Complexity and modeling
One of Friedrich Hayek's main contributions to early complexity theory is his distinction between the human capacity to predict the behavior of simple systems and its capacity to predict the behavior of complex systems through modeling. He believed that economics and the sciences of complex phenomena in general, which in his view included biology, psychology, and so on, could not be modeled after the sciences that deal with essentially simple phenomena like physics. Hayek would notably explain that complex phenomena, through modeling, can only allow pattern predictions, compared with the precise predictions that can be made out of non-complex phenomena.

Complexity and chaos theory
Complexity theory is rooted in chaos theory, which in turn has its origins more than a century ago in the work of the French mathematician Henri Poincaré. Chaos is sometimes viewed as extremely complicated information, rather than as an absence of order. Chaotic systems remain deterministic, though their long-term behavior can be difficult to predict with any accuracy. With perfect knowledge of the initial conditions and the relevant equations describing the chaotic system's behavior, one can theoretically make perfectly accurate predictions of the system, though in practice this is impossible to do with arbitrary accuracy. Ilya Prigogine argued that complexity is non-deterministic and gives no way whatsoever to precisely predict the future.The emergence of complexity theory shows a domain between deterministic order and randomness which is complex. This is referred to as the ""edge of chaos"".

When one analyzes complex systems, sensitivity to initial conditions, for example, is not an issue as important as it is within chaos theory, in which it prevails. As stated by Colander, the study of complexity is the opposite of the study of chaos. Complexity is about how a huge number of extremely complicated and dynamic sets of relationships can generate some simple behavioral patterns, whereas chaotic behavior, in the sense of deterministic chaos, is the result of a relatively small number of non-linear interactions.Therefore, the main difference between chaotic systems and complex systems is their history. Chaotic systems do not rely on their history as complex ones do. Chaotic behavior pushes a system in equilibrium into chaotic order, which means, in other words, out of what we traditionally define as 'order'. On the other hand, complex systems evolve far from equilibrium at the edge of chaos. They evolve at a critical state built up by a history of irreversible and unexpected events, which physicist Murray Gell-Mann called ""an accumulation of frozen accidents"". In a sense chaotic systems can be regarded as a subset of complex systems distinguished precisely by this absence of historical dependence. Many real complex systems are, in practice and over long but finite periods, robust. However, they do possess the potential for radical qualitative change of kind whilst retaining systemic integrity. Metamorphosis serves as perhaps more than a metaphor for such transformations.

Complexity and network science
A complex system is usually composed of many components and their interactions. Such a system can be represented by a network where nodes represent the components and links represent their interactions. For example, the Internet can be represented as a network composed of nodes (computers) and links (direct connections between computers), and the resilience of the Internet to failures has been studied using percolation theory, a form of complex systems analysis.
The failure and recovery of these networks is an open area of research.
Other examples of complex networks include social networks, financial institution interdependencies, traffic systems, airline networks, biological networks, and climate networks.
Finally, entire networks often interact in a complex manner; if an individual complex system can be represented as a network, then interacting complex systems can be modeled as networks of networks with dynamic properties.

General form of complexity computation
The computational law of reachable optimality is established as a general form of computation for ordered systems.
The computational law of reachable optimality has four key components as described below.
1. Reachability of Optimality: Any intended optimality shall be reachable. Unreachable optimality has no meaning for a member in the ordered system and even for the ordered system itself.
2. Prevailing and Consistency: Maximizing reachability to explore best available optimality is the prevailing computation logic for all members in the ordered system and is accommodated by the ordered system.
3. Conditionality: Realizable tradeoff between reachability and optimality depends primarily upon the initial bet capacity and how the bet capacity evolves along with the payoff table update path triggered by bet behavior and empowered by the underlying law of reward and punishment. Precisely, it is a sequence of conditional events where the next event happens upon reached status quo from experience path.
4. Robustness: The more challenge a reachable optimality can accommodate, the more robust it is in terms of path integrity.
There are also four computation features in the law of reachable optimality.
1. Optimal Choice: Computation in realizing Optimal Choice can be very simple or very complex. A simple rule in Optimal Choice is to accept whatever is reached, Reward As You Go (RAYG). A Reachable Optimality computation reduces into optimizing reachability when RAYG is adopted. The Optimal Choice computation can be more complex when multiple NE strategies present in a reached game.
2. Initial Status: Computation is assumed to start at an interesting beginning even the absolute beginning of an ordered system in nature may not and need not present. An assumed neutral Initial Status facilitates an artificial or a simulating computation and is not expected to change the prevalence of any findings.
3. Territory: An ordered system shall have a territory where the universal computation sponsored by the system will produce an optimal solution still within the territory.
4. Reaching Pattern: The forms of Reaching Pattern in the computation space, or the Optimality Driven Reaching Pattern in the computation space, primarily depend upon the nature and dimensions of measure space underlying a computation space and the law of punishment and reward underlying the realized experience path of reaching. There are five basic forms of experience path we are interested in, persistently positive reinforcement experience path, persistently negative reinforcement experience path, mixed persistent pattern experience path, decaying scale experience path and selection experience path.
The compound computation in the selection experience path includes current and lagging interaction, dynamic topological transformation and implies both invariance and variance characteristics in an ordered system's experience path.
Also, the computation law of reachable optimality gives out the boundary between the complexity model, chaotic model, and determination model. When RAYG is the Optimal Choice computation, and the reaching pattern is a persistently positive experience path, persistently negative experience path, or mixed persistent pattern experience path, the underlying computation shall be a simple system computation adopting determination rules. If the reaching pattern has no persistent pattern experienced in the RAYG regime, the underlying computation hints there is a chaotic system. When the optimal choice computation involves non-RAYG computation, it's a complexity computation driving the compound effect.

Notable scholars
See also
References
Further reading
Complexity Explained.
L.A.N. Amaral and J.M. Ottino, Complex networks — augmenting the framework for the study of complex system, 2004.
Chu, D.; Strand, R.; Fjelland, R. (2003). ""Theories of complexity"". Complexity. 8 (3): 19–30. Bibcode:2003Cmplx...8c..19C. doi:10.1002/cplx.10059.
Walter Clemens, Jr., Complexity Science and World Affairs, SUNY Press, 2013.
Gell-Mann, Murray (1995). ""Let's Call It Plectics"" (PDF). Complexity. 1 (5): 3–5. Bibcode:1996Cmplx...1e...3G. doi:10.1002/cplx.6130010502.
A. Gogolin, A. Nersesyan and A. Tsvelik, Theory of strongly correlated systems , Cambridge University Press, 1999.
Nigel Goldenfeld and Leo P. Kadanoff, Simple Lessons from Complexity, 1999
Kelly, K. (1995). Out of Control, Perseus Books Group.
Syed M. Mehmud (2011), A Healthcare Exchange Complexity Model
Preiser-Kapeller, Johannes, ""Calculating Byzantium. Social Network Analysis and Complexity Sciences as tools for the exploration of medieval social dynamics"". August 2010
Donald Snooks, Graeme (2008). ""A general theory of complex living systems: Exploring the demand side of dynamics"" (PDF). Complexity. 13 (6): 12–20. Bibcode:2008Cmplx..13f..12S. doi:10.1002/cplx.20225.
Stefan Thurner, Peter Klimek, Rudolf Hanel: Introduction to the Theory of Complex Systems, Oxford University Press, 2018, ISBN 978-0198821939
SFI @30, Foundations & Frontiers (2014).

External links
""The Open Agent-Based Modeling Consortium"".
""Complexity Science Focus"".
""Santa Fe Institute"".
""The Center for the Study of Complex Systems, Univ. of Michigan Ann Arbor"".
""INDECS"". (Interdisciplinary Description of Complex Systems)
""Introduction to Complexity - Free online course by Melanie Mitchell"". Archived from the original on 2018-08-30. Retrieved 2018-08-29.
Jessie Henshaw (October 24, 2013). ""Complex Systems"". Encyclopedia of Earth.
Introduction to complex systems-short course by Shlomo Havlin
Complex systems in scholarpedia.
Complex Systems Society
Complexity Science Hub Vienna
(Australian) Complex systems research network.
Complex Systems Modeling based on Luis M. Rocha, 1999.
CRM Complex systems research group
The Center for Complex Systems Research, Univ. of Illinois at Urbana-Champaign
FuturICT — Exploring and Managing our Future",https://en.wikipedia.org/wiki/Complex_system,"['All articles with dead external links', 'All articles with unsourced statements', 'Articles with dead external links from August 2019', 'Articles with dead external links from July 2020', 'Articles with permanently dead external links', 'Articles with short description', 'Articles with unsourced statements from April 2019', 'Articles with unsourced statements from February 2016', 'Articles with unsourced statements from November 2016', 'Commons category link is on Wikidata', 'Complex dynamics', 'Complex systems theory', 'Cybernetics', 'Emergence', 'Mathematical modeling', 'Short description is different from Wikidata', 'Systems science', 'Systems theory', 'Webarchive template wayback links', 'Wikipedia articles needing clarification from September 2011']",Data Science
36,Data analysis,"Data analysis is a process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.Data mining is a particular data analysis technique that focuses on statistical modeling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on the application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All of the above are varieties of data analysis.
Data integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination.

The process of data analysis
Analysis, refers to dividing a whole into its separate components for individual examination. Data analysis, is a process for obtaining raw data, and subsequently converting it into information useful for decision-making by users. Data, is collected and analyzed to answer questions, test hypotheses, or disprove theories.
Statistician John Tukey, defined data analysis in 1961, as:""Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.""There are several phases that can be distinguished, described below. The phases are iterative, in that feedback from later phases may result in additional work in earlier phases. The CRISP framework, used in data mining, has similar steps.

Data requirements
The data are necessary as inputs to the analysis, which is specified based upon the requirements of those directing the analysis or customers (who will use the finished product of the analysis). The general type of entity upon which the data will be collected is referred to as an experimental unit (e.g., a person or population of people). Specific variables regarding a population (e.g., age and income) may be specified and obtained.  Data may be numerical or categorical (i.e., a text label for numbers).

Data collection
Data are collected from a variety of sources. The requirements may be communicated by analysts to custodians of the data; such as, Information Technology personnel within an organization. The data may also be collected from sensors in the environment, including traffic cameras, satellites, recording devices, etc. It may also be obtained through interviews, downloads from online sources, or reading documentation.

Data processing
Data, when initially obtained, must be processed or organized for analysis. For instance, these may involve placing data into rows and columns in a table format (known as structured data) for further analysis, often through the use of spreadsheet or statistical software.

Data cleaning
Once processed and organized, the data may be incomplete, contain duplicates, or contain errors. The need for data cleaning, will arise from problems in the way that the datum are entered and stored. Data cleaning is the process of preventing and correcting these errors. Common tasks include record matching, identifying inaccuracy of data, overall quality of existing data, deduplication, and column segmentation. Such data problems can also be identified through a variety of analytical techniques. For example, with financial information, the totals for particular variables may be compared against separately published numbers, that are believed to be reliable. Unusual amounts, above or below predetermined thresholds, may also be reviewed.  There are several types of data cleaning, that are dependent upon the type of data in the set; this could be phone numbers, email addresses, employers, or other values. Quantitative data methods for outlier detection, can be used to get rid of data that appears to have a higher likelihood of being input incorrectly. Textual data spell checkers, can be used to lessen the amount of mis-typed words, however, it is harder to tell if the words themselves are correct.

Exploratory data analysis
Once the datasets are cleaned, it can then be analyzed. Analysts may apply a variety of techniques, referred to as exploratory data analysis, to begin understanding the messages contained within the obtained data. The process of data exploration may result in additional data cleaning or additional requests for data; thus, the initialization of the iterative phases mentioned in the lead paragraph of this section. Descriptive statistics, such as, the average or median, can be generated to aid in understanding the data. Data visualization is also a technique used, in which the analyst is able to examine the data in a graphical format in order to obtain additional insights, regarding the messages within the data.

Modeling and algorithms
Mathematical formulas or models (known as algorithms), may be applied to the data in order to identify relationships among the variables; for example, using correlation or causation. In general terms, models may be developed to evaluate a specific variable based on other variable(s) contained within the dataset, with some residual error depending on the implemented model's accuracy (e.g., Data = Model + Error).Inferential statistics, includes utilizing techniques that measure the relationships between particular variables. For example, regression analysis may be used to model whether a change in advertising (independent variable X), provides an explanation for the variation in sales (dependent variable Y). In mathematical terms, Y (sales) is a function of X (advertising). It may be described as (Y = aX + b + error), where the model is designed such that (a) and (b) minimize the error when the model predicts Y for a given range of values of X. Analysts may also attempt to build models that are descriptive of the data, in an aim to simplify analysis and communicate results.

Data product
A data product, is a computer application that takes data inputs and generates outputs, feeding them back into the environment. It may be based on a model or algorithm. For instance, an application that analyzes data about customer purchase history, and uses the results to recommend other purchases the customer might enjoy.

Communication
Once the data are analyzed, it may be reported in many formats to the users of the analysis to support their requirements. The users may have feedback, which results in additional analysis. As such, much of the analytical cycle is iterative.When determining how to communicate the results, the analyst may consider implementing a variety of data visualization techniques, to help clearly and efficiently communicate the message to the audience. Data visualization uses information displays (graphics such as, tables and charts) to help communicate key messages contained in the data. Tables are a valuable tool by enabling the ability of a user to query and focus on specific numbers; while charts (e.g., bar charts or line charts), may help explain the quantitative messages contained in the data.

Quantitative messages
Stephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message. Customers specifying requirements and analysts performing the data analysis may consider these messages during the course of the process.

Time-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend.
Ranking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by salespersons (the category, with each salesperson a categorical subdivision) during a single period.  A bar chart may be used to show the comparison across the salespersons.
Part-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%).  A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.
Deviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period.  A bar chart can show the comparison of the actual versus the reference amount.
Frequency distribution: Shows the number of observations of a particular variable for a given interval, such as the number of years in which the stock market return is between intervals such as 0–10%, 11–20%, etc. A histogram, a type of bar chart, may be used for this analysis.
Correlation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.
Nominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.
Geographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.

Techniques for analyzing quantitative data
Author Jonathan Koomey has recommended a series of best practices for understanding quantitative data.  These include:

Check raw data for anomalies prior to performing an analysis;
Re-perform important calculations, such as verifying columns of data that are formula driven;
Confirm main totals are the sum of subtotals;
Check relationships between numbers that should be related in a predictable way, such as ratios over time;
Normalize numbers to make comparisons easier, such as analyzing amounts per person or relative to GDP or as an index value relative to a base year;
Break problems into component parts by analyzing factors that led to the results, such as DuPont analysis of return on equity.For the variables under examination, analysts typically obtain descriptive statistics for them, such as the mean (average), median, and standard deviation. They may also analyze the distribution of the key variables to see how the individual values cluster around the mean.

 The consultants at McKinsey and Company named a technique for breaking a quantitative problem down into its component parts called the MECE principle. Each layer can be broken down into its components; each of the sub-components must be mutually exclusive of each other and collectively add up to the layer above them. The relationship is referred to as ""Mutually Exclusive and Collectively Exhaustive"" or MECE.  For example, profit by definition can be broken down into total revenue and total cost. In turn, total revenue can be analyzed by its components, such as the revenue of divisions A, B, and C (which are mutually exclusive of each other) and should add to the total revenue (collectively exhaustive).
Analysts may use robust statistical measurements to solve certain analytical problems. Hypothesis testing is used when a particular hypothesis about the true state of affairs is made by the analyst and data is gathered to determine whether that state of affairs is true or false. For example, the hypothesis might be that ""Unemployment has no effect on inflation"", which relates to an economics concept called the Phillips Curve. Hypothesis testing involves considering the likelihood of Type I and type II errors, which relate to whether the data supports accepting or rejecting the hypothesis.
Regression analysis may be used when the analyst is trying to determine the extent to which independent variable X affects dependent variable Y (e.g., ""To what extent do changes in the unemployment rate (X) affect the inflation rate (Y)?""). This is an attempt to model or fit an equation line or curve to the data, such that Y is a function of X.
Necessary condition analysis (NCA) may be used when the analyst is trying to determine the extent to which independent variable X allows variable Y (e.g., ""To what extent is a certain unemployment rate (X) necessary for a certain inflation rate (Y)?""). Whereas (multiple) regression analysis uses additive logic where each X-variable can produce the outcome and the X's can compensate for each other (they are sufficient but not necessary), necessary condition analysis (NCA) uses necessity logic, where one or more X-variables allow the outcome to exist, but may not produce it (they are necessary but not sufficient). Each single necessary condition must be present and compensation is not possible.

Analytical activities of data users
Users may have particular data points of interest within a data set, as opposed to the general messaging outlined above. Such low-level user analytic activities are presented in the following table. The taxonomy can also be organized by three poles of activities: retrieving values, finding data points, and arranging data points.

Barriers to effective analysis
Barriers to effective analysis may exist among the analysts performing the data analysis or among the audience. Distinguishing fact from opinion, cognitive biases, and innumeracy are all challenges to sound data analysis.

Confusing fact and opinion
Effective analysis requires obtaining relevant facts to answer questions, support a conclusion or formal opinion, or test hypotheses. Facts by definition are irrefutable, meaning that any person involved in the analysis should be able to agree upon them. For example, in August 2010, the Congressional Budget Office (CBO) estimated that extending the Bush tax cuts of 2001 and 2003 for the 2011–2020 time period would add approximately $3.3 trillion to the national debt. Everyone should be able to agree that indeed this is what CBO reported; they can all examine the report. This makes it a fact. Whether persons agree or disagree with the CBO is their own opinion.
As another example, the auditor of a public company must arrive at a formal opinion on whether financial statements of publicly traded corporations are ""fairly stated, in all material respects."" This requires extensive analysis of factual data and evidence to support their opinion. When making the leap from facts to opinions, there is always the possibility that the opinion is erroneous.

Cognitive biases
There are a variety of cognitive biases that can adversely affect analysis. For example, confirmation bias is the tendency to search for or interpret information in a way that confirms one's preconceptions. In addition, individuals may discredit information that does not support their views.
Analysts may be trained specifically to be aware of these biases and how to overcome them. In his book Psychology of Intelligence Analysis, retired CIA analyst Richards Heuer wrote that analysts should clearly delineate their assumptions and chains of inference and specify the degree and source of the uncertainty involved in the conclusions. He emphasized procedures to help surface and debate alternative points of view.

Innumeracy
Effective analysts are generally adept with a variety of numerical techniques. However, audiences may not have such literacy with numbers or numeracy; they are said to be innumerate.  Persons communicating the data may also be attempting to mislead or misinform, deliberately using bad numerical techniques.For example, whether a number is rising or falling may not be the key factor. More important may be the number relative to another number, such as the size of government revenue or spending relative to the size of the economy (GDP) or the amount of cost relative to revenue in corporate financial statements. This numerical technique is referred to as normalization or common-sizing. There are many such techniques employed by analysts, whether adjusting for inflation (i.e., comparing real vs. nominal data) or considering population increases, demographics, etc. Analysts apply a variety of techniques to address the various quantitative messages described in the section above.
Analysts may also analyze data under different assumptions or scenarios. For example, when analysts perform financial statement analysis, they will often recast the financial statements under different assumptions to help arrive at an estimate of future cash flow, which they then discount to present value based on some interest rate, to determine the valuation of the company or its stock.  Similarly, the CBO analyzes the effects of various policy options on the government's revenue, outlays and deficits, creating alternative future scenarios for key measures.

Other topics
Smart buildings
A data analytics approach can be used in order to predict energy consumption in buildings. The different steps of the data analysis process are carried out in order to realise smart buildings, where the building management and control operations including heating, ventilation, air conditioning, lighting and security are realised automatically by miming the needs of the building users and optimising resources like energy and time.

Analytics and business intelligence
Analytics is the ""extensive use of data, statistical and quantitative analysis, explanatory and predictive models, and fact-based management to drive decisions and actions."" It is a subset of business intelligence, which is a set of technologies and processes that use data to understand and analyze business performance.

Education
In education, most educators have access to a data system for the purpose of analyzing student data. These data systems present data to educators in an over-the-counter data format (embedding labels, supplemental documentation, and a help system and making key package/display and content decisions) to improve the accuracy of educators’ data analyses.

Practitioner notes
This section contains rather technical explanations that may assist practitioners but are beyond the typical scope of a Wikipedia article.

Initial data analysis
The most important distinction between the initial data analysis phase and the main analysis phase, is that during initial data analysis one refrains from any analysis that is aimed at answering the original research question. The initial data analysis phase is guided by the following four questions:

Quality of data
The quality of the data should be checked as early as possible. Data quality can be assessed in several ways, using different types of analysis: frequency counts, descriptive statistics (mean, standard deviation, median), normality (skewness, kurtosis, frequency histograms), normal imputation is needed.

Analysis of extreme observations: outlying observations in the data are analyzed to see if they seem to disturb the distribution.
Comparison and correction of differences in coding schemes: variables are compared with coding schemes of variables external to the data set, and possibly corrected if coding schemes are not comparable.
Test for common-method variance.The choice of analyses to assess the data quality during the initial data analysis phase depends on the analyses that will be conducted in the main analysis phase.

Quality of measurements
The quality of the measurement instruments should only be checked during the initial data analysis phase when this is not the focus or research question of the study. One should check whether structure of measurement instruments corresponds to structure reported in the literature.
There are two ways to assess measurement quality:

Confirmatory factor analysis
Analysis of homogeneity (internal consistency), which gives an indication of the reliability of a measurement instrument. During this analysis, one inspects the variances of the items and the scales, the Cronbach's α of the scales, and the change in the Cronbach's alpha when an item would be deleted from a scale

Initial transformations
After assessing the quality of the data and of the measurements, one might decide to impute missing data, or to perform initial transformations of one or more variables, although this can also be done during the main analysis phase.
Possible transformations of variables are:
Square root transformation (if the distribution differs moderately from normal)
Log-transformation (if the distribution differs substantially from normal)
Inverse transformation (if the distribution differs severely from normal)
Make categorical (ordinal / dichotomous) (if the distribution differs severely from normal, and no transformations help)

Did the implementation of the study fulfill the intentions of the research design?
One should check the success of the randomization procedure, for instance by checking whether background and substantive variables are equally distributed within and across groups. 
If the study did not need or use a randomization procedure, one should check the success of the non-random sampling, for instance by checking whether all subgroups of the population of interest are represented in sample.
Other possible data distortions that should be checked are:

dropout (this should be identified during the initial data analysis phase)
Item nonresponse (whether this is random or not should be assessed during the initial data analysis phase)
Treatment quality (using manipulation checks).

Characteristics of data sample
In any report or article, the structure of the sample must be accurately described. It is especially important to exactly determine the structure of the sample (and specifically the size of the subgroups) when subgroup analyses will be performed during the main analysis phase.
The characteristics of the data sample can be assessed by looking at:

Basic statistics of important variables
Scatter plots
Correlations and associations
Cross-tabulations

Final stage of the initial data analysis
During the final stage, the findings of the initial data analysis are documented, and necessary, preferable, and possible corrective actions are taken.
Also, the original plan for the main data analyses can and should be specified in more detail or rewritten. In order to do this, several decisions about the main data analyses can and should be made:

In the case of non-normals: should one transform variables; make variables categorical (ordinal/dichotomous); adapt the analysis method?
In the case of missing data: should one neglect or impute the missing data; which imputation technique should be used?
In the case of outliers: should one use robust analysis techniques?
In case items do not fit the scale: should one adapt the measurement instrument by omitting items, or rather ensure comparability with other (uses of the) measurement instrument(s)?
In the case of (too) small subgroups: should one drop the hypothesis about inter-group differences, or use small sample techniques, like exact tests or bootstrapping?
In case the randomization procedure seems to be defective: can and should one calculate propensity scores and include them as covariates in the main analyses?

Analysis
Several analyses can be used during the initial data analysis phase:
Univariate statistics (single variable)
Bivariate associations (correlations)
Graphical techniques (scatter plots)It is important to take the measurement levels of the variables into account for the analyses, as special statistical techniques are available for each level:
Nominal and ordinal variables
Frequency counts (numbers and percentages)
Associations
circumambulations (crosstabulations)
hierarchical loglinear analysis (restricted to a maximum of 8 variables)
loglinear analysis (to identify relevant/important variables and possible confounders)
Exact tests or bootstrapping (in case subgroups are small)
Computation of new variables
Continuous variables
Distribution
Statistics (M, SD, variance, skewness, kurtosis)
Stem-and-leaf displays
Box plots

Nonlinear analysis
Nonlinear analysis is often necessary when the data is recorded from a nonlinear system. Nonlinear systems can exhibit complex dynamic effects including bifurcations, chaos, harmonics and subharmonics that cannot be analyzed using simple linear methods.  Nonlinear data analysis is closely related to nonlinear system identification.

Main data analysis
In the main analysis phase analyses aimed at answering the research question are performed as well as any other relevant analysis needed to write the first draft of the research report.

Exploratory and confirmatory approaches
In the main analysis phase either an exploratory or confirmatory approach can be adopted. Usually the approach is decided before data is collected. In an exploratory analysis no clear hypothesis is stated before analysing the data, and the data is searched for models that describe the data well. In a confirmatory analysis clear hypotheses about the data are tested.
Exploratory data analysis should be interpreted carefully. When testing multiple models at once there is a high chance on finding at least one of them to be significant, but this can be due to a type 1 error. It is important to always adjust the significance level when testing multiple models with, for example, a Bonferroni correction. Also, one should not follow up an exploratory analysis with a confirmatory analysis in the same dataset. An exploratory analysis is used to find ideas for a theory, but not to test that theory as well. When a model is found exploratory in a dataset, then following up that analysis with a confirmatory analysis in the same dataset could simply mean that the results of the confirmatory analysis are due to the same type 1 error that resulted in the exploratory model in the first place. The confirmatory analysis therefore will not be more informative than the original exploratory analysis.

Stability of results
It is important to obtain some indication about how generalizable the results are. While this is often difficult to check, one can look at the stability of the results. Are the results reliable and reproducible? There are two main ways of doing that.

Cross-validation. By splitting the data into multiple parts, we can check if an analysis (like a fitted model) based on one part of the data generalizes to another part of the data as well. Cross-validation is generally inappropriate, though, if there are correlations within the data, e.g. with panel data. Hence other methods of validation sometimes need to be used. For more on this topic, see statistical model validation.
Sensitivity analysis. A procedure to study the behavior of a system or model when global parameters are (systematically) varied. One way to do that is via bootstrapping.

Free software for data analysis
Notable free software for data analysis include:

DevInfo – A database system endorsed by the United Nations Development Group for monitoring and analyzing human development.
ELKI – Data mining framework in Java with data mining oriented visualization functions.
KNIME – The Konstanz Information Miner, a user friendly and comprehensive data analytics framework.
Orange – A visual programming tool featuring interactive data visualization and methods for statistical data analysis, data mining, and machine learning.
Pandas – Python library for data analysis.
PAW – FORTRAN/C data analysis framework developed at CERN.
R – A programming language and software environment for statistical computing and graphics.
ROOT –  C++ data analysis framework developed at CERN.
SciPy – Python library for data analysis.
Julia - A programming language well-suited for numerical analysis and computational science.

International data analysis contests
Different companies or organizations hold data analysis contests to encourage researchers to utilize their data or to solve a particular question using data analysis. A few examples of well-known international data analysis contests are as follows. 

Kaggle competition held by Kaggle
LTPP data analysis contest held by FHWA and ASCE.

See also
References
Citations
Bibliography
Adèr, Herman J. (2008a). ""Chapter 14: Phases and initial steps in data analysis"".  In Adèr, Herman J.; Mellenbergh, Gideon J.; Hand, David J (eds.). Advising on research methods : a consultant's companion. Huizen, Netherlands: Johannes van Kessel Pub. pp. 333–356. ISBN 9789079418015. OCLC 905799857.
Adèr, Herman J. (2008b). ""Chapter 15: The main analysis phase"".  In Adèr, Herman J.; Mellenbergh, Gideon J.; Hand, David J (eds.). Advising on research methods : a consultant's companion. Huizen, Netherlands: Johannes van Kessel Pub. pp. 357–386. ISBN 9789079418015. OCLC 905799857.
Tabachnick, B.G. & Fidell, L.S. (2007). Chapter 4: Cleaning up your act. Screening data prior to analysis. In B.G. Tabachnick & L.S. Fidell (Eds.), Using Multivariate Statistics, Fifth Edition (pp. 60–116). Boston: Pearson Education, Inc. / Allyn and Bacon.

Further reading
Adèr, H.J. & Mellenbergh, G.J. (with contributions by D.J. Hand) (2008). Advising on Research Methods: A Consultant's Companion. Huizen, the Netherlands: Johannes van Kessel Publishing.
Chambers, John M.; Cleveland, William S.; Kleiner, Beat; Tukey, Paul A. (1983). Graphical Methods for Data Analysis, Wadsworth/Duxbury Press. ISBN 0-534-98052-X
Fandango, Armando (2008). Python Data Analysis, 2nd Edition. Packt Publishers.
Juran, Joseph M.; Godfrey, A. Blanton (1999). Juran's Quality Handbook, 5th Edition. New York: McGraw Hill. ISBN 0-07-034003-X
Lewis-Beck, Michael S. (1995). Data Analysis: an Introduction, Sage Publications Inc, ISBN 0-8039-5772-6
NIST/SEMATECH (2008) Handbook of Statistical Methods,
Pyzdek, T, (2003). Quality Engineering Handbook, ISBN 0-8247-4614-7
Richard Veryard (1984). Pragmatic Data Analysis. Oxford : Blackwell Scientific Publications. ISBN 0-632-01311-7
Tabachnick, B.G.; Fidell, L.S. (2007). Using Multivariate Statistics, 5th Edition. Boston: Pearson Education, Inc. / Allyn and Bacon, ISBN 978-0-205-45938-4",https://en.wikipedia.org/wiki/Data_analysis,"['Articles with short description', 'Big data', 'Computational fields of study', 'Data analysis', 'Data management', 'Scientific method', 'Short description is different from Wikidata', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers']",Data Science
37,Data archaeology,"Data archaeology refers to the art and science of recovering computer data encoded and/or encrypted in now obsolete media or formats. Data archaeology can also refer to recovering information from damaged electronic formats after natural disasters or human error.
The term originally appeared in 1993 as part of the Global Oceanographic Data Archaeology and Rescue Project (GODAR). The original impetus for data archaeology came from the need to recover computerized records of climatic conditions stored on old computer tape, which can provide valuable evidence for testing theories of climate change. These approaches allowed the reconstruction of an image of the Arctic that had been captured by the Nimbus 2 satellite on September 23, 1966, in higher resolution than ever seen before from this type of data.NASA also utilizes the services of data archaeologists to recover information stored on 1960s-era vintage computer tape, as exemplified by the Lunar Orbiter Image Recovery Project (LOIRP).

Recovery
There is a distinction between data recovery and data intelligibility. One may be able to recover data but not understand it. For data archaeology to be effective, the data must be intelligible.

Disaster recovery
Data archaeologists can also use data recovery after natural disasters such as fires, floods, earthquakes, or even hurricanes. For example, in 1995 during Hurricane Marilyn the National Media Lab assisted the National Archives and Records Administration in recovering data at risk due to damaged equipment. The hardware was damaged from rain, salt water, and sand, yet it was possible to clean some of the disks and refit them with new cases thus saving the data within.

Recovery techniques
When deciding whether or not to try and recover data, the cost must be taken into account. If there is enough time and money, most data will be able to be recovered. In the case of magnetic media, which are the most common type used for data storage, there are various techniques that can be used to recover the data depending on the type of damage.Humidity can cause tapes to become unusable as they begin to deteriorate and become sticky.  In this case, a heat treatment can be applied to fix this problem, by causing the oils and residues to either be reabsorbed into the tape or evaporate off the surface of the tape.  However, this should only be done in order to provide access to the data so it can be extracted and copied to a medium that is more stable.Lubrication loss is another source of damage to tapes. This is most commonly caused by heavy use, but can also be a result of improper storage or natural evaporation.  As a result of heavy use, some of the lubricant can remain on the read-write heads which then collect dust and particles.  This can cause damage to the tape.  Loss of lubrication can be addressed by re-lubricating the tapes.  This should be done cautiously, as excessive re-lubrication can cause tape slippage, which in turn can lead to media being misread and the loss of data.Water exposure will damage tapes over time.  This often occurs in a disaster situation.  If the media is in salty or dirty water, it should be rinsed in fresh water.  The process of cleaning, rinsing, and drying wet tapes should be done at room temperature in order to prevent heat damage.  Older tapes should be recovered prior to newer tapes, as they are more susceptible to water damage.

Prevention
To prevent the need of data archaeology, creators and holders of digital documents should take care to employ digital preservation.

See also
Bit rot
Data curation
Data preservation
Digital dark age
Digital preservation
Knowledge discovery

References

World Wide Words: Data Archaeology
O'Donnell, James Joseph.  Avatars of the Word:  From Papyrus to Cyperspace  Harvard University Press, 1998.
Ross, Seamus & Gow, Ann (1999). Digital archaeology : rescuing neglected and damaged data resources (PDF). Electronic libraries programme studies. London & Bristol: British Library and Joint Information Systems Committee. ISBN 1-90050-851-6.",https://en.wikipedia.org/wiki/Data_archaeology,"['Archaeological sub-disciplines', 'Articles with short description', 'Data management', 'Digital preservation', 'Short description matches Wikidata']",Data Science
38,Data augmentation,"Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model. It is closely related to oversampling in data analysis.

Synthetic oversampling techniques for traditional machine learning
Data augmentation for image classification
Transformations of images
Geometric transformations, flipping, color modification, cropping, rotation, noise injection and random erasing are used to augment image in deep learning.

Introducing new synthetic images
If the issue of data scarcity is faced, the simple yet effective techniques such as transformations may pose a limited solution. If a dataset is too small, then a transformed image set via rotation and mirroring etc. may still be too small for a given problem. Another solution is the sourcing of entirely new and synthetic images through various techniques, for example the use of Generative adversarial networks to create new synthetic images for data augmentation. Additionally, image recognition algorithms show improvement when transferring from synthetic images generated by the Unity Game Engine, that is, to improve learning of real world data by augmenting the training process with rendered images from virtual environments.

Data augmentation for Signal Processing
Biological Signals
Synthetic data augmentation is of paramount importance for machine learning classification, particularly for biological data, which tend to be high dimensional and scarce. The applications of robotic control and augmentation in disabled and able-bodied subjects still rely mainly on subject-specific analyses. Data scarcity is notable in signal processing problems such as for Parkinson's Disease Electromyography signals, which are difficult to source - Zanini, et al. noted that it is possible to use a Generative adversarial network (in particular, a DCGAN) to perform style transfer in order to generate synthetic electromyographic signals that corresponded to those exhibited by sufferers of Parkinson's Disease.The approaches are also important in electroencephalography (brainwaves). Wang, et al. explored the idea of using Deep Convolutional Neural Networks for EEG-Based Emotion Recognition, results show that emotion recognition was improved when data augmentation was used.

It has also been noted that OpenAI's GPT-2 model is capable of learning from, and generating synthetic biological signals such as EEG and EMG. In this study, it was noted that recognition was improved via data augmentation. It was also noted that statistical machine learning models trained on the synthetic domain could classify the human data, and vice versa. In the image, a comparison is given by some examples of EEG produced by the GPT-2 model and a human brain. 
A common approach is to generate synthetic signals by re-arranging components of real data. Lotte proposed a method of ""Artificial Trial Generation Based on Analogy"" where three data examples 
  
    
      
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        
          x
          
            3
          
        
      
    
    {\displaystyle x_{1},x_{2},x_{3}}
   provide examples and an artificial 
  
    
      
        
          x
          
            s
            y
            n
            t
            h
            e
            t
            i
            c
          
        
      
    
    {\displaystyle x_{synthetic}}
   is formed which is to 
  
    
      
        
          x
          
            3
          
        
      
    
    {\displaystyle x_{3}}
   what 
  
    
      
        
          x
          
            2
          
        
      
    
    {\displaystyle x_{2}}
   is to 
  
    
      
        
          x
          
            1
          
        
      
    
    {\displaystyle x_{1}}
  . A transformation is applied to 
  
    
      
        
          x
          
            1
          
        
      
    
    {\displaystyle x_{1}}
   to make it more similar to 
  
    
      
        
          x
          
            2
          
        
      
    
    {\displaystyle x_{2}}
  , the same transformation is then applied to 
  
    
      
        
          x
          
            3
          
        
      
    
    {\displaystyle x_{3}}
   which generates 
  
    
      
        
          x
          
            s
            y
            n
            t
            h
            e
            t
            i
            c
          
        
      
    
    {\displaystyle x_{synthetic}}
  . This approach was shown to improve performance of a Linear Discriminant Analysis classifier on three different datasets.
Current research shows great impact can be derived from relatively simple techniques. For example, Freer observed that introducing noise into gathered data to form additional data points improved the learning ability of several models which otherwise performed relatively poorly. Tsinganos et al. studied the approaches of magnitude warping, wavelet decomposition, and synthetic surface EMG models (generative approaches) for hand gesture recognition, finding classification performance increases of up to +16% when augmented data was introduced during training. More recently, data augmentation studies have begun to focus on the field of deep learning, more specifically on the ability of generative models to create artificial data which is then introduced during the classification model training process. In 2018, Luo et al. observed that useful EEG signal data could be generated by Conditional Wasserstein Generative Adversarial Networks (GANs) which was then introduced to the training set in a classical train-test learning framework. The authors found classification performance was improved when such techniques were introduced.

Data augmentation for Speech Recognition
It has been noted that synthetic data generation of spoken MFCCs can improve the recognition of a speaker from their utterances via transfer learning from synthetic data which has been generated via a Character-level Recurrent Neural Network (RNN).

See also
Oversampling and undersampling in data analysis
Generative adversarial network
Data pre-processing
Convolutional neural network
Regularization (mathematics)
Data preparation
Data fusion


== References ==",https://en.wikipedia.org/wiki/Data_augmentation,"['All articles needing additional references', 'Articles needing additional references from September 2020', 'CS1 errors: missing periodical', 'CS1 maint: date and year', 'Machine learning']",Data Science
39,Data cleansing,"Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting.
After cleansing, a data set should be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores. Data cleaning differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at the time of entry, rather than on batches of data.

The actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code), or with fuzzy or approximate string matching (such as correcting records that partially match existing, known records). Some data cleansing solutions will clean data by cross-checking with a validated data set. A common data cleansing practice is data enhancement, where data is made more complete by adding related information. For example, appending addresses with any phone numbers related to that address. Data cleansing may also involve harmonization (or normalization) of data, which is the process of bringing together data of ""varying file formats, naming conventions, and columns"", and transforming it into one cohesive data set; a simple example is the expansion of abbreviations (""st, rd, etc."" to ""street, road, etcetera"").

Motivation
Administratively incorrect, inconsistent data can lead to false conclusions and misdirect investments on both public and private scales. For instance, the government may want to analyze population census figures to decide which regions require further spending and investment on infrastructure and services. In this case, it will be important to have access to reliable data to avoid erroneous fiscal decisions. In the business world, incorrect data can be costly. Many companies use customer information databases that record data like contact information, addresses, and preferences. For instance, if the addresses are inconsistent, the company will suffer the cost of resending mail or even losing customers.

Data quality
High-quality data needs to pass a set of quality criteria. Those include:

Validity: The degree to which the measures conform to defined business rules or constraints (see also Validity (statistics)). When modern database technology is used to design data-capture systems, validity is fairly easy to ensure: invalid data arises mainly in legacy contexts (where constraints were not implemented in software) or where inappropriate data-capture technology was used (e.g., spreadsheets, where it is very hard to limit what a user chooses to enter into a cell, if cell validation is not used). Data constraints fall into the following categories:
Data-Type Constraints – e.g., values in a particular column must be of a particular data type, e.g., Boolean, numeric (integer or real), date, etc.
Range Constraints: typically, numbers or dates should fall within a certain range. That is, they have minimum and/or maximum permissible values.
Mandatory Constraints: Certain columns cannot be empty.
Unique Constraints: A field, or a combination of fields, must be unique across a dataset. For example, no two persons can have the same social security number.
Set-Membership constraints: The values for a column come from a set of discrete values or codes. For example, a person's gender may be Female, Male, or Other.
Foreign-key constraints: This is the more general case of set membership. The set of values in a column is defined in a column of another table that contains unique values. For example, in a US taxpayer database, the ""state"" column is required to belong to one of the US's defined states or territories: the set of permissible states/territories is recorded in a separate State table. The term foreign key is borrowed from relational database terminology.
Regular expression patterns: Occasionally, text fields will have to be validated this way. For example, phone numbers may be required to have the pattern (999) 999-9999.
Cross-field validation: Certain conditions that utilize multiple fields must hold. For example, in laboratory medicine, the sum of the components of the differential white blood cell count must be equal to 100 (since they are all percentages). In a hospital database, a patient's date of discharge from the hospital cannot be earlier than the date of admission.
Accuracy: The degree of conformity of a measure to a standard or a true value - see also Accuracy and precision. Accuracy is very hard to achieve through data-cleansing in the general case because it requires accessing an external source of data that contains the true value: such ""gold standard"" data is often unavailable. Accuracy has been achieved in some cleansing contexts, notably customer contact data, by using external databases that match up zip codes to geographical locations (city and state) and also help verify that street addresses within these zip codes actually exist.
Completeness: The degree to which all required measures are known. Incompleteness is almost impossible to fix with data cleansing methodology: one cannot infer facts that were not captured when the data in question was initially recorded. (In some contexts, e.g., interview data, it may be possible to fix incompleteness by going back to the original source of data, i.e. re-interviewing the subject, but even this does not guarantee success because of problems of recall - e.g., in an interview to gather data on food consumption, no one is likely to remember exactly what one ate six months ago. In the case of systems that insist certain columns should not be empty, one may work around the problem by designating a value that indicates ""unknown"" or ""missing"", but the supplying of default values does not imply that the data has been made complete.)
Consistency: The degree to which a set of measures are equivalent in across systems (see also Consistency). Inconsistency occurs when two data items in the data set contradict each other: e.g., a customer is recorded in two different systems as having two different current addresses, and only one of them can be correct. Fixing inconsistency is not always possible: it requires a variety of strategies - e.g., deciding which data were recorded more recently, which data source is likely to be most reliable (the latter knowledge may be specific to a given organization), or simply trying to find the truth by testing both data items (e.g., calling up the customer).
Uniformity: The degree to which a set data measures are specified using the same units of measure in all systems ( see also Unit of measure). In datasets pooled from different locales, weight may be recorded either in pounds or kilos and must be converted to a single measure using an arithmetic transformation.The term integrity encompasses accuracy, consistency and some aspects of validation (see also data integrity) but is rarely used by itself in data-cleansing contexts because it is insufficiently specific. (For example, ""referential integrity"" is a term used to refer to the enforcement of foreign-key constraints above.)

Process
Data auditing: The data is audited with the use of statistical and database methods to detect anomalies and contradictions: this eventually indicates the characteristics of the anomalies and their locations. Several commercial software packages will let you specify constraints of various kinds (using a grammar that conforms to that of a standard programming language, e.g., JavaScript or Visual Basic) and then generate code that checks the data for violation of these constraints. This process is referred to below in the bullets ""workflow specification"" and ""workflow execution."" For users who lack access to high-end cleansing software, Microcomputer database packages such as Microsoft Access or File Maker Pro will also let you perform such checks, on a constraint-by-constraint basis, interactively with little or no programming required in many cases.
Workflow specification: The detection and removal of anomalies are performed by a sequence of operations on the data known as the workflow. It is specified after the process of auditing the data and is crucial in achieving the end product of high-quality data. In order to achieve a proper workflow, the causes of the anomalies and errors in the data have to be closely considered.
Workflow execution: In this stage, the workflow is executed after its specification is complete and its correctness is verified. The implementation of the workflow should be efficient, even on large sets of data, which inevitably poses a trade-off because the execution of a data-cleansing operation can be computationally expensive.
Post-processing and controlling: After executing the cleansing workflow, the results are inspected to verify correctness. Data that could not be corrected during the execution of the workflow is manually corrected, if possible. The result is a new cycle in the data-cleansing process where the data is audited again to allow the specification of an additional workflow to further cleanse the data by automatic processing.Good quality source data has to do with “Data Quality Culture” and must be initiated at the top of the organization. It is not just a matter of implementing strong validation checks on input screens, because almost no matter how strong these checks are, they can often still be circumvented by the users. There is a nine-step guide for organizations that wish to improve data quality:
Declare a high-level commitment to a data quality culture
Drive process reengineering at the executive level
Spend money to improve the data entry environment
Spend money to improve application integration
Spend money to change how processes work
Promote end-to-end team awareness
Promote interdepartmental cooperation
Publicly celebrate data quality excellence
Continuously measure and improve data qualityOthers include:

Parsing:  for the detection of syntax errors. A parser decides whether a string of data is acceptable within the allowed data specification. This is similar to the way a parser works with grammars and languages.
Data transformation: Data transformation allows the mapping of the data from its given format into the format expected by the appropriate application. This includes value conversions or translation functions, as well as normalizing numeric values to conform to minimum and maximum values.
Duplicate elimination: Duplicate detection requires an algorithm for determining whether data contains duplicate representations of the same entity. Usually, data is sorted by a key that would bring duplicate entries closer together for faster identification.
Statistical methods: By analyzing the data using the values of mean, standard deviation, range, or clustering algorithms, it is possible for an expert to find values that are unexpected and thus erroneous. Although the correction of such data is difficult since the true value is not known, it can be resolved by setting the values to an average or other statistical value. Statistical methods can also be used to handle missing values which can be replaced by one or more plausible values, which are usually obtained by extensive data augmentation algorithms.

System
The essential job of this system is to find a suitable balance between fixing dirty data and maintaining the data as close as possible to the original data from the source production system. This is a challenge for the Extract, transform, load architect. The system should offer an architecture that can cleanse data, record quality events and measure/control quality of data in the data warehouse. A good start is to perform a thorough data profiling analysis that will help define to the required complexity of the data cleansing system and also give an idea of the current data quality in the source system(s).

Quality screens
Part of the data cleansing system is a set of diagnostic filters known as quality screens. They each implement a test in the data flow that, if it fails, records an error in the Error Event Schema. Quality screens are divided into three categories:

Column screens. Testing the individual column, e.g. for unexpected values like NULL values; non-numeric values that should be numeric; out of range values; etc.
Structure screens. These are used to test for the integrity of different relationships between columns (typically foreign/primary keys) in the same or different tables. They are also used for testing that a group of columns is valid according to some structural definition to which it should adhere.
Business rule screens. The most complex of the three tests. They test to see if data, maybe across multiple tables, follow specific business rules. An example could be, that if a customer is marked as a certain type of customer, the business rules that define this kind of customer should be adhered to.When a quality screen records an error, it can either stop the dataflow process, send the faulty data somewhere else than the target system or tag the data.
The latter option is considered the best solution because the first option requires, that someone has to manually deal with the issue each time it occurs and the second implies that data are missing from the target system (integrity) and it is often unclear what should happen to these data.

Criticism of existing tools and processes
Most data cleansing tools have limitations in usability:

Project costs: costs typically in the hundreds of thousands of dollars
Time: mastering large-scale data-cleansing software is time-consuming
Security: cross-validation requires sharing information, giving an application access across systems, including sensitive legacy systems

Error event schema
The Error Event schema holds records of all error events thrown by the quality screens. It consists of an Error Event Fact table with foreign keys to three dimension tables that represent date (when), batch job (where) and screen (who produced error). It also holds information about exactly when the error occurred and the severity of the error. Also, there is an Error Event Detail Fact table with a foreign key to the main table that contains detailed information about in which table, record and field the error occurred and the error condition.

See also
Data editing
Data mining
Iterative proportional fitting
Record linkage
Single customer view
Triangulation (social science)

References
Sources
Han, J., Kamber, M.  Data Mining: Concepts and Techniques, Morgan Kaufmann, 2001. ISBN 1-55860-489-8.
Kimball, R., Caserta, J. The Data Warehouse ETL Toolkit, Wiley and Sons, 2004. ISBN 0-7645-6757-8.
Muller H., Freytag J., Problems, Methods, and Challenges in Comprehensive Data Cleansing, Humboldt-Universitat zu Berlin, Germany, 2003.
Rahm, E., Hong, H. Data Cleaning: Problems and Current Approaches, University of Leipzig, Germany, 2000.

External links
Computerworld: Data Scrubbing (February 10, 2003)
Erhard Rahm, Hong Hai Do: Data Cleaning: Problems and Current Approaches",https://en.wikipedia.org/wiki/Data_cleansing,"['Articles with short description', 'Business intelligence', 'Data quality', 'Short description is different from Wikidata']",Data Science
40,Data collection,"Data collection is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. Data collection is a research component in all study fields, including physical and social sciences, humanities, and business. While methods vary by discipline, the emphasis on ensuring accurate and honest collection remains the same. The goal for all data collection is to capture quality evidence that allows analysis to lead to the formulation of convincing and credible answers to the questions that have been posed.Data collection and validation consists of four steps when it involves taking a census and seven steps when it involves sampling

Importance
Regardless of the field of study or preference for defining data (quantitative or qualitative), accurate data collection is essential to maintain research integrity. The selection of appropriate data collection instruments (existing, modified, or newly developed) and delineated instructions for their correct use reduce the likelihood of errors.
A formal data collection process is necessary as it ensures that the data gathered are both defined and accurate. This way, subsequent decisions based on arguments embodied in the findings are made using valid data. The process provides both a baseline from which to measure and in certain cases an indication of what to improve.
There are 5 common data collection methods:

closed-ended surveys and quizzes,
open-ended surveys and questionnaires,
1-on-1 interviews,
focus groups, and
direct observation.

Data integrity issues
The main reason for maintaining data integrity is to support the observation of errors in the data collection process. Those errors may be made intentionally (deliberate falsification) or non-intentionally (random or systematic errors).
There are two approaches that may protect data integrity and secure scientific validity of study results invented by Craddick, Crawford, Rhodes, Redican, Rukenbrod and Laws in 2003:

Quality assurance – all actions carried out before data collection
Quality control – all actions carried out during and after data collection

Quality assurance
Its main focus is prevention which is primarily a cost-effective activity to protect the integrity of data collection. Standardization of protocol best demonstrates this cost-effective activity, which is developed in a comprehensive and detailed procedures manual for data collection. The risk of failing to identify problems and errors in the research process is evidently caused by poorly written guidelines. Listed are several examples of such failures:

Uncertainty of timing, methods and identification of the responsible person
Partial listing of items needed to be collected
Vague description of data collection instruments instead of rigorous step-by-step instructions on administering tests
Failure to recognize exact content and strategies for training and retraining staff members responsible for data collection
Unclear instructions for using, making adjustments to, and calibrating data collection equipment
No predetermined mechanism to document changes in procedures that occur during the investigation

Quality control
Since quality control actions occur during or after the data collection all the details are carefully documented. There is a necessity for a clearly defined communication structure as a precondition for establishing monitoring systems. Uncertainty about the flow of information is not recommended as a poorly organized communication structure leads to lax monitoring and can also limit the opportunities for detecting errors. Quality control is also responsible for the identification of actions necessary for correcting faulty data collection practices and also minimizing such future occurrences. A team is more likely to not realize the necessity to perform these actions if their procedures are written vaguely and are not based on feedback or education.
Data collection problems that necessitate prompt action:

Systematic errors
Violation of protocol
Fraud or scientific misconduct
Errors in individual data items
Individual staff or site performance problems

Data collection on z/OS
z/OS is a widely used operating system for IBM mainframe. It is designed to offer a stable, secure, and continuously available environment for applications running on the mainframe. Operational data is data that z/OS system produces when it runs. This data indicates the health of the system and can be used to identify sources of performance and availability issues in the system. The analysis of operational data by analytics platforms provide insights and recommended actions to make the system work more efficiently, and to help resolve or prevent problems. IBM Z Common Data Provider collects IT operational data from z/OS systems, transforms it to a consumable format, and streams it to analytics platforms.IBM Z Common Data Provider supports the collection of the following operational data:
System Management Facilities (SMF) data
Log data from the following sources:
Job log, the output which is written to a data definition (DD) by a running job
z/OS UNIX log file, including the UNIX System Services system log (syslogd)
Entry-sequenced Virtual Storage Access Method (VSAM) cluster
z/OS system log (SYSLOG)
IBM Tivoli NetView for z/OS messages
IBM WebSphere Application Server for z/OS High Performance Extensible Logging (HPEL) log
IBM Resource Measurement Facility (RMF) Monitor III reports
User application data, the operational data from users' own applications

DMPs and data collection
DMP is the abbreviation for data management platform. It is a centralized storage and analytical system for data. Mainly used by marketers, DMPs exist to compile and transform large amounts of data into discernible information. Marketers may want to receive and utilize first, second and third-party data. DMPs enable this, because they are the aggregate system of DSPs (demand side platform) and SSPs (supply side platform). When in comes to advertising, DMPs are integral for optimizing and guiding marketers in future campaigns. This system and their effectiveness is proof that categorized, analyzed, and compiled data is far more useful than raw data.

See also

References
External links
Bureau of Statistics, Guyana by Arun Sooknarine",https://en.wikipedia.org/wiki/Data_collection,"['All articles needing additional references', 'Articles needing additional references from April 2017', 'Commons link from Wikidata', 'Data collection', 'Design of experiments', 'Survey methodology']",Data Science
41,Data corruption,"Data corruption refers to errors in computer data that occur during writing, reading, storage, transmission, or processing, which introduce unintended changes to the original data. Computer, transmission, and storage systems use a number of measures to provide end-to-end data integrity, or lack of errors.
In general, when data corruption occurs, a file containing that data will produce unexpected results when accessed by the system or the related application. Results could range from a minor loss of data to a system crash. For example, if a document file is corrupted, when a person tries to open that file with a document editor they may get an error message, thus the file might not be opened or might open with some of the data corrupted (or in some cases, completely corrupted, leaving the document unintelligible). The adjacent image is a corrupted image file in which most of the information has been lost.
Some types of malware may intentionally corrupt files as part of their payloads, usually by overwriting them with inoperative or garbage code, while a non-malicious virus may also unintentionally corrupt files when it accesses them. If a virus or trojan with this payload method manages to alter files critical to the running of the computer's operating system software or physical hardware, the entire system may be rendered unusable.
Some programs can give a suggestion to repair the file automatically (after the error), and some programs cannot repair it. It depends on the level of corruption, and the built-in functionality of the application to handle the error. There are various causes of the corruption.

Overview
There are two types of data corruption associated with computer systems: undetected and detected.  Undetected data corruption, also known as silent data corruption, results in the most dangerous errors as there is no indication that the data is incorrect.  Detected
data corruption may be permanent with the loss of data, or may be temporary when some part of the system is able to detect and correct the error; there is no data corruption in the latter case.
Data corruption can occur at any level in a system, from the host to the storage medium.  Modern systems attempt to detect corruption at many layers and then recover or correct the corruption; this is almost always successful but very rarely the information arriving in the systems memory is corrupted and can cause unpredictable results.
Data corruption during transmission has a variety of causes. Interruption of data transmission causes information loss. Environmental conditions can interfere with data transmission, especially when dealing with wireless transmission methods. Heavy clouds can block satellite transmissions. Wireless networks are susceptible to interference from devices such as microwave ovens.
Hardware and software failure are the two main causes for data loss.  Background radiation, head crashes, and aging or wear of the storage device fall into the former category, while software failure typically occurs due to bugs in the code.
Cosmic rays cause most soft errors in DRAM.

Silent
Some errors go unnoticed, without being detected by the disk firmware or the host operating system; these errors are known as silent data corruption.
There are many error sources beyond the disk storage subsystem itself. For instance, cables might be slightly loose, the power supply might be unreliable, external vibrations such as a loud sound, the network might introduce undetected corruption, cosmic radiation and many other causes of soft memory errors, etc. In 39,000 storage systems that were analyzed, firmware bugs accounted for 5–10% of storage failures. All in all, the error rates as observed by a CERN study on silent corruption are far higher than one in every 1016 bits. Webshop Amazon.com has acknowledged similar high data corruption rates in their systems.One problem is that hard disk drive capacities have increased substantially, but their error rates remain unchanged. The data corruption rate has always been roughly constant in time, meaning that modern disks are not much safer than old disks. In old disks the probability of data corruption was very small because they stored tiny amounts of data. In modern disks the probability is much larger because they store much more data, whilst not being safer. That way, silent data corruption has not been a serious concern while storage devices remained relatively small and slow. In modern times and with the advent of larger drives and very fast RAID setups, users are capable of transferring 1016 bits in a reasonably short time, thus easily reaching the data corruption thresholds.As an example, ZFS creator Jeff Bonwick stated that the fast database at Greenplum, which is a database software company specializing in large-scale data warehousing and analytics, faces silent corruption every 15 minutes.  As another example, a real-life study performed by NetApp on more than 1.5 million HDDs over 41 months found more than 400,000 silent data corruptions, out of which more than 30,000 were not detected by the hardware RAID controller.  Another study, performed by CERN over six months and involving about 97 petabytes of data, found that about 128 megabytes of data became permanently corrupted.Silent data corruption may result in cascading failures, in which the system may run for a period of time with undetected initial error causing increasingly more problems until it is ultimately detected.  For example, a failure affecting file system metadata can result in multiple files being partially damaged or made completely inaccessible as the file system is used in its corrupted state.

Countermeasures
When data corruption behaves as a Poisson process, where each bit of data has an independently low probability of being changed, data corruption can generally be detected by the use of checksums, and can often be corrected by the use of error correcting codes.
If an uncorrectable data corruption is detected, procedures such as automatic retransmission or restoration from backups can be applied. Certain levels of RAID disk arrays have the ability to store and evaluate parity bits for data across a set of hard disks and can reconstruct corrupted data upon the failure of a single or multiple disks, depending on the level of RAID implemented.  Some CPU architectures employ various transparent checks to detect and mitigate data corruption in CPU caches, CPU buffers and instruction pipelines; an example is Intel Instruction Replay technology, which is available on Intel Itanium processors.Many errors are detected and corrected by the hard disk drives using the ECC codes which are stored on disk for each sector.  If the disk drive detects multiple read errors on a sector it may make a copy of the failing sector on another part of the disk, by remapping the failed sector of the disk to a spare sector without the involvement of the operating system (though this may be delayed until the next write to the sector).  This ""silent correction"" can be monitored using S.M.A.R.T. and tools available for most operating systems to automatically check the disk drive for impending failures by watching for deteriorating SMART parameters.
Some file systems, such as Btrfs, HAMMER, ReFS, and ZFS, use internal data and metadata checksumming to detect silent data corruption.  In addition, if a corruption is detected and the file system uses integrated RAID mechanisms that provide data redundancy, such file systems can also reconstruct corrupted data in a transparent way.  This approach allows improved data integrity protection covering the entire data paths, which is usually known as end-to-end data protection, compared with other data integrity approaches that do not span different layers in the storage stack and allow data corruption to occur while the data passes boundaries between the different layers.Data scrubbing is another method to reduce the likelihood of data corruption, as disk errors are caught and recovered from before multiple errors accumulate and overwhelm the number of parity bits.  Instead of parity being checked on each read, the parity is checked during a regular scan of the disk, often done as a low priority background process. Note that the ""data scrubbing"" operation activates a parity check.  If a user simply runs a normal program that reads data from the disk, then the parity would not be checked unless parity-check-on-read was both supported and enabled on the disk subsystem.
If appropriate mechanisms are employed to detect and remedy data corruption, data integrity can be maintained. This is particularly important in commercial applications (e.g. banking), where an undetected error could either corrupt a database index or change data to drastically affect an account balance, and in the use of encrypted or compressed data, where a small error can make an extensive dataset unusable.

See also
References
External links
SoftECC: A System for Software Memory Integrity Checking
A Tunable, Software-based DRAM Error Detection and Correction Library for HPC
Detection and Correction of Silent Data Corruption for Large-Scale High-Performance Computing
End-to-end Data Integrity for File Systems: A ZFS Case Study
DRAM Errors in the Wild: A Large-Scale Field Study
A study on silent corruptions, and an associated paper on data integrity (CERN, 2007)
End-to-end Data Protection in SAS and Fibre Channel Hard Disk Drives (HGST)",https://en.wikipedia.org/wiki/Data_corruption,"['All articles needing additional references', 'Articles needing additional references from November 2013', 'Data quality', 'Product expiration']",Data Science
42,Data curation,"Data curation is the organization and integration of data collected from various sources. It involves annotation, publication and presentation of the data such that the value of the data is maintained over time, and the data remains available for reuse and preservation. Data curation includes ""all the processes needed for principled and controlled data creation, maintenance, and management, together with the capacity to add value to data"". In science, data curation may indicate the process of extraction of important information from scientific texts, such as research articles by experts, to be converted into an electronic format, such as an entry of a biological database.In the modern era of big data, the curation of data has become more prominent, particularly for software processing high volume and complex data systems.  The term is also used in historical occasions and the humanities, where increasing cultural and scholarly data from digital humanities projects requires the expertise and analytical practices of data curation. In broad terms, curation means a range of activities and processes done to create, manage, maintain, and validate a component. Specifically, data curation is the attempt to determine what information is worth saving and for how long.

History and practice
The user, rather than the database itself, typically initiates data curation and maintains metadata. According to the University of Illinois' Graduate School of Library and Information Science, ""Data curation is the active and on-going management of data through its lifecycle of interest and usefulness to scholarship, science, and education; curation activities enable data discovery and retrieval, maintain quality, add value, and provide for re-use over time.""  The data curation workflow is distinct from data quality management, data protection, lifecycle management, and data movement.Census data has been available in tabulated punch card form since the early 20th century and has been electronic since the 1960s. The Inter-university Consortium for Political and Social Research (ICPSR) website marks 1962 as the date of their first Survey Data Archive.Deep background on data libraries appeared in a 1982 issue of the Illinois journal, Library Trends. For historical background on the data archive movement, see ""Social Scientific Information Needs for Numeric Data: The Evolution of the International Data Archive Infrastructure.""  The exact curation process undertaken within any organisation depends on the volume of data, how much noise the data contains, and what the expected future use of the data means to its dissemination.The crises in space data led to the 1999 creation of the Open Archival Information System (OAIS) model, stewarded by the Consultative Committee for Space Data Systems (CCSDS), which was formed in 1982.The term data curation is sometimes used in the context of biological databases, where specific biological information is firstly obtained from a range of research articles and then stored within a specific category of database. For instance, information about anti-depressant drugs can be obtained from various sources and, after checking whether they are available as a database or not, they are saved under a drug's database's anti-depressive category. Enterprises are also utilizing data curation within their operational and strategic processes to ensure data quality and accuracy.

Projects and studies
The Dissemination Information Packages (DIPS) for Information Reuse (DIPIR) project is studying research data produced and used by quantitative social scientists, archaeologists, and zoologists. The intended audience is researchers who use secondary data and the digital curators, digital repository managers, data center staff, and others who collect, manage, and store digital information.The Protein Data Bank was established in 1971 at Brookhaven National Laboratory, and has grown into a global project. A database for three-dimensional structural data of proteins and other large biological molecules, the PDB contains over 120,000 structures, all standardized, validated against experimental data, and annotated.
FlyBase, the primary repository of genetic and molecular data for the insect family Drosophilidae, dates back to 1992. FlyBase annotates the entire Drosophila melanogaster genome.The Linguistic Data Consortium is a data repository for linguistic data, dating back to 1992.The Sloan Digital Sky Survey began surveying the night sky in 2000. Computer scientist Jim Gray, while working on the data architecture of the SDSS, championed the idea of data curation in the sciences.DataNet was a research program of the U.S. National Science Foundation Office of Cyberinfrastructure, funding data management projects in the sciences. DataONE (Data Observation Network for Earth) is one of the projects funded through DataNet, helping the environmental science community preserve and share data.

See also
Biocurator
Data archaeology
Data degradation
Data format management
Data preservation
Data stewardship
Data wrangling
Digital curation – the curation of published documents, rather than raw data
Digital preservation
Informationist – an individual with extensive expertise in data curation

References
External links
Curation of ecological and environmental data: DataONE
Data management tools and services spanning multiple scientific disciplines: DataConservancy",https://en.wikipedia.org/wiki/Data_curation,"['Archival science', 'Data management', 'Digital preservation', 'Records management', 'Webarchive template wayback links']",Data Science
43,Data compression,"In signal processing, data compression, source coding, or bit-rate reduction is the process of encoding information using fewer bits than the original representation. Any particular compression is either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information. Typically, a device that performs data compression is referred to as an encoder, and one that performs the reversal of the process (decompression) as a decoder.
The process of reducing the size of a data file is often referred to as data compression. In the context of data transmission, it is called source coding; encoding done at the source of the data before it is stored or transmitted. Source coding should not be confused with channel coding, for error detection and correction or line coding, the means for mapping data onto a signal.
Compression is useful because it reduces the resources required to store and transmit data. Computational resources are consumed in the compression and decompression processes. Data compression is subject to a space–time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data.

Lossless
Lossless data compression algorithms usually exploit statistical redundancy to represent data without losing any information, so that the process is reversible. Lossless compression is possible because most real-world data exhibits statistical redundancy. For example, an image may have areas of color that do not change over several pixels; instead of coding ""red pixel, red pixel, ..."" the data may be encoded as ""279 red pixels"". This is a basic example of run-length encoding; there are many schemes to reduce file size by eliminating redundancy.
The Lempel–Ziv (LZ) compression methods are among the most popular algorithms for lossless storage.  DEFLATE is a variation on LZ optimized for decompression speed and compression ratio, but compression can be slow. In the mid-1980s, following work by Terry Welch, the Lempel–Ziv–Welch (LZW) algorithm rapidly became the method of choice for most general-purpose compression systems. LZW is used in GIF images, programs such as PKZIP, and hardware devices such as modems. LZ methods use a table-based compression model where table entries are substituted for repeated strings of data. For most LZ methods, this table is generated dynamically from earlier data in the input. The table itself is often Huffman encoded. Grammar-based codes like this can compress highly repetitive input extremely effectively, for instance, a biological data collection of the same or closely related species, a huge versioned document collection, internet archival, etc. The basic task of grammar-based codes is constructing a context-free grammar deriving a single string. Other practical grammar compression algorithms include Sequitur and Re-Pair.
The strongest modern lossless compressors use probabilistic models, such as prediction by partial matching. The Burrows–Wheeler transform can also be viewed as an indirect form of statistical modelling. In a further refinement of the direct use of probabilistic modelling, statistical estimates can be coupled to an algorithm called arithmetic coding. Arithmetic coding is a more modern coding technique that uses the mathematical calculations of a finite-state machine to produce a string of encoded bits from a series of input data symbols. It can achieve superior compression compared to other techniques such as the better-known Huffman algorithm. It uses an internal memory state to avoid the need to perform a one-to-one mapping of individual input symbols to distinct representations that use an integer number of bits, and it clears out the internal memory only after encoding the entire string of data symbols. Arithmetic coding applies especially well to adaptive data compression tasks where the statistics vary and are context-dependent, as it can be easily coupled with an adaptive model of the probability distribution of the input data. An early example of the use of arithmetic coding was in an optional (but not widely used) feature of the JPEG image coding standard. It has since been applied in various other designs including H.263, H.264/MPEG-4 AVC and HEVC for video coding.

Lossy
In the late 1980s, digital images became more common, and standards for lossless image compression emerged. In the early 1990s, lossy compression methods began to be widely used. In these schemes, some loss of information is accepted as dropping nonessential detail can save storage space. There is a corresponding trade-off between preserving information and reducing size. Lossy data compression schemes are designed by research on how people perceive the data in question. For example, the human eye is more sensitive to subtle variations in luminance than it is to the variations in color. JPEG image compression works in part by rounding off nonessential bits of information.  A number of popular compression formats exploit these perceptual differences, including psychoacoustics for sound, and psychovisuals for images and video.
Most forms of lossy compression are based on transform coding, especially the discrete cosine transform (DCT). It was first proposed in 1972 by Nasir Ahmed, who then developed a working algorithm with T. Natarajan and K. R. Rao in 1973, before introducing it in January 1974. DCT is the most widely used lossy compression method, and is used in multimedia formats for images (such as JPEG and HEIF), video (such as MPEG, AVC and HEVC) and audio (such as MP3, AAC and Vorbis).
Lossy image compression is used in digital cameras, to increase storage capacities. Similarly, DVDs, Blu-ray and streaming video use lossy video coding formats. Lossy compression is extensively used in video.
In lossy audio compression, methods of psychoacoustics are used to remove non-audible (or less audible) components of the audio signal. Compression of human speech is often performed with even more specialized techniques; speech coding is distinguished as a separate discipline from general-purpose audio compression. Speech coding is used in internet telephony, for example, audio compression is used for CD ripping and is decoded by the audio players.Lossy compression can cause generation loss.

Theory
The theoretical basis for compression is provided by information theory and, more specifically, algorithmic information theory for lossless compression and rate–distortion theory for lossy compression. These areas of study were essentially created by Claude Shannon, who published fundamental papers on the topic in the late 1940s and early 1950s. Other topics associated with compression include coding theory and statistical inference.

Machine learning
There is a close connection between machine learning and compression. A system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution). An optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for ""general intelligence"".An alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space ℵ, such that C(.) maps an input string x, corresponds to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.According to AIXI theory, a connection more directly explained in Hutter Prize, the best possible compression of x is the smallest possible software which generates x. For example, in that model, a zip file's compressed size includes both the zip file and the unzipping software, since you can't unzip it without both, but there may be an even smaller combined form.

Data differencing
Data compression can be viewed as a special case of data differencing. Data differencing consists of producing a difference given a source and a target, with patching reproducing the target given a source and a difference. Since there is no separate source and target in data compression, one can consider data compression as data differencing with empty source data, the compressed file corresponding to a difference from nothing. This is the same as considering absolute entropy (corresponding to data compression) as a special case of relative entropy (corresponding to data differencing) with no initial data.
The term differential compression is used to emphasize the data differencing connection.

Uses
Image
Entropy coding originated in the 1940s with the introduction of Shannon–Fano coding, the basis for Huffman coding which was developed in 1950. Transform coding dates back to the late 1960s, with the introduction of fast Fourier transform (FFT) coding in 1968 and the Hadamard transform in 1969.An important image compression technique is the discrete cosine transform (DCT), a technique developed in the early 1970s. DCT is the basis for JPEG, a lossy compression format which was introduced by the Joint Photographic Experts Group (JPEG) in 1992. JPEG greatly reduces the amount of data required to represent an image at the cost of a relatively small reduction in image quality and has become the most widely used image file format. Its highly efficient DCT-based compression algorithm was largely responsible for the wide proliferation of digital images and digital photos.Lempel–Ziv–Welch (LZW) is a lossless compression algorithm developed in 1984. It is used in the GIF format, introduced in 1987. DEFLATE, a lossless compression algorithm specified in 1996, is used in the Portable Network Graphics (PNG) format.Wavelet compression, the use of wavelets in image compression, began after the development of DCT coding. The JPEG 2000 standard was introduced in 2000. In contrast to the DCT algorithm used by the original JPEG format, JPEG 2000 instead uses discrete wavelet transform (DWT) algorithms. JPEG 2000 technology, which includes the Motion JPEG 2000 extension, was selected as the video coding standard for digital cinema in 2004.

Audio
Audio data compression, not to be confused with dynamic range compression, has the potential to reduce the transmission bandwidth and storage requirements of audio data. Audio compression algorithms are implemented in software as audio codecs. In both lossy and lossless compression, information redundancy is reduced, using methods such as coding, quantization discrete cosine transform and linear prediction to reduce the amount of information used to represent the uncompressed data.
Lossy audio compression algorithms provide higher compression and are used in numerous audio applications including Vorbis and MP3. These algorithms almost all rely on psychoacoustics to eliminate or reduce fidelity of less audible sounds, thereby reducing the space required to store or transmit them.The acceptable trade-off between loss of audio quality and transmission or storage size depends upon the application. For example, one 640 MB compact disc (CD) holds approximately one hour of uncompressed high fidelity music, less than 2 hours of music compressed losslessly, or 7 hours of music compressed in the MP3 format at a medium bit rate. A digital sound recorder can typically store around 200 hours of clearly intelligible speech in 640 MB.Lossless audio compression produces a representation of digital data that can be decoded to an exact digital duplicate of the original. Compression ratios are around 50–60% of the original size, which is similar to those for generic lossless data compression. Lossless codecs use curve fitting or linear prediction as a basis for estimating the signal. Parameters describing the estimation and the difference between the estimation and the actual signal are coded separately.A number of lossless audio compression formats exist. See list of lossless codecs for a listing. Some formats are associated with a distinct system, such as Direct Stream Transfer, used in Super Audio CD and Meridian Lossless Packing, used in DVD-Audio, Dolby TrueHD, Blu-ray and HD DVD.
Some audio file formats feature a combination of a lossy format and a lossless correction; this allows stripping the correction to easily obtain a lossy file. Such formats include MPEG-4 SLS (Scalable to Lossless), WavPack, and OptimFROG DualStream.
When audio files are to be processed, either by further compression or for editing, it is desirable to work from an unchanged original (uncompressed or losslessly compressed). Processing of a lossily compressed file for some purpose usually produces a final result inferior to the creation of the same compressed file from an uncompressed original. In addition to sound editing or mixing, lossless audio compression is often used for archival storage, or as master copies.

Lossy audio compression
Lossy audio compression is used in a wide range of applications. In addition to standalone audio-only applications of file playback in MP3 players or computers, digitally compressed audio streams are used in most video DVDs, digital television, streaming media on the Internet, satellite and cable radio, and increasingly in terrestrial radio broadcasts. Lossy compression typically achieves far greater compression than lossless compression, by discarding less-critical data based on psychoacoustic optimizations.Psychoacoustics recognizes that not all data in an audio stream can be perceived by the human auditory system. Most lossy compression reduces redundancy by first identifying perceptually irrelevant sounds, that is, sounds that are very hard to hear. Typical examples include high frequencies or sounds that occur at the same time as louder sounds. Those irrelevant sounds are coded with decreased accuracy or not at all.
Due to the nature of lossy algorithms, audio quality suffers a digital generation loss when a file is decompressed and recompressed. This makes lossy compression unsuitable for storing the intermediate results in professional audio engineering applications, such as sound editing and multitrack recording. However, lossy formats such as MP3 are very popular with end users as the file size is reduced to 5-20% of the original size and a megabyte can store about a minute's worth of music at adequate quality.

Coding methods
To determine what information in an audio signal is perceptually irrelevant, most lossy compression algorithms use transforms such as the modified discrete cosine transform (MDCT) to convert time domain sampled waveforms into a transform domain, typically the frequency domain. Once transformed, component frequencies can be prioritized according to how audible they are. Audibility of spectral components is assessed using the absolute threshold of hearing and the principles of simultaneous masking—the phenomenon wherein a signal is masked by another signal separated by frequency—and, in some cases, temporal masking—where a signal is masked by another signal separated by time. Equal-loudness contours may also be used to weight the perceptual importance of components. Models of the human ear-brain combination incorporating such effects are often called psychoacoustic models.Other types of lossy compressors, such as the linear predictive coding (LPC) used with speech, are source-based coders. LPC uses a model of the human vocal tract to analyze speech sounds and infer the parameters used by the model to produce them moment to moment. These changing parameters are transmitted or stored and used to drive another model in the decoder which reproduces the sound.
Lossy formats are often used for the distribution of streaming audio or interactive communication (such as in cell phone networks). In such applications, the data must be decompressed as the data flows, rather than after the entire data stream has been transmitted. Not all audio codecs can be used for streaming applications.Latency is introduced by the methods used to encode and decode the data. Some codecs will analyze a longer segment, called a frame, of the data to optimize efficiency, and then code it in a manner that requires a larger segment of data at one time to decode. The inherent latency of the coding algorithm can be critical; for example, when there is a two-way transmission of data, such as with a telephone conversation, significant delays may seriously degrade the perceived quality.
In contrast to the speed of compression, which is proportional to the number of operations required by the algorithm, here latency refers to the number of samples that must be analyzed before a block of audio is processed. In the minimum case, latency is zero samples (e.g., if the coder/decoder simply reduces the number of bits used to quantize the signal). Time domain algorithms such as LPC also often have low latencies, hence their popularity in speech coding for telephony. In algorithms such as MP3, however, a large number of samples have to be analyzed to implement a psychoacoustic model in the frequency domain, and latency is on the order of 23 ms.

Speech encoding
Speech encoding is an important category of audio data compression. The perceptual models used to estimate what a human ear can hear are generally somewhat different from those used for music. The range of frequencies needed to convey the sounds of a human voice are normally far narrower than that needed for music, and the sound is normally less complex. As a result, speech can be encoded at high quality using a relatively low bit rate.
If the data to be compressed is analog (such as a voltage that varies with time), quantization is employed to digitize it into numbers (normally integers). This is referred to as analog-to-digital (A/D) conversion. If the integers generated by quantization are 8 bits each, then the entire range of the analog signal is divided into 256 intervals and all the signal values within an interval are quantized to the same number. If 16-bit integers are generated, then the range of the analog signal is divided into 65,536 intervals.
This relation illustrates the compromise between high resolution (a large number of analog intervals) and high compression (small integers generated). This application of quantization is used by several speech compression methods. This is accomplished, in general, by some combination of two approaches:

Only encoding sounds that could be made by a single human voice.
Throwing away more of the data in the signal—keeping just enough to reconstruct an ""intelligible"" voice rather than the full frequency range of human hearing.Perhaps the earliest algorithms used in speech encoding (and audio data compression in general) were the A-law algorithm and the μ-law algorithm.

History
In 1950, Bell Labs filed the patent on differential pulse-code modulation (DPCM). Adaptive DPCM (ADPCM) was introduced by P. Cummiskey, Nikil S. Jayant and James L. Flanagan at Bell Labs in 1973.Perceptual coding was first used for speech coding compression, with linear predictive coding (LPC). Initial concepts for LPC date back to the work of Fumitada Itakura (Nagoya University) and Shuzo Saito (Nippon Telegraph and Telephone) in 1966. During the 1970s, Bishnu S. Atal and Manfred R. Schroeder at Bell Labs developed a form of LPC called adaptive predictive coding (APC), a perceptual coding algorithm that exploited the masking properties of the human ear, followed in the early 1980s with the code-excited linear prediction (CELP) algorithm which achieved a significant compression ratio for its time. Perceptual coding is used by modern audio compression formats such as MP3 and AAC.
Discrete cosine transform (DCT), developed by Nasir Ahmed, T. Natarajan and K. R. Rao in 1974, provided the basis for the modified discrete cosine transform (MDCT) used by modern audio compression formats such as MP3 and AAC. MDCT was proposed by J. P. Princen, A. W. Johnson and A. B. Bradley in 1987, following earlier work by Princen and Bradley in 1986. The MDCT is used by modern audio compression formats such as Dolby Digital, MP3, and Advanced Audio Coding (AAC).The world's first commercial broadcast automation audio compression system was developed by Oscar Bonello, an engineering professor at the University of Buenos Aires. In 1983, using the psychoacoustic principle of the masking of critical bands first published in 1967, he started developing a practical application based on the recently developed IBM PC computer, and the broadcast automation system was launched in 1987 under the name Audicom. Twenty years later, almost all the radio stations in the world were using similar technology manufactured by a number of companies.
A literature compendium for a large variety of audio coding systems was published in the IEEE's Journal on Selected Areas in Communications (JSAC), in February 1988. While there were some papers from before that time, this collection documented an entire variety of finished, working audio coders, nearly all of them using perceptual (i.e. masking) techniques and some kind of frequency analysis and back-end noiseless coding. Several of these papers remarked on the difficulty of obtaining good, clean digital audio for research purposes. Most, if not all, of the authors in the JSAC edition were also active in the MPEG-1 Audio committee, which created the MP3 format.

Video
Video compression is a practical implementation of source coding in information theory. In practice, most video codecs are used alongside audio compression techniques to store the separate but complementary data streams as one combined package using so-called container formats.Uncompressed video requires a very high data rate. Although lossless video compression codecs perform at a compression factor of 5 to 12, a typical H.264 lossy compression video has a compression factor between 20 and 200.The two key video compression techniques used in video coding standards are the discrete cosine transform (DCT) and motion compensation (MC). Most video coding standards, such as the H.26x and MPEG formats, typically use motion-compensated DCT video coding (block motion compensation).

Encoding theory
Video data may be represented as a series of still image frames. Such data usually contains abundant amounts of spatial and temporal redundancy. Video compression algorithms attempt to reduce redundancy and store information more compactly.
Most video compression formats and codecs exploit both spatial and temporal redundancy (e.g. through difference coding with motion compensation). Similarities can be encoded by only storing differences between e.g. temporally adjacent frames (inter-frame coding) or spatially adjacent pixels (intra-frame coding).
Inter-frame compression (a temporal delta encoding) is one of the most powerful compression techniques. It (re)uses data from one or more earlier or later frames in a sequence to describe the current frame. Intra-frame coding, on the other hand, uses only data from within the current frame, effectively being still-image compression.A class of specialized formats used in camcorders and video editing use less complex compression schemes that restrict their prediction techniques to intra-frame prediction.
Usually video compression additionally employs lossy compression techniques like quantization that reduce aspects of the source data that are (more or less) irrelevant to the human visual perception by exploiting perceptual features of human vision. For example, small differences in color are more difficult to perceive than are changes in brightness. Compression algorithms can average a color across these similar areas to reduce space, in a manner similar to those used in JPEG image compression. As in all lossy compression, there is a trade-off between video quality and bit rate, cost of processing the compression and decompression, and system requirements. Highly compressed video may present visible or distracting artifacts.
Other methods than the prevalent DCT-based transform formats, such as fractal compression, matching pursuit and the use of a discrete wavelet transform (DWT), have been the subject of some research, but are typically not used in practical products (except for the use of wavelet coding as still-image coders without motion compensation). Interest in fractal compression seems to be waning, due to recent theoretical analysis showing a comparative lack of effectiveness of such methods.

Inter-frame coding
Inter-frame coding works by comparing each frame in the video with the previous one. Individual frames of a video sequence are compared from one frame to the next, and the video compression codec sends only the differences to the reference frame. If the frame contains areas where nothing has moved, the system can simply issue a short command that copies that part of the previous frame into the next one. If sections of the frame move in a simple manner, the compressor can emit a (slightly longer) command that tells the decompressor to shift, rotate, lighten, or darken the copy. This longer command still remains much shorter than intraframe compression. Usually the encoder will also transmit a residue signal which describes the remaining more subtle differences to the reference imagery. Using entropy coding, these residue signals have a more compact representation than the full signal. In areas of video with more motion, the compression must encode more data to keep up with the larger number of pixels that are changing. Commonly during explosions, flames, flocks of animals, and in some panning shots, the high-frequency detail leads to quality decreases or to increases in the variable bitrate.

Hybrid block-based transform formats
Today, nearly all commonly used video compression methods (e.g., those in standards approved by the ITU-T or ISO) share the same basic architecture that dates back to H.261 which was standardized in 1988 by the ITU-T. They mostly rely on the DCT, applied to rectangular blocks of neighboring pixels, and temporal prediction using motion vectors, as well as nowadays also an in-loop filtering step.
In the prediction stage, various deduplication and difference-coding techniques are applied that help decorrelate data and describe new data based on already transmitted data.
Then rectangular blocks of (residue) pixel data are transformed to the frequency domain to ease targeting irrelevant information in quantization and for some spatial redundancy reduction. The discrete cosine transform (DCT) that is widely used in this regard was introduced by N. Ahmed, T. Natarajan and K. R. Rao in 1974.In the main lossy processing stage that data gets quantized in order to reduce information that is irrelevant to human visual perception.
In the last stage statistical redundancy gets largely eliminated by an entropy coder which often applies some form of arithmetic coding.
In an additional in-loop filtering stage various filters can be applied to the reconstructed image signal. By computing these filters also inside the encoding loop they can help compression because they can be applied to reference material before it gets used in the prediction process and they can be guided using the original signal. The most popular example are deblocking filters that blur out blocking artefacts from quantization discontinuities at transform block boundaries.

History
In 1967, A.H. Robinson and C. Cherry proposed a run-length encoding bandwidth compression scheme for the transmission of analog television signals. Discrete cosine transform (DCT), which is fundamental to modern video compression, was introduced by Nasir Ahmed, T. Natarajan and K. R. Rao in 1974.H.261, which debuted in 1988, commercially introduced the prevalent basic architecture of video compression technology. It was the first video coding format based on DCT compression, which would subsequently become the standard for all of the major video coding formats that followed. H.261 was developed by a number of companies, including Hitachi, PictureTel, NTT, BT and Toshiba.The most popular video coding standards used for codecs have been the MPEG standards. MPEG-1 was developed by the Motion Picture Experts Group (MPEG) in 1991, and it was designed to compress VHS-quality video. It was succeeded in 1994 by MPEG-2/H.262, which was developed by a number of companies, primarily Sony, Thomson and Mitsubishi Electric. MPEG-2 became the standard video format for DVD and SD digital television. In 1999, it was followed by MPEG-4/H.263, which was a major leap forward for video compression technology. It was developed by a number of companies, primarily Mitsubishi Electric, Hitachi and Panasonic.The most widely used video coding format is H.264/MPEG-4 AVC. It was developed in 2003 by a number of organizations, primarily Panasonic, Godo Kaisha IP Bridge and LG Electronics. AVC commercially introduced the modern context-adaptive binary arithmetic coding (CABAC) and context-adaptive variable-length coding (CAVLC) algorithms. AVC is the main video encoding standard for Blu-ray Discs, and is widely used by streaming internet services such as YouTube, Netflix, Vimeo, and iTunes Store, web software such as Adobe Flash Player and Microsoft Silverlight, and various HDTV broadcasts over terrestrial and satellite television.

Genetics
Genetics compression algorithms are the latest generation of lossless algorithms that compress data (typically sequences of nucleotides) using both conventional compression algorithms and genetic algorithms adapted to the specific datatype. In 2012, a team of scientists from Johns Hopkins University published a genetic compression algorithm that does not use a reference genome for compression. HAPZIPPER was tailored for HapMap data and achieves over 20-fold compression (95% reduction in file size), providing 2- to 4-fold better compression and in much faster time than the leading general-purpose compression utilities. For this, Chanda, Elhaik, and Bader introduced MAF based encoding (MAFE), which reduces the heterogeneity of the dataset by sorting SNPs by their minor allele frequency, thus homogenizing the dataset. Other algorithms in 2009 and 2013 (DNAZip and GenomeZip) have compression ratios of up to 1200-fold—allowing 6 billion basepair diploid human genomes to be stored in 2.5 megabytes (relative to a reference genome or averaged over many genomes). For a benchmark in genetics/genomics data compressors, see

Outlook and currently unused potential
It is estimated that the total amount of data that is stored on the world's storage devices could be further compressed with existing compression algorithms by a remaining average factor of 4.5:1. It is estimated that the combined technological capacity of the world to store information provides 1,300 exabytes of hardware digits in 2007, but when the corresponding content is optimally compressed, this only represents 295 exabytes of Shannon information.

See also
References
External links
Data Compression Basics (Video)
Video compression 4:2:2 10-bit and its benefits
Why does 10-bit save bandwidth (even when content is 8-bit)?
Which compression technology should be used
Wiley – Introduction to Compression Theory
EBU subjective listening tests on low-bitrate audio codecs
Audio Archiving Guide: Music Formats (Guide for helping a user pick out the right codec)
MPEG 1&2 video compression intro (pdf format) at the Wayback Machine (archived September 28, 2007)
hydrogenaudio wiki comparison
Introduction to Data Compression by Guy E Blelloch from CMU
HD Greetings – 1080p Uncompressed source material for compression testing and research
Explanation of lossless signal compression method used by most codecs
Interactive blind listening tests of audio codecs over the internet
TestVid – 2,000+ HD and other uncompressed source video clips for compression testing
Videsignline – Intro to Video Compression
Data Footprint Reduction Technology
What is Run length Coding in video compression.",https://en.wikipedia.org/wiki/Data_compression,"['All Wikipedia articles written in American English', 'Articles with short description', 'Data compression', 'Digital audio', 'Digital television', 'Film and video technology', 'Short description matches Wikidata', 'Use American English from March 2021', 'Utility software types', 'Video compression', 'Videotelephony', 'Webarchive template wayback links', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NDL identifiers']",Data Science
44,Data degradation,"Data degradation is the gradual corruption of computer data due to an accumulation of non-critical failures in a data storage device. The phenomenon is also known as data decay, data rot or bit rot.

Visual example
Below are several digital images illustrating data degradation, all consisting of 326,272 bits. The original photo is displayed on the left. In the next image to the right, a single bit was changed from 0 to 1. In the next two images, two and three bits were flipped. On Linux systems, the binary difference between files can be revealed using cmp command (e.g. cmp -b bitrot-original.jpg bitrot-1bit-changed.jpg).

In RAM
Data degradation in dynamic random-access memory (DRAM) can occur when the electric charge of a bit in DRAM disperses, possibly altering program code or stored data. DRAM may be altered by cosmic rays or other high-energy particles. Such data degradation is known as a soft error. ECC memory can be used to mitigate this type of data degradation.

In storage
Data degradation results from the gradual decay of storage media over the course of years or longer. Causes vary by medium:

Solid-state media, such as EPROMs, flash memory and other solid-state drives, store data using electrical charges, which can slowly leak away due to imperfect insulation. The chip itself is not affected by this, so reprogramming it approximately once per decade prevents decay. An undamaged copy of the master data is required for the reprogramming.
Magnetic media, such as hard disk drives, floppy disks and magnetic tapes, may experience data decay as bits lose their magnetic orientation. Periodic refreshing by rewriting the data can alleviate this problem. In warm/humid conditions these media, especially those poorly protected against ambient air, are prone to the physical decomposition of the storage medium.
Optical media, such as CD-R, DVD-R and BD-R, may experience data decay from the breakdown of the storage medium. This can be mitigated by storing discs in a dark, cool, low humidity location. ""Archival quality"" discs are available with an extended lifetime, but are still not permanent. However, data integrity scanning that measures the rates of various types of errors is able to predict data decay on optical media well ahead of uncorrectable data loss occurring.
Paper media, such as punched cards and punched tape, may literally rot. Mylar punched tape is another approach that does not rely on electromagnetic stability.

Component and system failures
Most disk, disk controller and higher-level systems are subject to a slight chance of unrecoverable failure. With ever-growing disk capacities, file sizes, and increases in the amount of data stored on a disk, the likelihood of the occurrence of data decay and other forms of uncorrected and undetected data corruption increases.Higher-level software systems may be employed to mitigate the risk of such underlying failures by increasing redundancy and implementing integrity checking and self-repairing algorithms. The ZFS file system was designed to address many of these data corruption issues. The Btrfs file system also includes data protection and recovery mechanisms, as does ReFS.

See also


== References ==",https://en.wikipedia.org/wiki/Data_degradation,"['All articles needing additional references', 'All articles that may contain original research', 'All articles with unsourced statements', 'Articles needing additional references from February 2018', 'Articles that may contain original research from April 2015', 'Articles with multiple maintenance issues', 'Articles with unsourced statements from December 2020', 'CS1 maint: location', 'CS1 maint: uses authors parameter', 'Computer jargon', 'Data quality']",Data Science
45,Data editing,"Data editing is defined as the process involving the review and adjustment of collected survey data. Data editing helps define guidelines that will reduce potential bias and ensure consistent estimates leading to a clear analysis of the data set by correct inconsistent data using the methods later in this article.  The purpose is to control the quality of the collected data. Data editing can be performed manually, with the assistance of a computer or a combination of both.

Editing methods
Editing methods refer to a range of procedures and processes used for detecting and handling errors in data. Data editing is used with the goal to improve the quality of statistical data produced. These modifications can greatly improve the quality of analytics created by aiming to detect and correct errors. Examples of different techniques to data editing such as micro-editing, macro-editing, selective editing, or the different tools used to achieve data editings such as graphical editing and interactive editing.

Interactive editing
The term interactive editing is commonly used for modern computer-assisted manual editing. Most interactive data editing tools applied at National Statistical Institutes (NSIs) allow one to check the specified edits during or after data entry, and if necessary to correct erroneous data immediately. Several approaches can be followed to correct erroneous data:

Re-contact the respondent
Compare the respondent's data to his data from the previous year
Compare the respondent's data to data from similar respondents
Use the subject matter knowledge of the human editorInteractive editing is a standard way to edit data. It can be used to edit both categorical and continuous data. Interactive editing reduces the time frame needed to complete the cyclical process of review and adjustment. Interactive editing also requires an understanding of the data set and the possible results that would come from an analysis of the data.

Selective editing
Selective editing is an umbrella term for several methods to identify the influential errors,  and outliers. Selective editing techniques aim to apply interactive editing to a well-chosen subset of the records, such that the limited time and resources available for interactive editing are allocated to those records where it has the most effect on the quality of the final estimates of published figures. In selective editing, data is split into two streams:

The critical stream
The non-critical streamThe critical stream consists of records that are more likely to contain influential errors. These critical records are edited in a traditional interactive manner. The records in the non-critical stream which are unlikely to contain influential errors are not edited in a computer-assisted manner.

Data Editing Techniques
Data editing can be accomplished in many ways and primarily depends on the data set that is being explored.

Validity and Completeness of Data
The validity of a data set depends on the completeness of the responses provided by the respondents. One method of data editing is to ensure that all responses are complete in fields that require a numerical or non-numerical answer. See the example below.

Duplicate data entry
Verifying that the data is unique is an important aspect of data editing to ensure that all data provided was only entered once. This reduces the possibility for repeated data that could skew analytics reporting. See the example below.

Outliers
It is common to find outliers in data sets, which as described before are values that do not fit a model of data well. These extreme values can be found based on the distribution of data points from previous data series or parallel data series for the same data set. The values can be considered erroneous and require further analysis for checking and determining the validity of the response. See the example below.

Logical Inconsistencies
Logical consistency is the presence of logical relationships and interdependence between the variables. This editing requires a certain understanding around the dataset and the ability to identify errors in data based on previous reports or information. This type of data editing is used to account for the differences between data fields or variables. See the example below.

Macro editing
There are two methods of macro editing:

Aggregation method
This method is followed in almost every statistical agency before publication: verifying whether figures to be published seems plausible. This is accomplished by comparing quantities in publication tables with the same quantities in previous publications. If an unusual value is observed, a micro-editing procedure is applied to the individual records and fields contributing to the suspicious quantity.

Distribution method
Data available is used to characterize the distribution of the variables. Then all individual values are compared with the distribution. Records containing values that could be considered uncommon (given the distribution) are candidates for further inspection and possibly for editing.

Automatic editing
In automatic editing records are edited by a computer without human intervention. Prior knowledge on the values of a single variable or a combination of variables can be formulated as a set of edit rules which specify or constrain the admissible values

Determinants of Data Editing
Data editing has its limitations with the capacity and resources of any given study. These determinants can have a positive or negative impact on the post-analysis of the data set. Below are several determinants of data editing. Available resources: 
Time allocated to the project
Money and budget constraintsAvailable Software:
Tools used to analyze the data
Tools available to identify errors in the data set
Immediate availability of software depending on the objectives and goals of the dataData Source: 
Limitations of respondents to answer according to expectations
Missing information from respondents that are not readily available
Follow-ups are difficult to maintain in large data poolsCoordination of Data Editing Procedure: 
Subjective views on the data set
Disagreements between the overall objectives of the data
Methods used to handle data editing

See also
Data cleansing
Data pre-processing
Data wrangling
Iterative proportional fitting
Triangulation (social science)

Notes


== References ==",https://en.wikipedia.org/wiki/Data_editing,"['Quantitative research', 'Survey methodology']",Data Science
46,Data extraction,"Data extraction is the act or process of retrieving data out of (usually unstructured or poorly structured) data sources for further data processing or data storage (data migration). The import into the intermediate extracting system is thus usually followed by data transformation and possibly the addition of metadata prior to export to another stage in the data workflow.
Usually, the term data extraction is applied when (experimental) data is first imported into a computer from primary sources, like measuring or recording devices. Today's electronic devices will usually present an electrical connector (e.g. USB) through which 'raw data' can be streamed into a personal computer.

Data sources
Typical unstructured data sources include web pages, emails, documents, PDFs, scanned text, mainframe reports, spool files, classifieds, etc. which is further used for sales or marketing leads. Extracting data from these unstructured sources has grown into a considerable technical challenge where as historically data extraction has had to deal with changes in physical hardware formats, the majority of current data extraction deals with extracting data from these unstructured data sources, and from different software formats. This growing process of data extraction from the web is referred to as ""Web data extraction"" or ""Web scraping"".

Imposing structure
The act of adding structure to unstructured data takes a number of forms

Using text pattern matching such as regular expressions to identify small or large-scale structure e.g. records in a report and their associated data from headers and footers;
Using a table-based approach to identify common sections within a limited domain e.g. in emailed resumes, identifying skills, previous work experience, qualifications etc. using a standard set of commonly used headings (these would differ from language to language), e.g. Education might be found under Education/Qualification/Courses;
Using text analytics to attempt to understand the text and link it to other information

See also
Information extraction
Data retrieval
Extract, transform, load (ETL)
Data mining


== References ==",https://en.wikipedia.org/wiki/Data_extraction,"['All articles needing additional references', 'Articles needing additional references from August 2020', 'Data management', 'Data warehousing']",Data Science
47,Data farming,"Data farming  is the process of using designed computational experiments to “grow” data, which can then be analyzed using statistical and visualization techniques to obtain insight into complex systems.  These methods can be applied to any computational model.
Data farming differs from Data mining, as the following metaphors indicate: 

Miners seek valuable nuggets of ore buried in the earth, but have no control over what is out there or how hard it is to extract the nuggets from their surroundings. ...  Similarly, data miners seek to uncover valuable nuggets of information buried within massive amounts of data.  Data-mining techniques use statistical and graphical measures to try to identify interesting correlations or clusters in the data set.
Farmers cultivate the land to maximize their yield.  They manipulate the environment to their advantage using irrigation, pest control, crop rotation, fertilizer, and more.  Small-scale designed experiments let them determine whether these treatments are effective.  Similarly, data farmers manipulate simulation models to their advantage, using large-scale designed experimentation to grow data from their models in a manner that easily lets them extract useful information. ...the results can reveal root cause-and-effect relationships between the model input factors and the model responses, in addition to rich graphical and statistical views of these relationships.

A NATO modeling and simulation task group has documented the data farming process in the Final Report of MSG-088.
Here, data farming uses collaborative processes in combining rapid scenario prototyping, simulation modeling, design of experiments, high performance computing, and analysis and visualization in an iterative loop-of-loops.

History
The science of Design of Experiments (DOE) has been around for over a century, pioneered by R.A. Fisher for agricultural studies. Many of the classic experiment designs can be used in simulation studies. However, computational experiments have far fewer restrictions than do real-world experiments, in terms of costs, number of factors, time required, ability to replicate, ability to automate, etc.  Consequently, a framework specifically oriented toward large-scale simulation experiments is warranted.
People have been conducting computational experiments for as long as computers have been around.  The term “data farming” is more recent, coined in 1998 in conjunction with the Marine Corp's Project Albert, in which small agent-based distillation models (a type of stochastic simulation) were created to capture specific military challenges. These models were run thousands or millions of times at the Maui High Performance Computer Center and other facilities. Project Albert analysts would work with the military subject matter experts to refine the models and interpret the results.
Initially, the use of brute-force full factorial (gridded) designs meant that the simulations needed to run very quickly and the studies required high-performance computing. Even so, only a small number of factors (at a limited number of levels) could be investigated, due to the curse of dimensionality.
The SEED Center for Data Farming at the Naval Postgraduate School also worked closely with Project Albert in model generation, output analysis, and the creation of new experimental designs to better leverage the computing capabilities at Maui and other facilities. Recent breakthroughs in designs specifically developed for data farming can be found in 
 
,
among others.

Workshops
A series of international data farming workshops have been held since 1998 by the SEED Center for Data Farming. International Data Farming Workshop 1 occurred in 1991, and since then 16 more workshops have taken place. The workshops have seen a diverse array of representation from participating countries, such as Canada, Singapore, Mexico, Turkey, and the United States.The International Data Farming Workshops operate through collaboration between various teams of experts. The most recent workshop held in 2008 saw over 100 teams participating. The teams of data farmers are assigned a specific area of study, such as robotics, homeland security, and disaster relief. Different forms of data farming are experimented with and utilized by each group, such as the Pythagoras ABM, the Logistics Battle Command model, and the agent-based sensor effector model (ABSEM).

References
External links
SEED Center for Data Farming website, with links to numerous papers, applications, designs, and software.
An article on the 27th Data Farming Workshop in Finland in Defense Media Network from January 2014
An article on data farming in Defense News from January 2013
An article summarizing data farming in the June 2005 issue of SIGNAL
MITRE Corporation research paper on data farming",https://en.wikipedia.org/wiki/Data_farming,"['Cluster computing', 'Data analysis', 'Design of experiments', 'Scientific modeling', 'Simulation']",Data Science
48,Data fusion,"Data fusion is the process of integrating multiple data sources to produce more consistent, accurate, and useful information than that provided by any individual data source.
Data fusion processes are often categorized as low, intermediate, or high, depending on the processing stage at which fusion takes place. Low-level data fusion combines several sources of raw data to produce new raw data. The expectation is that fused data is more informative and synthetic than the original inputs.
For example, sensor fusion is also known as (multi-sensor) data fusion and is a subset of information fusion.
The concept of data fusion has origins in the evolved capacity of humans and animals to incorporate information from multiple senses to improve their ability to survive. For example, a combination of sight, touch, smell, and taste may indicate whether a substance is edible.

The JDL/DFIG model
In the mid-1980s, the Joint Directors of Laboratories formed the Data Fusion Subpanel (which later became known as the Data Fusion Group).  With the advent of the World Wide Web, data fusion thus included data, sensor, and information fusion. The JDL/DFIG introduced a model of data fusion that divided the various processes.  Currently, the six levels with the Data Fusion Information Group (DFIG) model are:
Level 0: Source Preprocessing (or Data Assessment)
Level 1: Object Assessment
Level 2: Situation Assessment
Level 3: Impact Assessment (or Threat Refinement)
Level 4: Process Refinement (or Resource Management)
Level 5: User Refinement (or Cognitive Refinement)
Level 6: Mission Refinement (or Mission Management)
Although the JDL Model (Level 1–4) is still in use today, it is often criticized for its implication that the levels necessarily happen in order and also for its lack of adequate representation of the potential for a human-in-the-loop. The DFIG model (Level 0–5) explored the implications of situation awareness, user refinement, and mission management. Despite these shortcomings, the JDL/DFIG models are useful for visualizing the data fusion process, facilitating discussion and common understanding, and important for systems-level information fusion design.

Geospatial applications
In the geospatial (GIS) domain, data fusion is often synonymous with data integration.  In these applications, there is often a need to combine diverse data sets into a unified (fused) data set which includes all of the data points and time steps from the input data sets.  The fused data set is different from a simple combined superset in that the points in the fused data set contain attributes and metadata which might not have been included for these points in the original data set.
A simplified example of this process is shown below where data set ""α"" is fused with data set β to form the fused data set δ.  Data points in set ""α"" have spatial coordinates X and Y and attributes A1 and A2.  Data points in set β have spatial coordinates X and Y and attributes B1 and B2.  The fused data set contains all points and attributes. 

In a simple case where all attributes are uniform across the entire analysis domain, the attributes may be simply assigned: M?, N?, Q?, R? to M, N, Q, R.  In a real application, attributes are not uniform and some type of interpolation is usually required to properly assign attributes to the data points in the fused set.

In a much more complicated application, marine animal researchers use data fusion to combine animal tracking data with bathymetric, meteorological, sea surface temperature (SST) and animal habitat data to examine and understand habitat utilization and animal behavior in reaction to external forces such as weather or water temperature.  Each of these data sets exhibit a different spatial grid and sampling rate so a simple combination would likely create erroneous assumptions and taint the results of the analysis.  But through the use of data fusion, all data and attributes are brought together into a single view in which a more complete picture of the environment is created.  This enables scientists to identify key locations and times and form new insights into the interactions between the environment and animal behaviors.
In the figure at right, rock lobsters are studied off the coast of Tasmania.  Hugh Pederson of the University of Tasmania used data fusion software to fuse southern rock lobster tracking data (color-coded for in yellow and black for day and night, respectively) with bathymetry and habitat data to create a unique 4D picture of rock lobster behavior.

Data integration
In applications outside of the geospatial domain, differences in the usage of the terms Data integration and Data fusion apply.  In areas such as business intelligence, for example, data integration is used to describe the combining of data, whereas data fusion is integration followed by reduction or replacement. Data integration might be viewed as set combination wherein the larger set is retained, whereas fusion is a set reduction technique with improved confidence.

Application areas
From multiple traffic sensing modalities
The data from the different sensing technologies can be combined in intelligent ways to determine the traffic state accurately. A Data fusion based approach that utilizes the road side collected acoustic, image and sensor data has been shown to combine the advantages of the different individual methods.

Decision fusion
In many cases, geographically-dispersed sensors are severely energy- and bandwidth-limited. Therefore, the raw data concerning a certain phenomenon are often summarized in a few bits from each sensor. When inferring on a binary event (i.e., 
  
    
      
        
          
            
              H
            
          
          
            0
          
        
      
    
    {\displaystyle {\mathcal {H}}_{0}}
    or 
  
    
      
        
          
            
              H
            
          
          
            1
          
        
      
    
    {\displaystyle {\mathcal {H}}_{1}}
   ), in the extreme case only binary decisions are sent from sensors to a Decision Fusion Center (DFC) and combined in order to obtain improved classification performance.

For enhanced contextual awareness
With a multitude of built-in sensors including motion sensor, environmental sensor, position sensor, a modern mobile device typically gives mobile applications access to a number of sensory data which could be leveraged to enhance the contextual awareness. Using signal processing and data fusion techniques such as feature generation, feasibility study and principal component analysis (PCA) such sensory data will greatly improve the positive rate of classifying the motion and contextual relevant status of the device. Many context-enhanced information techniques are provided by Snidaro, et al.

See also
References
Sources
General referencesHall, Dave L.; Llinas, James (1997). ""Introduction to Multisensor Data Fusion"". Proceedings of the IEEE. 85 (1): 6–23. doi:10.1109/5.554205.
Blasch, Erik; Kadar, Ivan; Salerno, John; Kokar, Mieczyslaw M.; Das, Subrata; Powell, Gerald M.; Corkill, Daniel D.; Ruspini, Enrique H. (2006). ""Issues and Challenges in Situation Assessment (Level 2 Fusion)"" (PDF). Journal of Advances in Information Fusion. 1 (2). Archived from the original (PDF) on 2015-05-27.

Bibliography
Hall, David L.; McMullen, Sonya A. H. (2004). Mathematical Techniques in Multisensor Data Fusion, Second Edition. Norwood, MA: Artech House, Inc. ISBN 978-1-5805-3335-5.
Mitchell, H. B. (2007). Multi-sensor Data Fusion – An Introduction. Berlin: Springer-Verlag. ISBN 978-3-540-71463-7.
Das, S. (2008). High-Level Data Fusion. Norwood, MA: Artech House Publishers. ISBN 978-1-59693-281-4.

External links
Discriminant Correlation Analysis (DCA)
Sensordata Fusion, An Introduction
International Society of Information Fusion
Sensor Fusion for Nanopositioning",https://en.wikipedia.org/wiki/Data_fusion,"['CS1 maint: multiple names: authors list', 'Data analysis', 'Pages using div col with small parameter']",Data Science
49,Data format management,"Data format management (DFM) is the application of a systematic approach to the selection and use of the data formats used to encode information for storage on a computer. 
In practical terms, data format management is the analysis of data formats and their associated technical, legal or economic attributes which can either enhance or detract from the ability of a digital asset or a given information systems to meet specified objectives.
Data format management is necessary as the amount of information and number of people creating it grows. This is especially the case as the information with which users are working is difficult to generate, store, costly to acquire, or to be shared.
Data format management as an analytic tool or approach is data format neutral.    
Historically individuals, organization and businesses have been categorized by their type of computer or their operating system.  Today, however, it is primarily productivity software, such as spreadsheet or word processor programs, and the way these programs store information that also defines an entity. For instance, when browsing the web it is not important which kind of computer is responsible for hosting a site, only that the information it publishes is in a format that is readable by the viewing browser. In this instance the data format of the published information has more to do with defining compatibilities than the underlying hardware or operating system. 
Several initiatives have been established to record those data formats commonly used and the software available to read them, for example the Pronom project at the UK National Archives.

See also
Data curation
Data preservation
Digital preservation
File format
Information technology governance
National Digital Library Program (NDLP)
National Digital Information Infrastructure and Preservation Program (NDIIPP)

External links
The Library of Congress, Sustainability of Digital Formats",https://en.wikipedia.org/wiki/Data_format_management,['Computer data'],Data Science
50,Data integration,"Data integration involves combining data residing in different sources and providing users with a unified view of them. This process becomes significant in a variety of situations, which include both commercial (such as when two similar companies need to merge their databases) and scientific (combining research results from different bioinformatics repositories, for example) domains.  Data integration appears with increasing frequency as the volume (that is, big data) and the need to share existing data explodes.  It has become the focus of extensive theoretical work, and numerous open problems remain unsolved. Data integration encourages collaboration between internal as well as external users. The data being integrated must be received from a heterogeneous database system and transformed to a single coherent data store that provides synchronous data across a network of files for clients. A common use of data integration is in data mining when analyzing and extracting information from existing databases that can be useful for Business information.

History
Issues with combining heterogeneous data sources, often referred to as information silos, under a single query interface have existed for some time. In the early 1980s, computer scientists began designing systems for interoperability of heterogeneous databases. The first data integration system driven by structured metadata was designed at the University of Minnesota in 1991, for the Integrated Public Use Microdata Series (IPUMS). IPUMS used a data warehousing approach, which extracts, transforms, and loads data from heterogeneous sources into a unique view schema so data from different sources become compatible. By making thousands of population databases interoperable, IPUMS demonstrated the feasibility of large-scale data integration. The data warehouse approach offers a tightly coupled architecture because the data are already physically reconciled in a single queryable repository, so it usually takes little time to resolve queries.The data warehouse approach is less feasible for data sets that are frequently updated, requiring the extract, transform, load (ETL) process to be continuously re-executed for synchronization.  Difficulties also arise in constructing data warehouses when one has only a query interface to summary data sources and no access to the full data.  This problem frequently emerges when integrating several commercial query services like travel or classified advertisement web applications.
As of 2009 the trend in data integration favored the loose coupling of data and providing a unified query-interface to access real time data over a mediated schema (see Figure 2), which allows information to be retrieved directly from original databases. This is consistent with the SOA approach popular in that era. This approach relies on mappings between the mediated schema and the schema of original sources, and translating a query into decomposed queries to match the schema of the original databases.  Such mappings can be specified in two ways: as a mapping from entities in the mediated schema to entities in the original sources (the ""Global-as-View"" (GAV) approach), or as a mapping from entities in the original sources to the mediated schema (the ""Local-as-View"" (LAV) approach).  The latter approach requires more sophisticated inferences to resolve a query on the mediated schema, but makes it easier to add new data sources to a (stable) mediated schema.
As of 2010 some of the work in data integration research concerns the semantic integration problem.  This problem addresses not the structuring of the architecture of the integration, but how to resolve semantic conflicts between heterogeneous data sources.  For example, if two companies merge their databases, certain concepts and definitions in their respective schemas like ""earnings"" inevitably have different meanings.  In one database it may mean profits in dollars (a floating-point number), while in the other it might represent the number of sales (an integer).  A common strategy for the resolution of such problems involves the use of ontologies which explicitly define schema terms and thus help to resolve semantic conflicts. This approach represents ontology-based data integration. On the other hand, the problem of combining research results from different bioinformatics repositories requires bench-marking of the similarities, computed from different data sources, on a single criterion such as positive predictive value. This enables the data sources to be directly comparable and can be integrated even when the natures of experiments are distinct.As of 2011 it was determined that current data modeling methods were imparting data isolation into every data architecture in the form of islands of disparate data and information silos. This data isolation is an unintended artifact of the data modeling methodology that results in the development of disparate data models. Disparate data models, when instantiated as databases, form disparate databases. Enhanced data model methodologies have been developed to eliminate the data isolation artifact and to promote the development of integrated data models. One enhanced data modeling method recasts data models by augmenting them with structural metadata in the form of standardized data entities.  As a result of recasting multiple data models, the set of recast data models will now share one or more commonality relationships that relate the structural metadata now common to these data models.  Commonality relationships are a peer-to-peer type of entity relationships that relate the standardized data entities of multiple data models.  Multiple data models that contain the same standard data entity may participate in the same commonality relationship.  When integrated data models are instantiated as databases and are properly populated from a common set of master data, then these databases are integrated.
Since 2011, data hub approaches have been of greater interest than fully structured (typically relational) Enterprise Data Warehouses. Since 2013, data lake approaches have risen to the level of Data Hubs. (See all three search terms popularity on Google Trends.) These approaches combine unstructured or varied data into one location, but do not necessarily require an (often complex) master relational schema to structure and define all data in the Hub.
Data integration plays a big role in business regarding data collection used for studying the market. Converting the raw data retrieved from consumers into coherent data is something businesses try to do when considering what steps they should take next. Organizations are more frequently using data mining for collecting information and patterns from their databases, and this process helps them develop new business strategies to increase business performance and perform economic analyses more efficiently. Compiling the large amount of data they collect to be stored in their system is a form of data integration adapted for Business intelligence to improve their chances of success.

Example
Consider a web application where a user can query a variety of information about cities (such as crime statistics, weather, hotels, demographics, etc.).  Traditionally, the information must be stored in a single database with a single schema.  But any single enterprise would find information of this breadth somewhat difficult and expensive to collect.  Even if the resources exist to gather the data, it would likely duplicate data in existing crime databases, weather websites, and census data.
A data-integration solution may address this problem by considering these external resources as materialized views over a virtual mediated schema, resulting in ""virtual data integration"".  This means application-developers construct a virtual schema—the mediated schema—to best model the kinds of answers their users want.  Next, they design ""wrappers"" or adapters for each data source, such as the crime database and weather website.  These adapters simply transform the local query results (those returned by the respective websites or databases) into an easily processed form for the data integration solution (see figure 2).  When an application-user queries the mediated schema, the data-integration solution transforms this query into appropriate queries over the respective data sources.  Finally, the virtual database combines the results of these queries into the answer to the user's query.
This solution offers the convenience of adding new sources by simply constructing an adapter or an application software blade for them.  It contrasts with ETL systems or with a single database solution, which require manual integration of entire new data set into the system. The virtual ETL solutions leverage virtual mediated schema to implement data harmonization; whereby the data are copied from the designated ""master"" source to the defined targets, field by field. Advanced data virtualization is also built on the concept of object-oriented modeling in order to construct virtual mediated schema or virtual metadata repository, using hub and spoke architecture.
Each data source is disparate and as such is not designed to support reliable joins between data sources.  Therefore, data virtualization as well as data federation depends upon accidental data commonality to support combining data and information from disparate data sets.  Because of this lack of data value commonality across data sources, the return set may be inaccurate, incomplete, and impossible to validate.
One solution is to recast disparate databases to integrate these databases without the need for ETL. The recast databases support commonality constraints where referential integrity may be enforced between databases.  The recast databases provide designed data access paths with data value commonality across databases.

Theory
The theory of data integration forms a subset of database theory and formalizes the underlying concepts of the problem in first-order logic.  Applying the theories gives indications as to the feasibility and difficulty of data integration.  While its definitions may appear abstract, they have sufficient generality to accommodate all manner of integration systems, including those that include nested relational / XML databases and those that treat databases as programs.  Connections to particular databases systems such as Oracle or DB2 are provided by implementation-level technologies such as JDBC and are not studied at the theoretical level.

Definitions
Data integration systems are formally defined as a tuple 
  
    
      
        
          ⟨
          
            G
            ,
            S
            ,
            M
          
          ⟩
        
      
    
    {\displaystyle \left\langle G,S,M\right\rangle }
   where 
  
    
      
        G
      
    
    {\displaystyle G}
   is the global (or mediated) schema, 
  
    
      
        S
      
    
    {\displaystyle S}
   is the heterogeneous set of source schemas, and 
  
    
      
        M
      
    
    {\displaystyle M}
   is the mapping that maps queries between the source and the global schemas.  Both 
  
    
      
        G
      
    
    {\displaystyle G}
   and 
  
    
      
        S
      
    
    {\displaystyle S}
   are expressed in languages over alphabets composed of symbols for each of their respective relations.  The mapping 
  
    
      
        M
      
    
    {\displaystyle M}
   consists of assertions between queries over 
  
    
      
        G
      
    
    {\displaystyle G}
   and queries over 
  
    
      
        S
      
    
    {\displaystyle S}
  .  When users pose queries over the data integration system, they pose queries over 
  
    
      
        G
      
    
    {\displaystyle G}
   and the mapping then asserts connections between the elements in the global schema and the source schemas.
A database over a schema is defined as a set of sets, one for each relation (in a relational database).  The database corresponding to the source schema 
  
    
      
        S
      
    
    {\displaystyle S}
   would comprise the set of sets of tuples for each of the heterogeneous data sources and is called the source database.  Note that this single source database may actually represent a collection of disconnected databases.  The database corresponding to the virtual mediated schema 
  
    
      
        G
      
    
    {\displaystyle G}
   is called the global database.  The global database must satisfy the mapping 
  
    
      
        M
      
    
    {\displaystyle M}
   with respect to the source database.  The legality of this mapping depends on the nature of the correspondence between 
  
    
      
        G
      
    
    {\displaystyle G}
   and 
  
    
      
        S
      
    
    {\displaystyle S}
  .  Two popular ways to model this correspondence exist: Global as View or GAV and Local as View or LAV.

GAV systems model the global database as a set of views over 
  
    
      
        S
      
    
    {\displaystyle S}
  .  In this case 
  
    
      
        M
      
    
    {\displaystyle M}
   associates to each element of 
  
    
      
        G
      
    
    {\displaystyle G}
   a query over 
  
    
      
        S
      
    
    {\displaystyle S}
  . Query processing becomes a straightforward operation due to the well-defined associations between 
  
    
      
        G
      
    
    {\displaystyle G}
   and 
  
    
      
        S
      
    
    {\displaystyle S}
  .  The burden of complexity falls on implementing mediator code instructing the data integration system exactly how to retrieve elements from the source databases.  If any new sources join the system, considerable effort may be necessary to update the mediator, thus the GAV approach appears preferable when the sources seem unlikely to change.
In a GAV approach to the example data integration system above, the system designer would first develop mediators for each of the city information sources and then design the global schema around these mediators.  For example, consider if one of the sources served a weather website.  The designer would likely then add a corresponding element for weather to the global schema.  Then the bulk of effort concentrates on writing the proper mediator code that will transform predicates on weather into a query over the weather website.  This effort can become complex if some other source also relates to weather, because the designer may need to write code to properly combine the results from the two sources.
On the other hand, in LAV, the source database is modeled as a set of views over 
  
    
      
        G
      
    
    {\displaystyle G}
  .  In this case 
  
    
      
        M
      
    
    {\displaystyle M}
   associates to each element of 
  
    
      
        S
      
    
    {\displaystyle S}
   a query over 
  
    
      
        G
      
    
    {\displaystyle G}
  .  Here the exact associations between 
  
    
      
        G
      
    
    {\displaystyle G}
   and 
  
    
      
        S
      
    
    {\displaystyle S}
   are no longer well-defined.  As is illustrated in the next section, the burden of determining how to retrieve elements from the sources is placed on the query processor.  The benefit of an LAV modeling is that new sources can be added with far less work than in a GAV system, thus the LAV approach should be favored in cases where the mediated schema is less stable or likely to change.In an LAV approach to the example data integration system above, the system designer designs the global schema first and then simply inputs the schemas of the respective city information sources.  Consider again if one of the sources serves a weather website.  The designer would add corresponding elements for weather to the global schema only if none existed already.  Then programmers write an adapter or wrapper for the website and add a schema description of the website's results to the source schemas.  The complexity of adding the new source moves from the designer to the query processor.

Query processing
The theory of query processing in data integration systems is commonly expressed using conjunctive queries and Datalog, a purely declarative logic programming language.  One can loosely think of a conjunctive query as a logical function applied to the relations of a database such as ""
  
    
      
        f
        (
        A
        ,
        B
        )
      
    
    {\displaystyle f(A,B)}
   where 
  
    
      
        A
        <
        B
      
    
    {\displaystyle A<B}
  "".  If a tuple or set of tuples is substituted into the rule and satisfies it (makes it true), then we consider that tuple as part of the set of answers in the query.  While formal languages like Datalog express these queries concisely and without ambiguity, common SQL queries count as conjunctive queries as well.
In terms of data integration, ""query containment"" represents an important property of conjunctive queries.  A query 
  
    
      
        A
      
    
    {\displaystyle A}
   contains another query 
  
    
      
        B
      
    
    {\displaystyle B}
   (denoted 
  
    
      
        A
        ⊃
        B
      
    
    {\displaystyle A\supset B}
  ) if the results of applying 
  
    
      
        B
      
    
    {\displaystyle B}
   are a subset of the results of applying 
  
    
      
        A
      
    
    {\displaystyle A}
   for any database.  The two queries are said to be equivalent if the resulting sets are equal for any database.  This is important because in both GAV and LAV systems, a user poses conjunctive queries over a virtual schema represented by a set of views, or ""materialized"" conjunctive queries.  Integration seeks to rewrite the queries represented by the views to make their results equivalent or maximally contained by our user's query.  This corresponds to the problem of answering queries using views (AQUV).In GAV systems, a system designer writes mediator code to define the query-rewriting.  Each element in the user's query corresponds to a substitution rule just as each element in the global schema corresponds to a query over the source.  Query processing simply expands the subgoals of the user's query according to the rule specified in the mediator and thus the resulting query is likely to be equivalent.  While the designer does the majority of the work beforehand, some GAV systems such as Tsimmis involve simplifying the mediator description process.
In LAV systems, queries undergo a more radical process of rewriting because no mediator exists to align the user's query with a simple expansion strategy.  The integration system must execute a search over the space of possible queries in order to find the best rewrite.  The resulting rewrite may not be an equivalent query but maximally contained, and the resulting tuples may be incomplete.  As of 2011 the GQR algorithm is the leading query rewriting algorithm for LAV data integration systems.
In general, the complexity of query rewriting is NP-complete.  If the space of rewrites is relatively small, this does not pose a problem — even for integration systems with hundreds of sources.

In the life sciences
Large-scale questions in science, such as global warming, invasive species spread, and resource depletion, are increasingly requiring the collection of disparate data sets for meta-analysis. This type of data integration is especially challenging for ecological and environmental data because metadata standards are not agreed upon and there are many different data types produced in these fields. National Science Foundation initiatives such as Datanet are intended to make data integration easier for scientists by providing cyberinfrastructure and setting standards. The five funded Datanet initiatives are DataONE, led by William Michener at the University of New Mexico; The Data Conservancy, led by Sayeed Choudhury of Johns Hopkins University; SEAD: Sustainable Environment through Actionable Data, led by Margaret Hedstrom of the University of Michigan; the DataNet Federation Consortium, led by Reagan Moore of the University of North Carolina; and Terra Populus, led by Steven Ruggles of the University of Minnesota.  The Research Data Alliance, has more recently explored creating global data integration frameworks. The OpenPHACTS project, funded through the European Union Innovative Medicines Initiative, built a drug discovery platform by linking datasets from providers such as European Bioinformatics Institute, Royal Society of Chemistry, UniProt, WikiPathways and DrugBank.

See also
References


== External links ==",https://en.wikipedia.org/wiki/Data_integration,"['All articles containing potentially dated statements', 'Articles containing potentially dated statements from 2009', 'Articles containing potentially dated statements from 2010', 'Articles containing potentially dated statements from 2011', 'Articles needing additional categories from June 2019', 'CS1 German-language sources (de)', 'CS1 errors: missing periodical', 'CS1 maint: multiple names: authors list', 'Data integration', 'Data management', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers']",Data Science
51,Columbia University,"Columbia University (also known as Columbia, and officially as Columbia University in the City of New York) is a private Ivy League research university in New York City. Established in 1754 on the grounds of Trinity Church in Manhattan, Columbia is the oldest institution of higher education in New York and the fifth-oldest institution of higher learning in the United States. It is one of nine colonial colleges founded prior to the Declaration of Independence, seven of which belong to the Ivy League. Columbia is ranked among the top universities in the world by major education publications.
Columbia was established as King's College by royal charter from King George II of Great Britain in reaction to the founding of Princeton College. It was renamed Columbia College in 1784 following the American Revolution, and in 1787 was placed under a private board of trustees headed by former students Alexander Hamilton and John Jay. In 1896, the campus was moved to its current location in Morningside Heights and renamed Columbia University.Columbia scientists and scholars have played an important role in scientific breakthroughs including brain-computer interface; the laser and maser; nuclear magnetic resonance; the first nuclear pile; the first nuclear fission reaction in the Americas; the first evidence for plate tectonics and continental drift; and much of the initial research and planning for the Manhattan Project during World War II. Columbia is organized into twenty schools, including four undergraduate schools and 15 graduate schools. The university's research efforts include the Lamont–Doherty Earth Observatory, the Goddard Institute for Space Studies, and accelerator laboratories with major technology firms such as IBM. Columbia is a founding member of the Association of American Universities and was the first school in the United States to grant the M.D. degree. With over 14 million volumes, Columbia University Library is the third largest private research library in the United States.The university's endowment stands at $11.26 billion in 2020, among the largest of any academic institution. As of October 2020, Columbia's alumni, faculty, and staff have included: five Founding Fathers of the United States—among them a co-author of the United States Constitution and a co-author of the Declaration of Independence; three U.S. presidents; 29 foreign heads of state; ten justices of the United States Supreme Court, one of whom currently serves; 96 Nobel laureates; five Fields Medalists; 122 National Academy of Sciences members; 53 living billionaires; eleven Olympic medalists; 33 Academy Award winners; and 125 Pulitzer Prize recipients.

History
Colonial period
Discussions regarding the founding of a college in the Province of New York began as early as 1704, at which time Colonel Lewis Morris wrote to the Society for the Propagation of the Gospel in Foreign Parts, the missionary arm of the Church of England, persuading the society that New York City was an ideal community in which to establish a college. However, it was not until the founding of the College of New Jersey (renamed Princeton) across the Hudson River in New Jersey that the City of New York seriously considered founding a college. In 1746, an act was passed by the general assembly of New York to raise funds for the foundation of a new college. In 1751, the assembly appointed a commission of ten New York residents, seven of whom were members of the Church of England, to direct the funds accrued by the state lottery towards the foundation of a college.Classes were initially held in July 1754 and were presided over by the college's first president, Dr. Samuel Johnson. Dr. Johnson was the only instructor of the college's first class, which consisted of a mere eight students. Instruction was held in a new schoolhouse adjoining Trinity Church, located on what is now lower Broadway in Manhattan. The college was officially founded on October 31, 1754, as King's College by royal charter of King George II, making it the oldest institution of higher learning in the State of New York and the fifth oldest in the United States.In 1763, Dr. Johnson was succeeded in the presidency by Myles Cooper, a graduate of The Queen's College, Oxford, and an ardent Tory. In the charged political climate of the American Revolution, his chief opponent in discussions at the college was an undergraduate of the class of 1777, Alexander Hamilton. The Irish anatomist, Samuel Clossy, was appointed professor of natural philosophy in October 1765 and later the college's first professor of anatomy in 1767. The American Revolutionary War broke out in 1776, and was catastrophic for the operation of King's College, which suspended instruction for eight years beginning in 1776 with the arrival of the Continental Army. The suspension continued through the military occupation of New York City by British troops until their departure in 1783. The college's library was looted and its sole building requisitioned for use as a military hospital first by American and then British forces. Loyalists were forced to abandon their King's College in New York, but some led by Bishop Charles Inglis fled to Windsor, Nova Scotia, where they founded King's Collegiate School.

18th century
After the Revolution, the college turned to the State of New York in order to restore its vitality, promising to make whatever changes to the school's charter the state might demand. The legislature agreed to assist the college, and on May 1, 1784, it passed ""an Act for granting certain privileges to the College heretofore called King's College"". The Act created a Board of Regents to oversee the resuscitation of King's College, and, in an effort to demonstrate its support for the new Republic, the legislature stipulated that ""the College within the City of New York heretofore called King's College be forever hereafter called and known by the name of Columbia College"", a reference to Columbia, an alternative name for America. The Regents finally became aware of the college's defective constitution in February 1787 and appointed a revision committee, which was headed by John Jay and Alexander Hamilton. In April of that same year, a new charter was adopted for the college granted the power to a separate board of 24 trustees.On May 21, 1787, William Samuel Johnson, the son of Dr. Samuel Johnson, was unanimously elected president of Columbia College. Prior to serving at the university, Johnson had participated in the First Continental Congress and been chosen as a delegate to the Constitutional Convention. For a period in the 1790s, with New York City as the federal and state capital and the country under successive Federalist governments, a revived Columbia thrived under the auspices of Federalists such as Hamilton and Jay. Both President George Washington and Vice President John Adams attended the college's commencement on May 6, 1789, as a tribute of honor to the many alumni of the school who had been involved in the American Revolution.

19th century to present
In November 1813, the college agreed to incorporate its medical school with The College of Physicians and Surgeons, a new school created by the Regents of New York, forming Columbia University College of Physicians and Surgeons. The college's enrollment, structure, and academics stagnated for the majority of the 19th century, with many of the college presidents doing little to change the way that the college functioned. In 1857, the college moved from the King's College campus at Park Place to a primarily Gothic Revival campus on 49th Street and Madison Avenue, where it remained for the next forty years. During the last half of the 19th century, under the leadership of President F.A.P. Barnard, the president that Barnard College is named after, the institution rapidly assumed the shape of a modern university. Barnard College was created in 1889 as a response to the university's refusal to accept women. By this time, the college's investments in New York real estate became a primary source of steady income for the school, mainly owing to the city's expanding population. University president Seth Low moved the campus from 49th Street to its present location, a more spacious campus in the developing neighborhood of Morningside Heights. Under the leadership of Low's successor, Nicholas Murray Butler, who served for over four decades, Columbia rapidly became the nation's major institution for research, setting the ""multiversity"" model that later universities would adopt. Prior to becoming the president of Columbia University, Butler founded Teachers College, as a school to prepare home economists and manual art teachers for the children of the poor, with philanthropist Grace Hoadley Dodge. Teachers College is currently affiliated as the university's Graduate School of Education.Research into the atom by faculty members John R. Dunning, I. I. Rabi, Enrico Fermi and Polykarp Kusch placed Columbia's physics department in the international spotlight in the 1940s after the first nuclear pile was built to start what became the Manhattan Project. In 1928, Seth Low Junior College was established by Columbia University in order to mitigate the number of Jewish applicants to Columbia College. The college was closed in 1936 due to the adverse effects of the Great Depression and its students were subsequently taught at Morningside Heights, although they did not belong to any college but to the university at large.There was an evening school called University Extension, which taught night classes, for a fee, to anyone willing to attend. In 1947, the program was reorganized as an undergraduate college and designated the School of General Studies in response to the return of GIs after World War II. In 1995, the School of General Studies was again reorganized as a full-fledged liberal arts college for non-traditional students (those who have had an academic break of one year or more, or are pursuing dual-degrees) and was fully integrated into Columbia's traditional undergraduate curriculum. Within the same year, the Division of Special Programs—later the School of Continuing Education, and now the School of Professional Studies—was established to reprise the former role of University Extension. While the School of Professional Studies only offered non-degree programs for lifelong learners and high school students in its earliest stages, it now offers degree programs in a diverse range of professional and inter-disciplinary fields.

In the aftermath of World War II, the discipline of international relations became a major scholarly focus of the university, and in response, the School of International and Public Affairs was founded in 1946, drawing upon the resources of the faculties of political science, economics, and history.During the 1960s Columbia experienced large-scale student activism, which reached a climax in the spring of 1968 when hundreds of students occupied buildings on campus. The incident forced the resignation of Columbia's president, Grayson Kirk and the establishment of the University Senate.Though several schools within the university had admitted women for years, Columbia College first admitted women in the fall of 1983, after a decade of failed negotiations with Barnard College, the all-female institution affiliated with the university, to merge the two schools. Barnard College still remains affiliated with Columbia, and all Barnard graduates are issued diplomas signed by the Presidents of Columbia University and Barnard College.During the late 20th century, the university underwent significant academic, structural, and administrative changes as it developed into a major research university. For much of the 19th century, the university consisted of decentralized and separate faculties specializing in Political Science, Philosophy, and Pure Science. In 1979, these faculties were merged into the Graduate School of Arts and Sciences. In 1991, the faculties of Columbia College, the School of General Studies, the Graduate School of Arts and Sciences, the School of the Arts, and the School of Professional Studies were merged into the Faculty of Arts and Sciences, leading to the academic integration and centralized governance of these schools. In 2010, the School of International and Public Affairs, which was previously a part of the Faculty of Arts and Sciences, became an independent faculty.

Campus
According to New York Magazine, Columbia University is the second largest landowner in New York City, after the Catholic Church.

Morningside Heights
The majority of Columbia's graduate and undergraduate studies are conducted in Morningside Heights on Seth Low's late-19th century vision of a university campus where all disciplines could be taught at one location. The campus was designed along Beaux-Arts planning principles by the architects McKim, Mead & White. Columbia's main campus occupies more than six city blocks, or 32 acres (13 ha), in Morningside Heights, New York City, a neighborhood that contains a number of academic institutions. The university owns over 7,800 apartments in Morningside Heights, housing faculty, graduate students, and staff. Almost two dozen undergraduate dormitories (purpose-built or converted) are located on campus or in Morningside Heights. Columbia University has an extensive tunnel system more than a century old, with the oldest portions predating the present campus. Some of these remain accessible to the public, while others have been cordoned off.

The Nicholas Murray Butler Library, known simply as Butler Library, is the largest single library in the Columbia University Library System, and is one of the largest buildings on the campus. Proposed as ""South Hall"" by the university's former president Nicholas Murray Butler as expansion plans for Low Memorial Library stalled, the new library was funded by Edward Harkness, benefactor of Yale's residential college system, and designed by his favorite architect, James Gamble Rogers. It was completed in 1934 and renamed for Butler in 1946. The library design is neo-classical in style. Its facade features a row of columns in the Ionic order above which are inscribed the names of great writers, philosophers, and thinkers, most of whom are read by students engaged in the Core Curriculum of Columbia College. As of 2012, Columbia's library system includes over 11.9  million volumes, making it the eighth largest library system and fifth largest collegiate library system in the United States.

Several buildings on the Morningside Heights campus are listed on the National Register of Historic Places. Low Memorial Library, a National Historic Landmark and the centerpiece of the campus, is listed for its architectural significance. Philosophy Hall is listed as the site of the invention of FM radio. Also listed is Pupin Hall, another National Historic Landmark, which houses the physics and astronomy departments. Here the first experiments on the fission of uranium were conducted by Enrico Fermi. The uranium atom was split there ten days after the world's first atom-splitting in Copenhagen, Denmark.

A statue by sculptor Daniel Chester French called Alma Mater is centered on the front steps of Low Memorial Library. McKim, Mead & White invited French to build the sculpture in order to harmonize with the larger composition of the court and library in the center of the campus. Draped in an academic gown, the female figure of Alma Mater wears a crown of laurels and sits on a throne. The scroll-like arms of the throne end in lamps, representing sapientia and doctrina. A book signifying knowledge, balances on her lap, and an owl, the attribute of wisdom, is hidden in the folds of her gown. Her right hand holds a scepter composed of four sprays of wheat, terminating with a crown of King's College which refers to Columbia's origin as a royal charter institution in 1754. A local actress named Mary Lawton was said to have posed for parts of the sculpture. The statue was dedicated on September 23, 1903, as a gift of Mr. & Mrs. Robert Goelet, and was originally covered in golden leaf. During the Columbia University protests of 1968 a bomb damaged the sculpture, but it has since been repaired. The small hidden owl on the sculpture is also the subject of many Columbia legends, the main legend being that the first student in the freshmen class to find the hidden owl on the statue will be valedictorian, and that any subsequent Columbia male who finds it will marry a Barnard student, given that Barnard is a women's college.""The Steps"", alternatively known as ""Low Steps"" or the ""Urban Beach"", are a popular meeting area for Columbia students. The term refers to the long series of granite steps leading from the lower part of campus (South Field) to its upper terrace. With a design inspired by the City Beautiful movement, the steps of Low Library provides Columbia University and Barnard College students, faculty, and staff with a comfortable outdoor platform and space for informal gatherings, events, and ceremonies. McKim's classical facade epitomizes late 19th-century new-classical designs, with its columns and portico marking the entrance to an important structure. On warm days when the weather is favorable, the Low Steps often become a popular gathering place for students to sunbathe, eat lunch, or play frisbee.

Other campuses
In April 2007, the university purchased more than two-thirds of a 17 acres (6.9 ha) site for a new campus in Manhattanville, an industrial neighborhood to the north of the Morningside Heights campus. Stretching from 125th Street to 133rd Street, Columbia Manhattanville houses buildings for Columbia's Business School, School of International and Public Affairs, Columbia School of the Arts, and the Jerome L. Greene Center for Mind, Brain, and Behavior, where research will occur on neurodegenerative diseases such as Parkinson's and Alzheimer's. The $7 billion expansion plan included demolishing all buildings, except three that are historically significant, eliminating the existing light industry and storage warehouses, and relocating tenants in 132 apartments. Replacing these buildings created 6.8 million square feet (630,000 m2) of space for the university. Community activist groups in West Harlem fought the expansion for reasons ranging from property protection and fair exchange for land, to residents' rights. Subsequent public hearings drew neighborhood opposition. As of December 2008, the State of New York's Empire State Development Corporation approved use of eminent domain, which, through declaration of Manhattanville's ""blighted"" status, gives governmental bodies the right to appropriate private property for public use. On May 20, 2009, the New York State Public Authorities Control Board approved the Manhanttanville expansion plan and the first buildings are under construction. Columbia Transportation is the shuttle bus service of the university, it operates between all campuses.NewYork-Presbyterian Hospital is affiliated with the medical schools of both Columbia University and Cornell University. According to U.S. News & World Report's ""2019–20 Best Hospitals Honor Roll and Medical Specialties Rankings"", it is ranked fifth overall and third among university hospitals. Columbia's medical school has a strategic partnership with New York State Psychiatric Institute, and is affiliated with 19 other hospitals in the U.S. and four hospitals overseas. Health-related schools are located at the Columbia University Medical Center, a 20 acres (8.1 ha) campus located in the neighborhood of Washington Heights, fifty blocks uptown. Other teaching hospitals affiliated with Columbia through the NewYork-Presbyterian network include the Payne Whitney Clinic in Manhattan, and the Payne Whitney Westchester, a psychiatric institute located in White Plains, New York. On the northern tip of Manhattan island (in the neighborhood of Inwood), Columbia owns 26-acre (11 ha) Baker Field, which includes the Lawrence A. Wien Stadium as well as facilities for field sports, outdoor track, and tennis. There is a third campus on the west bank of the Hudson River, the 157-acre (64 ha) Lamont-Doherty Earth Observatory and Earth Institute in Palisades, New York. A fourth is the 60-acre (24 ha) Nevis Laboratories in Irvington, New York for the study of particle and motion physics. A satellite site in Paris, France holds classes at Reid Hall.

Sustainability
In 2006, the university established the Office of Environmental Stewardship to initiate, coordinate and implement programs to reduce the university's environmental footprint. The U.S. Green Building Council selected the university's Manhattanville plan for the Leadership in Energy and Environmental Design (LEED) Neighborhood Design pilot program. The plan commits to incorporating smart growth, new urbanism and ""green"" building design principles. Columbia is one of the 2030 Challenge Partners, a group of nine universities in the city of New York that have pledged to reduce their greenhouse emissions by 30% within the next ten years. Columbia University adopts LEED standards for all new construction and major renovations. The university requires a minimum of Silver, but through its design and review process seeks to achieve higher levels. This is especially challenging for lab and research buildings with their intensive energy use; however, the university also uses lab design guidelines that seek to maximize energy efficiency while protecting the safety of researchers.Every Thursday and Sunday of the month, Columbia hosts a greenmarket where local farmers can sell their produce to residents of the city. In addition, from April to November Hodgson's farm, a local New York gardening center, joins the market bringing a large selection of plants and blooming flowers. The market is one of the many operated at different points throughout the city by the non-profit group GrowNYC. Dining services at Columbia spends 36 percent of its food budget on local products, in addition to serving sustainably harvested seafood and fair trade coffee on campus. Columbia has been rated ""B+"" by the 2011 College Sustainability Report Card for its environmental and sustainability initiatives.According to the A. W. Kuchler U.S. potential natural vegetation types, Columbia University would have a dominant vegetation type of Appalachian Oak (104) with a dominant vegetation form of Eastern Hardwood Forest (25).

Transportation
Columbia Transportation is the bus service of the university, operated by Academy Bus Lines. The buses are open to all Columbia faculty, students, Dodge Fitness Center members, and anyone else who holds a Columbia ID card. In addition, all TSC students can ride the buses.

Academics
Undergraduate admissions and financial aid
Columbia University received nearly 40,100 applications for the class of 2024 (entering 2020) and a total of around 2,450 were admitted to the two schools for an overall acceptance rate of 6.1%, making Columbia the third most selective college in the United States behind Stanford and Harvard as well as the second most selective college in the Ivy League. Columbia is a racially diverse school, with approximately 52% of all students identifying themselves as persons of color. Additionally, 50% of all undergraduates received grants from Columbia. The average grant size awarded to these students is $46,516. In 2015–2016, annual undergraduate tuition at Columbia was $50,526 with a total cost of attendance of $65,860 (including room and board).On April 11, 2007, Columbia University announced a $400 million to $600 million donation from media billionaire alumnus John Kluge to be used exclusively for undergraduate financial aid. The donation is among the largest single gifts to higher education. Its exact value will depend on the eventual value of Kluge's estate at the time of his death; however, the generous donation has helped change financial aid policy at Columbia. Annual gifts, fund-raising, and an increase in spending from the university's endowment have allowed Columbia to extend generous financial aid packages to qualifying students. As of 2008, undergraduates from families with incomes as high as $60,000 a year will have the projected cost of attending the university, including room, board, and academic fees, fully paid for by the university. That same year, the university ended loans for incoming and then-current students who were on financial aid, replacing loans that were traditionally part of aid packages with grants from the university. However, this does not apply to international students, transfer students, visiting students, or students in the School of General Studies. In the fall of 2010, admission to Columbia's undergraduate colleges Columbia College and the Fu Foundation School of Engineering and Applied Science (also known as SEAS or Columbia Engineering) began accepting the Common Application. The policy change made Columbia one of the last major academic institutions and the last Ivy League university to switch to the Common Application.Scholarships are also given to undergraduate students by the admissions committee. Designations include John W. Kluge Scholars, John Jay Scholars, C. Prescott Davis Scholars, Global Scholars, Egleston Scholars, and Science Research Fellows. Named scholars are selected by the admission committee from first-year applicants. According to Columbia, the first four designated scholars ""distinguish themselves for their remarkable academic and personal achievements, dynamism, intellectual curiosity, the originality and independence of their thinking, and the diversity that stems from their different cultures and their varied educational experiences.""In 1919, Columbia established a student application process characterized by The New York Times as ""the first modern college application."" The application required a photograph of the applicant, the maiden name of the applicant's mother, and the applicant's religious background.Columbia University received $1 million in Hanban funds over five years to begin a Confucius Institute. Professor Robert Barnett, the director of the Modern Tibetan Studies Program, described a ""strange silence about Tibet and other sensitive issues when it comes to Columbia, academics, and talks of China.""

Organization
Columbia University is an independent, privately supported, nonsectarian institution of higher education. Its official corporate name is ""The Trustees of Columbia University in the City of New York."" The university's first charter was granted in 1754 by King George II; however, its modern charter was first enacted in 1787 and last amended in 1810 by the New York State Legislature. The university is governed by 24 trustees, customarily including the president, who serves ex officio. The trustees themselves are responsible for choosing their successors. Six of the 24 are nominated from a pool of candidates recommended by the Columbia Alumni Association. Another six are nominated by the board in consultation with the executive committee of the University Senate. The remaining 12, including the president, are nominated by the trustees themselves through their internal processes. The term of office for trustees is six years. Generally, they serve for no more than two consecutive terms. The trustees appoint the president and other senior administrative officers of the university, and review and confirm faculty appointments as required. They determine the university's financial and investment policies, authorize the budget, supervise the endowment, direct the management of the university's real estate and other assets, and otherwise oversee the administration and management of the university.The University Senate was established by the trustees after a university-wide referendum in 1969. It succeeded to the powers of the University Council, which was created in 1890 as a body of faculty, deans, and other administrators to regulate inter-Faculty affairs and consider issues of university-wide concern. The University Senate is a unicameral body consisting of 107 members drawn from all constituencies of the university. These include the president of the university, the provost, the deans of Columbia College and the Graduate School of Arts and Sciences, all who serve ex officio, and five additional representatives, appointed by the president, from the university's administration. The president serves as the Senate's presiding officer. The Senate is charged with reviewing the educational policies, physical development, budget, and external relations of the university. It oversees the welfare and academic freedom of the faculty and the welfare of students.The president of Columbia University, who is selected by the trustees in consultation with the executive committee of the University Senate and who serves at the trustees' pleasure, is the chief executive officer of the university. Assisting the president in administering the university are the provost, the senior executive vice president, the executive vice president for health and biomedical sciences, several other vice presidents, the general counsel, the secretary of the university, and the deans of the faculties, all of whom are appointed by the trustees on the nomination of the president and serve at their pleasure. Lee C. Bollinger became the 19th president of Columbia University on June 1, 2002. A prominent advocate of affirmative action, he played a leading role in the twin Supreme Court cases—Grutter v Bollinger and Gratz v Bollinger—that upheld and clarified the importance of diversity as a compelling justification for affirmative action in higher education. A leading First Amendment scholar, he is widely published on freedom of speech and press, and serves on the faculty of Columbia Law School.Columbia has three official undergraduate colleges: Columbia College (CC), the liberal arts college offering the Bachelor of Arts degree; the Fu Foundation School of Engineering and Applied Science (also known as SEAS or Columbia Engineering), the engineering and applied science school offering the Bachelor of Science degree; and The School of General Studies (GS), the liberal arts college offering the Bachelor of Arts degree to non-traditional students undertaking full- or part-time study. Barnard College is a women's liberal arts college and an academic affiliate in which students receive a Bachelor of Arts degree from Columbia University. Their degrees are signed by the Presidents of Columbia University and Barnard College. Barnard students are also eligible to cross-register classes that are available through the Barnard Catalogue and alumnae can join the Columbia Alumni Association.Joint degree programs are available through Union Theological Seminary, the Jewish Theological Seminary of America, as well as through the Juilliard School. Teachers College and Barnard College are faculties of the university; both colleges' presidents are deans under the university governance structure. The Columbia University Senate includes faculty and student representatives from Teachers College and Barnard College who serve two-year terms; all senators are accorded full voting privileges regarding matters impacting the entire university. Teachers College is an affiliated, financially independent graduate school with their own Board of Trustees. Pursuant to an affiliation agreement, Columbia is given the authority to confer ""appropriate degrees and diplomas"" to the graduates of Teachers College. The degrees are signed by Presidents of Teachers College and Columbia University.
Columbia's General Studies school also has joint undergraduate programs available through University College London, Sciences Po, City University of Hong Kong, Trinity College Dublin, and the Juilliard School.The university also has several Columbia Global Centers, in Amman, Beijing, Istanbul, Paris, Mumbai, Rio de Janeiro, Santiago, Asunción and Nairobi. According to The Nation, some American universities ""restrict discussions of Chinese human-rights abuses for fear of losing access to the country. Columbia University, for instance, canceled several talks on topics sensitive to the Communist Party because it was worried about offending Beijing.""

International partnerships
Columbia students can study abroad for a semester or a year at partner institutions such as Sciences Po, École des hautes études en sciences sociales (EHESS), École normale supérieure (ENS), Panthéon-Sorbonne University, King's College London, London School of Economics, and the University of Warwick. Select students can study at either the University of Oxford or the University of Cambridge for a year if approved by both Columbia and either Oxford or Cambridge.

Rankings
Columbia University is ranked 3rd overall among U.S. national universities and 7th globally for 2020 by U.S. News & World Report. QS University Rankings listed Columbia as 5th in the United States. Ranked 15th among U.S. colleges for 2020 by The Wall Street Journal and Times Higher Education, in recent years it has been ranked as high as 2nd. Individual colleges and schools were also nationally ranked by U.S. News & World Report for its 2021 edition. Columbia Law School was ranked for 4th, the Mailman School of Public Health 4th, the School of Social Work tied for 3rd, Columbia Business School 8th, the College of Physicians and Surgeons tied for 6th for research (and tied for 31st for primary care), the School of Nursing tied for 11th in the master's program and tied for 1st in the doctorate nursing program, and the Fu Foundation School of Engineering and Applied Science (graduate) was ranked tied for 14th.In 2019, Columbia was ranked 8th in the world by Academic Ranking of World Universities, 7th in the world by U.S. News & World Report 18th in the world by QS World University Rankings, and 16th globally by Times Higher Education World University Rankings.
Rankings by other organizations include the Graduate School of Architecture, Planning and Preservation #2, and its Graduate School of Journalism #1.Between 1996 and 2008, 18 Columbia affiliates have won Nobel Prizes, of whom nine are faculty members while one is an adjunct senior research scientist (Daniel Tsui) and the other a Global Fellow (Kofi Annan). Columbia faculty awarded the Nobel Prize include Richard Axel, Martin Chalfie, Eric Kandel, Tsung-Dao Lee, Robert Mundell, Orhan Pamuk, Edmund S. Phelps, Joseph Stiglitz, and Horst L. Stormer. Other awards and honors won by faculty include 30 MacArthur Foundation Award winners, 4 National Medal of Science recipients, 43 National Academy of Sciences Award winners, 20 National Academy of Engineering Award winners, 38 Institute of Medicine of the National Academies Award recipients and 143 American Academy of Arts and Sciences Award winners.In 2015, Columbia University was ranked the first in the state by average professor salaries. In 2011, the Mines ParisTech : Professional Ranking World Universities ranked Columbia 3rd best university for forming CEOs in the US and 12th worldwide.

Research
Columbia is classified among ""R1: Doctoral Universities – Very high research activity"". Columbia was the first North American site where the uranium atom was split. The College of Physicians and Surgeons played a central role in developing the modern understanding of neuroscience with the publication of Principles of Neural Science, described by historian of science Katja Huenther as the ""neuroscience 'bible'"". The book was written by a team of Columbia researchers that included Nobel Prize winner Eric Kandel, James H. Schwartz, and Thomas Jessell. Columbia was the birthplace of FM radio and the laser. The first brain-computer interface capable of translating brain signals into speech was developed by neuroengineers at Columbia. The MPEG-2 algorithm of transmitting high quality audio and video over limited bandwidth was developed by Dimitris Anastassiou, a Columbia professor of electrical engineering. Biologist Martin Chalfie was the first to introduce the use of Green Fluorescent Protein (GFP) in labeling cells in intact organisms. Other inventions and products related to Columbia include Sequential Lateral Solidification (SLS) technology for making LCDs, System Management Arts (SMARTS), Session Initiation Protocol (SIP) (which is used for audio, video, chat, instant messaging and whiteboarding), pharmacopeia, Macromodel (software for computational chemistry), a new and better recipe for glass concrete, Blue LEDs, and Beamprop (used in photonics).
Columbia scientists have been credited with about 175 new inventions in the health sciences each year. More than 30 pharmaceutical products based on discoveries and inventions made at Columbia reached the market. These include Remicade (for arthritis), Reopro (for blood clot complications), Xalatan (for glaucoma), Benefix, Latanoprost (a glaucoma treatment), shoulder prosthesis, homocysteine (testing for cardiovascular disease), and Zolinza (for cancer therapy). Columbia Technology Ventures (formerly Science and Technology Ventures), as of 2008, manages some 600 patents and more than 250 active license agreements. Patent-related deals earned Columbia more than $230 million in the 2006 fiscal year, according to the university, more than any university in the world. Columbia owns many unique research facilities, such as the Columbia Institute for Tele-Information dedicated to telecommunications and the Goddard Institute for Space Studies, which is an astronomical observatory affiliated with NASA.

Military and veteran enrollment
Columbia is a long-standing participant of the United States Department of Veterans Affairs Yellow Ribbon Program, allowing eligible veterans to pursue a Columbia undergraduate degree regardless of socioeconomic status for over 70 years. As a part of the Eisenhower Leader Development Program (ELDP) in partnership with the U.S. Military Academy at West Point, Columbia is the only school in the Ivy League to offer a graduate degree program in organizational psychology to aid military officers in tactical decision making and strategic management.

Student life
Students
In 2017, Columbia University's student population was 32,429 (8,868 students in undergraduate programs and 23,561 in postgraduate programs), with 42% of the student population identifying themselves as a minority and 28% born outside of the United States. Twenty-six percent of students at Columbia have family incomes below $60,000, making it one of the most socioeconomically diverse top-tier colleges. Sixteen percent of students at Columbia receive Federal Pell Grants, which mostly go to students whose family incomes are below $40,000. Seventeen percent of students are the first member of their family to attend a four-year college.On-campus housing is guaranteed for all four years as an undergraduate. Columbia College and the Fu Foundation School of Engineering and Applied Science (also known as SEAS or Columbia Engineering) share housing in the on-campus residence halls. First-year students usually live in one of the large residence halls situated around South Lawn: Hartley Hall, Wallach Hall (originally Livingston Hall), John Jay Hall, Furnald Hall or Carman Hall. Upperclassmen participate in a room selection process, wherein students can pick to live in a mix of either corridor- or apartment-style housing with their friends. The Columbia University School of General Studies, Barnard College and graduate schools have their own apartment-style housing in the surrounding neighborhood.Columbia University is home to many fraternities, sororities, and co-educational Greek organizations. Approximately 10–15% of undergraduate students are associated with Greek life. Many Barnard women also join Columbia sororities. There has been a Greek presence on campus since the establishment in 1836 of the Delta chapter of Alpha Delta Phi. The InterGreek Council is the self-governing student organization that provides guidelines and support to its member organizations within each of the three councils at Columbia, the Interfraternity Council, Panhellenic Council, and Multicultural Greek Council. The three council presidents bring their affiliated chapters together once a month to meet as one Greek community. The InterGreek Council meetings provide opportunity for member organizations to learn from each other, work together and advocate for community needs.

Publications
The Columbia Daily Spectator is the nation's second-oldest student newspaper; and The Blue and White, a monthly literary magazine established in 1890, discusses campus life and local politics in print and on its daily blog, dubbed the Bwog. The Morningside Post is a student-run multimedia news publication. Its content: student-written investigative news, international affairs analysis, opinion, and satire.
Political publications include The Current, a journal of politics, culture and Jewish Affairs; the Columbia Political Review, the multi-partisan political magazine of the Columbia Political Union; and AdHoc, which denotes itself as the ""progressive"" campus magazine and deals largely with local political issues and arts events.
Columbia Magazine is the alumni magazine of Columbia, serving all 340,000+ of the university's alumni. Arts and literary publications include The Columbia Review, the nation's oldest college literary magazine; Columbia, a nationally regarded literary journal; the Columbia Journal of Literary Criticism; and The Mobius Strip, an online arts and literary magazine. Inside New York is an annual guidebook to New York City, written, edited, and published by Columbia undergraduates. Through a distribution agreement with Columbia University Press, the book is sold at major retailers and independent bookstores.

Columbia is home to numerous undergraduate academic publications. The Columbia Undergraduate Science Journal prints original science research in its two annual publications. The Journal of Politics & Society is a journal of undergraduate research in the social sciences, published and distributed nationally by the Helvidius Group; Publius is an undergraduate journal of politics established in 2008 and published biannually; the Columbia East Asia Review allows undergraduates throughout the world to publish original work on China, Japan, Korea, Tibet, and Vietnam and is supported by the Weatherhead East Asian Institute; and The Birch, is an undergraduate journal of Eastern European and Eurasian culture that is the first national student-run journal of its kind; the Columbia Political Review, the undergraduate magazine on politics operated by the Columbia Political Union; the Columbia Economics Review, the undergraduate economic journal on research and policy supported by the Columbia Economics Department; and the Columbia Science Review is a science magazine that prints general interest articles and faculty profiles.The Fed a triweekly satire and investigative newspaper, and the Jester of Columbia, the newly (and frequently) revived campus humor magazine both inject humor into local life. Other publications include The Columbian, the undergraduate colleges' annually published yearbook the Gadfly, a biannual journal of popular philosophy produced by undergraduates; and Rhapsody in Blue, an undergraduate urban studies magazine. Professional journals published by academic departments at Columbia University include Current Musicology and The Journal of Philosophy. During the spring semester, graduate students in the Journalism School publish The Bronx Beat, a bi-weekly newspaper covering the South Bronx.
Founded in 1961 under the auspices of Columbia University's Graduate School of Journalism, the Columbia Journalism Review (CJR) examines day-to-day press performance as well as the forces that affect that performance. The magazine is published six times a year, and offers a reporting, analysis, criticism, and commentary. CJR.org, its web site, delivers real-time criticism and reporting, giving CJR a presence in the ongoing conversation about the media.

Broadcasting
Columbia is home to two pioneers in undergraduate campus radio broadcasting, WKCR-FM and CTV. Many undergraduates are also involved with Barnard's radio station, WBAR. WKCR, the student run radio station that broadcasts to the Tri-State area, claims to be the oldest FM radio station in the world, owing to the university's affiliation with Major Edwin Armstrong. The station went operational on July 18, 1939, from a 400-foot antenna tower in Alpine, New Jersey, broadcasting the first FM transmission in the world. Initially, WKCR wasn't a radio station, but an organization concerned with the technology of radio communications. As membership grew, however, the nascent club turned its efforts to broadcasting. Armstrong helped the students in their early efforts, donating a microphone and turntables when they designed their first makeshift studio in a dorm room. The station has its studios on the second floor of Alfred Lerner Hall on the Morningside campus with its main transmitter tower at 4 Times Square in Midtown Manhattan. Columbia Television (CTV) is the nation's second oldest Student television station and home of CTV News, a weekly live news program produced by undergraduate students.

Debate and Model UN
The Philolexian Society is a literary and debating club founded in 1802, making it the oldest student group at Columbia, as well as the third oldest collegiate literary society in the country. The society annually administers the Joyce Kilmer Bad Poetry Contest. The Columbia Parliamentary Debate Team competes in tournaments around the country as part of the American Parliamentary Debate Association, and hosts both high school and college tournaments on Columbia's campus, as well as public debates on issues affecting the university.The Columbia International Relations Council and Association (CIRCA), oversees Columbia's Model United Nations activities. CIRCA hosts college and high school Model UN conferences, hosts speakers influential in international politics to speak on campus, trains students from underprivileged schools in New York in Model UN and oversees a competitive team, which travels to colleges around the country and to an international conference every year. The competitive team consistently wins best and outstanding delegation awards and is considered one of the top teams in the country.

Technology and entrepreneurship
The Columbia University Organization of Rising Entrepreneurs (CORE) was founded in 1999. The student-run group aims to foster entrepreneurship on campus. Each year CORE hosts dozens of events, including talks, #StartupColumbia, a conference and venture competition for $250,000, and Ignite@CU, a weekend for undergrads interested in design, engineering, and entrepreneurship. Notable speakers include Peter Thiel, Jack Dorsey, Alexis Ohanian, Drew Houston, and Mark Cuban. By 2006, CORE had awarded graduate and undergraduate students over $100,000 in seed capital.
CampusNetwork, an on-campus social networking site called Campus Network that preceded Facebook, was created and popularized by Columbia engineering student Adam Goldberg in 2003. Mark Zuckerberg later asked Goldberg to join him in Palo Alto to work on Facebook, but Goldberg declined the offer. The Fu Foundation School of Engineering and Applied Science offers a minor in Technical Entrepreneurship through its Center for Technology, Innovation, and Community Engagement. SEAS' entrepreneurship activities focus on community building initiatives in New York and worldwide, made possible through partners such as Microsoft Corporation.Columbia is a top supplier of young engineering entrepreneurs for New York City. Over the past 20 years, graduates of Columbia established over 100 technology companies. Mayor Bloomberg has provided over $6.7 million towards entrepreneurial programs that partner with Columbia and other universities in New York. Professor Chris Wiggins of the Fu Foundation School of Engineering and Applied Science is working in conjunction with Professors Evan Korth of New York University and Hilary Mason, chief scientist at bit.ly to facilitate the growth of student tech-startups in an effort to transform a traditionally financially centered New York City into the next Silicon Valley. Their website, hackny.org, is a gathering ground of ideas and discussions for New York's young entrepreneurial community, the Silicon Alley.On June 14, 2010, Mayor Michael R. Bloomberg launched the NYC Media Lab to promote innovations in New York's media industry. Situated at the New York University Tandon School of Engineering, the lab is a consortium of Columbia University, New York University, and New York City Economic Development Corporation acting to connect companies with universities in new technology research. The Lab is modeled after similar ones at MIT and Stanford. A $250,000 grant from the New York City Economic Development Corporation was used to establish the NYC Media Lab. Each year, the lab will host a range of roundtable discussions between the private sector and academic institutions. It will support research projects on topics of content format, next-generation search technologies, computer animation for film and gaming, emerging marketing techniques, and new devices development. The lab will also create a media research and development database. Columbia University will coordinate the long-term direction of the media lab as well as the involvement of its faculty and those of other universities.

Athletics
A member institution of the National Collegiate Athletic Association (NCAA) in Division I FCS, Columbia fields varsity teams in 29 sports and is a member of the Ivy League. The football Lions play home games at the 17,000-seat Robert K. Kraft Field at Lawrence A. Wien Stadium. The Baker Athletics Complex also includes facilities for baseball, softball, soccer, lacrosse, field hockey, tennis, track, and rowing, as well as the new Campbell Sports Center, opened in January 2013. The basketball, fencing, swimming & diving, volleyball, and wrestling programs are based at the Dodge Physical Fitness Center on the main campus.

Former students include Baseball Hall of Famers Lou Gehrig and Eddie Collins, football Hall of Famer Sid Luckman, Marcellus Wiley, and world champion women's weightlifter Karyn Marshall. On May 17, 1939, fledgling NBC broadcast a doubleheader between the Columbia Lions and the Princeton Tigers at Columbia's Baker Field, making it the first televised regular athletic event in history.Columbia University athletics has a long history, with many accomplishments in athletic fields. In 1870, Columbia played against Rutgers University in the second intercollegiate rugby football game in the history of the sport. Eight years later, Columbia crew won the famed Henley Royal Regatta in the first-ever defeat for an English crew rowing in English waters. In 1900, Olympian and Columbia College student Maxie Long set the first official world record in the 400 meters with a time of 47.8 seconds. In 1983, Columbia men's soccer went 18–0 and was ranked first in the nation, but lost to Indiana 1–0 in double overtime in the NCAA championship game; nevertheless, the team went further toward the NCAA title than any Ivy League soccer team in history. The football program unfortunately is best known for its record of futility set during the 1980s: between 1983 and 1988, the team lost 44 games in a row, which is still the record for the NCAA Football Championship Subdivision. The streak was broken on October 8, 1988, with a 16–13 victory over archrival Princeton University. That was the Lions' first victory at Wien Stadium, which had been opened during the losing streak and was already four years old. A new tradition has developed with the Liberty Cup. The Liberty Cup is awarded annually to the winner of the football game between Fordham and Columbia Universities, two of the only three NCAA Division I football teams in New York City. The tradition began in 2002, a year after the Fordham-Columbia game was postponed due to the September 11 attacks.

World Leaders Forum
Established in 2003 by university president Lee C. Bollinger, the World Leaders Forum at Columbia University provides the opportunity for undergraduate and graduate students alike to listen to world leaders in government, religion, industry, finance, and academia. The World Leaders Forum is a year-around event series that strives to provide a platform for uninhibited speech among nations and cultures, while educating students about problems and progress around the globe.All Columbia undergraduates and graduates, as well as students of Barnard College and other Columbia affiliated schools, can register to participate in the World Leaders Forum using their student IDs. Even for individuals who do not have the privilege to attend the event live, they can watch the forum via online videos on Columbia University's website.Past forum speakers include former president of the United States Bill Clinton, the prime minister of India Atal Bihari Vajpayee, former president of Ghana John Agyekum Kufuor, president of Afghanistan Hamid Karzai, prime minister of Russia Vladimir Putin, president of the Republic of Mozambique Joaquim Alberto Chissano, president of the Republic of Bolivia Carlos Diego Mesa Gisbert, president of the Republic of Romania Ion Iliescu, president of the Republic of Latvia Vaira Vīķe-Freiberga, the first female president of Finland Tarja Halonen, President Yudhoyono of Indonesia, President Pervez Musharraf of the Islamic Republic of Pakistan, Iraq President Jalal Talabani, the 14th Dalai Lama, president of the Islamic Republic of Iran Mahmoud Ahmadinejad, financier George Soros, Mayor of New York City Michael R. Bloomberg, President Václav Klaus of the Czech Republic, President Cristina Fernández de Kirchner of Argentina, former Secretary-General of the United Nations Kofi Annan, and Al Gore.

Other
The Columbia University Orchestra was founded by composer Edward MacDowell in 1896, and is the oldest continually operating university orchestra in the United States. Undergraduate student composers at Columbia may choose to become involved with Columbia New Music, which sponsors concerts of music written by undergraduate students from all of Columbia's schools.There are a number of performing arts groups at Columbia dedicated to producing student theater, including the Columbia Players, King's Crown Shakespeare Troupe (KCST), Columbia Musical Theater Society (CMTS), NOMADS (New and Original Material Authored and Directed by Students), LateNite Theatre, Columbia University Performing Arts League (CUPAL), Black Theatre Ensemble (BTE), sketch comedy group Chowdah, and improvisational troupes Alfred and Fruit Paunch. The Columbia University Marching Band tells jokes during the campus tradition of Orgo Night.

The Columbia Queer Alliance is the central Columbia student organization that represents the bisexual, lesbian, gay, transgender, and questioning student population. It is the oldest gay student organization in the world, founded as the Student Homophile League in 1967 by students including lifelong activist Stephen Donaldson. Columbia University campus military groups include the U.S. Military Veterans of Columbia University and Advocates for Columbia ROTC. In the 2005–06 academic year, the Columbia Military Society, Columbia's student group for ROTC cadets and Marine officer candidates, was renamed the Hamilton Society for ""students who aspire to serve their nation through the military in the tradition of Alexander Hamilton"".The university also houses an independent nonprofit organization, Community Impact, which strives to serve disadvantaged people in the Harlem, Washington Heights, and Morningside Heights communities. From its earliest inception as a single service initiative formed in 1981 by Columbia University undergraduates, Community Impact has grown into Columbia University's largest student service organization. CI provides food, clothing, shelter, education, job training, and companionship for residents in its surrounding communities. CI consists of a dedicated corps of about 950 Columbia University student volunteers participating in 25 community service programs, which serve more than 8,000 people each year.

Student activism
1936 protest against Nazis
In 1936, Robert Burke, CC '38 led a rally outside President Butler's mansion to protest Columbia's friendly relationship with the Nazis. Burke was expelled, and was never readmitted. The university has never apologized for expelling him.

Protests of 1968
Students initiated a major demonstration in 1968 over two main issues. The first was Columbia's proposed gymnasium in neighboring Morningside Park, perceived as a segregated facility, with limited access by the black residents of neighboring Harlem. A second issue was the Columbia administration's failure to resign its institutional membership in the Pentagon's weapons research think-tank, the Institute for Defense Analyses (IDA). Students barricaded themselves inside Low Library, Hamilton Hall, and several other university buildings during the protests, and New York City police were called onto the campus to arrest or forcibly remove the students.The protests achieved two of their stated goals. Columbia disaffiliated from the IDA and scrapped the plans for the controversial gym, building a subterranean physical fitness center under the north end of campus instead. A popular myth states that the gym's plans were eventually used by Princeton University for the expansion of its athletic facilities, but as Jadwin Gymnasium was already 50% complete by 1966 (when the Columbia gym was announced) this was clearly not correct. At least 30 Columbia students were suspended by the administration as a result of the protests. Many of the Class of '68 walked out of their graduation and held a countercommencement on Low Plaza with a picnic following at Morningside Park, the place where the protests began. The protests hurt Columbia financially as many potential students chose to attend other universities and some alumni refused to donate money to the school. Allan Bloom, a professor of philosophy at the University of Chicago,
believed that the protest efforts at Columbia were responsible for pushing higher education further toward the liberal left. As a result of the protests, Bloom stated, ""American universities were no longer places of intellectual and academic debate, but rather places of 'political correctness' and liberalism.""

Protests against racism and apartheid
Further student protests, including hunger strike and more barricades of Hamilton Hall and the Business School during the late 1970s and early 1980s, were aimed at convincing the university trustees to divest all of the university's investments in companies that were seen as active or tacit supporters of the apartheid regime in South Africa. A notable upsurge in the protests occurred in 1978, when following a celebration of the tenth anniversary of the student uprising in 1968, students marched and rallied in protest of university investments in South Africa. The Committee Against Investment in South Africa (CAISA) and numerous student groups including the Socialist Action Committee, the Black Student Organization and the Gay Students group joined together and succeeded in pressing for the first partial divestment of a U.S. university.
The initial (and partial) Columbia divestment,
focused largely on bonds and financial institutions directly involved with the South African regime. It followed a year-long campaign first initiated by students who had worked together to block the appointment of former United States Secretary of State Henry Kissinger to an endowed chair at the university in 1977.Broadly backed by student groups and many faculty members the Committee Against Investment in South Africa held teach-ins and demonstrations through the year focused on the trustees ties to the corporations doing business with South Africa. Trustee meetings were picketed and interrupted by demonstrations culminating in May 1978 in the takeover of the Graduate School of Business.

Columbia Unbecoming
In the early 2000s, professor Joseph Massad, held an elective course called Palestinian and Israeli Politics and Societies at Columbia. Students felt the views he espoused in the course were anti-Israel and some of them tried to disrupt his class and get him fired. In 2004, students got together with the pro-Israel campus group the David Project and produced a film called Columbia Unbecoming, accusing Massad and two other professors of intimidating or treating unfairly students with pro-Israel views. The film led to a committee being appointed by Bollinger which exonerated the professors in the spring of 2005. However, the committee's report criticized Columbia's inadequate grievance procedures.

Ahmadinejad speech controversy
The School of International and Public Affairs extends invitations to heads of state and heads of government who come to New York City for the opening of the fall session of the United Nations General Assembly. In 2007, Iranian President Mahmoud Ahmadinejad was one of those invited to speak on campus. Ahmadinejad accepted his invitation and spoke on September 24, 2007, as part of Columbia University's World Leaders Forum. The invitation proved to be highly controversial. Hundreds of demonstrators swarmed the campus on September 24 and the speech itself was televised worldwide. University President Lee C. Bollinger tried to allay the controversy by letting Ahmadenijad speak, but with a negative introduction (given personally by Bollinger). This did not mollify those who were displeased with the fact that the Iranian leader had been invited onto the campus. Columbia students, though, turned out en masse to listen to the speech on the South Lawn. An estimated 2,500 undergraduates and graduates came out for the historic occasion.
During his speech, Ahmadinejad criticized Israel's policies towards the Palestinians; called for research on the historical accuracy of the Holocaust; raised questions as to who initiated the 9/11 attacks; defended Iran's nuclear power program, criticizing the UN's policy of sanctions on his country; and attacked U.S. foreign policy in the Middle East. In response to a question about Iran's treatment of women and homosexuals, he asserted that women are respected in Iran and that ""In Iran, we don't have homosexuals like in your country…In Iran, we do not have this phenomenon. I don't know who told you this."" The latter statement drew laughter from the audience. The Manhattan District Attorney's Office accused Columbia of accepting grant money from the Alavi Foundation to support faculty ""sympathetic"" to Iran's Islamic republic.

ROTC controversy
Beginning in 1969, during the Vietnam War, the university did not allow the U.S. military to have Reserve Officers' Training Corps (ROTC) programs on campus, though Columbia students could participate in ROTC programs at other local colleges and universities. At a forum at the university during the 2008 presidential election campaign, both John McCain and Barack Obama said that the university should consider reinstating ROTC on campus. After the debate, the president of the university, Lee C. Bollinger, stated that he did not favor reinstating Columbia's ROTC program, because of the military's anti-gay policies. In November 2008, Columbia's undergraduate student body held a referendum on the question of whether or not to invite ROTC back to campus, and the students who voted were almost evenly divided on the issue. ROTC lost the vote (which would not have been binding on the administration, and did not include graduate students, faculty, or alumni) by a fraction of a percentage point.In April 2010 during Admiral Mike Mullen's address at Columbia, President Lee C. Bollinger stated that the ROTC would be readmitted to campus if the admiral's plans for revoking the don't ask, don't tell policy were successful. In February 2011 during one of three town-hall meetings on the ROTC ban, former Army staff sergeant Anthony Maschek, a Purple Heart recipient for injuries sustained during his service in Iraq, was booed and hissed at by some students during his speech promoting the idea of allowing the ROTC on campus. In April 2011 the Columbia University Senate voted to welcome the ROTC program back on campus. Secretary of the Navy Ray Mabus and Columbia University President Lee C. Bollinger signed an agreement to reinstate Naval Reserve Officers Training Corps (NROTC) program at Columbia for the first time in more than 40 years on May 26, 2011. The agreement was signed at a ceremony on board the USS Iwo Jima, docked in New York for the Navy's annual Fleet Week.

Divestment from private prisons
In February 2014, after learning that the university had over $10 million invested in the private prison industry, a group of students delivered a letter President Bollinger's office requesting a meeting and officially launching the Columbia Prison Divest (CPD) campaign. As of June 30, 2013, Columbia held investments in Corrections Corporation of America, the largest private prison company in the United States, as well as G4S, the largest multinational security firm in the world. Students demanded that the university divest these holdings from the industry and instate a ban on future investments in the private prison industry. Aligning themselves with the growing Black Lives Matter movement and in conversation with the heightened attention on race and the system of mass incarceration, CPD student activists hosted events to raise awareness of the issue and worked to involve large numbers of members of the Columbia and West Harlem community in campaign activities. After eighteen months of student driven organizing, the Board of Trustees of Columbia University voted to support the petition for divestment from private prison companies, which was confirmed to student leaders on June 22, 2015. The Columbia Prison Divest campaign was the first campaign to successfully get a U.S. university to divest from the private prison industry.

Tuition strike
In January 2021, more than 1000 Columbia University students initiated a tuition strike, demanding that the university lower its tuition rates by 10% amid financial burdens and the move to online classes prompted by the COVID-19 pandemic. Tuition for undergraduates is $58,920 for an academic year, and the total costs eclipse $80,000 when expenses including fees, room and board, books and travel are factored in. It is the largest tuition strike at the university in nearly 50 years. Students have stated they have won a number of concessions, as the university announced it would freeze tuition, suspend fees on late payments, increase spring financial aid and provide a limited amount of summer grants. a university spokesperson, however, stated that the decisions occurred several months prior to the strike. Students have also asked the university to end its expansion into and gentrification of West Harlem, defund its university police force, to divest from its investments in oil and gas companies, and bargain in good faith with campus unions. The university in February 2021 announced that the Board of Trustees had finally formalized its commitment to divest from publicly traded oil and gas companies. The strike had been largely organized by the campus chapter of Young Democratic Socialists of America, which had partnered with other student groups to support the action.

Starting in March 2021, members of the Graduate Workers of Columbia–United Auto Workers (a graduate student employee union) have been on strike over issues related to securing a labor contract with the university.

Traditions
Orgo Night
In one of the school's longest-lasting traditions, begun in 1975, at midnight before the Organic Chemistry exam—often the first day of final exams—the Columbia University Marching Band invaded and briefly occupied the main undergraduate reading room in Butler Library to distract and entertain studying students with some forty-five minutes of raucous jokes and music, beginning and ending with the singing of the school's fight song, ""Roar, Lion, Roar"". After the main show before a crowd that routinely began filling the room well before the announced midnight start time, the Band led a procession to several campus locations, including the residential quadrangle of Barnard College for more music and temporary relief from the stress of last-minute studying.
In December 2016, following several years of complaints from students who said that some Orgo Night scripts and advertising posters were offensive to minority groups, as well as a The New York Times article on the Band's crass treatment of sexual assault on campus,
University administrators banned the Marching Band from performing its Orgo Night show in the traditional Butler Library location. Protests and allegations of censorship followed, but University President Lee Bollinger said that complaints and publicity about the shows had ""nothing to do with"" the prohibition. The Band instead performed—at midnight, as usual—outside the main entrance of Butler Library.
The Band's official alumni organization, the Columbia University Band Alumni Association, registered protests with the administration, and an ad hoc group of alumni writing under the name ""A. Hamiltonius"" published a series of pamphlets addressing their dissatisfaction with the ban, but at the end of the spring 2017 semester the university administration held firm, prompting the Marching Band to again stage its show outside the building. For Orgo Night December 2017, Band members quietly infiltrated the library with their musical instruments during the evening and popped up at midnight to perform the show inside despite the ban. Prior to the spring 2018 exam period, the administration warned the group's leaders against a repeat and restated the injunction, warning of sanctions; the Band again staged its Orgo Night show in front of the library.

Tree-Lighting and Yule Log ceremonies
The campus Tree-Lighting Ceremony was inaugurated in 1998. It celebrates the illumination of the medium-sized trees lining College Walk in front of Kent and Hamilton Halls on the east end and Dodge and Journalism Halls on the west, just before finals week in early December. The lights remain on until February 28. Students meet at the sun-dial for free hot chocolate, performances by a cappella groups, and speeches by the university president and a guest.Immediately following the College Walk festivities is one of Columbia's older holiday traditions, the lighting of the Yule Log. The Christmas ceremony dates to a period prior to the American Revolutionary War, but lapsed before being revived by University President Nicholas Murray Butler in the early 20th century. A troop of students dressed as Continental Army soldiers carry the eponymous log from the sun-dial to the lounge of John Jay Hall, where it is lit amid the singing of seasonal carols. The Christmas ceremony is accompanied by a reading of A Visit From St. Nicholas by Clement Clarke Moore and Yes, Virginia, There is a Santa Claus by Francis Pharcellus Church.

The Varsity Show
The Varsity Show is an annual musical written by and for students and was established in 1894, making it one of Columbia's oldest traditions. Past writers and directors have included Columbians Richard Rodgers and Oscar Hammerstein, Lorenz Hart, I.A.L. Diamond, Herman Wouk and Eric Garcetti. The show has one of the largest operating budgets of all university events.

Notable people
The university has graduated many notable alumni, including five Founding Fathers of the United States, an author of the United States Constitution and a member of the Committee of Five; As of 2011, there were 125 Pulitzer Prize winners and 39 Oscar winners, as well as three United States presidents. As of 2006, there were 101 National Academy members who were alumni.In a 2016 ranking of universities worldwide with respect to living graduates who are billionaires, Columbia ranked second, after Harvard.Former U.S. Presidents Theodore Roosevelt and Franklin Delano Roosevelt attended the law school. Other political figures educated at Columbia include former U.S President Barack Obama, Associate Justice of the U.S. Supreme Court Ruth Bader Ginsburg, former U.S. Secretary of State Madeleine Albright, former chairman of the U.S. Federal Reserve Bank Alan Greenspan, U.S. Attorney General Eric Holder, and U.S. Solicitor General Donald Verrilli Jr. Dwight D. Eisenhower served as the thirteenth president of Columbia University from 1948 to 1953. The university has also educated 26 foreign heads of state, including president of Georgia Mikheil Saakashvili, president of East Timor Jose Ramos Horta, president of Estonia Toomas Hendrik Ilves and other historical figures such as Wellington Koo, Radovan Karadžić, Gaston Eyskens, and T. V. Soong. The author of India's constitution Dr. B. R. Ambedkar was also an alumnus of Columbia.Alumni of Columbia have occupied top positions in Wall Street and the rest of the business world. Notable members of the Astor family attended Columbia, while other business graduates include investor Warren Buffett, former CEO of PBS and NBC Larry Grossman, chairman of Wal-Mart S. Robson Walton and Bain Capital Co-Managing Partner, Jonathan Lavine. CEO's of top Fortune 500 companies include James P. Gorman of Morgan Stanley, Robert J. Stevens of Lockheed Martin, Philippe Dauman of Viacom, Ursula Burns of Xerox, and Vikram Pandit of Citigroup. Notable labor organizer and women's educator Louise Leonard McLaren received her degree of Master of Arts from Columbia.In science and technology, Columbia alumni include: founder of IBM Herman Hollerith; inventor of FM radio Edwin Armstrong; Francis Mechner; integral in development of the nuclear submarine Hyman Rickover; founder of Google China Kai-Fu Lee; scientists Stephen Jay Gould, Robert Millikan, Helium–neon laser inventor Ali Javan and Mihajlo Pupin; chief-engineer of the New York City Subway, William Barclay Parsons; philosophers Irwin Edman and Robert Nozick; economist Milton Friedman; psychologist Harriet Babcock; and sociologists Lewis A. Coser and Rose Laub Coser.Many Columbia alumni have gone on to renowned careers in the arts, including composers Richard Rodgers, Oscar Hammerstein II, Lorenz Hart, and Art Garfunkel. Four United States Poet Laureates received their degrees from Columbia. Columbia alumni have made an indelible mark in the field of American poetry and literature, with such people as Jack Kerouac and Allen Ginsberg, pioneers of the Beat Generation, and Langston Hughes, a seminal figure in the Harlem Renaissance, all having attended the university. Other notable writers who attended Columbia include authors Isaac Asimov, J.D. Salinger, Upton Sinclair, Danielle Valore Evans, and Hunter S. Thompson.University alumni have also been very prominent in the film industry, with 33 alumni and former students winning a combined 43 Academy Awards (as of 2011). Some notable Columbia alumni that have gone on to work in film include directors Sidney Lumet (12 Angry Men) and Kathryn Bigelow (The Hurt Locker), screenwriters Howard Koch (Casablanca) and Joseph L. Mankiewicz (All About Eve), and actors James Cagney and Ed Harris.
Notable Columbia University alumni include:

See also
Notes
References
Further reading
Robert A. McCaughey: Stand, Columbia: A History of Columbia University in the City of New York, 1754–2004, Columbia University Press, 2003, ISBN 0-231-13008-2
Living Legacies at Columbia, ed. by Wm Theodore De Bary, Columbia University Press, 2006, ISBN 0-231-13884-9

External links
Official website 
Columbia Athletics website
""Columbia University"" . Encyclopædia Britannica. 6 (11th ed.). 1911.
""Columbia University"" . New International Encyclopedia. 1905.",https://en.wikipedia.org/wiki/Columbia_University,"['1754 establishments in New York', 'All Wikipedia articles in need of updating', 'All articles containing potentially dated statements', 'All articles with unsourced statements', 'All articles with vague or ambiguous time', 'Articles containing potentially dated statements from 2006', 'Articles containing potentially dated statements from 2008', 'Articles containing potentially dated statements from 2011', 'Articles containing potentially dated statements from 2012', 'Articles containing potentially dated statements from December 2008', 'Articles containing potentially dated statements from June 2013', 'Articles containing potentially dated statements from October 2020', 'Articles with short description', 'Articles with unsourced statements from April 2017', 'Articles with unsourced statements from November 2015', 'CS1: Julian–Gregorian uncertainty', 'CS1 errors: missing periodical', 'Colonial colleges', 'Columbia University', 'Commons category link is on Wikidata', 'Coordinates on Wikidata', 'Educational institutions established in 1754', 'Good articles', 'McKim, Mead & White buildings', 'New York (state) in the American Revolution', 'Pages using New York City Subway service templates', 'Pages using infobox university with the image name parameter', 'Private universities and colleges in New York (state)', 'Short description matches Wikidata', 'Universities and colleges in Manhattan', 'Use mdy dates from July 2019', 'Vague or ambiguous time from April 2012', 'Vague or ambiguous time from January 2013', 'Webarchive template wayback links', 'Wikipedia articles in need of updating from July 2019', 'Wikipedia articles incorporating a citation from the 1911 Encyclopaedia Britannica with Wikisource reference', 'Wikipedia articles incorporating a citation from the New International Encyclopedia', 'Wikipedia articles with BNE identifiers', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with CINII identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with MusicBrainz place identifiers', 'Wikipedia articles with NKC identifiers', 'Wikipedia articles with SNAC-ID identifiers', 'Wikipedia articles with SUDOC identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with VcBA identifiers', 'Wikipedia articles with WORLDCATID identifiers', 'Wikipedia semi-protected pages']",Data Science
52,Data integrity,"Data integrity is the maintenance of, and the assurance of, data accuracy and consistency over its entire life-cycle and is a critical aspect to the design, implementation, and usage of any system that stores, processes, or retrieves data. The term is broad in scope and may have widely different meanings depending on the specific context –  even under the same general umbrella of computing. It is at times used as a proxy term for data quality, while data validation is a pre-requisite for data integrity.
Data integrity is the opposite of data corruption. The overall intent of any data integrity technique is the same: ensure data is recorded exactly as intended (such as a database correctly rejecting mutually exclusive possibilities). Moreover, upon later retrieval, ensure the data is the same as when it was originally recorded. In short, data integrity aims to prevent unintentional changes to information. Data integrity is not to be confused with data security, the discipline of protecting data from unauthorized parties.
Any unintended changes to data as the result of a storage, retrieval or processing operation, including malicious intent, unexpected hardware failure, and human error, is failure of data integrity. If the changes are the result of unauthorized access, it may also be a failure of data security. Depending on the data involved this could manifest itself as benign as a single pixel in an image appearing a different color than was originally recorded, to the loss of vacation pictures or a business-critical database, to even catastrophic loss of human life in a life-critical system.

Integrity types
Physical integrity
Physical integrity deals with challenges which are associated with correctly storing and fetching the data itself. Challenges with physical integrity may include electromechanical faults, design flaws, material fatigue, corrosion, power outages, natural disasters, and other special environmental hazards such as ionizing radiation, extreme temperatures, pressures and g-forces. Ensuring physical integrity includes methods such as redundant hardware, an uninterruptible power supply, certain types of RAID arrays, radiation hardened chips, error-correcting memory, use of a clustered file system, using file systems that employ block level checksums such as ZFS, storage arrays that compute parity calculations such as exclusive or or use a cryptographic hash function and even having a watchdog timer on critical subsystems.
Physical integrity often makes extensive use of error detecting algorithms known as error-correcting codes. Human-induced data integrity errors are often detected through the use of simpler checks and algorithms, such as the Damm algorithm or Luhn algorithm. These are used to maintain data integrity after manual transcription from one computer system to another by a human intermediary (e.g. credit card or bank routing numbers). Computer-induced transcription errors can be detected through hash functions.
In production systems, these techniques are used together to ensure various degrees of data integrity. For example, a computer file system may be configured on a fault-tolerant RAID array, but might not provide block-level checksums to detect and prevent silent data corruption. As another example, a database management system might be compliant with the ACID properties, but the RAID controller or hard disk drive's internal write cache might not be.

Logical integrity
This type of integrity is concerned with the correctness or rationality of a piece of data, given a particular context. This includes topics such as referential integrity and entity integrity in a relational database or correctly ignoring impossible sensor data in robotic systems. These concerns involve ensuring that the data ""makes sense"" given its environment. Challenges include software bugs, design flaws, and human errors. Common methods of ensuring logical integrity include things such as check constraints, foreign key constraints, program assertions, and other run-time sanity checks.
Both physical and logical integrity often share many common challenges such as human errors and design flaws, and both must appropriately deal with concurrent requests to record and retrieve data, the latter of which is entirely a subject on its own.
If a data sector only has a logical error, it can be reused by overwriting it with new data. In case of a physical error, the affected data sector is permanently unuseable.

Databases
Data integrity contains guidelines for data retention, specifying or guaranteeing the length of time data can be retained in a particular database.  To achieve data integrity, these rules are consistently and routinely applied to all data entering the system, and any relaxation of enforcement could cause errors in the data. Implementing checks on the data as close as possible to the source of input (such as human data entry), causes less erroneous data to enter the system. Strict enforcement of data integrity rules results in lower error rates, and time saved troubleshooting and tracing erroneous data and the errors it causes to algorithms.
Data integrity also includes rules defining the relations a piece of data can have, to other pieces of data, such as a Customer record being allowed to link to purchased Products, but not to unrelated data such as Corporate Assets. Data integrity often includes checks and correction for invalid data, based on a fixed schema or a predefined set of rules. An example being textual data entered where a date-time value is required. Rules for data derivation are also applicable, specifying how a data value is derived based on algorithm, contributors and conditions. It also specifies the conditions on how the data value could be re-derived.

Types of integrity constraints
Data integrity is normally enforced in a database system by a series of integrity constraints or rules. Three types of integrity constraints are an inherent part of the relational data model: entity integrity, referential integrity and domain integrity.

Entity integrity concerns the concept of a primary key. Entity integrity is an integrity rule which states that every table must have a primary key and that the column or columns chosen to be the primary key should be unique and not null.
Referential integrity concerns the concept of a foreign key. The referential integrity rule states that any foreign-key value can only be in one of two states. The usual state of affairs is that the foreign-key value refers to a primary key value of some table in the database. Occasionally, and this will depend on the rules of the data owner, a foreign-key value can be null. In this case, we are explicitly saying that either there is no relationship between the objects represented in the database or that this relationship is unknown.
Domain integrity specifies that all columns in a relational database must be declared upon a defined domain. The primary unit of data in the relational data model is the data item. Such data items are said to be non-decomposable or atomic. A domain is a set of values of the same type. Domains are therefore pools of values from which actual values appearing in the columns of a table are drawn.
User-defined integrity refers to a set of rules specified by a user, which do not belong to the entity, domain and referential integrity categories.If a database supports these features, it is the responsibility of the database to ensure data integrity as well as the consistency model for the data storage and retrieval. If a database does not support these features, it is the responsibility of the applications to ensure data integrity while the database supports the consistency model for the data storage and retrieval.
Having a single, well-controlled, and well-defined data-integrity system increases

stability (one centralized system performs all data integrity operations)
performance (all data integrity operations are performed in the same tier as the consistency model)
re-usability (all applications benefit from a single centralized data integrity system)
maintainability (one centralized system for all data integrity administration).Modern databases support these features (see Comparison of relational database management systems), and it has become the de facto responsibility of the database to ensure data integrity. Companies, and indeed many database systems, offer products and services to migrate legacy systems to modern databases.

Examples
An example of a data-integrity mechanism is the parent-and-child relationship of related records. If a parent record owns one or more related child records all of the referential integrity processes are handled by the database itself, which automatically ensures the accuracy and integrity of the data so that no child record can exist without a parent (also called being orphaned) and that no parent loses their child records. It also ensures that no parent record can be deleted while the parent record owns any child records. All of this is handled at the database level and does not require coding integrity checks into each application.

File systems
Various research results show that neither widespread filesystems (including UFS, Ext, XFS, JFS and NTFS) nor hardware RAID solutions provide sufficient protection against data integrity problems.Some filesystems (including Btrfs and ZFS) provide internal data and metadata checksumming that is used for detecting silent data corruption and improving data integrity.  If a corruption is detected that way and internal RAID mechanisms provided by those filesystems are also used, such filesystems can additionally reconstruct corrupted data in a transparent way.  This approach allows improved data integrity protection covering the entire data paths, which is usually known as end-to-end data protection.

Data integrity as applied to various industries
The U.S. Food and Drug Administration has created draft guidance on data integrity for the pharmaceutical manufacturers required to adhere to U.S. Code of Federal Regulations 21 CFR Parts 210–212. Outside the U.S., similar data integrity guidance has been issued by the United Kingdom (2015), Switzerland (2016), and Australia (2017).
Various standards for the manufacture of medical devices address data integrity either directly or indirectly, including ISO 13485, ISO 14155, and ISO 5840.
In early 2017, the Financial Industry Regulatory Authority (FINRA), noting data integrity problems with automated trading and money movement surveillance systems, stated it would make ""the development of a data integrity program to monitor the accuracy of the submitted data"" a priority. In early 2018, FINRA said it would expand its approach on data integrity to firms' ""technology change management policies and procedures"" and Treasury securities reviews.
Other sectors such as mining and product manufacturing are increasingly focusing on the importance of data integrity in associated automation and production monitoring assets.
Cloud storage providers have long faced significant challenges ensuring the integrity or provenance of customer data and tracking violations.

See also
End-to-end data integrity
Message authentication
National Information Systems Security Glossary
Single version of the truth
Optical disc § Surface error scanning

References
Further reading
 This article incorporates public domain material from the General Services Administration document: ""Federal Standard 1037C"". (in support of MIL-STD-188)
Xiaoyun Wang; Hongbo Yu (2005). ""How to Break MD5 and Other Hash Functions"" (PDF). EUROCRYPT. ISBN 3-540-25910-4. Archived from the original (PDF) on 2009-05-21. Retrieved 2009-05-10.",https://en.wikipedia.org/wiki/Data_integrity,"['Data quality', 'Transaction processing', 'Wikipedia articles incorporating text from MIL-STD-188', 'Wikipedia articles incorporating text from the Federal Standard 1037C', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers']",Data Science
53,Data loading,"In computing, extract, transform, load (ETL) is the general procedure of copying data from one or more sources into a destination system which represents the data differently from the source(s) or in a different context than the source(s). The ETL process became a popular concept in the 1970s and is often used in data warehousing.Data extraction involves extracting data from homogeneous or heterogeneous sources; data transformation processes data by data cleaning and transforming them into a proper storage format/structure for the purposes of querying and analysis; finally, data loading describes the insertion of data into the final target database such as an operational data store, a data mart, data lake or a data warehouse.A properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms data so that separate sources can be used together, and finally delivers data in a presentation-ready format so that application developers can build applications and end users can make decisions.Since the data extraction takes time, it is common to execute the three phases in pipeline. While the data is being extracted, another transformation process executes while processing the data already received and prepares it for loading while the data loading begins without waiting for the completion of the previous phases.
ETL systems commonly integrate data from multiple applications (systems), typically developed and supported by different vendors or hosted on separate computer hardware. The separate systems containing the original data are frequently managed and operated by different employees. For example, a cost accounting system may combine data from payroll, sales, and purchasing.

Extract
The first part of an ETL process involves extracting the data from the source system(s). In many cases, this represents the most important aspect of ETL, since extracting data correctly sets the stage for the success of subsequent processes. Most data-warehousing projects combine data from different source systems. Each separate system may also use a different data organization and/or format. Common data-source formats include relational databases, XML, JSON and flat files, but may also include non-relational database structures such as Information Management System (IMS) or other data structures such as Virtual Storage Access Method (VSAM) or Indexed Sequential Access Method (ISAM), or even formats fetched from outside sources by means such as web spidering or screen-scraping. The streaming of the extracted data source and loading on-the-fly to the destination database is another way of performing ETL when no intermediate data storage is required. 
An intrinsic part of the extraction involves data validation to confirm whether the data pulled from the sources has the correct/expected values in a given domain (such as a pattern/default or list of values). If the data fails the validation rules, it is rejected entirely or in part. The rejected data is ideally reported back to the source system for further analysis to identify and to rectify the incorrect records.

Transform
In the data transformation stage, a series of rules or functions are applied to the extracted data in order to prepare it for loading into the end target.
An important function of transformation is data cleansing, which aims to pass only ""proper"" data to the target. The challenge when different systems interact is in the relevant systems' interfacing and communicating. Character sets that may be available in one system may not be so in others.
In other cases, one or more of the following transformation types may be required to meet the business and technical needs of the server or data warehouse:

Selecting only certain columns to load: (or selecting null columns not to load). For example, if the source data has three columns (aka ""attributes""), roll_no, age, and salary, then the selection may take only roll_no and salary. Or, the selection mechanism may ignore all those records where salary is not present (salary = null).
Translating coded values: (e.g., if the source system codes male as ""1"" and female as ""2"", but the warehouse codes male as ""M"" and female as ""F"")
Encoding free-form values: (e.g., mapping ""Male"" to ""M"")
Deriving a new calculated value: (e.g., sale_amount = qty * unit_price)
Sorting or ordering the data based on a list of columns to improve search performance
Joining data from multiple sources (e.g., lookup, merge) and deduplicating the data
Aggregating (for example, rollup — summarizing multiple rows of data — total sales for each store, and for each region, etc.)
Generating surrogate-key values
Transposing or pivoting (turning multiple columns into multiple rows or vice versa)
Splitting a column into multiple columns (e.g., converting a comma-separated list, specified as a string in one column, into individual values in different columns)
Disaggregating repeating columns
Looking up and validating the relevant data from tables or referential files
Applying any form of data validation; failed validation may result in a full rejection of the data, partial rejection, or no rejection at all, and thus none, some, or all of the data is handed over to the next step depending on the rule design and exception handling; many of the above transformations may result in exceptions, e.g., when a code translation parses an unknown code in the extracted data

Load
The load phase loads the data into the end target, which can be any data store including a simple delimited flat file or a data warehouse. Depending on the requirements of the organization, this process varies widely. Some data warehouses may overwrite existing information with cumulative information; updating extracted data is frequently done on a daily, weekly, or monthly basis. Other data warehouses (or even other parts of the same data warehouse) may add new data in a historical form at regular intervals — for example, hourly. To understand this, consider a data warehouse that is required to maintain sales records of the last year. This data warehouse overwrites any data older than a year with newer data. However, the entry of data for any one year window is made in a historical manner. The timing and scope to replace or append are strategic design choices dependent on the time available and the business needs. More complex systems can maintain a history and audit trail of all changes to the data loaded in the data warehouse.As the load phase interacts with a database, the constraints defined in the database schema — as well as in triggers activated upon data load — apply (for example, uniqueness, referential integrity, mandatory fields), which also contribute to the overall data quality performance of the ETL process.

For example, a financial institution might have information on a customer in several departments and each department might have that customer's information listed in a different way. The membership department might list the customer by name, whereas the accounting department might list the customer by number. ETL can bundle all of these data elements and consolidate them into a uniform presentation, such as for storing in a database or data warehouse.
Another way that companies use ETL is to move information to another application permanently. For instance, the new application might use another database vendor and most likely a very different database schema. ETL can be used to transform the data into a format suitable for the new application to use.
An example would be an Expense and Cost Recovery System (ECRS) such as used by accountancies, consultancies, and legal firms. The data usually ends up in the time and billing system, although some businesses may also utilize the raw data for employee productivity reports to Human Resources (personnel dept.) or equipment usage reports to Facilities Management.

Real-life ETL cycle
The typical real-life ETL cycle consists of the following execution steps:

Cycle initiation
Build reference data
Extract (from sources)
Validate
Transform (clean, apply business rules, check for data integrity, create aggregates or disaggregates)
Stage (load into staging tables, if used)
Audit reports (for example, on compliance with business rules. Also, in case of failure, helps to diagnose/repair)
Publish (to target tables)
Archive

Challenges
ETL processes can involve considerable complexity, and significant operational problems can occur with improperly designed ETL systems.
The range of data values or data quality in an operational system may exceed the expectations of designers at the time validation and transformation rules are specified. Data profiling of a source during data analysis can identify the data conditions that must be managed by transform rules specifications, leading to an amendment of validation rules explicitly and implicitly implemented in the ETL process.
Data warehouses are typically assembled from a variety of data sources with different formats and purposes. As such, ETL is a key process to bring all the data together in a standard, homogeneous environment.
Design analysis should establish the scalability of an ETL system across the lifetime of its usage — including understanding the volumes of data that must be processed within service level agreements. The time available to extract from source systems may change, which may mean the same amount of data may have to be processed in less time. Some ETL systems have to scale to process terabytes of data to update data warehouses with tens of terabytes of data. Increasing volumes of data may require designs that can scale from daily batch to multiple-day micro batch to integration with message queues or real-time change-data-capture for continuous transformation and update.

Performance
ETL vendors benchmark their record-systems at multiple TB (terabytes) per hour (or ~1 GB per second) using powerful servers with multiple CPUs, multiple hard drives, multiple gigabit-network connections, and much memory.
In real life, the slowest part of an ETL process usually occurs in the database load phase. Databases may perform slowly because they have to take care of concurrency, integrity maintenance, and indices. Thus, for better performance, it may make sense to employ:

Direct path extract method or bulk unload whenever is possible (instead of querying the database) to reduce the load on source system while getting high-speed extract
Most of the transformation processing outside of the database
Bulk load operations whenever possibleStill, even using bulk operations, database access is usually the bottleneck in the ETL process. Some common methods used to increase performance are:

Partition tables (and indices): try to keep partitions similar in size (watch for null values that can skew the partitioning)
Do all validation in the ETL layer before the load: disable integrity checking (disable constraint ...) in the target database tables during the load
Disable triggers (disable trigger ...) in the target database tables during the load: simulate their effect as a separate step
Generate IDs in the ETL layer (not in the database)
Drop the indices (on a table or partition) before the load - and recreate them after the load (SQL: drop index ...; create index ...)
Use parallel bulk load when possible — works well when the table is partitioned or there are no indices (Note: attempting to do parallel loads into the same table (partition) usually causes locks — if not on the data rows, then on indices)
If a requirement exists to do insertions, updates, or deletions, find out which rows should be processed in which way in the ETL layer, and then process these three operations in the database separately; you often can do bulk load for inserts, but updates and deletes commonly go through an API (using SQL)Whether to do certain operations in the database or outside may involve a trade-off. For example, removing duplicates using distinct may be slow in the database; thus, it makes sense to do it outside. On the other side, if using distinct significantly (x100) decreases the number of rows to be extracted, then it makes sense to remove duplications as early as possible in the database before unloading data.
A common source of problems in ETL is a big number of dependencies among ETL jobs. For example, job ""B"" cannot start while job ""A"" is not finished. One can usually achieve better performance by visualizing all processes on a graph, and trying to reduce the graph making maximum use of parallelism, and making ""chains"" of consecutive processing as short as possible. Again, partitioning of big tables and their indices can really help.
Another common issue occurs when the data are spread among several databases, and processing is done in those databases sequentially. Sometimes database replication may be involved as a method of copying data between databases — it can significantly slow down the whole process. The common solution is to reduce the processing graph to only three layers:

Sources
Central ETL layer
TargetsThis approach allows processing to take maximum advantage of parallelism. For example, if you need to load data into two databases, you can run the loads in parallel (instead of loading into the first — and then replicating into the second).
Sometimes processing must take place sequentially. For example, dimensional (reference) data are needed before one can get and validate the rows for main ""fact"" tables.

Parallel processing
A recent development in ETL software is the implementation of parallel processing. It has enabled a number of methods to improve overall performance of ETL when dealing with large volumes of data.
ETL applications implement three main types of parallelism:

Data: By splitting a single sequential file into smaller data files to provide parallel access
Pipeline: allowing the simultaneous running of several components on the same data stream, e.g. looking up a value on record 1 at the same time as adding two fields on record 2
Component: The simultaneous running of multiple processes on different data streams in the same job, e.g. sorting one input file while removing duplicates on another fileAll three types of parallelism usually operate combined in a single job or task.
An additional difficulty comes with making sure that the data being uploaded is relatively consistent. Because multiple source databases may have different update cycles (some may be updated every few minutes, while others may take days or weeks), an ETL system may be required to hold back certain data until all sources are synchronized. Likewise, where a warehouse may have to be reconciled to the contents in a source system or with the general ledger, establishing synchronization and reconciliation points becomes necessary.

Rerunnability, recoverability
Data warehousing procedures usually subdivide a big ETL process into smaller pieces running sequentially or in parallel. To keep track of data flows, it makes sense to tag each data row with ""row_id"", and tag each piece of the process with ""run_id"". In case of a failure, having these IDs help to roll back and rerun the failed piece.
Best practice also calls for checkpoints, which are states when certain phases of the process are completed. Once at a checkpoint, it is a good idea to write everything to disk, clean out some temporary files, log the state, etc.

Virtual ETL
As of 2010, data virtualization had begun to advance ETL processing. The application of data virtualization to ETL allowed solving the most common ETL tasks of data migration and application integration for multiple dispersed data sources. Virtual ETL operates with the abstracted representation of the objects or entities gathered from the variety of relational, semi-structured, and unstructured data sources. ETL tools can leverage object-oriented modeling and work with entities' representations persistently stored in a centrally located hub-and-spoke architecture. Such a collection that contains representations of the entities or objects gathered from the data sources for ETL processing is called a metadata repository and it can reside in memory or be made persistent. By using a persistent metadata repository, ETL tools can transition from one-time projects to persistent middleware, performing data harmonization and data profiling consistently and in near-real time.

Dealing with keys
Unique keys play an important part in all relational databases, as they tie everything together. A unique key is a column that identifies a given entity, whereas a foreign key is a column in another table that refers to a primary key. Keys can comprise several columns, in which case they are composite keys. In many cases, the primary key is an auto-generated integer that has no meaning for the business entity being represented, but solely exists for the purpose of the relational database - commonly referred to as a surrogate key.
As there is usually more than one data source getting loaded into the warehouse, the keys are an important concern to be addressed. For example: customers might be represented in several data sources, with their Social Security Number as the primary key in one source, their phone number in another, and a surrogate in the third. Yet a data warehouse may require the consolidation of all the customer information into one dimension.
A recommended way to deal with the concern involves adding a warehouse surrogate key, which is used as a foreign key from the fact table.Usually, updates occur to a dimension's source data, which obviously must be reflected in the data warehouse.
If the primary key of the source data is required for reporting, the dimension already contains that piece of information for each row. If the source data uses a surrogate key, the warehouse must keep track of it even though it is never used in queries or reports; it is done by creating a lookup table that contains the warehouse surrogate key and the originating key. This way, the dimension is not polluted with surrogates from various source systems, while the ability to update is preserved.
The lookup table is used in different ways depending on the nature of the source data.
There are 5 types to consider; three are included here:

Type 1
The dimension row is simply updated to match the current state of the source system; the warehouse does not capture history; the lookup table is used to identify the dimension row to update or overwrite
Type 2
A new dimension row is added with the new state of the source system; a new surrogate key is assigned; source key is no longer unique in the lookup table
Fully logged
A new dimension row is added with the new state of the source system, while the previous dimension row is updated to reflect it is no longer active and time of deactivation.

Tools
By using an established ETL framework, one may increase one's chances of ending up with better connectivity and scalability. A good ETL tool must be able to communicate with the many different relational databases and read the various file formats used throughout an organization. ETL tools have started to migrate into Enterprise Application Integration, or even Enterprise Service Bus, systems that now cover much more than just the extraction, transformation, and loading of data. Many ETL vendors now have data profiling, data quality, and metadata capabilities. A common use case for ETL tools include converting CSV files to formats readable by relational databases. A typical translation of millions of records is facilitated by ETL tools that enable users to input csv-like data feeds/files and import it into a database with as little code as possible.
ETL tools are typically used by a broad range of professionals — from students in computer science looking to quickly import large data sets to database architects in charge of company account management, ETL tools have become a convenient tool that can be relied on to get maximum performance. ETL tools in most cases contain a GUI that helps users conveniently transform data, using a visual data mapper, as opposed to writing large programs to parse files and modify data types.
While ETL tools have traditionally been for developers and IT staff, the new trend is to provide these capabilities to business users so they can themselves create connections and data integrations when needed, rather than going to the IT staff. Gartner refers to these non-technical users as Citizen Integrators.

ETL Vs. ELT
Extract, load, transform (ELT) is a variant of ETL where the extracted data is loaded into the target system first.
The architecture for the analytics pipeline shall also consider where to cleanse and enrich data as well as how to conform dimensions.Cloud-based data warehouses like Amazon Redshift, Google BigQuery, and Snowflake Computing have been able to provide highly scalable computing power. This lets businesses forgo preload transformations and replicate raw data into their data warehouses, where it can transform them as needed using SQL.
After having used ELT, data may be processed further and stored in a data mart.There are pros and cons to each approach. Most data integration tools skew towards ETL, while ELT is popular in database and data warehouse appliances. Similarly, it is possible to perform TEL (Transform, Extract, Load) where data is first transformed on a blockchain (as a way of recording changes to data, e.g., token burning) before extracting and loading into another data store.

See also
Architecture patterns (EA reference architecture)
Create, read, update and delete (CRUD)
Data cleansing
Data integration
Data mart
Data mediation
Data migration
Electronic data interchange (EDI)
Enterprise architecture
Expense and cost recovery system (ECRS)
Hartmann pipeline
Legal Electronic Data Exchange Standard (LEDES)
Metadata discovery
Online analytical processing
Online transaction processing (OLTP)
Spatial ETL


== References ==","https://en.wikipedia.org/wiki/Extract,_transform,_load","['All articles containing potentially dated statements', 'All articles lacking in-text citations', 'All articles needing additional references', 'All articles that may contain original research', 'All articles with unsourced statements', 'Articles containing potentially dated statements from 2009', 'Articles containing potentially dated statements from 2010', 'Articles lacking in-text citations from November 2011', 'Articles needing additional references from May 2019', 'Articles that may contain original research from December 2011', 'Articles with multiple maintenance issues', 'Articles with unsourced statements from December 2011', 'Data warehousing', 'Extract, transform, load tools', 'Wikipedia articles with MA identifiers']",Data Science
54,Data library,"A data library, data archive, or data repository is a collection of numeric and/or geospatial data sets for secondary use in research. A data library is normally part of a larger institution (academic, corporate, scientific, medical, governmental, etc.) established for research data archiving and to serve the data users of that organisation. The data library tends to house local data collections and provides access to them through various means (CD-/DVD-ROMs or central server for download).  A data library may also maintain subscriptions to licensed data resources for its users to access the information. Whether a data library is also considered a data archive may depend on the extent of unique holdings in the collection, whether long-term preservation services are offered, and whether it serves a broader community (as national data archives do). Most public data libraries are listed in the Registry of Research Data Repositories.

Importance of data libraries and data librarianship
In August 2001, the Association of Research Libraries (ARL) published SPEC Kit 263: Numeric Data Products and Services, presenting results from a survey of ARL member institutions involved in collecting and providing services for numeric data resources.

Services offered by data libraries and data librarians
Library service providing support at the institutional level for the use of numerical and other types of  datasets in research. Amongst the support activities typically available:

Reference Assistance — locating numeric or geospatial datasets containing measurable variables on a particular topic or group of topics, in response to a user query.
User Instruction — providing hands-on training to groups of users in locating data resources on particular topics, how to download data and read it into spreadsheet, statistical, database, or GIS packages, how to interpret codebooks and other documentation.
Technical Assistance - including easing registration procedures, troubleshooting problems with the dataset, such as errors in the documentation, reformatting data into something a user can work with, and helping with statistical methodology.
Collection Development & Management - acquire, maintain, and manage a collection of data files used for secondary analysis by the local user community; purchase institutional data subscriptions; act as a site representative to data providers and national data archives for the institution.
Preservation and Data Sharing Services - act on a strategy of preservation of datasets in the collection, such as media refreshment and file format migration; download and keep records on updated versions from a central repository. Also, assist users in preparing original data for secondary use by others; either for deposit in a central or institutional repository, or for less formal ways of sharing data. This may also involve marking up the data into an appropriate XML standard, such as the Data Documentation Initiative, or adding other metadata to facilitate online discovery.

Associations
IASSIST (International Association for Social Science Information and Service Technology)
DISC-UK (Data Information Specialists Committee—United Kingdom)
APDU (Association of Public Data Users - USA)
CAPDU (Canadian Association of Public Data Users)

Examples of Data Libraries
Natural sciences
The following list refers to scientific data archives.

CISL Research Data Archive
Dryad
ESO/ST-ECF Science Archive Facility
International Tree-Ring Data Bank
Inter-university Consortium for Political and Social Research
Knowledge Network for Biocomplexity
National Archive of Computerized Data on Aging
National Archive of Criminal Justice Data [1]
National Climatic Data Center
National Geophysical Data Center
National Snow and Ice Data Center
National Oceanographic Data Center
Oak Ridge National Laboratory Distributed Active Archive Center
Pangaea - Data Publisher for Earth & Environmental Science
World Data Center
DataONE
4TU.Centre for Research Data

Social sciences
In the social sciences, data libraries are referred to as data archives. Data archives are professional institutions for the acquisition, preparation, preservation, and dissemination of social and behavioral data. Data archives in the social sciences evolved in the 1950s and have been perceived as an international movement: 

By 1964 the International Social Science Council (ISSC) had sponsored a second conference on Social Science Data Archives and had a standing Committee on Social Science Data, both of which stimulated the data archives movement. By the beginning of the twenty-first century, most developed countries and some developing countries had organized formal and well-functioning national data archives. In addition, college and university campuses often have `data libraries' that make data available to their faculty, staff, and students; most of these bear minimal archival responsibility, relying for that function on a national institution (Rockwell, 2001, p. 3227).
 re3data.org is a global registry of research data repository indexing data archives from all disciplines: http://www.re3data.org
CESSDA Members are data archives and other organisations that archive social science data and provide data for secondary use: https://www.cessda.eu/About/Consortium
Consortium of European Social Science Data Archives: http://www.cessda.org/
Finnish Social Science Data Archive (FSD): http://www.fsd.uta.fi/
The Danish Data Archives: http://www.sa.dk/content/us/about_us ; specific page (only in Danish): https://web.archive.org/web/20150318230743/http://www.sa.dk/dda/default.htm
Inter-university Consortium for Political and Social Research: http://www.icpsr.umich.edu/
The Roper Center for Public Opinion Research: https://ropercenter.cornell.edu/
The Social Science Data Archive: http://dataarchives.ss.ucla.edu/
The NCAR Research Data Archive:  http://rda.ucar.edu
Cornell Institute for Social and Economic Research: https://ciser.cornell.edu/data/data-archive/

References
Clubb, J., Austin, E., and Geda, C. ""'Sharing research data in the social sciences.'"" In Sharing Research Data, S. Fienberg, M. Martin, and M. Straf, Eds. National Academy Press, Washington, D.C., 1985, 39-88.
Geraci, D., Humphrey, C., and Jacobs, J. Data Basics. Canadian Library Association, Ottawa, ON, 2005.
Martinez, Luis & Macdonald, Stuart, ""'Supporting local data users in the UK academic community'"". Ariadne, issue 44, July 2005.
See the IASSIST Bibliography of Selected Works for articles tracing the history of data libraries and its relationship to the archivist profession, going back to the 1960s and '70s up to 1996.
See IASSIST Quarterly articles from 1993 to the present, focusing on data libraries, data archives, data support, and information technology for the social sciences.

See also
Data curation
Digital curation
Digital preservation
Data center
Open Data


== References ==",https://en.wikipedia.org/wiki/Data_library,"['All Wikipedia articles needing context', 'All pages needing cleanup', 'Digital libraries', 'Information technology', 'Types of library', 'Wikipedia articles needing context from September 2014', 'Wikipedia introduction cleanup from September 2014']",Data Science
55,Data management,"Data Management comprises all disciplines related to managing data as a valuable resource.

Concept
The concept of data management arose in the 1980s as technology moved from sequential processing  (first punched cards, then magnetic tape) to random access storage.  Since it was now possible to store a discrete fact and quickly access it using random access disk technology, those suggesting that data management was more important than business process management used arguments such as ""a customer's home address is stored in 75 (or some other large number) places in our computer systems.""  However, during this period, random access processing was not competitively fast, so those suggesting ""process management"" was more important than ""data management"" used batch processing time as their primary argument. As application software evolved into real-time, interactive usage, it became obvious that both management processes were important.  If the data was not well defined, the data would be mis-used in applications. If the process wasn't well defined, it was impossible to meet user needs.

Topics
Topics in data management include:

Usage
In modern management usage, the term data is increasingly replaced by information or even knowledge in a non-technical context. Thus data management has become information management or knowledge management. This trend obscures the raw data processing and renders interpretation implicit. The distinction between data and derived value is illustrated by the information ladder.
However, data has staged a comeback with the popularisation of the term big data, which refers to the collection and analyses of massive sets of data.
Several organisations have established data management centers (DMC) for their operations.

Integrated data management
Integrated data management (IDM) is a tools approach to facilitate data management and improve performance. IDM consists of an integrated, modular environment to manage enterprise application data, and optimize data-driven applications over its lifetime. IDM's purpose is to:

Produce enterprise-ready applications faster
Improve data access, speed iterative testing
Empower collaboration between architects, developers and DBAs
Consistently achieve service level targets
Automate and simplify operations
Provide contextual intelligence across the solution stack
Support business growth
Accommodate new initiatives without expanding infrastructure
Simplify application upgrades, consolidation and retirement
Facilitate alignment, consistency and governance
Define business policies and standards up front;  share, extend, and apply throughout the lifecycle

See also
References
External links
 Media related to Data management at Wikimedia Commons
Data management at Curlie",https://en.wikipedia.org/wiki/Data_management,"['All articles needing additional references', 'All articles with unsourced statements', 'Articles needing additional references from April 2020', 'Articles with Curlie links', 'Articles with unsourced statements from June 2016', 'Commons category link from Wikidata', 'Data management', 'Information technology management']",Data Science
56,Data migration,"Data migration is the process of selecting, preparing, extracting, and transforming data and permanently transferring it from one computer storage system to another. Additionally, the validation of migrated data for completeness and the decommissioning of legacy data storage are considered part of the entire data migration process. Data migration is a key consideration for any system implementation, upgrade, or consolidation, and it is typically performed in such a way as to be as automated as possible, freeing up human resources from tedious tasks. Data migration occurs for a variety of reasons, including server or storage equipment replacements, maintenance or upgrades, application migration, website consolidation, disaster recovery, and data center relocation.

The standard phases
As of 2011, ""nearly 40 percent of data migration projects were over time, over budget, or failed entirely."" As such, to achieve an effective data migration, proper planning is critical. While the specifics of a data migration plan may vary—sometimes significantly—from project to project, the computing company IBM suggests there are three main phases to most any data migration project: planning, migration, and post-migration. Each of those phases has its own steps. During planning, dependencies and requirements are analyzed, migration scenarios get developed and tested, and a project plan that incorporates the prior information is created. During the migration phase, the plan is enacted, and during post-migration, the completeness and thoroughness of the migration is validated, documented, closed out, including any necessary decommissioning of legacy systems. For applications of moderate to high complexity, these data migration phases may be repeated several times before the new system is considered to be fully validated and deployed.
Planning: The data, applications, etc. that will be migrated are selected based on business, project, and technical requirements and dependencies. Hardware and bandwidth requirements are analyzed. Feasible migration and back-out scenarios are developed, as well as the associated tests, automation scripts, mappings, and procedures. Data cleansing and transformation requirements are also gauged for data formats to improve data quality and to eliminate redundant or obsolete information. Migration architecture is decided on and developed, any necessary software licenses are obtained, and change management processes are started.Migration: Hardware and software requirements are validated, and migration procedures are customized as necessary. Some sort of pre-validation testing may also occur to ensure requirements and customized settings function as expected. If all is deemed well, migration begins, including the primary acts of data extraction, where data is read from the old system, and data loading, where data is written to the new system. Additional verification steps ensure the developed migration plan was enacted in full.Post-migration: After data migration, results are subjected to data verification to determine whether data was accurately translated, is complete, and supports processes in the new system. During verification, there may be a need for a parallel run of both systems to identify areas of disparity and forestall erroneous data loss. Additional documentation and reporting of the migration project is conducted, and once the migration is validated complete, legacy systems may also be decommissioned. Migration close-out meetings will officially end the migration process.

Project versus process
There is a difference between data migration and data integration activities. Data migration is a project by means of which data will be moved or copied from one environment to another, and removed or decommissioned in the source. During the migration (which can take place over months or even years), data can flow in multiple directions, and there may be multiple migrations taking place simultaneously. The ETL (extract, transform, load) actions will be necessary, although the means of achieving these may not be those traditionally associated with the ETL acronym.
Data integration, by contrast, is a permanent part of the IT architecture, and is responsible for the way data flows between the various applications and data stores—and is a process rather than a project activity. Standard ETL technologies designed to supply data from operational systems to data warehouses would fit within the latter category.

Categories
Data is stored on various media in files or databases, and is generated and consumed by software applications, which in turn support business processes. The need to transfer and convert data can be driven by multiple business requirements, and the approach taken to the migration depends on those requirements. Four major migration categories are proposed on this basis.

Storage migration
A business may choose to rationalize the physical media to take advantage of more efficient storage technologies. This will result in having to move physical blocks of data from one tape or disk to another, often using virtualization techniques. The data format and content itself will not usually be changed in the process and can normally be achieved with minimal or no impact to the layers above.

Database migration
Similarly, it may be necessary to move from one database vendor to another, or to upgrade the version of database software being used. The latter case is less likely to require a physical data migration, but this can happen with major upgrades. In these cases a physical transformation process may be required since the underlying data format can change significantly. This may or may not affect behavior in the applications layer, depending largely on whether the data manipulation language or protocol has changed. However, some modern applications are written to be almost entirely agnostic to the database technology, so a change from Sybase, MySQL, DB2 or SQL Server to Oracle should only require a testing cycle to be confident that both functional and non-functional performance has not been adversely affected.

Application migration
Changing application vendor—for instance a new CRM or ERP platform—will inevitably involve substantial transformation as almost every application or suite operates on its own specific data model and also interacts with other applications and systems within the enterprise application integration environment. Furthermore, to allow the application to be sold to the widest possible market, commercial off-the-shelf packages are generally configured for each customer using metadata. Application programming interfaces (APIs) may be supplied by vendors to protect the integrity of the data they have to handle. It is also possible to script the web interfaces of vendors to automatically migrate data.

Business process migration
Business processes operate through a combination of human and application systems actions, often orchestrated by business process management tools. When these change they can require the movement of data from one store, database or application to another to reflect the changes to the organization and information about customers, products and operations. Examples of such migration drivers are mergers and acquisitions, business optimization, and reorganization to attack new markets or respond to competitive threat.The first two categories of migration are usually routine operational activities that the IT department takes care of without the involvement of the rest of the business. The last two categories directly affect the operational users of processes and applications, are necessarily complex, and delivering them without significant business downtime can be challenging. A highly adaptive approach, concurrent synchronization, a business-oriented audit capability, and clear visibility of the migration for stakeholders—through a project management office or data governance team—are likely to be key requirements in such migrations.

Migration as a form of digital preservation
Migration, which focuses on the digital object itself, is the act of transferring, or rewriting data from an out-of-date medium to a current medium and has for many years been considered the only viable approach to long-term preservation of digital objects. Reproducing brittle newspapers onto microfilm is an example of such migration.

Disadvantages
Migration addresses the possible obsolescence of the data carrier, but does not address the fact that certain technologies which run the data may be abandoned altogether, leaving migration useless.
Time-consuming – migration is a continual process, which must be repeated every time a medium reaches obsolescence, for all data objects stored on a certain media.
Costly – an institution must purchase additional data storage media at each migration.

See also
Data conversion
Data curation
Data preservation
Data transformation
Digital Preservation
Extract, transform, load
System migration

References
External links
Data Migration at Curlie",https://en.wikipedia.org/wiki/Data_migration,"['All articles containing potentially dated statements', 'Articles containing potentially dated statements from 2011', 'Articles with Curlie links', 'Data management', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers']",Data Science
57,Data loss,"Data loss is an error condition in information systems in which information is destroyed by failures (like failed spindle motors or head crashes on hard drives) or neglect (like mishandling, careless handling or storage under unsuitable conditions) in storage, transmission, or processing. Information systems implement backup and disaster recovery equipment and processes to prevent data loss or restore lost data. Data loss can also occur if the physical medium containing the data is lost or stolen. 
Data loss is distinguished from data unavailability, which may arise from a network outage. Although the two have substantially similar consequences for users, data unavailability is temporary, while data loss may be permanent. Data loss is also distinct from data breach, an incident where data falls into the wrong hands, although the term data loss has been used in those incidents.

Types
Procedural
Intentional Action
Intentional deletion of a file or program
Unintentional Action
Accidental deletion of a file or program
Misplacement of CDs or Memory sticks
Administration errors
Inability to read unknown file format
Failure
Power failure, resulting in data in volatile memory not being saved to permanent memory.
Hardware failure, such as a head crash in a hard disk.
A software crash or freeze, resulting in data not being saved.
Software bugs or poor usability, such as not confirming a file delete command.
Business failure (vendor bankruptcy), where data is stored with a software vendor using Software-as-a-service and SaaS data escrow has not been provisioned.
Data corruption, such as file system corruption or database corruption.
Disaster
Natural disaster, earthquake, flood, tornado, etc.
Fire
Crime
Theft, hacking, SQL injection, sabotage, etc.
A malicious act, such as a worm, virus, Ransomware, hacking or theft of physical media.Studies show hardware failure and human error are the two most common causes of data loss, accounting for roughly three quarters of all incidents. Another cause of data loss is a natural disaster, which is a greater risk dependent on where the hardware is located. While the probability of data loss due to natural disaster is small, the only way to prepare for such an event is to store backup data in a separate physical location. As such, the best backup plans always include at least one copy being stored off-site.

Cost
The cost of a data loss event is directly related to the value of the data and the length of time that it is unavailable yet needed. For an enterprise in particular, the definition of cost extends beyond the financial and can also include time.
Consider:

The cost of continuing without the data
The cost of recreating the data
The cost of notifying users in the event of a compromise

Prevention
The frequency of data loss and the impact can be greatly mitigated by taking proper precautions, those of which necessary can vary depending on the type of data loss. For example, multiple power circuits with battery backup and a generator only protect against power failures, though using an Uninterruptable Power Supply can protect drive against sudden power spikes. Similarly, using a journaling file system and RAID storage only protect against certain types of software and hardware failure.For hard disk drives, which are a physical storage medium, ensuring minimal vibration and movement will help protect against damaging the components internally, as can maintaining a suitable drive temperature.Regular data backups are an important asset to have when trying to recover after a data loss event, but they do not prevent user errors or system failures. As such, a data backup plan needs to be established and run in unison with a disaster recovery plan in order to lower risk.

Data recovery
Data recovery is often performed by specialized commercial services that have developed often proprietary methods to recover data from physically damaged media. Service costs at data recovery labs are usually dependent on type of damage and type of storage medium, as well as the required security or cleanroom procedures.
File system corruption can frequently be repaired by the user or the system administrator. For example, a deleted file is typically not immediately overwritten on disk, but more often simply has its entry deleted from the file system index. In such a case, the deletion can be easily reversed.
Successful recovery from data loss generally requires implementation of an effective backup strategy. Without an implemented backup strategy, recovery requires reinstallation of programs and regeneration of data. Even with an effective backup strategy, restoring a system to the precise state it was in prior to the Data Loss Event is extremely difficult. Some level of compromise between granularity of recoverability and cost is necessary. Furthermore, a Data Loss Event may not be immediately apparent. An effective backup strategy must also consider the cost of maintaining the ability to recover lost data for long periods of time.
A highly effective backup system would have duplicate copies of every file and program that were immediately accessible whenever a Data Loss Event was noticed. However, in most situations, there is an inverse correlation between the value of a unit of data and the length of time it takes to notice the loss of that data. Taking this into consideration, many backup strategies decrease the granularity of restorability as the time increases since the potential Data Loss Event. By this logic, recovery from recent Data Loss Events is easier and more complete than recovery from Data Loss Events that happened further in the past.
Recovery is also related to the type of Data Loss Event. Recovering a single lost file is substantially different from recovering an entire system that was destroyed in a disaster. An effective backup regimen has some proportionality between the magnitude of Data Loss and the magnitude of effort required to recover. For example, it should be far easier to restore the single lost file than to recover the entire system.

Initial steps upon data loss
If data loss occurs, a successful recovery must ensure that the deleted data is not over-written. For this reason — one should avoid all write operations to the affected storage device. This includes not starting the system to which the affected device is connected. This is because many operating systems create temporary files in order to boot, and these may overwrite areas of lost data — rendering it unrecoverable. Viewing web pages has the same effect — potentially overwriting lost files with the temporary HTML and image files created when viewing a web page. File operations such as copying, editing, or deleting should also be avoided.
Upon realizing data loss has occurred, it is often best to shut down the computer and remove the drive in question from the unit. Re-attach this drive to a secondary computer with a write blocker device and then attempt to recover lost data. If possible, create an image of the drive in order to establish a secondary copy of the data. This can then be tested on, with recovery attempted, abolishing the risk of harming the source data.

See also
Data spill
Data truncation
List of data recovery software

References
External links
Data Loss Prevention
Some Data Loss Event",https://en.wikipedia.org/wiki/Data_loss,"['All articles with unsourced statements', 'Articles with unsourced statements from October 2018', 'Computer data', 'Data recovery']",Data Science
58,Data mining,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use. Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.

Etymology
In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983. Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation; researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities. Currently, the terms data mining and knowledge discovery are used interchangeably.
In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations. The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.

Background
The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns. in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.

Process
The knowledge discovery in databases (KDD) process is commonly defined with the stages:

Selection
Pre-processing
Transformation
Data mining
Interpretation/evaluation.It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:

Business understanding
Data understanding
Data preparation
Modeling
Evaluation
Deploymentor a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.
Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners. The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models, and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.

Pre-processing
Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.

Data mining
Data mining involves six common classes of tasks:
Anomaly detection (outlier/change/deviation detection) – The identification of unusual data records, that might be interesting or data errors that require further investigation.
Association rule learning (dependency modeling) – Searches for relationships between variables. For example, a supermarket might gather data on customer purchasing habits. Using association rule learning, the supermarket can determine which products are frequently bought together and use this information for marketing purposes. This is sometimes referred to as market basket analysis.
Clustering – is the task of discovering groups and structures in the data that are in some way or another ""similar"", without using known structures in the data.
Classification – is the task of generalizing known structure to apply to new data. For example, an e-mail program might attempt to classify an e-mail as ""legitimate"" or as ""spam"".
Regression – attempts to find a function that models the data with the least error that is, for estimating the relationships among data or datasets.
Summarization – providing a more compact representation of the data set, including visualization and report generation.

Results validation
Data mining can unintentionally be misused, and can then produce results that appear to be significant; but which do not actually predict future behavior and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening.The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by data mining algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish ""spam"" from ""legitimate"" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.
If the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.

Research
The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD). Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings, and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".Computer science conferences on data mining include:

CIKM Conference – ACM Conference on Information and Knowledge Management
European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases
KDD Conference – ACM SIGKDD Conference on Knowledge Discovery and Data MiningData mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases

Standards
There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.

Notable uses
Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.

Privacy concerns and ethics
While the term ""data mining"" itself may have no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics. In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent). This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.It is recommended to be aware of the following before data are collected:
The purpose of the data collection and any (known) data mining projects;
How the data will be used;
Who will be able to mine the data and use the data and their derivatives;
The status of security surrounding access to the data;
How collected data can be updated.Data may also be modified so as to become anonymous, so that individuals may not readily be identified. However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.

Situation in Europe
Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.–E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.

Situation in the United States
In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their ""informed consent"" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, ""'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals."" This underscores the necessity for data anonymity in data aggregation and mining practices.
U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.

Copyright law
Situation in Europe
Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception. The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. 
The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe. The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.

Situation in the United States
US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining.

Software
Free open-source data mining software and applications
The following applications are available under free/open-source licenses. Public access to application source code is also available.

Carrot2: Text and search results clustering framework.
Chemicalize.org: A chemical structure miner and web search engine.
ELKI: A university research project with advanced cluster analysis and outlier detection methods written in the Java language.
GATE: a natural language processing and language engineering tool.
KNIME: The Konstanz Information Miner, a user-friendly and comprehensive data analytics framework.
Massive Online Analysis (MOA): a real-time big data stream mining with concept drift tool in the Java programming language.
MEPX - cross-platform tool for regression and classification problems based on a Genetic Programming variant.
ML-Flex: A software package that enables users to integrate with third-party machine-learning packages written in any programming language, execute classification analyses in parallel across multiple computing nodes, and produce HTML reports of classification results.
mlpack: a collection of ready-to-use machine learning algorithms written in the C++ language.
NLTK (Natural Language Toolkit): A suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python language.
OpenNN: Open neural networks library.
Orange: A component-based data mining and machine learning software suite written in the Python language.
PSPP: Data mining and statistics software under the GNU Project similar to SPSS
R: A programming language and software environment for statistical computing, data mining, and graphics. It is part of the GNU Project.
scikit-learn is an open-source machine learning library for the Python programming language
Torch: An open-source deep learning library for the Lua programming language and scientific computing framework with wide support for machine learning algorithms.
UIMA: The UIMA (Unstructured Information Management Architecture) is a component framework for analyzing unstructured content such as text, audio and video – originally developed by IBM.
Weka: A suite of machine learning software applications written in the Java programming language.

Proprietary data-mining software and applications
The following applications are available under proprietary licenses.

Angoss KnowledgeSTUDIO: data mining tool
LIONsolver: an integrated software application for data mining, business intelligence, and modeling that implements the Learning and Intelligent OptimizatioN (LION) approach.
Megaputer Intelligence: data and text mining software is called PolyAnalyst.
Microsoft Analysis Services: data mining software provided by Microsoft.
NetOwl: suite of multilingual text and entity analytics products that enable data mining.
Oracle Data Mining: data mining software by Oracle Corporation.
PSeven: platform for automation of engineering simulation and analysis, multidisciplinary optimization and data mining provided by DATADVANCE.
Qlucore Omics Explorer: data mining software.
RapidMiner: An environment for machine learning and data mining experiments.
SAS Enterprise Miner: data mining software provided by the SAS Institute.
SPSS Modeler: data mining software provided by IBM.
STATISTICA Data Miner: data mining software provided by StatSoft.
Tanagra: Visualisation-oriented data mining software, also for teaching.
Vertica: data mining software provided by Hewlett-Packard.
Google Cloud Platform: automated custom ML models managed by Google.
Amazon SageMaker: managed service provided by Amazon for creating & productionising custom ML models.

See also
Methods
Application domains
Application examples

Related topicsFor more information about extracting information out of data (as opposed to analyzing data) , see:

Other resourcesInternational Journal of Data Warehousing and Mining

References
Further reading
External links
Knowledge Discovery Software at Curlie
Data Mining Tool Vendors at Curlie",https://en.wikipedia.org/wiki/Data_mining,"['All articles with specifically marked weasel-worded phrases', 'Articles to be expanded from September 2011', 'Articles with Curlie links', 'Articles with short description', 'Articles with specifically marked weasel-worded phrases from August 2019', 'CS1 maint: multiple names: authors list', 'Commons category link is on Wikidata', 'Data mining', 'Formal sciences', 'Short description is different from Wikidata', 'Webarchive template wayback links', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NDL identifiers', 'Wikipedia articles with multiple identifiers']",Data Science
59,Data pre-processing,"Data preprocessing is an important step in the data mining process. The phrase ""garbage in, garbage out"" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: −100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), and missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running any analysis. 
Often, data preprocessing is the most important phase of a machine learning project, especially in computational biology.If there is much irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase is more difficult. Data preparation and filtering steps can take considerable amount of processing time. Data preprocessing includes cleaning, Instance selection, normalization, transformation, feature extraction and selection, etc. The product of data preprocessing is the final training set.
Data preprocessing may affect the way in which outcomes of the final data processing can be interpreted. This aspect should be carefully considered when interpretation of the results is a key point, such in the multivariate processing of chemical data (chemometrics).

Tasks of data preprocessing
Data cleansing
Data editing
Data reduction
Data wrangling

Example
In this example we have 5 Adults in our dataset who have the Sex of Male or Female and whether they are pregnant or not. We can detect that Adult 3 and 5 are impossible data combinations.

We can perform a Data cleansing and choose to delete such data from our table. We remove such data because we can determine that such data existing in the dataset is caused by user entry errors or data corruption. A reason that one might have to delete such data is because the impossible data will affect the calculation or data manipulation process in the later steps of the data mining process. 

We can perform a Data editing and change the Sex of the Adult by knowing that the Adult is Pregnant we can make the assumption that the Adult is Female and make changes accordingly. We edit the dataset to have a clearer analysis of the data when performing data manipulation in the later steps within the data mining process.

We can use a form of Data reduction and sort the data by Sex and by doing this we can simplify our dataset and choose what Sex we want to focus on more.

Data mining
The origins of data preprocessing are located in data mining. The idea is to aggregate existing information and search in the content. Later it was recognized, that for machine learning and neural networks a data preprocessing step is needed too. So it has become to a universal technique which is used in computing in general.
Data preprocessing allows for the removal of unwanted data with the use of data cleansing, this allows the user to have a dataset to contain more valuable information after the preprocessing stage for data manipulation later in the data mining process. Editing such dataset to either correct data corruption or human error is a crucial step to get accurate quantifiers like true positives ,true negatives, False positives and false negatives found in a Confusion matrix that are commonly used for a medical diagnosis. Users are able to join data files together and use preprocessing to filter any unnecessary noise from the data which can allow for higher accuracy. Users use Python programming scripts accompanied by the pandas library which give them the ability to import data from a Comma-separated values as a data-frame.The data-frame is then used to manipulate data that can be challenging otherwise to do in Excel. pandas (software) which is a powerful tool allows for data analysis and manipulation; which makes data visualizations, statistical operations and much more, a lot easier. Many also use the R (programming language) to do such tasks as well. 
The reason why a user transforms existing files into a new one is because of many reasons. Data preprocessing has the objective to add missing values, aggregate information, label data with categories (Data binning) and smooth a trajectory. More advanced techniques like principle component analysis and feature selection are working with statistical formulas and are applied to complex datasets which are recorded by GPS trackers and motion capture devices.

Semantic data preprocessing
Complex problems are asking for more elaborated analyzing techniques of existing information. Instead of creating a simple script for aggregating different numerical values into one, it make sense to focus on semantic based data preprocessing. Here is the idea to build a dedicated ontology which explains on a higher level what the problem is about. The Protégé (software) is the standard tool for this purpose. A second more advanced technique is Fuzzy preprocessing. Here is the idea to ground numerical values with linguistic information. Raw data are transformed into natural language.

References
External links
Online Data Processing Compendium
Data preprocessing in predictive data mining. Knowledge Eng. Review 34: e1 (2019)",https://en.wikipedia.org/wiki/Data_pre-processing,"['All articles with unsourced statements', 'Articles with unsourced statements from March 2021', 'CS1 maint: multiple names: authors list', 'Machine learning']",Data Science
60,Data quality,"Data quality refers to the state of qualitative or quantitative pieces of information. There are many definitions of data quality, but data is generally considered high quality if it is ""fit for [its] intended uses in operations, decision making and planning"". Moreover, data is deemed of high quality if it correctly represents the real-world construct to which it refers. Furthermore, apart from these definitions, as the number of data sources increases, the question of internal data consistency becomes significant, regardless of fitness for use for any particular external purpose. People's views on data quality can often be in disagreement, even when discussing the same set of data used for the same purpose. When this is the case, data governance is used to form agreed upon definitions and standards for data quality. In such cases, data cleansing, including standardization, may be required in order to ensure data quality.

Definitions
Defining data quality in a sentence is difficult due to the many contexts data are used in, as well as the varying perspectives among end users, producers, and custodians of data.From a consumer perspective, data quality is:
""data that are fit for use by data consumers""
data ""meeting or exceeding consumer expectations""
data that ""satisfies the requirements of its intended use""From a business perspective, data quality is:

data that is ""'fit for use' in their intended operational, decision-making and other roles"" or that exhibits ""'conformance to standards' that have been set, so that fitness for use is achieved""
data that ""are fit for their intended uses in operations, decision making and planning""
""the capability of data to satisfy the stated business, system, and technical requirements of an enterprise""From a standards-based perspective, data quality is:

the ""degree to which a set of inherent characteristics (quality dimensions) of an object (data) fulfills requirements""
""the usefulness, accuracy, and correctness of data for its application""Arguably, in all these cases, ""data quality"" is a comparison of the actual state of a particular set of data to a desired state, with the desired state being typically referred to as ""fit for use,"" ""to specification,"" ""meeting consumer expectations,"" ""free of defect,"" or ""meeting requirements."" These expectations, specifications, and requirements are usually defined by one or more individuals or groups, standards organizations, laws and regulations, business policies, or software development policies. Drilling down further, those expectations, specifications, and requirements are stated in terms of characteristics or dimensions of the data, such as:
accessibility or availability
accuracy or correctness
comparability
completeness or comprehensiveness
consistency, coherence, or clarity
credibility, reliability, or reputation
flexibility
plausibility
relevance, pertinence, or usefulness
timeliness or latency
uniqueness
validity or reasonablenessA systematic scoping review of the literature suggests that data quality dimensions and methods with real world data are not consistent in the literature, and as a result quality assessments are challenging due to the complex and heterogeneous nature of these data.In 2021, the work group Data Quality of DAMA Netherlands has carried out research into definitions of dimensions of data quality. It has collected definitions from various sources and compared them with each other. The working group also tested the definitions against criteria derived from a standard for concepts and definitions: ISO 704. The results is a list of 60 dimensions of data quality and its definitions.

History
Before the rise of the inexpensive computer data storage, massive mainframe computers were used to maintain name and address data for delivery services. This was so that mail could be properly routed to its destination. The mainframes used business rules to correct common misspellings and typographical errors in name and address data, as well as to track customers who had moved, died, gone to prison, married, divorced, or experienced other life-changing events. Government agencies began to make postal data available to a few service companies to cross-reference customer data with the National Change of Address registry (NCOA). This technology saved large companies millions of dollars in comparison to manual correction of customer data. Large companies saved on postage, as bills and direct marketing materials made their way to the intended customer more accurately. Initially sold as a service, data quality moved inside the walls of corporations, as low-cost and powerful server technology became available.Companies with an emphasis on marketing often focused their quality efforts on name and address information, but data quality is recognized as an important property of all types of data. Principles of data quality can be applied to supply chain data, transactional data, and nearly every other category of data found. For example, making supply chain data conform to a certain standard has value to an organization by: 1) avoiding overstocking of similar but slightly different stock; 2) avoiding false stock-out; 3) improving the understanding of vendor purchases to negotiate volume discounts; and 4) avoiding logistics costs in stocking and shipping parts across a large organization.For companies with significant research efforts, data quality can include developing protocols for research methods, reducing measurement error, bounds checking of data, cross tabulation, modeling and outlier detection, verifying data integrity, etc.

Overview
There are a number of theoretical frameworks for understanding data quality. A systems-theoretical approach influenced by American pragmatism expands the definition of data quality to include information quality, and emphasizes the inclusiveness of the fundamental dimensions of accuracy and precision on the basis of the theory of science (Ivanov, 1972). One framework, dubbed ""Zero Defect Data"" (Hansen, 1991) adapts the principles of statistical process control to data quality. Another framework seeks to integrate the product perspective (conformance to specifications) and the service perspective (meeting consumers' expectations) (Kahn et al. 2002). Another framework is based in semiotics to evaluate the quality of the form, meaning and use of the data (Price and Shanks, 2004). One highly theoretical approach analyzes the ontological nature of information systems to define data quality rigorously (Wand and Wang, 1996).
A considerable amount of data quality research involves investigating and describing various categories of desirable attributes (or dimensions) of data. Nearly 200 such terms have been identified and there is little agreement in their nature (are these concepts, goals or criteria?), their definitions or measures (Wang et al., 1993). Software engineers may recognize this as a similar problem to ""ilities"".
MIT has a Information Quality (MITIQ) Program, led by Professor Richard Wang, which produces a large number of publications and hosts a significant international conference in this field (International Conference on Information Quality, ICIQ). This program grew out of the work done by Hansen on the ""Zero Defect Data"" framework (Hansen, 1991).
In practice, data quality is a concern for professionals involved with a wide range of information systems, ranging from data warehousing and business intelligence to customer relationship management and supply chain management. One industry study estimated the total cost to the U.S. economy of data quality problems at over U.S. $600 billion per annum (Eckerson, 2002). Incorrect data – which includes invalid and outdated information – can originate from different data sources – through data entry, or data migration and conversion projects.In 2002, the USPS and PricewaterhouseCoopers released a report stating that 23.6 percent of all U.S. mail sent is incorrectly addressed.One reason contact data becomes stale very quickly in the average database – more than 45 million Americans change their address every year.In fact, the problem is such a concern that companies are beginning to set up a data governance team whose sole role in the corporation is to be responsible for data quality. In some organizations, this data governance function has been established as part of a larger Regulatory Compliance function - a recognition of the importance of Data/Information Quality to organizations.
Problems with data quality don't only arise from incorrect data; inconsistent data is a problem as well. Eliminating data shadow systems and centralizing data in a warehouse is one of the initiatives a company can take to ensure data consistency.
Enterprises, scientists, and researchers are starting to participate within data curation communities to improve the quality of their common data.The market is going some way to providing data quality assurance. A number of vendors make tools for analyzing and repairing poor quality data in situ, service providers can clean the data on a contract basis and consultants can advise on fixing processes or systems to avoid data quality problems in the first place. Most data quality tools offer a series of tools for improving data, which may include some or all of the following:

Data profiling - initially assessing the data to understand its current state, often including value distributions
Data standardization - a business rules engine that ensures that data conforms to standards
Geocoding - for name and address data. Corrects data to U.S. and Worldwide geographic standards
Matching or Linking - a way to compare data so that similar, but slightly different records can be aligned. Matching may use ""fuzzy logic"" to find duplicates in the data. It often recognizes that ""Bob"" and ""Bbo"" may be the same individual. It might be able to manage ""householding"", or finding links between spouses at the same address, for example. Finally, it often can build a ""best of breed"" record, taking the best components from multiple data sources and building a single super-record.
Monitoring - keeping track of data quality over time and reporting variations in the quality of data. Software can also auto-correct the variations based on pre-defined business rules.
Batch and Real time - Once the data is initially cleansed (batch), companies often want to build the processes into enterprise applications to keep it clean.There are several well-known authors and self-styled experts, with Larry English perhaps the most popular guru. In addition, IQ International - the International Association for Information and Data Quality was established in 2004 to provide a focal point for professionals and researchers in this field.
ISO 8000 is an international standard for data quality.

Data quality assurance
Data quality assurance is the process of data profiling to discover inconsistencies and other anomalies in the data, as well as performing data cleansing activities (e.g. removing outliers, missing data interpolation) to improve the data quality.
These activities can be undertaken as part of data warehousing or as part of the database administration of an existing piece of application software.

Data quality control
Data quality control is the process of controlling the usage of data for an application or a process. This process is performed both before and after a Data Quality Assurance (QA) process, which consists of discovery of data inconsistency and correction.
Before:

Restricts inputsAfter QA process the following statistics are gathered to guide the Quality Control (QC) process:

Severity of inconsistency
Incompleteness
Accuracy
Precision
Missing / UnknownThe Data QC process uses the information from the QA process to decide to use the data for analysis or in an application or business process. General example: if a Data QC process finds that the data contains too many errors or inconsistencies, then it prevents that data from being used for its intended process which could cause disruption. Specific example: providing invalid measurements from several sensors to the automatic pilot feature on an aircraft could cause it to crash. Thus, establishing a QC process provides data usage protection.

Optimum use of data quality
Data Quality (DQ) is a niche area required for the integrity of the data management by covering gaps of data issues. This is one of the key functions that aid data governance by monitoring data to find exceptions undiscovered by current data management operations. Data Quality checks may be defined at attribute level to have full control on its remediation steps.DQ checks and business rules may easily overlap if an organization is not attentive of its DQ scope. Business teams should understand the DQ scope thoroughly in order to avoid overlap. Data quality checks are redundant if business logic covers the same functionality and fulfills the same purpose as DQ. The DQ scope of an organization should be defined in DQ strategy and well implemented. Some data quality checks may be translated into business rules after repeated instances of exceptions in the past.Below are a few areas of data flows that may need perennial DQ checks:
Completeness and precision DQ checks on all data may be performed at the point of entry for each mandatory attribute from each source system. Few attribute values are created way after the initial creation of the transaction; in such cases, administering these checks becomes tricky and should be done immediately after the defined event of that attribute's source and the transaction's other core attribute conditions are met.
All data having attributes referring to Reference Data in the organization may be validated against the set of well-defined valid values of Reference Data to discover new or discrepant values through the validity DQ check. Results may be used to update Reference Data administered under Master Data Management (MDM).
All data sourced from a third party to organization's internal teams may undergo accuracy (DQ) check against the third party data. These DQ check results are valuable when administered on data that made multiple hops after the point of entry of that data but before that data becomes authorized or stored for enterprise intelligence.
All data columns that refer to Master Data may be validated for its consistency check. A DQ check administered on the data at the point of entry discovers new data for the MDM process, but a DQ check administered after the point of entry discovers the failure (not exceptions) of consistency.
As data transforms, multiple timestamps and the positions of that timestamps are captured and may be compared against each other and its leeway to validate its value, decay, operational significance against a defined SLA (service level agreement). This timeliness DQ check can be utilized to decrease data value decay rate and optimize the policies of data movement timeline.
In an organization complex logic is usually segregated into simpler logic across multiple processes. Reasonableness DQ checks on such complex logic yielding to a logical result within a specific range of values or static interrelationships (aggregated business rules) may be validated to discover complicated but crucial business processes and outliers of the data, its drift from BAU (business as usual) expectations, and may provide possible exceptions eventually resulting into data issues. This check may be a simple generic aggregation rule engulfed by large chunk of data or it can be a complicated logic on a group of attributes of a transaction pertaining to the core business of the organization. This DQ check requires high degree of business knowledge and acumen. Discovery of reasonableness issues may aid for policy and strategy changes by either business or data governance or both.
Conformity checks and integrity checks need not covered in all business needs, it's strictly under the database architecture's discretion.
There are many places in the data movement where DQ checks may not be required. For instance, DQ check for completeness and precision on not–null columns is redundant for the data sourced from database. Similarly, data should be validated for its accuracy with respect to time when the data is stitched across disparate sources. However, that is a business rule and should not be in the DQ scope.Regretfully, from a software development perspective, DQ is often seen as a nonfunctional requirement. And as such, key data quality checks/processes are not factored into the final software solution. Within Healthcare, wearable technologies or Body Area Networks, generate large volumes of data. The level of detail required to ensure data quality is extremely high and is often underestimated. This is also true for the vast majority of mHealth apps, EHRs and other health related software solutions. However, some open source tools exist that examine data quality. The primary reason for this, stems from the extra cost involved is added a higher degree of rigor within the software architecture.

Health data security and privacy
The use of mobile devices in health, or mHealth, creates new challenges to health data security and privacy, in ways that directly affect data quality. mHealth is an increasingly important strategy for delivery of health services in low- and middle-income countries. Mobile phones and tablets are used for collection, reporting, and analysis of data in near real time. However, these mobile devices are commonly used for personal activities, as well, leaving them more vulnerable to security risks that could lead to data breaches. Without proper security safeguards, this personal use could jeopardize the quality, security, and confidentiality of health data.

Data quality in public health
Data quality has become a major focus of public health programs in recent years, especially as demand for accountability increases. Work towards ambitious goals related to the fight against diseases such as AIDS, Tuberculosis, and Malaria must be predicated on strong Monitoring and Evaluation systems that produce quality data related to program implementation. These programs, and program auditors, increasingly seek tools to standardize and streamline the process of determining the quality of data, verify the quality of reported data, and assess the underlying data management and reporting systems for indicators. An example is WHO and MEASURE Evaluation's Data Quality Review Tool WHO, the Global Fund, GAVI, and MEASURE Evaluation have collaborated to produce a harmonized approach to data quality assurance across different diseases and programs.

Open data quality
There are a number of scientific works devoted to the analysis of the data quality in open data sources, such as Wikipedia, Wikidata, DBpedia and other. In the case of Wikipedia, quality analysis may relate to the whole article Modeling of quality there is carried out by means of various methods. Some of them use machine learning algorithms, including Random Forest, Support Vector Machine, and others. Methods for assessing data quality in Wikidata, DBpedia and other LOD sources differ.

Professional associations
IQ International—the International Association for Information and Data Quality
IQ International is a not-for-profit, vendor neutral, professional association formed in 2004, dedicated to building the information and data quality profession.

ECCMA  (Electronic Commerce Code Management Association)
The Electronic Commerce Code Management Association (ECCMA) is a member-based, international not-for-profit association committed to improving data quality through the implementation of international standards. ECCMA is the current project leader for the development of ISO 8000 and ISO 22745, which are the international standards for data quality and the exchange of material and service master data, respectively.
ECCMA provides a platform for collaboration amongst subject experts on data quality and data governance around the world to build and maintain global, open standard dictionaries that are used to unambiguously label information. The existence of these dictionaries of labels allows information to be passed from one computer system to another without losing meaning.

See also
Data validation
Record linkage
Information quality
Master data management
Data governance
Database normalization
Data visualization
Data Analysis
Clinical data management

References
Further reading
Baškarada, S; Koronios, A (2014). ""A Critical Success Factors Framework for Information Quality Management"". Information Systems Management. 31 (4): 1–20. doi:10.1080/10580530.2014.958023. S2CID 33018618.
Baamann, Katharina, ""Data Quality Aspects of Revenue Assurance"", Article
Eckerson, W. (2002) ""Data Warehousing Special Report: Data quality and the bottom line"", Article
Ivanov, K. (1972) ""Quality-control of information: On the concept of accuracy of information in data banks and in management information systems"". The University of Stockholm and The Royal Institute of Technology. Doctoral dissertation.
Hansen, M. (1991) Zero Defect Data, MIT. Masters thesis [1]
Kahn, B., Strong, D., Wang, R. (2002) ""Information Quality Benchmarks: Product and Service Performance,"" Communications of the ACM, April 2002. pp. 184–192. Article
Price, R. and Shanks, G. (2004) A Semiotic Information Quality Framework, Proc. IFIP International Conference on Decision Support Systems (DSS2004): Decision Support in an Uncertain and Complex World, Prato. Article
Redman, T. C. (2008) Data Driven: Profiting From Our Most Important Business Asset
Wand, Y. and Wang, R. (1996) ""Anchoring Data Quality Dimensions in Ontological Foundations,"" Communications of the ACM, November 1996. pp. 86–95. Article
Wang, R., Kon, H. & Madnick, S. (1993), Data Quality Requirements Analysis and Modelling, Ninth International Conference of Data Engineering, Vienna, Austria. Article
Fournel Michel, Accroitre la qualité et la valeur des données de vos clients, éditions Publibook, 2007. ISBN 978-2-7483-3847-8.
Daniel F., Casati F., Palpanas T., Chayka O., Cappiello C. (2008) ""Enabling Better Decisions through Quality-aware Reports"", International Conference on Information Quality (ICIQ), MIT. Article
Jack E. Olson (2003), ""Data Quality: The Accuracy dimension"", Morgan Kaufmann Publishers
Woodall P., Oberhofer M., and Borek A. (2014), ""A Classification of Data Quality Assessment and Improvement Methods"". International Journal of Information Quality 3 (4), 298–321. doi:10.1504/ijiq.2014.068656.
Woodall, P., Borek, A., and Parlikad, A. (2013), ""Data Quality Assessment: The Hybrid Approach."" Information & Management 50 (7), 369–382.

External links
Data quality course, from the Global Health Learning Center",https://en.wikipedia.org/wiki/Data_quality,"['All articles with specifically marked weasel-worded phrases', 'All articles with unsourced statements', 'Articles with specifically marked weasel-worded phrases from June 2012', 'Articles with specifically marked weasel-worded phrases from May 2015', 'Articles with unsourced statements from May 2015', 'CS1 maint: multiple names: authors list', 'Data quality', 'Information science', 'Webarchive template wayback links']",Data Science
61,Data recovery,"In computing, data recovery is a process of salvaging (retrieving) inaccessible, lost, corrupted, damaged or formatted data from secondary storage, removable media or files, when the data stored in them cannot be accessed in a usual way. The data is most often salvaged from storage media such as internal or external hard disk drives (HDDs), solid-state drives (SSDs), USB flash drives, magnetic tapes, CDs, DVDs, RAID subsystems, and other electronic devices. Recovery may be required due to physical damage to the storage devices or logical damage to the file system that prevents it from being mounted by the host operating system (OS).
The most common data recovery scenario involves an operating system failure, malfunction of a storage device, logical failure of storage devices, accidental damage or deletion, etc. (typically, on a single-drive, single-partition, single-OS system), in which case the ultimate goal is simply to copy all important files from the damaged media to another new drive. This can be easily accomplished using a Live CD or DVD by booting directly from a ROM instead of the corrupted drive in question. Many Live CDs or DVDs provide a means to mount the system drive and backup drives or removable media, and to move the files from the system drive to the backup media with a file manager or optical disc authoring software. Such cases can often be mitigated by disk partitioning and consistently storing valuable data files (or copies of them) on a different partition from the replaceable OS system files.
Another scenario involves a drive-level failure, such as a compromised file system or drive partition, or a hard disk drive failure. In any of these cases, the data is not easily read from the media devices. Depending on the situation, solutions involve repairing the logical file system, partition table or master boot record, or updating the firmware or drive recovery techniques ranging from software-based recovery of corrupted data, hardware- and software-based recovery of damaged service areas (also known as the hard disk drive's ""firmware""), to hardware replacement on a physically damaged drive which allows for extraction of data to a new drive. If a drive recovery is necessary, the drive itself has typically failed permanently, and the focus is rather on a one-time recovery, salvaging whatever data can be read.
In a third scenario, files have been accidentally ""deleted"" from a storage medium by the users. Typically, the contents of deleted files are not removed immediately from the physical drive; instead, references to them in the directory structure are removed, and thereafter space the deleted data occupy is made available for later data overwriting. In the mind of end users, deleted files cannot be discoverable through a standard file manager, but the deleted data still technically exists on the physical drive. In the meantime, the original file contents remain, often in a number of disconnected fragments, and may be recoverable if not overwritten by other data files.
The term ""data recovery"" is also used in the context of forensic applications or espionage, where data which have been encrypted or hidden, rather than damaged, are recovered. Sometimes data present in the computer gets encrypted or hidden due to reasons like virus attack which can only be recovered by some computer forensic experts.

Physical damage
A wide variety of failures can cause physical damage to storage media, which may result from human errors and natural disasters. CD-ROMs can have their metallic substrate or dye layer scratched off; hard disks can suffer from a multitude of mechanical failures, such as head crashes, PCB failure and failed motors; tapes can simply break.
Physical damage to a hard drive, even in cases where a head crash has occurred, does not necessarily mean there will be permanent loss of data. The techniques employed by many professional data recovery companies can typically salvage most, if not all, of the data that had been lost when the failure occurred.
Of course there are exceptions to this, such as cases where severe damage to the hard drive platters may have occurred. However, if the hard drive can be repaired and a full image or clone created, then the logical file structure can be rebuilt in most instances.
Most physical damage cannot be repaired by end users. For example, opening a hard disk drive in a normal environment can allow airborne dust to settle on the platter and become caught between the platter and the read/write head. During normal operation, read/write heads float 3 to 6 nanometers above the platter surface, and the average dust particles found in a normal environment are typically around 30,000 nanometers in diameter. When these dust particles get caught between the read/write heads and the platter, they can cause new head crashes that further damage the platter and thus compromise the recovery process. Furthermore, end users generally do not have the hardware or technical expertise required to make these repairs. Consequently, data recovery companies are often employed to salvage important data with the more reputable ones using class 100 dust- and static-free cleanrooms.

Recovery techniques
Recovering data from physically damaged hardware can involve multiple techniques. Some damage can be repaired by replacing parts in the hard disk. This alone may make the disk usable, but there may still be logical damage. A specialized disk-imaging procedure is used to recover every readable bit from the surface. Once this image is acquired and saved on a reliable medium, the image can be safely analyzed for logical damage and will possibly allow much of the original file system to be reconstructed.

Hardware repair
A common misconception is that a damaged printed circuit board (PCB) may be simply replaced during recovery procedures by an identical PCB from a healthy drive. While this may work in rare circumstances on hard disk drives manufactured before 2003, it will not work on newer drives.  Electronics boards of modern drives usually contain drive-specific adaptation data (generally a map of bad sectors and tuning parameters) and other information required to properly access data on the drive. Replacement boards often need this information to effectively recover all of the data. The replacement board may need to be reprogrammed. Some manufacturers (Seagate, for example) store this information on a serial EEPROM chip, which can be removed and transferred to the replacement board.Each hard disk drive has what is called a system area or service area; this portion of the drive, which is not directly accessible to the end user, usually contains drive's firmware and adaptive data that helps the drive operate within normal parameters.  One function of the system area is to log defective sectors within the drive; essentially telling the drive where it can and cannot write data.
The sector lists are also stored on various chips attached to the PCB, and they are unique to each hard disk drive. If the data on the PCB do not match what is stored on the platter, then the drive will not calibrate properly. In most cases the drive heads will click because they are unable to find the data matching what is stored on the PCB.

Logical damage
The term ""logical damage"" refers to situations in which the error is not a problem in the hardware and requires software-level solutions.

Corrupt partitions and file systems, media errors
In some cases, data on a hard disk drive can be unreadable due to damage to the partition table or file system, or to (intermittent) media errors. In the majority of these cases, at least a portion of the original data can be recovered by repairing the damaged partition table or file system using specialized data recovery software such as Testdisk; software like dd rescue can image media despite intermittent errors, and image raw data when there is partition table or file system damage. This type of data recovery can be performed by people without expertise in drive hardware as it requires no special physical equipment or access to platters.
Sometimes data can be recovered using relatively simple methods and tools; more serious cases can require expert intervention, particularly if parts of files are irrecoverable. Data carving is the recovery of parts of damaged files using knowledge of their structure.

Overwritten data
After data has been physically overwritten on a hard disk drive, it is generally assumed that the previous data are no longer possible to recover. In 1996, Peter Gutmann, a computer scientist, presented a paper that suggested overwritten data could be recovered through the use of magnetic force microscopy. In 2001, he presented another paper on a similar topic.  To guard against this type of data recovery, Gutmann and Colin Plumb designed a method of irreversibly scrubbing data, known as the Gutmann method and used by several disk-scrubbing software packages.
Substantial criticism has followed, primarily dealing with the lack of any concrete examples of significant amounts of overwritten data being recovered. Although Gutmann's theory may be correct, there is no practical evidence that overwritten data can be recovered, while research has shown to support that overwritten data cannot be recovered.Solid-state drives (SSD) overwrite data differently from hard disk drives (HDD) which makes at least some of their data easier to recover. Most SSDs use flash memory to store data in pages and blocks, referenced by logical block addresses (LBA) which are managed by the flash translation layer (FTL). When the FTL modifies a sector it writes the new data to another location and updates the map so the new data appear at the target LBA. This leaves the pre-modification data in place, with possibly many generations, and recoverable by data recovery software.

Lost, deleted, and formatted data
Sometimes, data present in the physical drives (Internal/External Hard disk, Pen Drive, etc.) gets lost, deleted and formatted due to circumstances like virus attack, accidental deletion or accidental use of SHIFT+DELETE. In these cases, data recovery software are used to recover/restore the data files.

Logical bad sector
In the list of logical failures of hard disks, logical bad sector is the most common in which data files cannot be retrieved from a particular sector of the media drives. To resolve this, software is used to correct the logical sectors of the media drive.  If this is not enough, the hardware containing the logical bad sectors must be replaced.

Remote data recovery
Recovery experts do not always need to have physical access to the damaged hardware.  When the lost data can be recovered by software techniques, they can often perform the recovery using remote access software over the Internet, LAN or other connection to the physical location of the damaged media.  The process is essentially no different from what the end user could perform by themselves.Remote recovery requires a stable connection with an adequate bandwidth. However, it is not applicable where access to the hardware is required, as in cases of physical damage.

Four phases of data recovery
Usually, there are four phases when it comes to successful data recovery, though that can vary depending on the type of data corruption and recovery required.
Phase 1
Repair the hard disk drive
The hard drive is repaired in order to get it running in some form, or at least in a state suitable for reading the data from it. For example, if heads are bad they need to be changed; if the PCB is faulty then it needs to be fixed or replaced; if the spindle motor is bad the platters and heads should be moved to a new drive.Phase 2
Image the drive to a new drive or a disk image file
When a hard disk drive fails, the importance of getting the data off the drive is the top priority. The longer a faulty drive is used, the more likely further data loss is to occur. Creating an image of the drive will ensure that there is a secondary copy of the data on another device, on which it is safe to perform testing and recovery procedures without harming the source.Phase 3
Logical recovery of files, partition, MBR and filesystem structures
After the drive has been cloned to a new drive, it is suitable to attempt the retrieval of lost data. If the drive has failed logically, there are a number of reasons for that. Using the clone it may be possible to repair the partition table or master boot record (MBR) in order to read the file system's data structure and retrieve stored data.Phase 4
Repair damaged files that were retrieved
Data damage can be caused when, for example, a file is written to a sector on the drive that has been damaged. This is the most common cause in a failing drive, meaning that data needs to be reconstructed to become readable. Corrupted documents can be recovered by several software methods or by manually reconstructing the document using a hex editor.

Restore disk
The Windows operating system can be reinstalled on a computer that is already licensed for it. The reinstallation can be done by downloading the operating system or by using a ""restore disk"" provided by the computer manufacturer. Eric Lundgren was fined and sentenced to U.S. federal prison in April 2018 for producing 28,000 restore disks and intending to distribute them for about 25 cents each as a convenience to computer repair shops.

List of data recovery software
Bootable
Data recovery cannot always be done on a running system. As a result, a boot disk, live CD, live USB, or any other type of live distro contains a minimal operating system.

BartPE: a lightweight variant of Microsoft Windows XP or Windows Server 2003 32-bit operating systems, similar to a Windows Preinstallation Environment, which can be run from a live CD or live USB drive. Discontinued.
Finnix: a Debian-based Live CD with a focus on being small and fast, useful for computer and data rescue
Disk Drill Basic: capable of creating bootable Mac OS X USB drives for data recovery
Knoppix: contains utilities for data recovery under Linux
SpinRite: a FreeDOS-based data recovery tool for hard disks and magnetic storage devices
SystemRescueCD: an Arch Linux based live CD, useful for repairing unbootable computer systems and retrieving data after a system crash
Windows Preinstallation Environment (WinPE): A customizable Windows Boot DVD (made by Microsoft and distributed for free). Can be modified to boot to any of the programs listed.

Consistency checkers
CHKDSK: a consistency checker for DOS and Windows systems
Disk First Aid: a consistency checker for Mac OS 9
Disk Utility: a consistency checker for Mac OS X
fsck: a consistency checker for UNIX
gparted: a GUI for GNU parted, the GNU partition editor, capable of calling fsck

File recovery
CDRoller: recovers data from optical disc
Data Recovery Wizard: Windows file recovery utility by EaseUS
Disk Drill Basic: data recovery application for Mac OS X and Windows
dvdisaster: generates error-correction data for optical discs
GetDataBack: a Windows recovery program
Hetman Partition Recovery: data drive recovery solution
IsoBuster: recovers data from optical discs, USB sticks, flash drives and hard drives
Mac Data Recovery Guru: Mac OS X data recovery program which works on USB sticks, optical media, and hard drives
MiniTool Partition Wizard: for Windows 7 and later; includes data recovery
Norton Utilities: a suite of utilities that has a file recovery component
PhotoRec: advanced multi-platform program with text-based user interface used to recover files
Recover My Files: proprietary software for Windows 2000 and later—FAT, NTFS and HFS
Recovery Toolbox: freeware and shareware tools plus online services for various Windows 2000 and later programs
Recuva: proprietary software for Windows 2000 and later—FAT and NTFS
Stellar Data Recovery for Mac: data recovery utility for Mac OS
Stellar Data Recovery for Windows: data recovery utility for Windows
Stellar Photo Recovery: photo recovery utility for Mac OS and Windows
TestDisk: free, open source, multi-platform. recover files and lost partitions
TuneUp Utilities: a suite of utilities that has a file recovery component for Windows XP and later
Windows File Recovery: a command-line utility from Microsoft to recover deleted files for Windows 10 version 2004 and later

Forensics
Foremost: an open-source command-line file recovery program, originally developed by the U.S. Air Force Office of Special Investigations and NPS Center for Information Systems Security Studies and Research
Forensic Toolkit: by AccessData, used by law enforcement
Open Computer Forensics Architecture: An open-source program for Linux
The Coroner's Toolkit: a suite of utilities for assisting in forensic analysis of a UNIX system after a break-in
The Sleuth Kit: also known as TSK, a suite of forensic analysis tools developed by Brian Carrier for UNIX, Linux and Windows systems. TSK includes the Autopsy forensic browser.

Imaging tools
Clonezilla: a free disk cloning, disk imaging, data recovery, and deployment boot disk
ddrescue: an open-source tool similar to dd but with the ability to skip over and subsequently retry bad blocks on failing storage devices
dd: common byte-to-byte cloning tool found on Unix-like systems
Team Win Recovery Project: a free and open-source recovery system for Android devices

See also
References
Further reading
Tanenbaum, A. & Woodhull, A. S. (1997). Operating Systems: Design And Implementation, 2nd ed. New York: Prentice Hall.
Data recovery at Curlie",https://en.wikipedia.org/wiki/Data_recovery,"['All articles needing additional references', 'All articles with style issues', 'Articles needing additional references from February 2012', 'Articles needing more detailed references', 'Articles with Curlie links', 'Articles with multiple maintenance issues', 'Articles with short description', 'Backup', 'Computer data', 'Data management', 'Data recovery', 'Hard disk software', 'Short description matches Wikidata', 'Transaction processing', 'Use dmy dates from June 2016', 'Webarchive template wayback links', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with style issues from April 2016']",Data Science
62,Data preservation,"Data preservation is the act of conserving and maintaining both the safety and integrity of data. Preservation is done through formal activities that are governed by policies, regulations and strategies directed towards protecting and prolonging the existence and authenticity of data and its metadata. Data can be described as the elements or units in which knowledge and information is created,
 and metadata are the summarizing subsets of the elements of data; or the data about the data. The main goal of data preservation is to protect data from being lost or destroyed and to contribute to the reuse and progression of the data.

History
Most historical data collected over time has been lost or destroyed. War and natural disasters combined with the lack of materials and necessary practices to preserve and protect data has caused this. Usually, only the most important data sets were saved, such as government records and statistics, legal contracts and economic transactions. Scientific research and doctoral theses data have mostly been destroyed from improper storage and lack of data preservation awareness and execution. Over time, data preservation has evolved and has generated importance and awareness. We now have many different ways to preserve data and many different important organizations involved in doing so.
The first digital data preservation storage solutions appeared in the 1950s, which were usually flat or hierarchically structured.  While there were still issues with these solutions, it made storing data much cheaper, and more easily accessible. In the 1970s relational databases as well as spreadsheets appeared. Relational data bases structure data into tables using structured query languages which made them more efficient than the preceding storage solutions, and spreadsheets hold high volumes of numeric data which can be applied to these relational databases to produce derivative data. More recently, non-relational (non-structured query language) databases have appeared as complements to relational databases which hold high volumes of unstructured or semi-structured data.

Importance
The scope of data preservation is vast. Everything from governmental to business records to art essentially can be represented as data, and is amenable to be lost. This then leads to loss of human history, for perpetuity.
Data can be lost on a small or independent scale whether it's personal data loss, or data loss within businesses and organizations, as well as on a larger or national or global scale which can negatively and potentially permanently affect things such as environmental protection, medical research, homeland security, public health and safety, economic development and culture. The mechanisms of data loss are also as many as they are varied, spanning from disaster, wars, data breaches, negligence, all the way through simple forgetting to natural decay.
Ways in which data collections can be used when preserved and stored properly can be seen through the U.S. Geological Survey, which stores data collections on natural hazards, natural resources, and landscapes. The data collected by the Survey is used by federal and state land management agencies towards land use planning and management, and continually needs access to historical reference data.

In contrast
In contrast, data holdings are collections of gathered 1 data that are informally kept and do not prepare for long-term preservation. For example, a collection or back-up of personal files. Data holdings are generally the storage methods used in the past when data has been lost due to environmental and other historical disasters.Furthermore, data retention differs from data preservation in the sense that by definition, to retain an object (data) is to hold or keep possession or use of the object. To preserve an object is to protect, maintain and keep up for future use. Retention policies often circle around when data should be deleted on purpose as well, and held from public access, while preservation centers on permanence and more widely shared access.
Thus, data preservation exceeds the concept of having or possessing data or back up copies of data. Data preservation ensures persistent access to data by planning back-up and recovery strategies, preceding the event of a disaster or technological change.

Methods
Digital
Digital preservation, is similar to data preservation, but is mainly concerned with technological threats, and solely digital data. Essentially digital data is a set of formal activities to enable ongoing or persistent use and access of digital data exceeding the occurrence of technological malfunction or change. Digital preservation is aware of the inevitable change in technology and protocols, and prepares for data will need to be accessible across new types of technologies and platforms while being the integrity of the data and metadata being conserved.Technology, while providing great process in conserving data that may not have been possible in the past, is also changing at such a quick rate that digital data may not be accessible anymore due to the format being incompatible with new software. Without the use of data preservation much of our existing digital data is at risk.The majority of methods used towards data preservation today are digital methods, which are so far the most effective methods that exist.

Archives
Archives are a collection of historical documents and records. Archives contribute and work towards the preservation of data by collecting data that is well organized, while providing the appropriate metadata to confirm it.An example of an important data archive is The LONI Image Data Archive, which is an archive that collects data regarding clinical trials and clinical research studies.

Catalogues, directories and portals
Catalogues, directories and portals are consolidated resources which are kept by individual institutions, and are associated with data archives and holdings. In other words, the data is not presented on the site, but instead might act as metadata and aggregators, and may administer thorough inventories.

Repositories
Repositories are places where data archives and holdings can be accessed and stored. The goal of repositories is to make sure that all requirements and protocols of archives and holdings are being met, and data is being certified to ensure data integrity and user trust.Single-site Repositories  
A repository that holds all data sets on a single site.An example of a major single-site repository the Data Archiving and Networking Services which is a repository which provides ongoing access to digital research resources for the Netherlands.Multi-Site Repositories  
A repository that hosts data set on multiple institutional sites.An example of a well known multi-site repository is OpenAIRE which is a repository that hosts research data and publications collaborating all of the EU countries and more. OpenAIRE promotes open scholarship and seeks to improves discover-ability and re-usability of data.Trusted Digital Repository 
A repository that seeks to provide reliable, trusted access over a long period of time. The repository can be single or multi-sited but must cooperate with the Reference Model for an Open Archival Information System, as well as adhere to a set of rules or attributes that contribute to its trust such as having persistent financial responsibility, organizational buoyancy, administrative responsibility security and safety.An example of a trusted digital repository is The Digital Repository of Ireland (DRI) which is a multi-site repository that hosts Ireland's humanity and social science data sets.

Cyber Infrastructures
Cyber infrastructures which consists of archive collections which are made available through the system of hardware, technologies, software, policies, services and tools. Cyber infrastructures are geared towards the sharing of data supporting peer-to-peer collaborations and a cultural community.An example of a major cyber-infrastructure is The Canadian Geo-spatial Data Infrastructure which provides access to spatial data in Canada.

See also
Data curation
Data recovery
Data refuge
Data retention


== References ==",https://en.wikipedia.org/wiki/Data_preservation,"['Backup', 'CS1: long volume value', 'CS1 errors: missing periodical', 'Computer data', 'Data management', 'Data recovery', 'Transaction processing']",Data Science
63,Data scraping,"Data scraping is a technique in which a computer program extracts data from human-readable output coming from another program.

Description
Normally, data transfer between programs is accomplished using data structures suited for automated processing by computers, not people.  Such interchange formats and protocols are typically rigidly structured, well-documented, easily parsed, and keep ambiguity to a minimum.  Very often, these transmissions are not human-readable at all.
Thus, the key element that distinguishes data scraping from regular parsing is that the output being scraped is intended for display to an end-user, rather than as input to another program, and is therefore usually neither documented nor structured for convenient parsing.  Data scraping often involves ignoring binary data (usually images or multimedia data), display formatting, redundant labels, superfluous commentary, and other information which is either irrelevant or hinders automated processing.
Data scraping is most often done either to interface to a legacy system, which has no other mechanism which is compatible with current hardware, or to interface to a third-party system which does not provide a more convenient API.  In the second case, the operator of the third-party system will often see screen scraping as unwanted, due to reasons such as increased system load, the loss of advertisement revenue, or the loss of control of the information content.
Data scraping is generally considered an ad hoc, inelegant technique, often used only as a ""last resort"" when no other mechanism for data interchange is available.  Aside from the higher programming and processing overhead, output displays intended for human consumption often change structure frequently.  Humans can cope with this easily, but a computer program will fail. Depending on the quality and the extent of error handling logic present in the computer, this failure can result in error messages, corrupted output or even program crashes.

Technical variants
Screen scraping
Although the use of physical ""dumb terminal"" IBM 3270s is slowly diminishing, as more and more mainframe applications acquire Web interfaces, some Web applications merely continue to use the technique of screen scraping to capture old screens and transfer the data to modern front-ends.Screen scraping is normally associated with the programmatic collection of visual data from a source, instead of parsing data as in Web scraping. Originally, screen scraping referred to the practice of reading text data from a computer display terminal's screen. This was generally done by reading the terminal's memory through its auxiliary port, or by connecting the terminal output port of one computer system to an input port on another. The term screen scraping is also commonly used to refer to the bidirectional exchange of data. This could be the simple cases where the controlling program navigates through the user interface, or more complex scenarios where the controlling program is entering data into an interface meant to be used by a human.
As a concrete example of a classic screen scraper, consider a hypothetical legacy system dating from the 1960s—the dawn of computerized data processing. Computer to user interfaces from that era were often simply text-based dumb terminals which were not much more than virtual teleprinters (such systems are still in use today, for various reasons). The desire to interface such a system to more modern systems is common.  A robust solution will often require things no longer available, such as source code, system documentation, APIs, or programmers with experience in a 50-year-old computer system.  In such cases, the only feasible solution may be to write a screen scraper that ""pretends"" to be a user at a terminal.  The screen scraper might connect to the legacy system via Telnet, emulate the keystrokes needed to navigate the old user interface, process the resulting display output, extract the desired data, and pass it on to the modern system. A sophisticated and resilient implementation of this kind, built on a platform providing the governance and control required by a major enterprise—e.g. change control, security, user management, data protection, operational audit, load balancing, and queue management, etc.—could be said to be an example of robotic process automation software, called RPA or RPAAI for self-guided RPA 2.0 based on artificial intelligence.
In the 1980s, financial data providers such as Reuters, Telerate, and Quotron displayed data in 24×80 format intended for a human reader. Users of this data, particularly investment banks, wrote applications to capture and convert this character data as numeric data for inclusion into calculations for trading decisions without re-keying the data. The common term for this practice, especially in the United Kingdom, was page shredding, since the results could be imagined to have passed through a paper shredder. Internally Reuters used the term 'logicized' for this conversion process, running a sophisticated computer system on VAX/VMS called the Logicizer.More modern screen scraping techniques include capturing the bitmap data from the screen and running it through an OCR engine, or for some specialised automated testing systems, matching the screen's bitmap data against expected results. This can be combined in the case of GUI applications, with querying the graphical controls by programmatically obtaining references to their underlying programming objects. A sequence of screens is automatically captured and converted into a database.
Another modern adaptation to these techniques is to use, instead of a sequence of screens as input, a set of images or PDF files, so there are some overlaps with generic ""document scraping"" and report mining techniques.
There are many tools that can be used for screen scraping.

Web scraping
Web pages are built using text-based mark-up languages (HTML and XHTML), and frequently contain a wealth of useful data in text form.  However, most web pages are designed for human end-users and not for ease of automated use. Because of this, tool kits that scrape web content were created. A web scraper is an API or tool to extract data from a web site. Companies like Amazon AWS and Google provide web scraping tools, services, and public data available free of cost to end-users.
Newer forms of web scraping involve listening to data feeds from web servers.  For example, JSON is commonly used as a transport storage mechanism between the client and the webserver.
Recently, companies have developed web scraping systems that rely on using techniques in DOM parsing, computer vision and natural language processing to simulate the human processing that occurs when viewing a webpage to automatically extract useful information.Large websites usually use defensive algorithms to protect their data from web scrapers and to limit the number of requests an IP or IP network may send. This has caused an ongoing battle between website developers and scraping developers.

Report mining
Report mining is the extraction of data from human-readable computer reports. Conventional data extraction requires a connection to a working source system, suitable connectivity standards or an API, and usually complex querying. By using the source system's standard reporting options, and directing the output to a spool file instead of to a printer, static reports can be generated suitable for offline analysis via report mining. This approach can avoid intensive CPU usage during business hours, can minimise end-user licence costs for ERP customers, and can offer very rapid prototyping and development of custom reports. Whereas data scraping and web scraping involve interacting with dynamic output, report mining involves extracting data from files in a human-readable format, such as HTML, PDF, or text. These can be easily generated from almost any system by intercepting the data feed to a printer. This approach can provide a quick and simple route to obtaining data without the need to program an API to the source system.

See also
Comparison of feed aggregators
Data cleansing
Data munging
Importer (computing)
Information extraction
Open data
Mashup (web application hybrid)
Metadata
Web scraping
Search engine scraping

References
Further reading
Hemenway, Kevin and Calishain, Tara. Spidering Hacks. Cambridge, Massachusetts: O'Reilly, 2003. ISBN 0-596-00577-6.",https://en.wikipedia.org/wiki/Data_scraping,"['All articles containing potentially dated statements', 'All articles needing additional references', 'Articles containing potentially dated statements from 2007', 'Articles needing additional references from February 2011', 'Articles with short description', 'Data processing', 'Short description matches Wikidata']",Data Science
64,Data scrubbing,"Data scrubbing is an error correction technique that uses a background task to periodically inspect main memory or storage for errors, then corrects detected errors using redundant data in the form of different checksums or copies of data. Data scrubbing reduces the likelihood that single correctable errors will accumulate, leading to reduced risks of uncorrectable errors.
Data integrity is a high-priority concern in writing, reading, storage, transmission, or processing of the computer data in computer operating systems and in computer storage and data transmission systems.  However, only a few of the currently existing and used file systems provide sufficient protection against data corruption.To address this issue, data scrubbing provides routine checks of all inconsistencies in data and, in general, prevention of hardware or software failure. This ""scrubbing"" feature occurs commonly in memory, disk arrays, file systems, or FPGAs as a mechanism of error detection and correction.

RAID
With data scrubbing, a RAID controller may periodically read all hard disk drives in a RAID array and check for defective blocks before applications might actually access them. This reduces the probability of silent data corruption and data loss due to bit-level errors.In Dell PowerEdge RAID environments, a feature called ""patrol read"" can perform data scrubbing and preventive maintenance.
In OpenBSD, the bioctl(8) utility allows the system administrator to control these patrol reads through the BIOCPATROL ioctl on the /dev/bio pseudo-device; as of 2019, this functionality is supported in some device drivers for LSI Logic and Dell controllers — this includes mfi(4) since OpenBSD 5.8 (2015) and mfii(4) since OpenBSD 6.4 (2018).
In FreeBSD and DragonFly BSD, patrol can be controlled through a RAID controller-specific utility mfiutil(8) since FreeBSD 8.0 (2009) and 7.3 (2010).  The implementation from FreeBSD was used by the OpenBSD developers for adding patrol support to their generic bio(4) framework and the bioctl utility, without a need for a separate controller-specific utility.

In NetBSD in 2008, the bio(4) framework from OpenBSD was extended to feature support for consistency checks, which was implemented for /dev/bio pseudo-device under BIOCSETSTATE ioctl command, with the options being start and stop (BIOC_SSCHECKSTART_VOL and BIOC_SSCHECKSTOP_VOL, respectively); this is supported only by a single driver as of 2019 — arcmsr(4).Linux MD RAID, as a software RAID implementation, makes data consistency checks available and provides automated repairing of detected data inconsistencies. Such procedures are usually performed by setting up a weekly cron job. Maintenance is performed by issuing operations check, repair, or idle to each of the examined MD devices. Statuses of all performed operations, as well as general RAID statuses, are always available.

File systems
Btrfs
As a copy-on-write (CoW) file system for Linux, Btrfs provides fault isolation, corruption detection and correction, and file-system scrubbing. If the file system detects a checksum mismatch while reading a block, it first tries to obtain (or create) a good copy of this block from another device –  if its internal mirroring or RAID techniques are in use.Btrfs can initiate an online check of the entire file system by triggering a file system scrub job that is performed in the background. The scrub job scans the entire file system for integrity and automatically attempts to report and repair any bad blocks it finds along the way.

ZFS
The features of ZFS, which is a combined file system and logical volume manager, include the verification against data corruption modes, continuous integrity checking, and automatic repair.  Sun Microsystems designed ZFS from the ground up with a focus on data integrity and to protect the data on disks against issues such as disk firmware bugs and ghost writes.ZFS provides a repair utility called scrub that examines and repairs silent data corruption caused by data rot and other problems.

Memory
Due to the high integration density of contemporary computer memory chips, the individual memory cell structures became small enough to be vulnerable to cosmic rays and/or alpha particle emission. The errors caused by these phenomena are called soft errors. This can be a problem for DRAM- and SRAM-based memories.
Memory scrubbing does error-detection and correction of bit errors in computer RAM by using ECC memory, other copies of the data, or other error-detecting codes.

FPGA
Scrubbing is a technique used to reprogram an FPGA. It can be used periodically to avoid the accumulation of errors without the need to find one in the configuration bitstream, thus simplifying the design.
Numerous approaches can be taken with respect to scrubbing, from simply reprogramming the FPGA to partial reconfiguration. The simplest method of scrubbing is to completely reprogram the FPGA at some periodic rate (typically 1/10 the calculated upset rate). However, the FPGA is not operational during that reprogram time, on the order of micro to milliseconds. For situations that cannot tolerate that type of interruption, partial reconfiguration is available. This technique allows the FPGA to be reprogrammed while still operational.

See also
Data corruption
Error detection and correction
fsck - a tool for checking the consistency of a file system
CHKDSK - similar to fsck, used in Windows operating systems

References
External links
Soft Errors in Electronic Memory",https://en.wikipedia.org/wiki/Data_scrubbing,"['Articles with short description', 'Error detection and correction', 'Short description is different from Wikidata']",Data Science
65,Data reduction,"Data reduction is the transformation of numerical or alphabetical digital information derived empirically or experimentally into a corrected, ordered, and simplified form. The purpose of data reduction can be two-fold: reduce the number of data records by eliminating invalid data or produce summary data and statistics at different aggregation levels for various applications.When information is derived from instrument readings there may also be a transformation from analog to digital form. When the data are already in digital form the 'reduction' of the data typically involves some editing, scaling, encoding, sorting, collating, and producing tabular summaries. When the observations are discrete but the underlying phenomenon is continuous then smoothing and interpolation are often needed. The data reduction is often undertaken in the presence of reading or measurement errors. Some idea of the nature of these errors is needed before the most likely value may be determined.
An example in astronomy is the data reduction in the Kepler satellite. This satellite records 95-megapixel images once every six seconds, generating dozens of megabytes of data per second, which is orders-of-magnitudes more than the downlink bandwidth of 550 KBps. The on-board data reduction encompasses co-adding the raw frames for thirty minutes, reducing the bandwidth by a factor of 300. Furthermore, interesting targets are pre-selected and only the relevant pixels are processed, which is 6% of the total. This reduced data is then sent to Earth where it is processed further.
Research has also been carried out on the use of data reduction in wearable (wireless) devices for health monitoring and diagnosis applications. For example, in the context of epilepsy diagnosis, data reduction has been used to increase the battery lifetime of a wearable EEG device by selecting and only transmitting, EEG data that is relevant for diagnosis and discarding background activity.

Types of Data Reduction
Dimensionality Reduction
When dimensionality increases, data becomes increasingly sparse while density and distance between points, critical to clustering and outlier analysis, becomes less meaningful. Dimensionality reduction helps reduce noise in the data and allows for easier visualization, such as the example below where 3-dimensional data is transformed into 2 dimensions to show hidden parts. One method of dimensionality reduction is wavelet transform, in which data is transformed to preserver relative distance between objects at different levels of resolution, and is often used for image compression.

Numerosity Reduction
This method of data reduction reduces the data volume by choosing alternate, smaller forms of data representation. Numerosity reduction can be split into 2 groups: parametric and non-parametric methods. Parametric methods (regression, for example) assume the data fits some model, estimate model parameters, store only the parameters, and discard the data. One example of this is in the image below, where the volume of data to be processed is reduced based on more specific criteria. Another example would be a log-linear model, obtaining a value at a point in m-D space as the product on appropriate marginal subspaces. Non-parametric methods do not assume models, some examples being histograms, clustering, sampling, etc.

Best practices
These are common techniques used in data reduction.

Order by some aspect of size.
Table diagonalization, whereby rows and columns of tables are re-arranged to make patterns easier to see (refer to the diagram).
Round drastically to one, or at most two, effective digits (effective digits are ones that vary in that part of the data).
Use averages to provide a visual focus as well as a summary.
Use layout and labeling to guide the eye.
Remove chartjunk, such as pictures and lines.
Give a brief verbal summary.

See also
Data cleansing
Data editing
Data pre-processing
Data wrangling

References
Bibliography
Ehrenberg, Andrew S. C. (1975,1981), Data Reduction, John Wiley, Chichester. Reprinted in the Journal of Empirical Generalisations in Marketing Science, 2000, 5, 1-391
Ehrenberg, Andrew S. C. (1982) A Primer in Data Reduction: An Introductory Statistics Ehrenberg",https://en.wikipedia.org/wiki/Data_reduction,['Exploratory data analysis'],Data Science
66,Data security,"Data security  means protecting digital data, such as those in a database, from destructive forces and from the unwanted actions of unauthorized users, such as a cyberattack or a data breach.

Technologies
Disk encryption
Disk encryption refers to encryption technology that encrypts data on a hard disk drive. Disk encryption typically takes form in either software (see disk encryption software) or hardware (see disk encryption hardware). Disk encryption is often referred to as on-the-fly encryption (OTFE) or transparent encryption.

Software versus hardware-based mechanisms for protecting data
Software-based security solutions encrypt the data to protect it from theft. However, a malicious program or a hacker could corrupt the data in order to make it unrecoverable, making the system unusable. Hardware-based security solutions prevent read and write access to data, which provides very strong protection against tampering and unauthorized access.
Hardware based security or assisted computer security offers an alternative to software-only computer security. Security tokens such as those using PKCS#11 may be more secure due to the physical access required in order to be compromised. Access is enabled only when the token is connected and correct PIN is entered (see two-factor authentication). However, dongles can be used by anyone who can gain physical access to it. Newer technologies in hardware-based security solves this problem offering full proof security for data.Working off hardware-based security: A hardware device allows a user to log in, log out and set different levels through manual actions. The device uses biometric technology to prevent malicious users from logging in, logging out, and changing privilege levels. The current state of a user of the device is read by controllers in peripheral devices such as hard disks. Illegal access by a malicious user or a malicious program is interrupted based on the current state of a user by hard disk and DVD controllers making illegal access to data impossible. Hardware-based access control is more secure than protection provided by the operating systems as operating systems are vulnerable to malicious attacks by viruses and hackers. The data on hard disks can be corrupted after a malicious access is obtained. With hardware-based protection, software cannot manipulate the user privilege levels. It is impossible for a hacker or a malicious program to gain access to secure data protected by hardware or perform unauthorized privileged operations. This assumption is broken only if the hardware itself is malicious or contains a backdoor. The hardware protects the operating system image and file system privileges from being tampered. Therefore, a completely secure system can be created using a combination of hardware-based security and secure system administration policies.

Backups
Backups are used to ensure data which is lost can be recovered from another source. It is considered essential to keep a backup of any data in most industries and the process is recommended for any files of importance to a user.

Data masking
Data masking of structured data is the process of obscuring (masking) specific data within a database table or cell to ensure that data security is maintained and sensitive information is not exposed to unauthorized personnel. This may include masking the data from users (for example so banking customer representatives can only see the last 4 digits of a customers national identity number), developers (who need real production data to test new software releases but should not be able to see sensitive financial data), outsourcing vendors, etc.

Data erasure
Data erasure is a method of software based overwriting that completely wipes all electronic data residing on a hard drive or other digital media to ensure that no sensitive data is lost when an asset is retired or reused.

International laws and standards
International laws
In the UK, the Data Protection Act is used to ensure that personal data is accessible to those whom it concerns, and provides redress to individuals if there are inaccuracies. This is particularly important to ensure individuals are treated fairly, for example for credit checking purposes. The Data Protection Act states that only individuals and companies with legitimate and lawful reasons can process personal information and cannot be shared. Data Privacy Day is an international holiday started by the Council of Europe that occurs every January 28. 
Since the General Data Protection Regulation (GDPR) of the European Union (EU) became law on May 25, 2018, organizations may face significant penalties of up to €20 million or 4% of their annual revenue if they do not comply with the regulation.  It is intended that GDPR will force organizations to understand their data privacy risks and take the appropriate measures to reduce the risk of unauthorized disclosure of consumers’ private information.

International standards
The international standards ISO/IEC 27001:2013 and ISO/IEC 27002:2013 covers data security under the topic of information security, and one of its cardinal principles is that all stored information, i.e. data, should be owned so that it is clear whose responsibility it is to protect and control access to that data. The following are examples of organizations that help strengthen and standardize computing security:
The Trusted Computing Group is an organization that helps standardize computing security technologies.
The Payment Card Industry Data Security Standard (PCI DSS) is a proprietary international information security standard for organizations that handle cardholder information for the major debit, credit, prepaid, e-purse, automated teller machines, and point of sale cards.The General Data Protection Regulation (GDPR) proposed by the European Commission will strengthen and unify data protection for individuals within the European Union (EU), whilst addressing the export of personal data outside the EU.

See also
Copy protection
Cyber Security Regulations
Data-centric security
Data erasure
Data masking
Data recovery
Digital inheritance
Disk encryption
Comparison of disk encryption software
Identity-based security
Information security
IT network assurance
Merritt method
Pre-boot authentication
Privacy engineering
Privacy law
Security breach notification laws
Single sign-on
Smart card
Tokenization
Transparent data encryption
USB flash drive security

Notes and references
External links
Getting Ready for New Data Laws - Local Gov Magazine
EU General Data Protection Regulation (GDPR)
Countering ransomware attacks",https://en.wikipedia.org/wiki/Data_security,"['All articles needing additional references', 'All articles with unsourced statements', 'Articles needing additional references from February 2012', 'Articles with unsourced statements from April 2019', 'CS1 maint: multiple names: authors list', 'Commons category link from Wikidata', 'Data management', 'Data security', 'Webarchive template wayback links']",Data Science
67,Data steward,"A data steward is an oversight or data governance role within an organization, and is responsible for ensuring the quality and fitness for purpose of the organization's data assets, including the metadata for those data assets.  A data steward may share some responsibilities with a data custodian, such as the awareness, accessibility, release, appropriate use, security and management of data.  A data steward would also participate in the development and implementation of data assets.  A data steward may seek to improve the quality and fitness for purpose of other data assets their organization depends upon but is not responsible for.
Data stewards have a specialist role that utilizes an organization's data governance processes, policies, guidelines and responsibilities for administering an organizations' entire data in compliance with policy and/or regulatory obligations. The overall objective of a data steward is the data quality of the data assets, datasets, data records and data elements. This includes documenting metainformation for the data, such as definitions, related rules/governance, physical manifestation, and related data models (most of these properties being specific to an attribute/concept relationship), identifying owners/custodian's various responsibilities, relations insight pertaining to attribute quality, aiding with project requirement data facilitation and documentation of capture rules.
Data stewards begin the stewarding process with the identification of the data assets and elements which they will steward, with the ultimate result being standards, controls and data entry.  The steward works closely with business glossary standards analysts (for standards), with data architect/modelers (for standards), with  DQ analysts (for controls) and with operations team members (good-quality data going in per business rules) while entering data.
Data stewardship roles are common when organizations attempt to exchange data precisely and consistently between computer systems and to reuse data-related resources.  Master data management often makes references to the need for data stewardship for its implementation to succeed. Data stewardship must have precise purpose, fit for purpose or fitness.

Data steward responsibilities
A data steward ensures that each assigned data element:

Has clear and unambiguous data element definition
Does not conflict with other data elements in the metadata registry (removes duplicates, overlap etc.)
Has clear enumerated value definitions if it is of type Code
Is still being used (remove unused data elements)
Is being used consistently in various computer systems
Is being used, fit for purpose = Data Fitness
Has adequate documentation on appropriate usage and notes
Documents the origin and sources of authority on each metadata element
Is protected against unauthorised access or changeResponsibilities of data stewards vary between different organisations and institutions. For example, at Delft University of Technology, data stewards are perceived as the first contact point for any questions related to research data. They also have subject-specific background allowing them to easily connect with researchers and to contextualise data management problems to take into account disciplinary practices.

Types of data stewards
Depending on the set of data stewardship responsibilities assigned to an individual, there are 4 types (or dimensions of responsibility) of data stewards typically found within an organization:

Data object data steward - responsible for managing reference data and attributes of one business data entity
Business data steward - responsible for managing critical data, both reference and transactional, created or used by one business function
Process data steward - responsible for managing data across one business process
System data steward - responsible for managing data for at least one IT system

Benefits of data stewardship
Systematic data stewardship can foster:

Faster analysis
Consistent use of data management resources
Easy mapping of data between computer systems and exchange documents
Lower costs associated with migration to (for example) Service Oriented Architecture (SOA)
Better control of dangers associated with privacy, legal, errors, etc.Assignment of each data element to a person sometimes seems like an unimportant process. But many groups have found that users have greater trust and usage rates in systems where they can contact a person with questions on each data element.

Examples
Delft University of Technology (TU Delft) offers an example of data stewardship implementation at a research institution. In 2017 the Data Stewardship Project was initiated at TU Delft to address research data management needs in a disciplinary manner across the whole campus. Dedicated data stewards with subject-specific background were appointed at every TU Delft faculty to support researchers with data management questions and to act as a linking point with the other institutional support services. The project is coordinated centrally by TU Delft Library, and it has its own website, blog and a YouTube channel.The [1]EPA metadata registry furnishes an example of data stewardship.  Note that each data element therein has a ""POC""  (point of contact).

Data stewardship applications
A new market for data governance applications is emerging, one in which both technical and business staff — stewards — manage policies. These new applications, like previous generations, deliver a strong business glossary capability, but they do not stop there. Vendors are introducing additional features addressing the roles of business in addition to technical stewards' concerns.Information stewardship applications are business solutions used by business users acting in the role of information steward (interpreting and enforcing information governance policy, for example). These developing solutions represent, for the most part, an amalgam of a number of disparate, previously IT-centric tools already on the market, but are organized and presented in such a way that information stewards (a business role) can support the work of information policy enforcement as part of their normal, business-centric, day-to-day work in a range of use cases.
The initial push for the formation of this new category of packaged software came from operational use cases — that is, use of business data in and between transactional and operational business applications. This is where most of the master data management efforts are undertaken in organizations. However, there is also now a faster-growing interest in the new data lake arena for more analytical use cases.Some of the vendors in Metadata Management, like Alation, have started highlighting the importance of Data Stewards to employees interested in using data to make business decisions.

See also
Metadata
Metadata registry
Data curation
Data element
Data element definition
Representation term
ISO/IEC 11179

References
Universal Meta Data Models, by David Marco and Michael Jennings, Wiley, 2004, page 93-94 ISBN 0-471-08177-9
Metadata Solution by Adrinne Tannenbaum, Addison Wesley, 2002, page 412
Building and Managing the Meta Data Repository, by David Marco, Wiley, 2000, pages 61–62
The Data Warehouse Lifecycle Toolkit, by Ralph Kimball et. el., Wiley, 1998, also briefly mentions the role of data steward in the context of data warehouse project management on page 70.
Developing Geospatial Intelligence Stewardship for Multinational Operations, by Jeff Thomas, US Army Command General Staff College, 2010, www.dtic.mil/dtic/tr/fulltext/u2/a524227.pdf.


== Notes ==",https://en.wikipedia.org/wiki/Data_steward,"['All articles to be expanded', 'All articles with specifically marked weasel-worded phrases', 'All articles with unsourced statements', 'Articles to be expanded from July 2010', 'Articles using small message boxes', 'Articles with specifically marked weasel-worded phrases from July 2010', 'Articles with unsourced statements from October 2014', 'Data management', 'Information technology governance', 'Knowledge representation', 'Library occupations', 'Metadata', 'Technical communication', 'Wikipedia articles needing clarification from January 2021']",Data Science
68,Data storage,"Data storage is the recording (storing) of information (data) in a storage medium. Handwriting, phonographic recording, magnetic tape, and optical discs are all examples of storage media, some authors even propose that DNA is a natural data storage mechanism. Recording may be accomplished with virtually any form of energy. Electronic data storage requires electrical power to store and retrieve data. 
Data storage in a digital, machine-readable medium is sometimes called digital data. Computer data storage is one of the core functions of a general-purpose computer. Electronic documents can be stored in much less space than paper documents. Barcodes and magnetic ink character recognition (MICR) are two ways of recording machine-readable data on paper.

Recording media
A recording medium is a physical material that holds information. Newly created information is distributed and can be stored in four storage media–print, film, magnetic, and optical–and seen or heard in four information flows–telephone, radio and TV, and the Internet as well as being observed directly.  Digital information is stored on electronic media in many different recording formats.
With electronic media, the data and the recording media are sometimes referred to as ""software"" despite the more common use of the word to describe computer software. With (traditional art) static media, art materials such as crayons may be considered both equipment and medium as the wax, charcoal or chalk material from the equipment becomes part of the surface of the medium.
Some recording media may be temporary either by design or by nature. Volatile organic compounds may be used to preserve the environment or to purposely make data expire over time.  Data such as smoke signals or skywriting are temporary by nature. Depending on the volatility, a gas (e.g. atmosphere, smoke) or a liquid surface such as a lake would be considered a temporary recording medium if at all.

Global capacity, digitization, and trends
A 2003 UC Berkeley report estimated that about five exabytes of new information were produced in 2002 and that 92% of this data was stored on hard disk drives. This was about twice the data produced in 2000. The amount of data transmitted over telecommunication systems in 2002 was nearly 18 exabytes—three and a half times more than was recorded on non-volatile storage. Telephone calls constituted 98% of the telecommunicated information in 2002. The researchers' highest estimate for the growth rate of newly stored information (uncompressed) was more than 30% per year.
A 2011 Science (journal) article estimated that the year 2002 was the beginning of the digital age for information storage: an age in which more information is stored on digital storage devices than on analog storage devices. In 1986, approximately 1% of the world's capacity to store information was in digital format; this grew to 3% by 1993, to 25% by 2000, and to 97% by 2007. These figures correspond to less than three compressed exabytes in 1986, and 295 compressed exabytes in 2007. The quantity of digital storage doubled roughly every three years.In a more limited study, the International Data Corporation estimated that the total amount of digital data in 2007 was 281 exabytes, and that the total amount of digital data produced exceeded the global storage capacity for the first time.A study published in 2011 estimated that the world's technological capacity to store information in analog and digital devices grew from less than three (optimally compressed) exabytes in 1986, to 295 (optimally compressed) exabytes in 2007, and doubles roughly every three years.

See also
References
Further reading
Bennett, John C. (1997). ""'JISC/NPO Studies on the Preservation of Electronic Materials: A Framework of Data Types and Formats, and Issues Affecting the Long Term Preservation of Digital Material"". British Library Research and Innovation Report 50. 
History of Computer Data Storage
History of Storage from Cave Paintings to Electrons
The Evolution of Data Storage",https://en.wikipedia.org/wiki/Data_storage,"['All articles needing additional references', 'Articles needing additional references from February 2018', 'Articles with short description', 'CS1 errors: missing periodical', 'Data management', 'Data storage', 'Film and video technology', 'Mass media technology', 'Recording', 'Short description is different from Wikidata', 'Sound production technology', 'Webarchive template wayback links', 'Wikipedia articles with GND identifiers']",Data Science
69,Data validation,"In computer science, data validation is the process of ensuring data has undergone data cleansing to ensure they have, that is, that they are both correct and useful. It uses routines, often called ""validation rules"", ""validation constraints"", or ""check routines"", that check for correctness, meaningfulness, and security of data that are input to the system. The rules may be implemented through the automated facilities of a data dictionary, or by the inclusion of explicit application program validation logic of the computer and its application.
This is distinct from, which attempts to prove or disprove the correctness of algorithms for implementing a specification or property.

Overview
Data validation is intended to provide certain well-defined guarantees for fitness and consistency of data in an application or automated system. Data validation rules can be defined and designed using various methodologies, and be deployed in various contexts. Their implementation can use declarative data integrity rules, or procedure-based business rules.Note that the guarantees of data validation do not necessarily include accuracy, and it is possible for data entry errors such as misspellings to be accepted as valid. Other clerical and/or computer controls may be applied to reduce inaccuracy within a system.

Different kinds
In evaluating the basics of data validation, generalizations can be made regarding the different kinds of validation according to their scope, complexity, and purpose.
For example:

Data type validation;
Range and constraint validation;
Code and cross-reference validation;
Structured validation; and
Consistency validation

Data-type check
Data type validation is customarily carried out on one or more simple data fields.
The simplest kind of data type validation verifies that the individual characters provided through user input are consistent with the expected characters of one or more known primitive data types as defined in a programming language or data storage and retrieval mechanism.
For example, an integer field may require input to use only characters 0 through 9.

Simple range and constraint check
Simple range and constraint validation may examine input for consistency with a minimum/maximum range, or consistency with a test for evaluating a sequence of characters, such as one or more tests against regular expressions. For example, a counter value may be required to be a non-negative integer, and a password may be required to meet a minimum length and contain characters from multiple categories.

Code and cross-reference check
Code and cross-reference validation includes operations to verify that data is consistent with one or more possibly-external rules, requirements, or collections relevant to a particular organization, context or set of underlying assumptions. These additional validity constraints may involve cross-referencing supplied data with a known look-up table or directory information service such as LDAP.
For example, a user-provided country code might be required to identify a current geopolitical region.

Structured check
Structured validation allows for the combination of other kinds of validation, along with more complex processing. Such complex processing may include the testing of conditional constraints for an entire complex data object or set of process operations within a system.

Consistency check
Consistency validation ensures that data is logical. For example, the delivery date of an order can be prohibited from preceding its shipment date.

Example
Multiple kinds of data validation are relevant to 10-digit pre-2007 ISBNs (the 2005 edition of ISO 2108 required ISBNs to have 13 digits from 2007 onwards).

Size. A pre-2007 ISBN must consist of 10 digits, with optional hyphens or spaces separating its four parts.
Format checks. Each of the first 9 digits must be 0 through 9, and the 10th must be either 0 through 9 or an X.
Check digit. To detect transcription errors in which digits have been altered or transposed, the last digit of a pre-2007 ISBN must match the result of a mathematical formula incorporating the other 9 digits (ISBN-10 check digits).

Validation Types
Allowed character checks
Checks to ascertain that only expected characters are present in a field. For example a numeric field may only allow the digits 0–9, the decimal point and perhaps a minus sign or commas. A text field such as a personal name might disallow characters used for markup. An e-mail address might require at least one @ sign and various other structural details. Regular expressions can be effective ways to implement such checks.Batch totals
Checks for missing records. Numerical fields may be added together for all records in a batch. The batch total is entered and the computer checks that the total is correct, e.g., add the 'Total Cost' field of a number of transactions together.Cardinality check
Checks that record has a valid number of related records. For example, if a contact record is classified as ""customer"" then it must have at least one associated order (cardinality > 0). This type of rule can be complicated by additional conditions. For example, if a contact record in a payroll database is classified as ""former employee"" then it must not have any associated salary payments after the separation date (cardinality = 0).Check digits
Used for numerical data. To support error detection, an extra digit is added to a number which is calculated from the other digits.Consistency checks
Checks fields to ensure data in these fields correspond, e.g., if expiration date is in the past then status is not ""active"".Cross-system consistency checks
Compares data in different systems to ensure it is consistent. Systems may represent the same data differently, in which case comparison requires transformation (e.g., one system may store customer name in a single Name field as 'Doe, John Q', while another uses First_Name 'John' and Last_Name 'Doe' and Middle_Name 'Quality').Data type checks
Checks input conformance with typed data. For example, an input box accepting numeric data may reject the letter 'O'.File existence check
Checks that a file with a specified name exists. This check is essential for programs that use file handling.Format check
Checks that the data is in a specified format (template), e.g., dates have to be in the format YYYY-MM-DD. Regular expressions may be used for this kind of validation.Presence check
Checks that data is present, e.g., customers may be required to have an email address.Range check
Checks that the data is within a specified range of values, e.g., a probability must be between 0 and 1.Referential integrity
Values in two relational database tables can be linked through foreign key and primary key. If values in the foreign key field are not constrained by internal mechanisms, then they should be validated to ensure that the referencing table always refers to a row in the referenced table.Spelling and grammar check
Looks for spelling and grammatical errors.Uniqueness check
Checks that each value is unique. This can be applied to several fields (i.e. Address, First Name, Last Name).Table look up check
A table look up check compares data to a collection of allowed values.

Post-validation actions
Enforcement Action
Enforcement action typically rejects the data entry request and requires the input actor to make a change that brings the data into compliance. This is most suitable for interactive use, where a real person is sitting on the computer and making entry. It also works well for batch upload, where a file input may be rejected and a set of messages sent back to the input source for why the data is rejected.
Another form of enforcement action involves automatically changing the data and saving a conformant version instead of the original version. This is most suitable for cosmetic change. For example, converting an [all-caps] entry to a [Pascal case] entry does not need user input. An inappropriate use of automatic enforcement would be in situations where the enforcement leads to loss of business information. For example, saving a truncated comment if the length is longer than expected. This is not typically a good thing since it may result in loss of significant data.Advisory Action
Advisory actions typically allow data to be entered unchanged but sends a message to the source actor indicating those validation issues that were encountered. This is most suitable for non-interactive system, for systems where the change is not business critical, for cleansing steps of existing data and for verification steps of an entry process.Verification Action
Verification actions are special cases of advisory actions. In this case, the source actor is asked to verify that this data is what they would really want to enter, in the light of a suggestion to the contrary. Here, the check step suggests an alternative (e.g., a check of a mailing address returns a different way of formatting that address or suggests a different address altogether). You would want in this case, to give the user the option of accepting the recommendation or keeping their version. This is not a strict validation process, by design and is useful for capturing addresses to a new location or to a location that is not yet supported by the validation databases.Log of validation
Even in cases where data validation did not find any issues, providing a log of validations that were conducted and their results is important. This is helpful to identify any missing data validation checks in light of data issues and in improving the validation.

Validation and security
Failures or omissions in data validation can lead to data corruption or a security vulnerability. Data validation checks that data are fit for purpose, valid, sensible, reasonable and secure before they are processed. Some risk management practices to protect companies from fraud, reduce operational cost and maintain compliance with varying regulatory policies are Know your customer (KYC) and Customer Identification Program (CIP). Know your customer verification is the verification of the customers which is done either before or at the time they started their relationship with a business.

See also
Data verification
Verification and validation

References
External links
Data Validation, OWASP
Input Validation, OWASP Cheat Sheet Series, github.com",https://en.wikipedia.org/wiki/Data_validation,"['All articles needing additional references', 'Articles needing additional references from July 2012', 'Articles needing additional references from November 2016', 'Data quality', 'Data security', 'Webarchive template wayback links']",Data Science
70,Data warehouse,"In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis, and is considered a core component of business intelligence. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place that are used for creating analytical reports for workers throughout the enterprise.The data stored in the warehouse is uploaded from the operational systems (such as marketing or sales). The data may pass through an operational data store and may require data cleansing for additional operations to ensure data quality before it is used in the DW for reporting.
Extract, transform, load (ETL) and extract, load, transform (ELT) are the two main approaches used to build a data warehouse system.

ETL-based data warehousing
The typical extract, transform, load (ETL)-based data warehouse uses staging, data integration, and access layers to house its key functions. The staging layer or staging database stores raw data extracted from each of the disparate source data systems. The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an operational data store (ODS) database. The integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups, often called dimensions, and into facts and aggregate facts. The combination of facts and dimensions is sometimes called a star schema. The access layer helps users retrieve data.The main source of the data is cleansed, transformed, catalogued, and made available for use by managers and other business professionals for data mining, online analytical processing, market research and decision support. However, the means to retrieve and analyze data, to extract, transform, and load data, and to manage the data dictionary are also considered essential components of a data warehousing system. Many references to data warehousing use this broader context. Thus, an expanded definition for data warehousing includes business intelligence tools, tools to extract, transform, and load data into the repository, and tools to manage and retrieve metadata.
IBM InfoSphere DataStage, Ab Initio Software, Informatica – PowerCenter are some of the tools which are widely used to implement ETL-based data warehouse.

ELT-based data warehousing
ELT-based data warehousing gets rid of a separate ETL tool for data transformation. Instead, it maintains a staging area inside the data warehouse itself. In this approach, data gets extracted from heterogeneous source systems and are then directly loaded into the data warehouse, before any transformation occurs. All necessary transformations are then handled inside the data warehouse itself. Finally, the manipulated data gets loaded into target tables in the same data warehouse.

Benefits
A data warehouse maintains a copy of information from the source transaction systems. This architectural complexity provides the opportunity to:

Integrate data from multiple sources into a single database and data model. More congregation of data to single database so a single query engine can be used to present data in an ODS.
Mitigate the problem of database isolation level lock contention in transaction processing systems caused by attempts to run large, long-running analysis queries in transaction processing databases.
Maintain data history, even if the source transaction systems do not.
Integrate data from multiple source systems, enabling a central view across the enterprise. This benefit is always valuable, but particularly so when the organization has grown by merger.
Improve data quality, by providing consistent codes and descriptions, flagging or even fixing bad data.
Present the organization's information consistently.
Provide a single common data model for all data of interest regardless of the data's source.
Restructure the data so that it makes sense to the business users.
Restructure the data so that it delivers excellent query performance, even for complex analytic queries, without impacting the operational systems.
Add value to operational business applications, notably customer relationship management (CRM) systems.
Make decision–support queries easier to write.
Organize and disambiguate repetitive data

Generic
The environment for data warehouses and marts includes the following:

Source systems that provide data to the warehouse or mart;
Data integration technology and processes that are needed to prepare the data for use;
Different architectures for storing data in an organization's data warehouse or data marts;
Different tools and applications for a variety of users;
Metadata, data quality, and governance processes must be in place to ensure that the warehouse or mart meets its purposes.In regards to source systems listed above, R. Kelly Rainer states, ""A common source for the data in data warehouses is the company's operational databases, which can be relational databases"".Regarding data integration, Rainer states, ""It is necessary to extract data from source systems, transform them, and load them into a data mart or warehouse"".Rainer discusses storing data in an organization's data warehouse or data marts.Metadata is data about data. ""IT personnel need information about data sources; database, table, and column names; refresh schedules; and data usage measures"".Today, the most successful companies are those that can respond quickly and flexibly to market changes and opportunities. A key to this response is the effective and efficient use of data and information by analysts and managers. A ""data warehouse"" is a repository of historical data that is organized by the subject to support decision-makers in the organization. Once data is stored in a data mart or warehouse, it can be accessed.

Related systems (data mart, OLAPS, OLTP, predictive analytics)
A data mart is a simple form of a data warehouse that is focused on a single subject (or functional area), hence they draw data from a limited number of sources such as sales, finance or marketing. Data marts are often built and controlled by a single department within an organization. The sources could be internal operational systems, a central data warehouse, or external data. Denormalization is the norm for data modeling techniques in this system. Given that data marts generally cover only a subset of the data contained in a data warehouse, they are often easier and faster to implement.

Types of data marts include dependent, independent, and hybrid data marts.Online analytical processing (OLAP) is characterized by a relatively low volume of transactions. Queries are often very complex and involve aggregations. For OLAP systems, response time is an effective measure. OLAP applications are widely used by Data Mining techniques. OLAP databases store aggregated, historical data in multi-dimensional schemas (usually star schemas). OLAP systems typically have a data latency of a few hours, as opposed to data marts, where latency is expected to be closer to one day. The OLAP approach is used to analyze multidimensional data from multiple sources and perspectives. The three basic operations in OLAP are Roll-up (Consolidation), Drill-down, and Slicing & Dicing.
Online transaction processing (OLTP) is characterized by a large number of short on-line transactions (INSERT, UPDATE, DELETE). OLTP systems emphasize very fast query processing and maintaining data integrity in multi-access environments. For OLTP systems, effectiveness is measured by the number of transactions per second. OLTP databases contain detailed and current data. The schema used to store transactional databases is the entity model (usually 3NF). Normalization is the norm for data modeling techniques in this system.
Predictive analytics is about finding and quantifying hidden patterns in the data using complex mathematical models that can be used to predict future outcomes. Predictive analysis is different from OLAP in that OLAP focuses on historical data analysis and is reactive in nature, while predictive analysis focuses on the future. These systems are also used for customer relationship management (CRM).

History
The concept of data warehousing dates back to the late 1980s when IBM researchers Barry Devlin and Paul Murphy developed the ""business data warehouse"". In essence, the data warehousing concept was intended to provide an architectural model for the flow of data from operational systems to decision support environments. The concept attempted to address the various problems associated with this flow, mainly the high costs associated with it. In the absence of a data warehousing architecture, an enormous amount of redundancy was required to support multiple decision support environments. In larger corporations, it was typical for multiple decision support environments to operate independently. Though each environment served different users, they often required much of the same stored data. The process of gathering, cleaning and integrating data from various sources, usually from long-term existing operational systems (usually referred to as legacy systems), was typically in part replicated for each environment. Moreover, the operational systems were frequently reexamined as new decision support requirements emerged. Often new requirements necessitated gathering, cleaning and integrating new data from ""data marts"" that was tailored for ready access by users.
Key developments in early years of data warehousing:

1960s – General Mills and Dartmouth College, in a joint research project, develop the terms dimensions and facts.
1970s – ACNielsen and IRI provide dimensional data marts for retail sales.
1970s – Bill Inmon begins to define and discuss the term Data Warehouse.
1975 – Sperry Univac introduces MAPPER (MAintain, Prepare, and Produce Executive Reports), a database management and reporting system that includes the world's first 4GL. It is the first platform designed for building Information Centers (a forerunner of contemporary data warehouse technology).
1983 – Teradata introduces the DBC/1012 database computer specifically designed for decision support.
1984 – Metaphor Computer Systems, founded by David Liddle and Don Massaro, releases a hardware/software package and GUI for business users to create a database management and analytic system.
1985 - Sperry Corporation publishes an article (Martyn Jones and Philip Newman) on information centers, where they introduce the term MAPPER data warehouse in the context of information centers.
1988 – Barry Devlin and Paul Murphy publish the article ""An architecture for a business and information system"" where they introduce the term ""business data warehouse"".
1990 – Red Brick Systems, founded by Ralph Kimball, introduces Red Brick Warehouse, a database management system specifically for data warehousing.
1991 – Prism Solutions, founded by Bill Inmon, introduces Prism Warehouse Manager, software for developing a data warehouse.
1992 – Bill Inmon publishes the book Building the Data Warehouse.
1995 – The Data Warehousing Institute, a for-profit organization that promotes data warehousing, is founded.
1996 – Ralph Kimball publishes the book The Data Warehouse Toolkit.
2000 – Dan Linstedt releases in the public domain the Data vault modeling, conceived in 1990 as an alternative to Inmon and Kimball to provide long-term historical storage of data coming in from multiple operational systems, with emphasis on tracing, auditing and resilience to change of the source data model.
2008 – Bill Inmon, along with Derek Strauss and Genia Neushloss, publishes ""DW 2.0: The Architecture for the Next Generation of Data Warehousing"", explaining his top-down approach to data warehousing and coining the term, data-warehousing 2.0.
2012 – Bill Inmon develops and makes public technology known as ""textual disambiguation"". Textual disambiguation applies context to raw text and reformats the raw text and context into a standard data base format. Once raw text is passed through textual disambiguation, it can easily and efficiently be accessed and analyzed by standard business intelligence technology. Textual disambiguation is accomplished through the execution of textual ETL. Textual disambiguation is useful wherever raw text is found, such as in documents, Hadoop, email, and so forth.

Information storage
Facts
A fact is a value, or measurement, which represents a fact about the managed entity or system.
Facts, as reported by the reporting entity, are said to be at raw level; e.g., in a mobile telephone system, if a BTS (base transceiver station) receives 1,000 requests for traffic channel allocation, allocates for 820, and rejects the remaining, it would report three facts or measurements to a management system:

tch_req_total = 1000
tch_req_success = 820
tch_req_fail = 180Facts at the raw level are further aggregated to higher levels in various dimensions to extract more service or business-relevant information from it. These are called aggregates or summaries or aggregated facts.
For instance, if there are three BTS in a city, then the facts above can be aggregated from the BTS to the city level in the network dimension. For example:

tch_req_success_city = tch_req_success_bts1 + tch_req_success_bts2 + tch_req_success_bts3
avg_tch_req_success_city = (tch_req_success_bts1 + tch_req_success_bts2 + tch_req_success_bts3) / 3

Dimensional versus normalized approach for storage of data
There are three or more leading approaches to storing data in a data warehouse – the most important approaches are the dimensional approach and the normalized approach.
The dimensional approach refers to Ralph Kimball's approach in which it is stated that the data warehouse should be modeled using a Dimensional Model/star schema. The normalized approach, also called the 3NF model (Third Normal Form), refers to Bill Inmon's approach in which it is stated that the data warehouse should be modeled using an E-R model/normalized model.

Dimensional approach
In a dimensional approach, transaction data are partitioned into ""facts"", which are generally numeric transaction data, and ""dimensions"", which are the reference information that gives context to the facts. For example, a sales transaction can be broken up into facts such as the number of products ordered and the total price paid for the products, and into dimensions such as order date, customer name, product number, order ship-to and bill-to locations, and salesperson responsible for receiving the order.
A key advantage of a dimensional approach is that the data warehouse is easier for the user to understand and to use. Also, the retrieval of data from the data warehouse tends to operate very quickly. Dimensional structures are easy to understand for business users, because the structure is divided into measurements/facts and context/dimensions. Facts are related to the organization's business processes and operational system whereas the dimensions surrounding them contain context about the measurement (Kimball, Ralph 2008). Another advantage offered by dimensional model is that it does not involve a relational database every time. Thus, this type of modeling technique is very useful for end-user queries in data warehouse.
The model of facts and dimensions can also be understood as a data cube. Where the dimensions are the categorical coordinates in a multi-dimensional cube, the fact is a value corresponding to the coordinates.
The main disadvantages of the dimensional approach are the following:

To maintain the integrity of facts and dimensions, loading the data warehouse with data from different operational systems is complicated.
It is difficult to modify the data warehouse structure if the organization adopting the dimensional approach changes the way in which it does business.

Normalized approach
In the normalized approach, the data in the data warehouse are stored following, to a degree, database normalization rules. Tables are grouped together by subject areas that reflect general data categories (e.g., data on customers, products, finance, etc.). The normalized structure divides data into entities, which creates several tables in a relational database. When applied in large enterprises the result is dozens of tables that are linked together by a web of joins. Furthermore, each of the created entities is converted into separate physical tables when the database is implemented (Kimball, Ralph 2008).
The main advantage of this approach is that it is straightforward to add information into the database. Some disadvantages of this approach are that, because of the number of tables involved, it can be difficult for users to join data from different sources into meaningful information and to access the information without a precise understanding of the sources of data and of the data structure of the data warehouse.
Both normalized and dimensional models can be represented in entity-relationship diagrams as both contain joined relational tables. The difference between the two models is the degree of normalization (also known as Normal Forms). These approaches are not mutually exclusive, and there are other approaches. Dimensional approaches can involve normalizing data to a degree (Kimball, Ralph 2008).
In Information-Driven Business, Robert Hillard proposes an approach to comparing the two approaches based on the information needs of the business problem. The technique shows that normalized models hold far more information than their dimensional equivalents (even when the same fields are used in both models) but this extra information comes at the cost of usability. The technique measures information quantity in terms of information entropy and usability in terms of the Small Worlds data transformation measure.

Design methods
Bottom-up design
In the bottom-up approach, data marts are first created to provide reporting and analytical capabilities for specific business processes. These data marts can then be integrated to create a comprehensive data warehouse. The data warehouse bus architecture is primarily an implementation of ""the bus"", a collection of conformed dimensions and conformed facts, which are dimensions that are shared (in a specific way) between facts in two or more data marts.

Top-down design
The top-down approach is designed using a normalized enterprise data model. ""Atomic"" data, that is, data at the greatest level of detail, are stored in the data warehouse. Dimensional data marts containing data needed for specific business processes or specific departments are created from the data warehouse.

Hybrid design
Data warehouses (DW) often resemble the hub and spokes architecture. Legacy systems feeding the warehouse often include customer relationship management and enterprise resource planning, generating large amounts of data. To consolidate these various data models, and facilitate the extract transform load process, data warehouses often make use of an operational data store, the information from which is parsed into the actual DW. To reduce data redundancy, larger systems often store the data in a normalized way. Data marts for specific reports can then be built on top of the data warehouse.
A hybrid DW database is kept on third normal form to eliminate data redundancy. A normal relational database, however, is not efficient for business intelligence reports where dimensional modelling is prevalent. Small data marts can shop for data from the consolidated warehouse and use the filtered, specific data for the fact tables and dimensions required. The DW provides a single source of information from which the data marts can read, providing a wide range of business information. The hybrid architecture allows a DW to be replaced with a master data management repository where operational (not static) information could reside.
The data vault modeling components follow hub and spokes architecture. This modeling style is a hybrid design, consisting of the best practices from both third normal form and star schema. The data vault model is not a true third normal form, and breaks some of its rules, but it is a top-down architecture with a bottom up design. The data vault model is geared to be strictly a data warehouse. It is not geared to be end-user accessible, which, when built, still requires the use of a data mart or star schema-based release area for business purposes.

Data warehouse characteristics
There are basic features that define the data in the data warehouse that include subject orientation, data integration, time-variant, nonvolatile data, and data granularity.

Subject-oriented
Unlike the operational systems, the data in the data warehouse revolves around subjects of the enterprise. Subject orientation is not database normalization. Subject orientation can be really useful for decision making.
Gathering the required objects is called subject-oriented.

Integrated
The data found within the data warehouse is integrated. Since it comes from several operational systems, all inconsistencies must be removed. Consistencies include naming conventions, measurement of variables, encoding structures, physical attributes of data, and so forth.

Time-variant
While operational systems reflect current values as they support day-to-day operations, data warehouse data represents a long time horizon (up to 10 years) which means it stores mostly historical data. It is mainly meant for data mining and forecasting. (E.g. if a user is searching for a buying pattern of a specific customer, the user needs to look at data on the current and past purchases.)

Nonvolatile
The data in the data warehouse is read-only, which means it cannot be updated, created, or deleted (unless there is a regulatory or statutory obligation to do so).

Data warehouse options
Aggregation
In the data warehouse process, data can be aggregated in data marts at different levels of abstraction. The user may start looking at the total sale units of a product in an entire region. Then the user looks at the states in that region. Finally, they may examine the individual stores in a certain state. Therefore, typically, the analysis starts at a higher level and drills down to lower levels of details.

Data warehouse architecture
The different methods used to construct/organize a data warehouse specified by an organization are numerous. The hardware utilized, software created and data resources specifically required for the correct functionality of a data warehouse are the main components of the data warehouse architecture. All data warehouses have multiple phases in which the requirements of the organization are modified and fine-tuned.

Versus operational system
Operational systems are optimized for the preservation of data integrity and speed of recording of business transactions through use of database normalization and an entity-relationship model. Operational system designers generally follow Codd's 12 rules of database normalization to ensure data integrity. Fully normalized database designs (that is, those satisfying all Codd rules) often result in information from a business transaction being stored in dozens to hundreds of tables. Relational databases are efficient at managing the relationships between these tables. The databases have very fast insert/update performance because only a small amount of data in those tables is affected each time a transaction is processed. To improve performance, older data are usually periodically purged from operational systems.
Data warehouses are optimized for analytic access patterns. Analytic access patterns generally involve selecting specific fields and rarely if ever select *, which selects all fields/columns, as is more common in operational databases. Because of these differences in access patterns, operational databases (loosely, OLTP) benefit from the use of a row-oriented DBMS whereas analytics databases (loosely, OLAP) benefit from the use of a column-oriented DBMS. Unlike operational systems which maintain a snapshot of the business, data warehouses generally maintain an infinite history which is implemented through ETL processes that periodically migrate data from the operational systems over to the data warehouse.

Evolution in organization use
These terms refer to the level of sophistication of a data warehouse:

Offline operational data warehouse
Data warehouses in this stage of evolution are updated on a regular time cycle (usually daily, weekly or monthly) from the operational systems and the data is stored in an integrated reporting-oriented database.
Offline data warehouse
Data warehouses at this stage are updated from data in the operational systems on a regular basis and the data warehouse data are stored in a data structure designed to facilitate reporting.
On-time data warehouse
Online Integrated Data Warehousing represent the real-time Data warehouses stage data in the warehouse is updated for every transaction performed on the source data
Integrated data warehouse
These data warehouses assemble data from different areas of business, so users can look up the information they need across other systems.

References
Further reading
Davenport, Thomas H. and Harris, Jeanne G. Competing on Analytics: The New Science of Winning (2007) Harvard Business School Press. ISBN 978-1-4221-0332-6
Ganczarski, Joe. Data Warehouse Implementations: Critical Implementation Factors Study (2009) VDM Verlag ISBN 3-639-18589-7 ISBN 978-3-639-18589-8
Kimball, Ralph and Ross, Margy. The Data Warehouse Toolkit Third Edition (2013) Wiley, ISBN 978-1-118-53080-1
Linstedt, Graziano, Hultgren. The Business of Data Vault Modeling Second Edition (2010) Dan linstedt, ISBN 978-1-4357-1914-9
William Inmon. Building the Data Warehouse (2005) John Wiley and Sons, ISBN 978-81-265-0645-3",https://en.wikipedia.org/wiki/Data_warehouse,"['All articles needing additional references', 'All articles with unsourced statements', 'Articles needing additional references from July 2015', 'Articles with unsourced statements from June 2014', 'Business intelligence', 'Data management', 'Data warehousing', 'Information technology management', 'Wikipedia articles needing clarification from March 2017', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NDL identifiers']",Data Science
71,Data transformation,"In computing, Data transformation is the process of converting data from one format or structure into another format or structure. It is a fundamental aspect of most data integration and data management tasks such as data wrangling, data warehousing, data integration and application integration.
Data transformation can be simple or complex based on the required changes to the data between the source (initial) data and the target (final) data. Data transformation is typically performed via a mixture of manual and automated steps. Tools and technologies used for data transformation can vary widely based on the format, structure, complexity, and volume of the data being transformed.
A master data recast is another form of data transformation where the entire database of data values is transformed or recast without extracting the data from the database.  All data in a well designed database is directly or indirectly related to a limited set of master database tables by a network of foreign key constraints.  Each foreign key constraint is dependent upon a unique database index from the parent database table.  Therefore, when the proper master database table is recast with a different unique index, the directly and indirectly related data are also recast or restated.  The directly and indirectly related data may also still be viewed in the original form since the original unique index still exists with the master data.  Also, the database recast must be done in such a way as to not impact the applications architecture software.
When the data mapping is indirect via a mediating data model, the process is also called data mediation.

Data Transformation Process
Data transformation can be divided into the following steps, each applicable as needed based on the complexity of the transformation required.
Data discovery
Data mapping
Code generation
Code execution
Data reviewThese steps are often the focus of developers or technical data analysts who may use multiple specialized tools to perform their tasks.
The steps can be described as follows:
Data discovery is the first step in the data transformation process. Typically the data is profiled using profiling tools or sometimes using manually written profiling scripts to better understand the structure and characteristics of the data and decide how it needs to be transformed.
Data mapping is the process of defining how individual fields are mapped, modified, joined, filtered, aggregated etc. to produce the final desired output. Developers or technical data analysts traditionally perform data mapping since they work in the specific technologies to define the transformation rules (e.g. visual ETL tools, transformation languages).
Code generation is the process of generating executable code (e.g. SQL, Python, R, or other executable instructions) that will transform the data based on the desired and defined data mapping rules. Typically, the data transformation technologies generate this code based on the definitions or metadata defined by the developers.
Code execution is the step whereby the generated code is executed against the data to create the desired output. The executed code may be tightly integrated into the transformation tool, or it may require separate steps by the developer to manually execute the generated code.
Data review is the final step in the process, which focuses on ensuring the output data meets the transformation requirements. It is typically the business user or final end-user of the data that performs this step. Any anomalies or errors in the data that are found and communicated back to the developer or data analyst as new requirements to be implemented in the transformation process.

Types of Data Transformation
Batch Data Transformation
Traditionally, data transformation has been a bulk or batch process, whereby developers write code or implement transformation rules in a data integration tool, and then execute that code or those rules on large volumes of data. This process can follow the linear set of steps as described in the data transformation process above.
Batch data transformation is the cornerstone of virtually all data integration technologies such as data warehousing, data migration and application integration.When data must be transformed and delivered with low latency, the term “microbatch” is often used.  This refers to small batches of data (e.g. a small number of rows or small set of data objects) that can be processed very quickly and delivered to the target system when needed.

Benefits of Batch Data Transformation
Traditional data transformation processes have served companies well for decades. The various tools and technologies (data profiling, data visualization, data cleansing, data integration etc.) have matured and most (if not all) enterprises transform enormous volumes of data that feed internal and external applications, data warehouses and other data stores.

Limitations of Traditional Data Transformation
This traditional process also has limitations that hamper its overall efficiency and effectiveness.The people who need to use the data (e.g. business users) do not play a direct role in the data transformation process. Typically, users hand over the data transformation task to developers who have the necessary coding or technical skills to define the transformations and execute them on the data.This process leaves the bulk of the work of defining the required transformations to the developer.  The developer interprets the business user requirements and implements the related code/logic. This has the potential of introducing errors into the process (through misinterpreted requirements), and also increases the time to arrive at a solution.This problem has given rise to the need for agility and self-service in data integration (i.e. empowering the user of the data and enabling them to transform the data themselves interactively).There are companies that provide self-service data transformation tools. They are aiming to efficiently analyze, map and transform large volumes of data without the technical and process complexity that currently exists. While these companies use traditional batch transformation, their tools enable more interactivity for users through visual platforms and easily repeated scripts.

Interactive Data Transformation
Interactive data transformation (IDT) is an emerging capability that allows business analysts and business users the ability to directly interact with large datasets through a visual interface, understand the characteristics of the data (via automated data profiling or visualization), and change or correct the data through simple interactions such as clicking or selecting certain elements of the data.Although IDT follows the same data integration process steps as batch data integration, the key difference is that the steps are not necessarily followed in a linear fashion and typically don't require significant technical skills for completion.A number of companies, primarily start-ups such as Trifacta, Alteryx and Paxata provide interactive data transformation tools. They are aiming to efficiently analyze, map and transform large volumes of data without the technical and process complexity that currently exists.
IDT solutions provide an integrated visual interface that combines the previously disparate steps of data analysis, data mapping and code generation/execution and data inspection. IDT interfaces incorporate visualization to show the user patterns and anomalies in the data so they can identify erroneous or outlying values.Once they've finished transforming the data, the system can generate executable code/logic, which can be executed or applied to subsequent similar data sets.
By removing the developer from the process, IDT systems shorten the time needed to prepare and transform the data, eliminate costly errors in interpretation of user requirements and empower business users and analysts to control their data and interact with it as needed.

Transformational languages
There are numerous languages available for performing data transformation. Many transformation languages require a grammar to be provided. In many cases, the grammar is structured using something closely resembling Backus–Naur Form (BNF). There are numerous languages available for such purposes varying in their accessibility (cost) and general usefulness. Examples of such languages include:

AWK - one of the oldest and popular textual data transformation language;
Perl - a high-level language with both procedural and object-oriented syntax capable of powerful operations on binary or text data.
Template languages - specialized to transform data into documents (see also template processor);
TXL - prototyping language-based descriptions, used for source code or data transformation.
XSLT - the standard XML data transformation language (suitable by XQuery in many applications);Additionally, companies such as Trifacta and Paxata have developed domain-specific transformational languages (DSL) for servicing and transforming datasets. The development of domain-specific languages has been linked to increased productivity and accessibility for non-technical users. Trifacta's “Wrangle” is an example of such a domain specific language.Another advantage of the recent DSL trend is that a DSL can abstract the underlying execution of the logic defined in the DSL, but it can also utilize that same logic in various processing engines, such as Spark, MapReduce, and Dataflow. With a DSL, the transformation language is not tied to the engine.Although transformational languages are typically best suited for transformation, something as simple as regular expressions can be used to achieve useful transformation. A text editor like vim, emacs or TextPad supports the use of regular expressions with arguments. This would allow all instances of a particular pattern to be replaced with another pattern using parts of the original pattern. For example:

foo (""some string"", 42, gCommon);
bar (someObj, anotherObj);

foo (""another string"", 24, gCommon);
bar (myObj, myOtherObj);

could both be transformed into a more compact form like:

foobar(""some string"", 42, someObj, anotherObj);
foobar(""another string"", 24, myObj, myOtherObj);

In other words, all instances of a function invocation of foo with three arguments, followed by a function invocation with two arguments would be replaced with a single function invocation using some or all of the original set of arguments.
Another advantage to using regular expressions is that they will not fail the null transform test. That is, using your transformational language of choice, run a sample program through a transformation that doesn't perform any transformations. Many transformational languages will fail this test.

See also
File Formats, Transformation, and Migration (related wikiversity article)
Data Cleansing
Data Mapping
Data Integration
Data Preparation
Data Wrangling
Extract Transform Load
Information Integration

References
External links
Extraction and Transformation at Curlie",https://en.wikipedia.org/wiki/Data_transformation,"['Articles with Curlie links', 'Articles with example C++ code', 'Data management', 'Data warehousing', 'Metadata', 'Wikipedia articles with possible conflicts of interest from October 2017']",Data Science
72,Data wrangling,"Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one ""raw"" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. The goal of data wrangling is to assure quality and useful data. Data analysts typically spend the majority of their time in the process of data wrangling compared to the actual analysis of the data.
The process of data wrangling may include further munging, data visualization, data aggregation, training a statistical model, as well as many other potential uses.  Data wrangling typically follows a set of general steps which begin with extracting the data in a raw form from the data source, ""munging"" the raw data (e.g. sorting) or parsing the data into predefined data structures, and finally depositing the resulting content into a data sink for storage and future use.

Background
The ""wrangler"" non-technical term is often said to derive from work done by the United States Library of Congress's National Digital Information Infrastructure and Preservation Program (NDIIPP) and their program partner the Emory University Libraries based MetaArchive Partnership. The term ""mung"" has roots in munging as described in the Jargon File. The term ""Data Wrangler"" was also suggested as the best analogy to coder for someone working with data.One of the first mentions of data wrangling in a scientific context was by Donald Cline during the NASA/NOAA Cold Lands Processes Experiment. Cline stated the data wranglers “coordinate the acquisition of the entire collection of the experiment data.” Cline also specifies duties typically handled by a storage administrator for working with large amounts of data. This can occur in areas like major research projects and the making of films with a large amount of complex computer-generated imagery. In research, this involves both data transfer from research instrument to storage grid or storage facility as well as data manipulation for re-analysis via high performance computing instruments or access via cyberinfrastructure-based digital libraries.
With the upcoming of artificial intelligence in data science it has become increasingly important for automation of data wrangling to have very strict checks and balances, which is why the munging process of data has not been automated by machine learning. Data munging requires more than just an automated solution, it requires knowledge of what information should be removed and artificial intelligence is not to the point of understanding such things.

Connection to data mining
Data wrangling is a superset of data mining and requires processes that some data mining uses, but not always. The process of data mining is to find patterns within large data sets, where data wrangling transforms data in order to deliver insights about that data. Even though data wrangling is a superset of data mining does not mean that data mining does not use it, there are many use cases for data wrangling in data mining. Data wrangling can benefit data mining by removing data that does not benefit the overall set, or is not formatted properly, which will yield better results for the overall data mining process.
An example of data mining that is closely related to data wrangling is ignoring data from a set that is not connected to the goal: say there is a data set related to the state of Texas and the goal is to get statistics on the residents of Houston, the data in the set related to the residents of Dallas is not useful to the overall set and can be removed before processing to improve the efficiency of the data mining process.

Benefits
With an increase of raw data comes an increase in the amount of data that is not inherently useful, this increases time spent on cleaning and organizing data before it can be analyzed which is where data wrangling comes into play. The result of data wrangling can provide important metadata statistics for further insights about the data, it is important to ensure metadata is consistent otherwise it can cause roadblocks. Data wrangling allows analysts to analyze more complex data more quickly, achieve more accurate results, and because of this better decisions can be made. Many businesses have moved to data wrangling because of the success that it has brought.

Core Ideas
The main steps in data wrangling are as follows:

These steps are an iterative process that should yield a clean and usable data set that can then be used for analysis. This process is tedious but rewarding as it allows analysts to get the information they need out of a large set of data that would otherwise be unreadable.

The result of using the data wrangling process on this small data set shows a significantly easier data set to read. All names are now formatted the same way, {first name last name}, phone numbers are also formatted the same way { area code-XXX-XXXX}, dates are formatted numerically {mm-dd-YYYY}, and states are no longer abbreviated. The entry for Jacob Alan was deemed to be removed from the data set as it did not have fully formed data, the area code on the phone number is missing and the birth date had no year and was thus cleaned from the data set. Now that the resulting data set is cleaned and readable it is ready to be either deployed or evaluated.

Typical use
The data transformations are typically applied to distinct entities (e.g. fields, rows, columns, data values etc.) within a data set, and could include such actions as extractions, parsing, joining, standardizing, augmenting, cleansing, consolidating and filtering to create desired wrangling outputs that can be leveraged downstream. 
The recipients could be individuals, such as data architects or data scientists who will investigate the data further, business users who will consume the data directly in reports, or systems that will further process the data and write it into targets such as data warehouses, data lakes or downstream applications.

Modus operandi
Depending on the amount and format of the incoming data, data wrangling has traditionally been performed manually (e.g. via spreadsheets such as Excel), tools like KNIME or via scripts in languages such as Python or SQL. R, a language often used in data mining and statistical data analysis, is now also often used for data wrangling. Data wranglers typically have skills sets within: R or Python, SQL, PHP, Scala, and more languages typically used for analyzing data. 
Visual data wrangling systems were developed to make data wrangling accessible for non-programmers, and simpler for programmers. Some of these also include embedded AI recommenders and Programming by Example facilities to provide user assistance, and Program Synthesis techniques to autogenerate scalable dataflow code. Early prototypes of visual data wrangling tools include OpenRefine and the Stanford/Berkeley Wrangler research system; the latter evolved into Trifacta.
Other terms for these processes have included data franchising, data preparation and data munging.

Example
Given a set of data that contains information on medical patients your goal is to find correlation for a disease. Before you can start iterating through the data ensure that you have an understanding of the result, are you looking for patients who have the disease? Are there other diseases that can be the cause? Once an understanding of the outcome is achieve then the data wrangling process can begin. 
Start by determining the structure of the outcome, what is important to understand the disease diagnosis. 
Once a final structure is determined, clean the data by removing any data points that are not helpful or are malformed, this could include patients that have not been diagnosed with any disease. 
After cleaning look at the data again, is there anything that can be added to the data set that is already known that would benefit it? An example could be most common diseases in the area, America and India are very different when it comes to most common diseases. 
Now comes the validation step, determine validation rules for which data points need to be checked for validity, this could include date of birth or checking for specific diseases. 
After the validation step the data should now be organized and prepared for either deployment or evaluation. This process can be beneficial for determining correlations for disease diagnosis as it will reduce the vast amount of data into something that can be easily analyzed for an accurate result.

See also
Data Preparation
OpenRefine
Trifacta
Alteryx


== References ==",https://en.wikipedia.org/wiki/Data_wrangling,"['Articles with short description', 'Computer occupations', 'Data mapping', 'Short description is different from Wikidata']",Data Science
73,Database,"A database is an organized collection of data, generally stored and accessed electronically from a computer system. Where databases are more complex they are often developed using formal design and modeling techniques.
The database management system (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS software additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a ""database system"". Often the term ""database"" is also used to loosely refer to any of the DBMS, the database system or an application associated with the database.
Computer scientists may classify database-management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, referred to as NoSQL because they use different query languages.

Terminology and overview
Formally, a ""database"" refers to a set of related data and the way it is organized. Access to this data is usually provided by a ""database management system"" (DBMS) consisting of an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.
Because of the close relationship between them, the term ""database"" is often used casually to refer to both a database and the DBMS used to manipulate it.
Outside the world of professional information technology, the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system.Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups:

Data definition – Creation, modification and removal of definitions that define the organization of the data.
Update – Insertion, modification, and deletion of the actual data.
Retrieval – Providing information in a form directly usable or for further processing by other applications. The retrieved data may be made available in a form basically the same as it is stored in the database or in a new form obtained by altering or combining existing data from the database.
Administration – Registering and monitoring users, enforcing data security, monitoring performance, maintaining data integrity, dealing with concurrency control, and recovering information that has been corrupted by some event such as an unexpected system failure.Both a database and its DBMS conform to the principles of a particular database model. ""Database system"" refers collectively to the database model, database management system, and database.Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.Since DBMSs comprise a significant market, computer and storage vendors often take into account DBMS requirements in their own development plans.Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.

History
The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid 1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational, SQL/relational, and post-relational.
The two main early navigational data models were the hierarchical model and the CODASYL model (network model). These were characterized by the use of pointers (often physical disk addresses) to follow relationships from one record to another.
The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2018 they remain dominant: IBM DB2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS. The dominant database language, standardised SQL for the relational model, has influenced database languages for other data models.Object databases were developed in the 1980s to overcome the inconvenience of object–relational impedance mismatch, which led to the coining of the term ""post-relational"" and also the development of hybrid object–relational databases.
The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key–value stores and document-oriented databases. A competing ""next generation"" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs.

1960s, navigational DBMS
The introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term ""data-base"" in a specific technical sense.As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the CODASYL approach, and soon a number of commercial products based on this approach entered the market.
The CODASYL approach offered applications the ability to navigate around a linked data set which was formed into a large network. Applications could find records by one of three methods:

Use of a primary key (known as a CALC key, typically implemented by hashing)
Navigating relationships (called sets) from one record to another
Scanning all the records in a sequential orderLater systems added B-trees to provide alternate access paths. Many CODASYL databases also added a declarative query language for end users (as distinct from the navigational API). However CODASYL databases were complex and required significant training and effort to produce useful applications.
IBM also had their own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman's 1973 Turing Award presentation The Programmer as Navigator. IMS is classified by IBM as a hierarchical database. IDMS and Cincom Systems' TOTAL database are classified as network databases. IMS remains in use as of 2014.

1970s, relational DBMS
Edgar F. Codd worked at IBM in San Jose, California, in one of their offshoot offices that was primarily involved in the development of hard disk systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a ""search"" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks.In this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to organise the data as a number of ""tables"", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a  primary key by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized tables (or relations) aimed to ensure that each ""fact"" was only stored once, thus simplifying update operations. Virtual tables called views could present the data in different ways for different users, but views could not be directly updated.
Codd used mathematical terms to define the model: relations, tuples, and domains rather than tables, rows, and columns. The terminology that is now familiar came from early implementations. Codd would later criticize the tendency for practical implementations to depart from the mathematical foundations on which the model was based.

The use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of first-order predicate calculus; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit.
In the hierarchic and network models, records were allowed to have a complex internal structure. For example, the salary history of an employee might be represented as a ""repeating group"" within the employee record. In the relational model, the process of normalization led to such internal structures being replaced by data held in multiple tables, connected only by logical keys.
For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.
As well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic.
Codd's paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a ""language"" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.
IBM itself did one test implementation of the relational model, PRTV, and a production one, Business System 12, both now discontinued. Honeywell wrote MRDS for Multics, and now there are two new implementations: Alphora Dataphor and Rel. Most other DBMS implementations usually called relational are actually SQL DBMSs.
In 1970, the University of Michigan began development of the MICRO Information Management System based on D.L. Childs' Set-Theoretic Data model. MICRO was used to manage very large data sets by the US Department of Labor, the U.S. Environmental Protection Agency, and researchers from the University of Alberta, the University of Michigan, and Wayne State University. It ran on IBM mainframe computers using the Michigan Terminal System. The system remained in production until 1998.

Integrated approach
In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine.
Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued for certain applications by some companies like Netezza and Oracle (Exadata).

Late 1970s, SQL DBMS
IBM started working on a prototype system loosely based on Codd's concepts as System R in the early 1970s. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large ""chunk"". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language – SQL – had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS, and, later, Database 2 (DB2).
Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it wasn't until Oracle Version 2 when Ellison beat IBM to market in 1979.Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions).
In Sweden, Codd's paper was also read and Mimer SQL was developed from the mid-1970s at Uppsala University. In 1984, this project was consolidated into an independent enterprise.
Another data model, the entity–relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity–relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two have become irrelevant.

1980s, on the desktop
The 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: ""dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation."" dBASE was one of the top selling software titles in the 1980s and early 1990s.

1990s, object-oriented
The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be relations to objects and their attributes and not to individual fields. The term ""object–relational impedance mismatch"" described the inconvenience of translating between programmed objects and database tables. Object databases and object–relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object–relational mappings (ORMs) attempt to solve the same problem.

2000s, NoSQL and NewSQL
XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records.
NoSQL databases are often very fast, do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally.
In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.
NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system.

Use cases
Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software).
Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database.

Classification
One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases.

An in-memory database is a database that primarily resides in main memory, but is typically backed-up by non-volatile computer data storage. Main memory databases are faster than disk databases, and so are often used where response time is critical, such as in telecommunications network equipment.
An active database includes an event-driven architecture which can respond to conditions both inside and outside the database. Possible uses include security monitoring, alerting, statistics gathering and authorization. Many databases provide active database features in the form of database triggers.
A cloud database relies on cloud technology. Both the database and most of its DBMS reside remotely, ""in the cloud"", while its applications are both developed by programmers and later maintained and used by end-users through a web browser and Open APIs.
Data warehouses archive data from operational databases and often from external sources such as market research firms. The warehouse becomes the central source of data for use by managers and other end-users who may not have access to operational data. For example, sales data might be aggregated to weekly totals and converted from internal product codes to use UPCs so that they can be compared with ACNielsen data. Some basic and essential components of data warehousing include extracting, analyzing, and mining data, transforming, loading, and managing data so as to make them available for further use.
A deductive database combines logic programming with a relational database.
A distributed database is one in which both the data and the DBMS span multiple computers.
A document-oriented database is designed for storing, retrieving, and managing document-oriented, or semi structured, information. Document-oriented databases are one of the main categories of NoSQL databases.
An embedded database system is a DBMS which is tightly integrated with an application software that requires access to stored data in such a way that the DBMS is hidden from the application's end-users and requires little or no ongoing maintenance.
End-user databases consist of data developed by individual end-users. Examples of these are collections of documents, spreadsheets, presentations, multimedia, and other files. Several products exist to support such databases. Some of them are much simpler than full-fledged DBMSs, with more elementary DBMS functionality.
A federated database system comprises several distinct databases, each with its own DBMS. It is handled as a single database by a federated database management system (FDBMS), which transparently integrates multiple autonomous DBMSs, possibly of different types (in which case it would also be a heterogeneous database system), and provides them with an integrated conceptual view.
Sometimes the term multi-database is used as a synonym to federated database, though it may refer to a less integrated (e.g., without an FDBMS and a managed integrated schema) group of databases that cooperate in a single application. In this case, typically middleware is used for distribution, which typically includes an atomic commit protocol (ACP), e.g., the two-phase commit protocol, to allow distributed (global) transactions across the participating databases.
A graph database is a kind of NoSQL database that uses graph structures with nodes, edges, and properties to represent and store information. General graph databases that can store any graph are distinct from specialized graph databases such as triplestores and network databases.
An array DBMS is a kind of NoSQL DBMS that allows modeling, storage, and retrieval of (usually large) multi-dimensional arrays such as satellite images and climate simulation output.
In a hypertext or hypermedia database, any word or a piece of text representing an object, e.g., another piece of text, an article, a picture, or a film, can be hyperlinked to that object. Hypertext databases are particularly useful for organizing large amounts of disparate information. For example, they are useful for organizing online encyclopedias, where users can conveniently jump around the text. The World Wide Web is thus a large distributed hypertext database.
A knowledge base (abbreviated KB, kb or Δ) is a special kind of database for knowledge management, providing the means for the computerized collection, organization, and retrieval of knowledge. Also a collection of data representing problems with their solutions and related experiences.A mobile database can be carried on or synchronized from a mobile computing device.
Operational databases store detailed data about the operations of an organization. They typically process relatively high volumes of updates using transactions. Examples include customer databases that record contact, credit, and demographic information about a business's customers, personnel databases that hold information such as salary, benefits, skills data about employees, enterprise resource planning systems that record details about product components, parts inventory, and financial databases that keep track of the organization's money, accounting and financial dealings.
A parallel database seeks to improve performance through parallelization for tasks such as loading data, building indexes and evaluating queries.The major parallel DBMS architectures which are induced by the underlying hardware architecture are:
Shared memory architecture, where multiple processors share the main memory space, as well as other data storage.
Shared disk architecture, where each processing unit (typically consisting of multiple processors) has its own main memory, but all units share the other storage.
Shared-nothing architecture, where each processing unit has its own main memory and other storage.Probabilistic databases employ fuzzy logic to draw inferences from imprecise data.
Real-time databases process transactions fast enough for the result to come back and be acted on right away.
A spatial database can store the data with multidimensional features. The queries on such data include location-based queries, like ""Where is the closest hotel in my area?"".
A temporal database has built-in time aspects, for example a temporal data model and a temporal version of SQL. More specifically the temporal aspects usually include valid-time and transaction-time.
A terminology-oriented database builds upon an object-oriented database, often customized for a specific field.
An unstructured data database is intended to store in a manageable and protected way diverse objects that do not fit naturally and conveniently in common databases. It may include email messages, documents, journals, multimedia objects, etc. The name may be misleading since some objects can be highly structured. However, the entire possible object collection does not fit into a predefined structured framework. Most established DBMSs now support unstructured data in various ways, and new dedicated DBMSs are emerging.

Database interaction
Database management system
Connolly and Begg define database management system (DBMS) as a ""software system that enables users to define, create, maintain and control access to the database"". Examples of DBMS's include MySQL, PostgreSQL, Microsoft SQL Server, Oracle Database, and Microsoft Access.
The DBMS acronym is sometimes extended to indicate the underlying database model, with RDBMS for the relational, OODBMS for the object (oriented) and ORDBMS for the object–relational model. Other extensions can indicate some other characteristic, such as DDBMS for a distributed database management systems.
The functionality provided by a DBMS can vary enormously. The core functionality is the storage, retrieval and update of data. Codd proposed the following functions and services a fully-fledged general purpose DBMS should provide:
Data storage, retrieval and update
User accessible catalog or data dictionary describing the metadata
Support for transactions and concurrency
Facilities for recovering the database should it become damaged
Support for authorization of access and update of data
Access support from remote locations
Enforcing constraints to ensure data in the database abides by certain rulesIt is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities. The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine.
Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimise the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount.
The large major enterprise DBMSs have tended to increase in size and functionality and can have involved thousands of human years of development effort through their lifetime.Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client–server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier.A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with the database. A special purpose DBMS may use a private API and be specifically customised and linked to a single application. For example, an email system performing many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email.

Application
External interaction with the database will be via an application program that interfaces with the DBMS. This can range from a database tool that allows users to execute SQL queries textually or graphically, to a web site that happens to use a database to store and search information.

Application program interface
A programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possible indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET.

Database languages
Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages:

Data control language (DCL) – controls access to data;
Data definition language (DDL) – defines data types such as creating, altering, or dropping tables and the relationships among them;
Data manipulation language (DML) – performs tasks such as inserting, updating, or deleting data occurrences;
Data query language (DQL) – allows searching for information and computing derived information.Database languages are specific to a particular data model. Notable examples include:

SQL combines the roles of data definition, data manipulation, and query in a single language. It was one of the first commercial languages for the relational model, although it departs in some respects from the relational model as described by Codd (for example, the rows and columns of a table can be ordered). SQL became a standard of the American National Standards Institute (ANSI) in 1986, and of the International Organization for Standardization (ISO) in 1987. The standards have been regularly enhanced since and is supported (with varying degrees of conformance) by all mainstream commercial relational DBMSs.
OQL is an object model language standard (from the Object Data Management Group). It has influenced the design of some of the newer query languages like JDOQL and EJB QL.
XQuery is a standard XML query language implemented by XML database systems such as MarkLogic and eXist, by relational databases with XML capability such as Oracle and DB2, and also by in-memory XML processors such as Saxon.
SQL/XML combines XQuery with SQL.A database language may also incorporate features like:

DBMS-specific configuration and storage engine management
Computations to modify query results, like counting, summing, averaging, sorting, grouping, and cross-referencing
Constraint enforcement (e.g. in an automotive database, only allowing one engine type per car)
Application programming interface version of the query language, for programmer convenience

Storage
Database storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, ""data about the data"", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Putting data into permanent storage is generally the responsibility of the database engine a.k.a. ""storage engine"". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems' file systems as intermediates for storage layout), storage properties and configuration setting are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look in the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).
Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database.
Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases.

Materialized views
Often storage redundancy is employed to increase performance. A common example is storing materialized views, which consist of frequently needed external views or query results. Storing such views saves the expensive computing of them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy.

Replication
Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to a same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated.

Security
Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program).
Database access control deals with controlling who (a person or a certain computer program) is allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.
This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called ""subschemas"". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases.
Data security in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see physical security), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see data encryption).
Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this to the database. Monitoring can be set up to attempt to detect security breaches.

Transactions and concurrency
Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands).
The acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability.

Migration
A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help importing databases from other popular DBMSs.

Building, maintaining, and tuning
After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.).
When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation.
After the database is created, initialised and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc.

Backup and restore
Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state.

Static analysis
Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database system has many interesting applications, in particular, for security purposes, such as fine grained access control, watermarking, etc.

Miscellaneous features
Other DBMS features might include:

Database logs – This helps in keeping a history of the executed functions.
Graphics component for producing graphs and charts, especially in a data warehouse system.
Query optimizer – Performs query optimization on every query to choose an efficient query plan (a partial order (tree) of operations) to be executed to compute the query result. May be specific to a particular storage engine.
Tools or hooks for database design, application programming, application program maintenance, database performance analysis and monitoring, database configuration monitoring, DBMS hardware configuration (a DBMS and related database may span computers, networks, and storage units) and related database mapping (especially for a distributed DBMS), storage allocation and database layout monitoring, storage migration, etc.Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as ""DevOps for database"".

Design and modeling
The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity–relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like ""can a customer also be a supplier?"", or ""if a product is sold with two different forms of packaging, are those the same product or different products?"", or ""if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?"". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes.
Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data.
Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design).
The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary ""fact"" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency.
The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called physical database design, and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS.
Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself.

Models
A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format.
Common logical data models for databases include:

Navigational databases
Hierarchical database model
Network model
Graph database
Relational model
Entity–relationship model
Enhanced entity–relationship model
Object model
Document model
Entity–attribute–value model
Star schemaAn object–relational database combines the two related structures.
Physical data models include:

Inverted index
Flat fileOther models include:

Associative model
Multidimensional model
Array model
Multivalue modelSpecialized models are optimized for particular types of data:

XML database
Semantic model
Content store
Event store
Time series model

External, conceptual, and internal views
A database management system provides three views of the database data:

The external level defines how each group of end-users sees the organization of data in the database. A single database can have any number of views at the external level.
The conceptual level unifies the various external views into a compatible global view. It provides the synthesis of all the external views. It is out of the scope of the various database end-users, and is rather of interest to database application developers and database administrators.
The internal level (or physical level) is the internal organization of data inside a DBMS. It is concerned with cost, performance, scalability and other operational matters. It deals with storage layout of the data, using storage structures such as indexes to enhance performance. Occasionally it stores data of individual views (materialized views), computed from generic data, if performance justification exists for such redundancy. It balances all the external views' performance requirements, possibly conflicting, in an attempt to optimize overall performance across all activities.While there is typically only one conceptual (or logical) and physical (or internal) view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are the interest of the human resources department. Thus different departments need different views of the company's database.
The three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model. The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance.
The conceptual view provides a level of indirection between internal and external. On one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types.
Separating the external, conceptual and internal levels was a major feature of the relational database model implementations that dominate 21st century databases.

Research
Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, and related concurrency control techniques, query languages and query optimization methods, RAID, and more.
The database research area has several dedicated academic journals (for example, ACM Transactions on Database Systems-TODS, Data and Knowledge Engineering-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE).

See also
Notes
References
Sources
Further reading
Ling Liu and Tamer M. Özsu (Eds.) (2009).  ""Encyclopedia of Database Systems, 4100 p. 60 illus. ISBN 978-0-387-49616-0.
Gray, J. and Reuter, A. Transaction Processing: Concepts and Techniques, 1st edition,  Morgan Kaufmann Publishers, 1992.
Kroenke, David M. and David J. Auer. Database Concepts. 3rd ed. New York: Prentice, 2007.
Raghu Ramakrishnan and Johannes Gehrke, Database Management Systems
Abraham Silberschatz, Henry F. Korth, S. Sudarshan, Database System Concepts
Lightstone, S.; Teorey, T.; Nadeau, T. (2007). Physical Database Design: the database professional's guide to exploiting indexes, views, storage, and more. Morgan Kaufmann Press. ISBN 978-0-12-369389-1.
Teorey, T.; Lightstone, S. and Nadeau, T. Database Modeling & Design: Logical Design, 4th edition, Morgan Kaufmann Press, 2005. ISBN 0-12-685352-5

External links

DB File extension – information about files with the DB extension",https://en.wikipedia.org/wiki/Database,"['All articles containing potentially dated statements', 'All articles needing additional references', 'All articles with unsourced statements', 'Articles containing potentially dated statements from 2014', 'Articles containing potentially dated statements from 2018', 'Articles contradicting other articles', 'Articles needing additional references from March 2013', 'Articles with short description', 'Articles with unsourced statements from January 2020', 'Articles with unsourced statements from March 2013', 'Articles with unsourced statements from May 2012', 'CS1: long volume value', 'Database management systems', 'Databases', 'Pages containing links to subscription-only content', 'Pages using Sister project links with default search', 'Pages using Sister project links with wikidata mismatch', 'Pages using Sister project links with wikidata namespace mismatch', 'Short description matches Wikidata', 'Webarchive template wayback links', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NARA identifiers', 'Wikipedia articles with NDL identifiers']",Data Science
74,Data retention,"Data retention defines the policies of persistent data and records management for meeting legal and business data archival requirements. Although sometimes interchangeable, it is not to be confused with the Data Protection Act 1998.
The different data retention policies weigh legal and privacy concerns against economics and need-to-know concerns to determine the retention time, archival rules, data formats, and the permissible means of storage, access, and encryption.
In the field of telecommunications, data retention generally refers to the storage of call detail records (CDRs) of telephony and internet traffic and transaction data (IPDRs) by governments and commercial organisations. In the case of government data retention, the data that is stored is usually of telephone calls made and received, emails sent and received, and websites visited. Location data is also collected.
The primary objective in government data retention is traffic analysis and mass surveillance. By analysing the retained data, governments can identify the locations of individuals, an individual's associates and the members of a group such as political opponents. These activities may or may not be lawful, depending on the constitutions and laws of each country. In many jurisdictions access to these databases may be made by a government with little or no judicial oversight.In the case of commercial data retention, the data retained will usually be on transactions and web sites visited.
Data retention also covers data collected by other means (e.g., by Automatic number-plate recognition systems) and held by government and commercial organisations.

Data retention policy
A data retention policy is a recognized and proven protocol within an organization for retaining information for operational use while ensuring adherence to the laws and regulations concerning them. The objectives of a data retention policy are to keep important information for future use or reference, to organize information so it can be searched and accessed at a later date and to dispose of information that is no longer needed.The data retention policies within an organization are a set of guidelines that describes which data will be archived, how long it will be kept, what happens to the data at the end of the retention period (archive or destroy) and other factors concerning the retention of the data.A part of any effective data retention policy is the permanent deletion of the retained data; achieving secure deletion of data by encrypting the data when stored, and then deleting the encryption key after a specified retention period. Thus, effectively deleting the data object and its copies stored in online and offline locations.

Australia
In 2015, the Australian government introduced mandatory data retention laws that allows data to be retained up to two years. The scheme is estimated to cost at least AU$400 million per year to implement, working out to at least $16 per user per year. It will require telecommunication providers and ISPs to retain telephony, Internet and email metadata for two years, accessible without a warrant, and could possibly be used to target file sharing. The Attorney-General has broad discretion on which agencies are allowed to access metadata, including private agencies.The Greens were strongly opposed to the introduction of these laws, citing privacy concerns and the increased prospect of 'speculative invoicing' over alleged copyright infringement cases. The Labor Party initially opposed as well, but later agreed to passing the law after additional safeguards were put in place to afford journalists some protection.

European Union
On 15 March 2006, the European Union adopted the Data Retention Directive, on ""the retention of data generated or processed in connection with the provision of publicly available electronic communications services or of public communications networks and amending Directive 2002/58/EC"". It requires Member States to ensure that communications providers retain the necessary data as specified in the Directive for a period of between 6 months and 2 years  in order to:

Trace and identify the source of a communication;
Trace and identify the destination of a communication;
Identify the date, time, and duration of a communication;
Identify the type of communication;
Identify the communication device;
Identify the location of mobile communication equipment.The data is required to be available to ""competent"" national authorities in specific cases, ""for the purpose of the investigation, detection and prosecution of serious crime, as defined by each Member State in its national law"".
The Directive covers fixed telephony, mobile telephony, Internet access, email, and VoIP. Member States were required to transpose it into national law within 18 months—no later than September 2007. However, they may if they wish postpone the application of the Directive to Internet access, email, and VoIP for a further 18 months after this date. A majority of Member States exercised this option. All 28 EU States have notified the European Commission about the transposition of the Directive into their national law. Of these, however, Germany and Belgium have only transposed the legislation partially.A report evaluating the Directive was published by the European Commission in April 2011.It concluded that data retention was a valuable tool for ensuring criminal justice and public protection, but that it had achieved only limited harmonisation. There were serious concerns from service providers about the compliance costs and from civil society organisations who claim that mandatory data retention was an unacceptable infringement of the fundamental right to privacy and the protection of personal data. The Commission is now reviewing the legislation.
In response to the report, on May 31, 2011, the European Data Protection Supervisor expressed some concerns on the European Data Retention Directive, underlining that the Directive ""does not meet the requirements imposed by the fundamental rights to privacy and data protection"".On 8 April 2014, the Court of Justice of the European Union declared the Directive 2006/24/EC invalid for violating fundamental rights. The Council's Legal Services have been reported to have stated in closed session that paragraph 59 of the European Court of Justice's ruling ""suggests that general and blanket data retention is no longer possible"". A legal opinion funded by the Greens/EFA Group in the European Parliament finds that the blanket retention data of unsuspicious persons generally violates the EU Charter of Fundamental Rights, both in regard to national telecommunications data retention laws and to similar EU data retention schemes (PNR, TFTP, TFTS, LEA access to EES, Eurodac, VIS).

United Kingdom
Data Retention and Investigatory Powers Act 2014
The Data Retention and Investigatory Powers Act came into force in 2014. It is the answer by the United Kingdom parliament after a declaration of invalidity was made by the Court of Justice of the European Union in relation to Directive 2006/ 24/EC in order to make provision, about the retention of certain communications data.
In addition, the purpose of the act is to:

Amend the grounds for issuing interception warrants, or granting or giving certain authorizations or notices.
Make provision about the extraterritorial application of that Part and about the meaning of ""telecommunications service"" for the purposes of that Act;
Make provision about a review of the operation and regulation of investigatory powers; and for connected purposes.The act is also to ensure that communication companies in the UK retain communications data so that it continues to be available when it is needed by law enforcement agencies and others to investigate committed crimes and protect the public. Data protection law requires data that isn't of use to be deleted. This means that the intention of this Act could be using data retention to acquire further policing powers using, as the Act make data retention mandatory.
An element of this Act is the provision of the investigatory powers to be reported by 1 May 2015.

Controversy
The Data Retention and Investigatory Powers Act 2014 was referred to as the ""snooper's charter"" communications data bill. Theresa May, a strong supporter of the Parliament Act, said in a speech that ""If we (parliament) do not act, we risk sleepwalking into a society in which crime can no longer be investigated and terrorists can plot their murderous schemes undisrupted.""The United Kingdom parliament its new laws increasing the power of data retention is essential to tackling crime and protecting the public.  However, not all agree and believe that the primary objective in the data retention by the government is mass surveillance.
After Europe's highest court said the depth of data retention breaches citizens' fundamental right to privacy and the UK created its own Act, it has led to the British government being accused of breaking the law by forcing telecoms and internet providers to retain records of phone calls, texts and internet usage.
From this information, governments can identify an individual's associates, location, group memberships, political affiliations and other personal information.
In a television interview, the EU Advocate General Pedro Cruz Villalón highlighted the risk that the retained data might be used illegally in ways that are ""potentially detrimental to privacy or, more broadly, fraudulent or even malicious"".

Retention of other data
Postal data – retention period unknown
Information written on the outside of a postal item (such as a letter or parcel), online tracking of postal items, records of special postal items (such as records of registered, recorded or special delivery postal items), records of parcel consignment, delivery and collection.
Banking data – seven years
The Economist reported that UK banks are required to retain data on all financial transactions for seven years though this has not been verified. It is not clear whether data on credit card transactions is also retained for seven years.
Vehicle movement data – two years
Documents leaked from the Association of Chief Police Officers (ACPO) have revealed that the UK is planning to collect data from a nationwide network of automatic numberplate recognition cameras and store the data for two years in a controversial new centre being built at Hendon. This data could then be linked to other data held by the government and watchlists from the police and security services.

Access to retained data
The bodies that are able to access retained data in the United Kingdom are listed in the Regulation of Investigatory Powers Act 2000 (RIPA). These are the following:

Police forces, as defined in section 81(1) of RIPA
National Criminal Intelligence Service
Serious Organised Crime Agency, formerly the National Crime Squad
HM Customs and Excise
Inland Revenue (the latter two have been merged into HM Revenue and Customs)
Security Service
Secret Intelligence Service
Government Communications Headquarters (GCHQ)However, the Regulation of Investigatory Powers Act 2000 (RIPA) also gives the Home Secretary powers to change the list of bodies with access to retained data through secondary legislation. The list of authorised bodies now includes:
Food Standards Agency
Local authorities
National Health Service

Reasons for accessing retained data
The justifications for accessing retained data in the UK are set out in the Regulation of Investigatory Powers Act 2000 (RIPA). They include:

Interests of national security;
Preventing or detecting crime or of preventing disorder;
Economic well-being of the United Kingdom;
Public safety;
Protecting public health;
Assessing or collecting any tax, duty, levy or other imposition, contribution or charge payable to a government department;
Preventing death or injury in an emergency or any damage to a person's physical or mental health, or of mitigating any injury or damage to a person's physical or mental health;
Any other purpose not listed above which is specified for the purposes of this subsection by an order made by the Secretary of State.

Czech Republic
Implementation of the directive was part of Act. No. 259/2010 Coll. on electronic communications as later amended. Under Art. 97 (3), telecommunication data are to be stored between 6 and 12 months. The Czech Constitutional Court has deemed the law unconstitutional and found it to be infringing on the peoples right to privacy.As of July 2012, new legislation was on its way.

Italy
In July 2005 new legal requirements on data retention came into force in Italy.

Subscriber information
Internet cafés and public telephone shops with at least three terminals must seek a license permit within 30 days from the Ministry of Home Affairs. They must also store traffic data for a period which may be determined later by administrative decree. Wi-Fi hotspots and locations that do not store traffic data have to secure ID information from users before allowing them to log on. For example, users may be required to enter a number from an ID card or driving license. It is not clear how this information is validated. Mobile telephony users must identify themselves before service activation, or before a SIM card may be obtained. Resellers of mobile subscriptions or pre-paid cards must verify the identity of purchasers and retain a photocopy of identity cards.
Telephony data
Data, including location data, on fixed line and mobile telephony must be retained for 24 months. There is no requirement to store the content of calls. Telephony operators must retain a record of all unsuccessful dial attempts.
ISP data
Internet service providers must retain all data for at least 12 months. The law does not specify exactly what traffic data must be retained.  There is no requirement to store the content of internet communications.
Legality
The legislation of July 2005 enables data retention by outlawing all the relevant data protection provisions until 31 December 2007. Under the data protection provisions, service providers are obliged to store traffic data and user data for no less than 365 days, even if they no longer need it to process the communication or to send bills, policy requires user id information, location, tracking data be stored and kept on file for easy access by law enforcement and/or other authorities who request this information (permission must be asked to view sensitive user ID data on file). The traffic data which will now be retained can be used for anti-terrorism purposes and for general penal enforcement of criminal offences large and small.Italy already required the retention of telephony traffic data for 48 months, but without location data. Italy has adopted the EU Directive on Privacy and Electronic Communications 2002 but with an exemption to the requirement to erase traffic data.

Denmark
Denmark has implemented the EU data retention directive and much more, by logging all internet flow or sessions between operators and operators and consumers.
""2.2.1. Session logging (section 5(1) of the Executive Order) Providers of access to the internet must, in respect of the initiating and terminating package of an internet session, retain data that identifies the sending and receiving internet protocol address (in the following called IP address), the sending and receiving port number and the transmission protocol.""
""2.2.2. Sampling (section 5(4) of the Executive Order)  The obligation to retain data about the initiating and terminating package of an internet session does not apply to providers in case such retention is not technically feasible in their systems. In that case, data must instead be retained for every 500th package that is part of an end user's communication on the internet.""
""2.2.5. Hot spots (section 5(3) of the Executive Order) In addition to the internet data that must otherwise be retained, the provider must retain data that identifies the precise geographic or physical location of a hot spot and the identity of the communication equipment used. This means that a provider of internet access via a hot spot must retain data on a user's access to the internet and, at the same time, retain data that identifies the geographic location of the hot spot in question.""

Sweden
Sweden implemented the EU's 2006 Data Retention Directive in May 2012, and it was fined €3 million by the Court of Justice of the European Union for its belated transposition (the deadline was 15 September 2007). The directive allowed member states to determine the duration data is retained, ranging from six months to two years; the Riksdag, Sweden's legislature, opted for six months.In April 2014, however, the CJEU struck down the Data Retention Directive. PTS, Sweden's telecommunications regulator, told Swedish ISPs and telcos that they would no longer have to retain call records and internet metadata. But after two government investigations found that Sweden's data retention law did not break its obligations to the European Convention on Human Rights, the PTS reversed course. Most of Sweden's major telecommunications companies complied immediately, though Tele2 lodged an unsuccessful appeal. The one holdout ISP, Bahnhof, was given an order to comply by November 24 deadline or face a five million krona ($680,000) fine.

Germany
The German Bundestag had implemented the directive in ""Gesetz zur Neuregelung der Telekommunikationsüberwachung und anderer verdeckter Ermittlungsmaßnahmen sowie zur Umsetzung der Richtlinie 2006/24/EG"". The law became valid on 1 January 2008. Any communications data had to be retained for six months. On 2 March 2010, the Federal Constitutional Court of Germany ruled the law unconstitutional as a violation of the guarantee of the secrecy of correspondence. On 16 October 2015, a second law for shorter, up to 10 weeks long, data retention excluding email communication was passed by parliament. However, this act was ruled incompatible with German and European laws by an injunction of the Higher Administrative Court of North Rhine-Westphalia. As a result, on June 28, 2017, three days before the planned start of data retention, the Federal Network Agency suspended the introduction of data retention until a final decision in the principle proceedings.

Romania
The EU directive has been transposed into Romanian law as well, initially as Law 298/2008. However, the Constitutional Court of Romania subsequently struck down the law in 2009 as violating constitutional rights. The court held that the transposing act violated the constitutional rights of privacy, of confidentiality in communications, and of free speech. The European Commission has subsequently sued Romania in 2011 for non-implementation, threatening Romania with a fine of 30,000 euros per day. The Romanian parliament passed a new law in 2012, which was signed by president Traian Băsescu in June. The Law 82/2012 has been nicknamed ""Big Brother"" (using the untranslated English expression) by various Romanian non-governmental organizations opposing it. On July 8, 2014 this law too was declared unconstitutional by the Constitutional Court of Romania.

Slovakia
Slovakia has implemented the directive in Act No. 610/2003 Coll. on electronic communications as later amended. Telecommunication data are stored for six months in the case of data related to Internet, Internet email and Internet telephony (art. 59a (6) a), and for 12 months in the case of other types of communication (art. 59a (6) b).
In April 2014, the Slovak Constitutional Court preliminary suspended effectiveness of the Slovak implementation of Data Retention Directive and accepted the case for the further review. In April 2015 Constitutional court decided that some parts of Slovak laws implementing DR Directive are not in compliance with Slovak constitution and Convention for the Protection of Human Rights and Fundamental Freedoms. According to now invalid provisions of the Electronic Communications Act, the providers of electronic communications were obliged to store traffic data, localization data and data about the communicating parties for a period of 6 months (in the case Internet, email or VoIP communication) or for a period of 12 months (in case of other communication).

Russia
A 2016 anti-terrorist federal law 374-FZ known as Yarovaya Law requires all telecommunication providers to store phone call, text and email metadata, as well as the actual voice recordings for up to 6 months. Messaging services like Whatsapp are required to provide cryptographic backdoors to law-enforcement. The law has been widely criticized both in Russia and abroad as an infringement of human rights and a waste of resources.

Norway
The EU's Data Retention Directive has been implemented into Norwegian law in 2011, but this will not be in effect before 1 January 2015.

Serbia
On 29 June 2010, the Serbian parliament adopted the Law on Electronic Communications, according to which the operator must keep the data on electronic communications for 12 months. This provision was criticized as unconstitutional by opposition parties and by Ombudsman Saša Janković.

Switzerland
As from 7 July 2016, the Swiss Federal Law about the Surveillance of the Post and Telecommunications entered into force, passed by the Swiss government on 18 March 2016.

Mobile phones
Swiss mobile phone operators have to retain the following data for six months according to the BÜPF:

Phone numbers of incoming and outgoing calls
SIM- (Subscriber Identity Module), IMSI- (International Mobile Subscribers Identity) and IMEI-numbers (International Mobile Equipment Identity)
„the location and the electrical boresight of the antenna of the mobile phone with which the monitored person is connected to the communications system at the time of the communication""
date, time and duration of the connection

Email
All Internet service providers must retain the following data for six months:

type of the connections (telephone, xDSL, Cable, permanent line etc.) and if known login data, address information of the origin (MAC address, telephone number), name, address and occupation of the user and duration of the connection from beginning to end
time of the transmission or reception of an email, header information according to the SMTP-protocol and the IP addresses of the sending and receiving email application.Email application refers to SMTP-, POP3-, IMAP4, webmail- and remail-server.

Exemptions
Switzerland only applies data retention to the largest Internet service providers with over 100 million CHF in annual Swiss-sourced revenue. This notably exempts derived communications providers such as ProtonMail, a popular encrypted email service based in Switzerland.

United States
The National Security Agency (NSA) commonly records Internet metadata for the whole planet for up to a year in its MARINA database, where it is used for pattern-of-life analysis. U.S. persons are not exempt because metadata are not considered data under US law (section 702 of the FISA Amendments Act). Its equivalent for phone records is MAINWAY. The NSA records SMS and similar text messages worldwide through DISHFIRE.

Leveraging commercial data retention
Various United States agencies leverage the (voluntary) data retention practised by many U.S. commercial organizations through programs such as PRISM and MUSCULAR.
Amazon is known to retain extensive data on customer transactions. Google is also known to retain data on searches, and other transactions. If a company is based in the United States the Federal Bureau of Investigation (FBI) can obtain access to such information by means of a National Security Letter (NSL). The Electronic Frontier Foundation states that ""NSLs are secret subpoenas issued directly by the FBI without any judicial oversight. These secret subpoenas allow the FBI to demand that online service providers or ecommerce companies produce records of their customers' transactions. The FBI can issue NSLs for information about people who haven't committed any crimes.
NSLs are practically immune to judicial review. They are accompanied by gag orders that allow no exception for talking to lawyers and provide no effective opportunity for the recipients to challenge them in court. This secret subpoena authority, which was expanded by the controversial USA PATRIOT Act, could be applied to nearly any online service provider for practically any type of record, without a court ever knowing"". The Washington Post has published a well researched article on the FBI's use of National Security Letters.

Failed mandatory ISP retention legislation attempts
The United States does not have any Internet Service Provider (ISP) mandatory data retention laws similar to the European Data Retention Directive, which was retroactively invalidated in 2014 by the Court of Justice of the European Union. Some attempts to create mandatory retention legislation have failed:

In 1999 two models of mandatory data retention were suggested for the United States: What IP address was assigned to a customer at a specific time. In the second model, ""which is closer to what Europe adopted"", telephone numbers dialed, contents of Web pages visited, and recipients of e-mail messages must be retained by the ISP for an unspecified amount of time.
The Internet Stopping Adults Facilitating the Exploitation of Today's Youth Act (SAFETY Act) of 2009 also known as H.R. 1076 and S.436 would require providers of ""electronic communication or remote computing services"" to ""retain for a period of at least two years all records or other information pertaining to the identity of a user of a temporarily assigned network address the service assigns to that user"". This bill never became a law.

Arguments against data retention
It is often argued that data retention is necessary to combat terrorism, and other crimes. Data retention may assist the police and security services to identify potential terrorists and their accomplices before or after an attack has taken place. For example, the authorities in Spain and the United Kingdom stated that retained telephony data made a significant contribution to police enquires into the 2004 Madrid train bombings and the 2005 London bombings.The opponents of data retention make the following arguments:

The Madrid train bombings can also be seen as proof that the current data retention level is sufficient and hence the EU directive is not necessity.
Schemes for data retention do not make provisions for adequate regulation of the data retention process and for independent judicial oversight.
Data retention is an invasion of privacy and a disproportionate response to the threat of terrorism.
It is easy for terrorists to avoid having their communications recorded. The Home Office Voluntary Code of Practice of Data Retention admits that there are some internet protocols which cannot be effectively monitored. It would be possible for terrorists to avoid monitoring by using anonymous P2P technologies, internet cafés, anonymous proxies or several other methods. Some police officers in the EU are sceptical about the value of data retention. For example, Heinz Kiefer, president of Eurocop, the European Confederation of Police, issued a press statement saying ""it remains easy for criminals to avoid detection through fairly simple means, for example mobile phone cards can be purchased from foreign providers and frequently switched. The result would be that a vast effort is made with little more effect on criminals and terrorists than to slightly irritate them. Activities like these are unlikely to boost citizens' confidence in the EU's ability to deliver solutions to their demand for protection against serious crime and terrorism"".
The hardware and software required to store all the retained data would be extremely costly. The costs of retaining data would not only fall on Internet Service Providers and telephone companies, but also on all companies and other organisations which would need to retain records of traffic passing through their switchboards and servers.
Data retention gives excessive power to the state to monitor the lives of individual citizens.
Data retention may be abused by the police to monitor the activities of any group which may come into conflict with the state; including ones which are engaged in legitimate protests. The UK police have used anti-terrorism powers against groups opposed to the war in Iraq and protesters at an arms fair. The definition of terrorism in the UK Terrorism Act 2000 includes not only action, but the threat of action, involving serious violence against a person, or serious damage to property, for the purposes of  advancing a ""political, religious or ideological cause"". There is concern that the definition is vaguely worded and could be applied to supporters of animal liberation, anti-war demonstrators, and many others.
Even if data retention may be justified, the retention periods proposed in some cases are excessive. It has been argued that a period of five days for web activity logs and ninety days for all other data would be adequate for police purposes.
Data retention by search engines provides an unfair advantage to dominant search engines.

Protection against data retention
The current directive proposal (see above) would force ISPs to record the internet communications of its users. The basic assumption is that this information can be used to identify with whom someone, whether innocent citizen or terrorist, communicated throughout a specific timespan. Believing that such as mandate would be useful is ignoring that some very committed community of crypto professionals has been preparing for such legislation for decades. Below are some strategies available today to anyone to protect themselves, avoid such traces, and render such expensive and legally dubious logging operations useless.

Anonymizing proxy services: Web
There are anonymizing proxies that provide slightly more private web access.  Proxies must use HTTPS encryption in order to provide any level of protection at all.  Unfortunately, proxies require the user to place a large amount of trust in the proxy operator (since they see everything the user does over HTTP), and may be subject to traffic analysis.

P2P communications
Some P2P services like file transfer or voice over IP use other computers to allow communication between computers behind firewalls. This means that trying to follow a call between two citizens might, mistakenly, identify a third citizen unaware of the communication.

Privacy enhancing tools
For security conscious citizens with some basic technical knowledge, tools like I2P – The Anonymous Network, Tor, Mixmaster and the cryptography options integrated into any many modern mail clients can be employed.
I2P is an international peer-to-peer anonymizing network, which aims at not only evading data retention, but also at making spying by other parties impossible. The structure is similar to the one TOR (see next paragraph) uses, but there are substantial differences. It protects better against traffic analysis and offers strong anonymity and for net-internal traffic end-to-end encryption. Due to unidirectional tunnels it is less prone to timing attacks than Tor. In I2P, several services are available: anonymous browsing, anonymous e-mails, anonymous instant messenger, anonymous file-sharing, and anonymous hosting of websites, among others.
Tor is a project of the U.S. non-profit Tor Project to develop and improve an onion routing network to shield its users from traffic analysis. Mixmaster is a remailer service that allows anonymous email sending.
JAP is a project very similar to Tor. It is designed to route web requests through several proxies to hide the end user's Internet address. Tor support has been included into JAP.

Initiative against extensive data retention
The Arbeitskreis Vorratsdatenspeicherung (German Working Group on Data Retention) is an association of civil rights campaigners, data protection activists and Internet users. The Arbeitskreis coordinates the campaign against the introduction of data retention in Germany.An analysis of federal Crime Agency (BKA) statistics published on 27 January 2010 by civil liberties NGO AK Vorrat revealed that data retention did not make a prosecution of serious crime any more effective.As the EU Commission is currently considering changes to the controversial EU data retention directive, a coalition of more than 100 civil liberties, data protection and human rights associations, jurists, trade unions and others are urging the Commission to propose the repeal of the EU requirements regarding data retention in favour of a system of expedited preservation and targeted collection of traffic data.

Plans for extending data retention to social networks
In November 2012, answers to a parliamentary inquiry in the German Bundestag revealed plans of some EU countries including France to extend data retention to chats and social media. Furthermore, the German Federal Office for the Protection of the Constitution (Germany's domestic intelligence agency) has confirmed that it has been working with the ETSI LI Technical Committee since 2003.

See also
Data security
Data Retention Directive
Data retention hardware
Data Protection Act 1998
Computer data storage
Customer proprietary network information
Data privacy
Electronic discovery
Lawful interception
Mass surveillance
NSA call database
Privacy
Secrecy of correspondence
Traffic analysis
I2P - The Anonymous Network

References
External links
Data Retention on the Open Rights Group wiki
The Politics of the EU Court Data Retention Opinion: End to Mass Surveillance?
Bowden, Caspar: ""Closed Circuit Television For Inside Your Head: Blanket Traffic Data Retention and the Emergency Anti-Terrorism Legislation"". Computer and Telecommunications Law Review, March 2002.
Boehm, F. and Cole, M.: Data Retention after the Judgement of the Court of Justice of the European Union (2014). (PDF-file)
Breyer, P.: ""Telecommunications Data Retention and Human Rights: The Compatibility of Blanket Traffic Data Retention with the ECHR"", European Law Journal, May 2005. (PDF-File, 82 KB)
Centre for European Policy Studies (CEP): Policy Brief on Data Retention (2011). (PDF-File)
Crump, C.: Data retention: privacy, anonymity, and accountability online (2003). 56 Stanford Law Review 191-229.
Cybertelecom :: Records Keeping / Data Retention
Digital Rights Ireland: Digital Rights Ireland's challenge against the EU Data Retention Directive and Irish retention legislation on the grounds of European and Irish constitutional law.
Electronic Privacy Information Center: EPIC data retention page (to 2007)
European Digital Rights: EDRI news tracking page on data retention (current)
Feiler, L.: The Data Retention Directive (2008). Seminar paper. (PDF-File)
Frost & Sullivan Whitepaper: ""Meeting the challenges of Data Retention: Now and in the future""
Ganj, C.: ""The Lives of Other Judges: Effects of the Romanian Data Retention Judgment"" (December 4, 2009). (PDF-File)
Goemans, C. and Dumortier, J.: ""Mandatory retention of traffic data in the EU: possible impact on privacy and on-line anonymity. Digital Anonymity and the Law, series IT & Law/2, T.M.C. Asser Press, 2003, p 161-183. (PDF-File)
Milford, P.: ""The Data Retention Directive: too fast, too furious a response? (2008). LLM Dissertation – Southampton Business School. (PDF-File)
Mitrou, L.: ""Communications Data Retention: A Pandora's Box for Rights and Liberties?"" From Digital Privacy: Theory, Technologies, and Practices edited by Alessandro Acquisti, Stefanos Gritzalis, Costos Lambrinoudakis and Sabrina di Vimercati. Auerbach Publications, 2008. (PDF-File)
Morariu, M.: How Secure is to Remain Private? On the Controversies of the European Data Retention Directive (2009). Amsterdam Social Science, Vol. 1(2): p. 46-65. (PDF-File)
Statewatch: The surveillance of telecommunications in the EU.
UK Data Retention Requirements with full references to legislation, codes of practice, etc.
UK Home Office: Consultation papers on data retention and on access to communications data.
Walker, C., & Akdeniz, Y.: ""Anti-Terrorism laws and data retention: war is over?"". Northern Ireland Legal Quarterly, 54(2), Summer edition 2003, 159-182. (PDF-File, 97 KB)
Working Group on Data Retention: List of documents relating to communications data retention in the EU (current)",https://en.wikipedia.org/wiki/Data_retention,"['All Wikipedia articles in need of updating', 'All articles with dead external links', 'All articles with unsourced statements', 'Articles with German-language sources (de)', 'Articles with dead external links from June 2016', 'Articles with dead external links from September 2017', 'Articles with obsolete information from November 2017', 'Articles with permanently dead external links', 'Articles with unsourced statements from February 2011', 'Articles with unsourced statements from May 2013', 'Articles with unsourced statements from May 2015', 'Articles with unsourced statements from October 2012', 'CS1 Danish-language sources (da)', 'CS1 Norwegian-language sources (no)', 'CS1 Swedish-language sources (sv)', 'CS1 errors: missing periodical', 'CS1 maint: archived copy as title', 'CS1 maint: multiple names: authors list', 'Data laws', 'Data retention', 'Intelligence analysis', 'Mass surveillance', 'Privacy of telecommunications', 'Telephony', 'Webarchive template wayback links']",Data Science
75,Decision tree,"A decision tree  is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.
Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.

Overview
A decision tree is a flowchart-like structure in which each internal node represents a ""test"" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.
In decision analysis, a decision tree and the closely related influence diagram are used as a visual and analytical decision support tool, where the expected values (or expected utility) of competing alternatives are calculated.
A decision tree consists of three types of nodes:
Decision nodes – typically represented by squares
Chance nodes – typically represented by circles
End nodes – typically represented by trianglesDecision trees are commonly used in operations research and operations management. If, in practice, decisions have to be taken online with no recall under incomplete knowledge, a decision tree should be paralleled by a probability model as a best choice model or online selection model algorithm. Another use of decision trees is as a descriptive means for calculating conditional probabilities.
Decision trees, influence diagrams, utility functions, and other decision analysis tools and methods are taught to undergraduate students in schools of business, health economics, and public health, and are examples of operations research or management science methods.

Decision tree building blocks
Decision tree elements
Drawn from left to right, a decision tree has only burst nodes (splitting paths) but no sink nodes (converging paths). Therefore, used manually, they can grow very big and are then often hard to draw fully by hand. Traditionally, decision trees have been created manually – as the aside example shows – although increasingly, specialized software is employed.

Decision rules
The decision tree can be linearized into decision rules, where the outcome is the contents of the leaf node, and the conditions along the path form a conjunction in the if clause. In general, the rules have the form:

if condition1 and condition2 and condition3 then outcome.Decision rules can be generated by constructing association rules with the target variable on the right. They can also denote temporal or causal relations.

Decision tree using flowchart symbols
Commonly a decision tree is drawn using flowchart symbols as it is easier for many to read and understand. Note there is a conceptual error in the ""Proceed"" calculation of the tree shown below; the error relates to the calculation of ""costs"" awarded in a legal action.

Analysis example
Analysis can take into account the decision maker's (e.g., the company's) preference or utility function, for example:

The basic interpretation in this situation is that the company prefers B's risk and payoffs under realistic risk preference coefficients (greater than $400K—in that range of risk aversion, the company would need to model a third strategy, ""Neither A nor B"").
Another example, commonly used in operations research courses, is the distribution of lifeguards on beaches (a.k.a. the ""Life's a Beach"" example). The example describes two beaches with lifeguards to be distributed on each beach. There is maximum budget B that can be distributed among the two beaches (in total), and using a marginal returns table, analysts can decide how many lifeguards to allocate to each beach.

In this example, a decision tree can be drawn to illustrate the principles of diminishing returns on beach #1.

The decision tree illustrates that when sequentially distributing lifeguards, placing a first lifeguard on beach #1 would be optimal if there is only the budget for 1 lifeguard. But if there is a budget for two guards, then placing both on beach #2 would prevent more overall drownings.

Influence diagram
Much of the information in a decision tree can be represented more compactly as an influence diagram, focusing attention on the issues and relationships between events.

Association rule induction
Decision trees can also be seen as generative models of induction rules from empirical data. An optimal decision tree is then defined as a tree that accounts for most of the data, while minimizing the number of levels (or ""questions""). Several algorithms to generate such optimal trees have been devised, such as ID3/4/5, CLS, ASSISTANT, and CART.

Advantages and disadvantages
Among decision support tools, decision trees (and influence diagrams) have several advantages. Decision trees:

Are simple to understand and interpret. People are able to understand decision tree models after a brief explanation.
Have value even with little hard data. Important insights can be generated based on experts describing a situation (its alternatives, probabilities, and costs) and their preferences for outcomes.
Help determine worst, best and expected values for different scenarios.
Use a white box model. If a given result is provided by a model.
Can be combined with other decision techniques.Disadvantages of decision trees:

They are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree.
They are often relatively inaccurate.  Many other predictors perform better with similar data.  This can be remedied by replacing a single decision tree with a random forest of decision trees, but a random forest is not as easy to interpret as a single decision tree.
For data including categorical variables with different number of levels, information gain in decision trees is biased in favor of those attributes with more levels.
Calculations can get very complex, particularly if many values are uncertain and/or if many outcomes are linked.

See also
References
External links
Extensive Decision Tree tutorials and examples
Gallery of example decision trees
Gradient Boosted Decision Trees",https://en.wikipedia.org/wiki/Decision_tree,"['Articles with short description', 'Commons category link is on Wikidata', 'Decision analysis', 'Decision trees', 'Short description is different from Wikidata', 'Use dmy dates from April 2020', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers']",Data Science
76,Decision tree learning,"Decision tree learning is one of the predictive modelling approaches used in statistics, data mining and machine learning. It uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Decision trees are among the most popular machine learning algorithms given their intelligibility and simplicity.In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data (but the resulting classification tree can be an input for decision making). This page deals with decision trees in data mining.

General
Decision tree learning is a method commonly used in data mining. The goal is to create a model that predicts the value of a target variable based on several input variables.
A decision tree is a simple representation for classifying examples. For this section, assume that all of the input features have finite discrete domains, and there is a single target feature called the ""classification"". Each element of the domain of the classification is called a class.
A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature. The arcs coming from a node labeled with an input feature are labeled with each of the possible values of the target feature or the arc leads to a subordinate decision node on a different input feature. Each leaf of the tree is labeled with a class or a probability distribution over the classes, signifying that the data set has been classified by the tree into either a specific class, or into a particular probability distribution (which, if the decision tree is well-constructed, is skewed towards certain subsets of classes).
A tree is built by splitting the source set, constituting the root node of the tree, into subsets—which constitute the successor children. The splitting is based on a set of splitting rules based on classification features.  This process is repeated on each derived subset in a recursive manner called recursive partitioning.
The recursion is completed when the subset at a node has all the same values of the target variable, or when splitting no longer adds value to the predictions. This process of top-down induction of decision trees (TDIDT) is an example of a greedy algorithm, and it is by far the most common strategy for learning decision trees from data.In data mining, decision trees can be described also as the combination of mathematical and computational techniques to aid the description, categorization and generalization of a given set of data.
Data comes in records of the form:

  
    
      
        (
        
          
            x
          
        
        ,
        Y
        )
        =
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        
          x
          
            3
          
        
        ,
        .
        .
        .
        ,
        
          x
          
            k
          
        
        ,
        Y
        )
      
    
    {\displaystyle ({\textbf {x}},Y)=(x_{1},x_{2},x_{3},...,x_{k},Y)}
  The dependent variable, 
  
    
      
        Y
      
    
    {\displaystyle Y}
  , is the target variable that we are trying to understand, classify or generalize. The vector 
  
    
      
        
          
            x
          
        
      
    
    {\displaystyle {\textbf {x}}}
   is composed of the features, 
  
    
      
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        
          x
          
            3
          
        
      
    
    {\displaystyle x_{1},x_{2},x_{3}}
   etc., that are used for that task.

Decision tree types
Decision trees used in data mining are of two main types:

Classification tree analysis is when the predicted outcome is the class (discrete) to which the data belongs.
Regression tree analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patient's length of stay in a hospital).The term Classification And Regression Tree (CART) analysis is an umbrella term used to refer to both of the above procedures, first introduced by Breiman et al. in 1984. Trees used for regression and trees used for classification have some similarities - but also some differences, such as the procedure used to determine where to split.Some techniques, often called ensemble methods, construct more than one decision tree:

Boosted trees Incrementally building an ensemble by training each new instance to emphasize the training instances previously mis-modeled. A typical example is AdaBoost. These can be used for regression-type and classification-type problems.
Bootstrap aggregated (or bagged) decision trees, an early ensemble method, builds multiple decision trees by repeatedly resampling training data with replacement, and voting the trees for a consensus prediction.A random forest classifier is a specific type of bootstrap aggregating
Rotation forest – in which every decision tree is trained by first applying principal component analysis (PCA) on a random subset of the input features.A special case of a decision tree is a decision list, which is a one-sided decision tree, so that every internal node has exactly 1 leaf node and exactly 1 internal node as a child (except for the bottommost node, whose only child is a single leaf node).  While less expressive, decision lists are arguably easier to understand than general decision trees due to their added sparsity, permit non-greedy learning methods and monotonic constraints to be imposed.Notable decision tree algorithms include:

ID3 (Iterative Dichotomiser 3)
C4.5 (successor of ID3)
CART (Classification And Regression Tree)
Chi-square automatic interaction detection (CHAID). Performs multi-level splits when computing classification trees.
MARS: extends decision trees to handle numerical data better.
Conditional Inference Trees. Statistics-based approach that uses non-parametric tests as splitting criteria, corrected for multiple testing to avoid overfitting. This approach results in unbiased predictor selection and does not require pruning.ID3 and CART were invented independently at around the same time (between 1970 and 1980), yet follow a similar approach for learning a decision tree from training tuples.
It has also been proposed to leverage concepts of fuzzy set theory for the definition of a special version of decision tree, known as Fuzzy Decision Tree (FDT). 
In this type of fuzzy classification, generally an input vector 
  
    
      
        
          
            x
          
        
      
    
    {\displaystyle {\textbf {x}}}
   is associated with multiple classes, each with a different confidence value.
Boosted ensembles of FDTs have been recently investigated as well, and they have shown performances comparable to those of other very efficient fuzzy classifiers.

Metrics
Algorithms for constructing decision trees usually work top-down, by choosing a variable at each step that best splits the set of items. Different algorithms use different metrics for measuring ""best"".  These generally measure the homogeneity of the target variable within the subsets. Some examples are given below. These metrics are applied to each candidate subset, and the resulting values are combined (e.g., averaged) to provide a measure of the quality of the split.

Gini impurity
Used by the CART (classification and regression tree) algorithm for classification trees, Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. The Gini impurity can be computed by summing the probability 
  
    
      
        
          p
          
            i
          
        
      
    
    {\displaystyle p_{i}}
   of an item with label 
  
    
      
        i
      
    
    {\displaystyle i}
    being chosen times the probability 
  
    
      
        
          ∑
          
            k
            ≠
            i
          
        
        
          p
          
            k
          
        
        =
        1
        −
        
          p
          
            i
          
        
      
    
    {\displaystyle \sum _{k\neq i}p_{k}=1-p_{i}}
   of a mistake in categorizing that item.  It reaches its minimum (zero) when all cases in the node fall into a single target category.
The Gini impurity is also an information theoretic measure and corresponds to Tsallis Entropy with deformation coefficient 
  
    
      
        q
        =
        2
      
    
    {\displaystyle q=2}
  , which in physics is associated with the lack of information in out-of-equilibrium, non-extensive, dissipative and quantum systems. For the limit 
  
    
      
        q
        →
        1
      
    
    {\displaystyle q\to 1}
   one recovers the usual Boltzmann-Gibbs or Shannon entropy. In this sense, the Gini impurity is but a variation of the usual entropy measure for decision trees.
To compute Gini impurity for a set of items with 
  
    
      
        J
      
    
    {\displaystyle J}
   classes, suppose 
  
    
      
        i
        ∈
        {
        1
        ,
        2
        ,
        .
        .
        .
        ,
        J
        }
      
    
    {\displaystyle i\in \{1,2,...,J\}}
  , and let 
  
    
      
        
          p
          
            i
          
        
      
    
    {\displaystyle p_{i}}
   be the fraction of items labeled with class 
  
    
      
        i
      
    
    {\displaystyle i}
   in the set.

  
    
      
        
          I
          
            G
          
        
        ⁡
        (
        p
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            J
          
        
        
          (
          
            
              p
              
                i
              
            
            
              ∑
              
                k
                ≠
                i
              
            
            
              p
              
                k
              
            
          
          )
        
        =
        
          ∑
          
            i
            =
            1
          
          
            J
          
        
        
          p
          
            i
          
        
        (
        1
        −
        
          p
          
            i
          
        
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            J
          
        
        (
        
          p
          
            i
          
        
        −
        
          
            
              p
              
                i
              
            
          
          
            2
          
        
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            J
          
        
        
          p
          
            i
          
        
        −
        
          ∑
          
            i
            =
            1
          
          
            J
          
        
        
          
            
              p
              
                i
              
            
          
          
            2
          
        
        =
        1
        −
        
          ∑
          
            i
            =
            1
          
          
            J
          
        
        
          
            
              p
              
                i
              
            
          
          
            2
          
        
      
    
    {\displaystyle \operatorname {I} _{G}(p)=\sum _{i=1}^{J}\left(p_{i}\sum _{k\neq i}p_{k}\right)=\sum _{i=1}^{J}p_{i}(1-p_{i})=\sum _{i=1}^{J}(p_{i}-{p_{i}}^{2})=\sum _{i=1}^{J}p_{i}-\sum _{i=1}^{J}{p_{i}}^{2}=1-\sum _{i=1}^{J}{p_{i}}^{2}}

Information gain
Used by the ID3, C4.5 and C5.0 tree-generation algorithms. Information gain is based on the concept of entropy and information content from information theory.
Entropy is defined as below

  
    
      
        
          H
        
        (
        T
        )
        =
        
          I
          
            E
          
        
        ⁡
        
          (
          
            
              p
              
                1
              
            
            ,
            
              p
              
                2
              
            
            ,
            .
            .
            .
            ,
            
              p
              
                J
              
            
          
          )
        
        =
        −
        
          ∑
          
            i
            =
            1
          
          
            J
          
        
        
          
            p
            
              i
            
          
          
            log
            
              2
            
          
          ⁡
          
            p
            
              i
            
          
        
      
    
    {\displaystyle \mathrm {H} (T)=\operatorname {I} _{E}\left(p_{1},p_{2},...,p_{J}\right)=-\sum _{i=1}^{J}{p_{i}\log _{2}p_{i}}}
  where 
  
    
      
        
          p
          
            1
          
        
        ,
        
          p
          
            2
          
        
        ,
        .
        .
        .
      
    
    {\displaystyle p_{1},p_{2},...}
  are fractions that add up to 1 and represent the percentage of each class present in the child node that results from a split in the tree.

  
    
      
        =
        −
        
          ∑
          
            i
            =
            1
          
          
            J
          
        
        
          p
          
            i
          
        
        
          log
          
            2
          
        
        ⁡
        
          
            p
            
              i
            
          
        
        −
        
          ∑
          
            i
            =
            1
          
          
            J
          
        
        −
        Pr
        (
        i
        
          |
        
        a
        )
        
          log
          
            2
          
        
        ⁡
        
          Pr
          (
          i
          
            |
          
          a
          )
        
      
    
    {\displaystyle =-\sum _{i=1}^{J}p_{i}\log _{2}{p_{i}}-\sum _{i=1}^{J}-\Pr(i|a)\log _{2}{\Pr(i|a)}}
  Averaging over the possible values of 
  
    
      
        A
      
    
    {\displaystyle A}
  ,

  
    
      
        =
        −
        
          ∑
          
            i
            =
            1
          
          
            J
          
        
        
          p
          
            i
          
        
        
          log
          
            2
          
        
        ⁡
        
          
            p
            
              i
            
          
        
        −
        
          ∑
          
            a
          
        
        
          p
          (
          a
          )
          
            ∑
            
              i
              =
              1
            
            
              J
            
          
          −
          Pr
          (
          i
          
            |
          
          a
          )
          
            log
            
              2
            
          
          ⁡
          
            Pr
            (
            i
            
              |
            
            a
            )
          
        
      
    
    {\displaystyle =-\sum _{i=1}^{J}p_{i}\log _{2}{p_{i}}-\sum _{a}{p(a)\sum _{i=1}^{J}-\Pr(i|a)\log _{2}{\Pr(i|a)}}}
  That is, the expected information gain is the mutual information, meaning that on average, the reduction in the entropy of T is the mutual information.
Information gain is used to decide which feature to split on at each step in building the tree. Simplicity is best, so we want to keep our tree small. To do so, at each step we should choose the split that results in the most consistent child nodes. A commonly used measure of consistency is called information which is measured in bits. For each node of the tree, the information value ""represents the expected amount of information that would be needed to specify whether a new instance should be classified yes or no, given that the example reached that node"".Consider an example data set with four attributes: outlook (sunny, overcast, rainy), temperature (hot, mild, cool), humidity (high, normal), and windy (true, false), with a binary (yes or no) target variable, play, and 14 data points. To construct a decision tree on this data, we need to compare the information gain of each of four trees, each split on one of the four features. The split with the highest information gain will be taken as the first split and the process will continue until all children nodes each have consistent data, or until the information gain is 0.
To find the information gain of the split using windy, we must first calculate the information in the data before the split. The original data contained nine yes's and five no's.

  
    
      
        
          I
          
            E
          
        
        (
        [
        9
        ,
        5
        ]
        )
        =
        −
        
          
            9
            14
          
        
        
          log
          
            2
          
          

          
        
        ⁡
        
          
            9
            14
          
        
        −
        
          
            5
            14
          
        
        
          log
          
            2
          
        
        ⁡
        
          
            5
            14
          
        
        =
        0.94
      
    
    {\displaystyle I_{E}([9,5])=-{\frac {9}{14}}\log _{2}^{}{\frac {9}{14}}-{\frac {5}{14}}\log _{2}{\frac {5}{14}}=0.94}
  The split using the feature windy results in two children nodes, one for a windy value of true and one for a windy value of false. In this data set, there are six data points with a true windy value, three of which have a play (where play is the target variable) value of yes and three with a play value of no. The eight remaining data points with a windy value of false contain two no's and six yes's. The information of the windy=true node is calculated using the entropy equation above. Since there is an equal number of yes's and no's in this node, we have

  
    
      
        
          I
          
            E
          
        
        (
        [
        3
        ,
        3
        ]
        )
        =
        −
        
          
            3
            6
          
        
        
          log
          
            2
          
          

          
        
        ⁡
        
          
            3
            6
          
        
        −
        
          
            3
            6
          
        
        
          log
          
            2
          
          

          
        
        ⁡
        
          
            3
            6
          
        
        =
        −
        
          
            1
            2
          
        
        
          log
          
            2
          
          

          
        
        ⁡
        
          
            1
            2
          
        
        −
        
          
            1
            2
          
        
        
          log
          
            2
          
          

          
        
        ⁡
        
          
            1
            2
          
        
        =
        1
      
    
    {\displaystyle I_{E}([3,3])=-{\frac {3}{6}}\log _{2}^{}{\frac {3}{6}}-{\frac {3}{6}}\log _{2}^{}{\frac {3}{6}}=-{\frac {1}{2}}\log _{2}^{}{\frac {1}{2}}-{\frac {1}{2}}\log _{2}^{}{\frac {1}{2}}=1}
  For the node where windy=false there were eight data points, six yes's and two no's. Thus we have

  
    
      
        
          I
          
            E
          
        
        (
        [
        6
        ,
        2
        ]
        )
        =
        −
        
          
            6
            8
          
        
        
          log
          
            2
          
          

          
        
        ⁡
        
          
            6
            8
          
        
        −
        
          
            2
            8
          
        
        
          log
          
            2
          
          

          
        
        ⁡
        
          
            2
            8
          
        
        =
        −
        
          
            3
            4
          
        
        
          log
          
            2
          
          

          
        
        ⁡
        
          
            3
            4
          
        
        −
        
          
            1
            4
          
        
        
          log
          
            2
          
          

          
        
        ⁡
        
          
            1
            4
          
        
        =
        0.81
      
    
    {\displaystyle I_{E}([6,2])=-{\frac {6}{8}}\log _{2}^{}{\frac {6}{8}}-{\frac {2}{8}}\log _{2}^{}{\frac {2}{8}}=-{\frac {3}{4}}\log _{2}^{}{\frac {3}{4}}-{\frac {1}{4}}\log _{2}^{}{\frac {1}{4}}=0.81}
  To find the information of the split, we take the weighted average of these two numbers based on how many observations fell into which node.

  
    
      
        
          I
          
            E
          
        
        (
        [
        3
        ,
        3
        ]
        ,
        [
        6
        ,
        2
        ]
        )
        =
        
          I
          
            E
          
        
        (
        
          windy or not
        
        )
        =
        
          
            6
            14
          
        
        ⋅
        1
        +
        
          
            8
            14
          
        
        ⋅
        0.81
        =
        0.89
      
    
    {\displaystyle I_{E}([3,3],[6,2])=I_{E}({\text{windy or not}})={\frac {6}{14}}\cdot 1+{\frac {8}{14}}\cdot 0.81=0.89}
  Now we can calculate the information gain achieved by splitting on the windy feature.

  
    
      
        I
        G
        (
        
          windy
        
        )
        =
        
          I
          
            E
          
        
        (
        [
        9
        ,
        5
        ]
        )
        −
        
          I
          
            E
          
        
        (
        [
        3
        ,
        3
        ]
        ,
        [
        6
        ,
        2
        ]
        )
        =
        0.94
        −
        0.89
        =
        0.05
      
    
    {\displaystyle IG({\text{windy}})=I_{E}([9,5])-I_{E}([3,3],[6,2])=0.94-0.89=0.05}
  To build the tree, the information gain of each possible first split would need to be calculated. The best first split is the one that provides the most information gain. This process is repeated for each impure node until the tree is complete. This example is adapted from the example appearing in Witten et al.

Variance reduction
Introduced in CART, variance reduction is often employed in cases where the target variable is continuous (regression tree), meaning that use of many other metrics would first require discretization before being applied. The variance reduction of a node N is defined as the total reduction of the variance of the target variable Y due to the split at this node:

  
    
      
        
          I
          
            V
          
        
        (
        N
        )
        =
        
          
            1
            
              
                |
              
              S
              
                
                  |
                
                
                  2
                
              
            
          
        
        
          ∑
          
            i
            ∈
            S
          
        
        
          ∑
          
            j
            ∈
            S
          
        
        
          
            1
            2
          
        
        (
        
          y
          
            i
          
        
        −
        
          y
          
            j
          
        
        
          )
          
            2
          
        
        −
        
          (
          
            
              
                
                  
                    |
                  
                  
                    S
                    
                      t
                    
                  
                  
                    
                      |
                    
                    
                      2
                    
                  
                
                
                  
                    |
                  
                  S
                  
                    
                      |
                    
                    
                      2
                    
                  
                
              
            
            
              
                1
                
                  
                    |
                  
                  
                    S
                    
                      t
                    
                  
                  
                    
                      |
                    
                    
                      2
                    
                  
                
              
            
            
              ∑
              
                i
                ∈
                
                  S
                  
                    t
                  
                
              
            
            
              ∑
              
                j
                ∈
                
                  S
                  
                    t
                  
                
              
            
            
              
                1
                2
              
            
            (
            
              y
              
                i
              
            
            −
            
              y
              
                j
              
            
            
              )
              
                2
              
            
            +
            
              
                
                  
                    |
                  
                  
                    S
                    
                      f
                    
                  
                  
                    
                      |
                    
                    
                      2
                    
                  
                
                
                  
                    |
                  
                  S
                  
                    
                      |
                    
                    
                      2
                    
                  
                
              
            
            
              
                1
                
                  
                    |
                  
                  
                    S
                    
                      f
                    
                  
                  
                    
                      |
                    
                    
                      2
                    
                  
                
              
            
            
              ∑
              
                i
                ∈
                
                  S
                  
                    f
                  
                
              
            
            
              ∑
              
                j
                ∈
                
                  S
                  
                    f
                  
                
              
            
            
              
                1
                2
              
            
            (
            
              y
              
                i
              
            
            −
            
              y
              
                j
              
            
            
              )
              
                2
              
            
          
          )
        
      
    
    {\displaystyle I_{V}(N)={\frac {1}{|S|^{2}}}\sum _{i\in S}\sum _{j\in S}{\frac {1}{2}}(y_{i}-y_{j})^{2}-\left({\frac {|S_{t}|^{2}}{|S|^{2}}}{\frac {1}{|S_{t}|^{2}}}\sum _{i\in S_{t}}\sum _{j\in S_{t}}{\frac {1}{2}}(y_{i}-y_{j})^{2}+{\frac {|S_{f}|^{2}}{|S|^{2}}}{\frac {1}{|S_{f}|^{2}}}\sum _{i\in S_{f}}\sum _{j\in S_{f}}{\frac {1}{2}}(y_{i}-y_{j})^{2}\right)}
  where 
  
    
      
        S
      
    
    {\displaystyle S}
  , 
  
    
      
        
          S
          
            t
          
        
      
    
    {\displaystyle S_{t}}
  , and 
  
    
      
        
          S
          
            f
          
        
      
    
    {\displaystyle S_{f}}
   are the set of presplit sample indices, set of sample indices for which the split test is true, and set of sample indices for which the split test is false, respectively. Each of the above summands are indeed variance estimates, though, written in a form without directly referring to the mean.

Measure of ""goodness""
Used by CART in 1984, the measure of ""goodness"" is a function that seeks to optimize the balance of a candidate split's capacity to create pure children with its capacity to create equally-sized children. This process is repeated for each impure node until the tree is complete. The function 
  
    
      
        ϕ
        (
        s
        
          |
        
        t
        )
      
    
    {\displaystyle \phi (s|t)}
  , where 
  
    
      
        s
      
    
    {\displaystyle s}
   is a candidate split at node 
  
    
      
        t
      
    
    {\displaystyle t}
  , is defined as below

  
    
      
        ϕ
        (
        s
        
          |
        
        t
        )
        =
        2
        
          P
          
            L
          
        
        
          P
          
            R
          
        
        
          ∑
          
            j
            =
            1
          
          
            C
            l
            a
            s
            s
            C
            o
            u
            n
            t
          
        
        
          |
        
        P
        (
        j
        
          |
        
        
          t
          
            L
          
        
        )
        −
        P
        (
        j
        
          |
        
        
          t
          
            R
          
        
        )
        
          |
        
      
    
    {\displaystyle \phi (s|t)=2P_{L}P_{R}\sum _{j=1}^{ClassCount}|P(j|t_{L})-P(j|t_{R})|}
  where 
  
    
      
        
          t
          
            L
          
        
      
    
    {\displaystyle t_{L}}
   and 
  
    
      
        
          t
          
            R
          
        
      
    
    {\displaystyle t_{R}}
   are the left and right children of node 
  
    
      
        t
      
    
    {\displaystyle t}
   using split 
  
    
      
        s
      
    
    {\displaystyle s}
  , respectively; 
  
    
      
        
          P
          
            L
          
        
      
    
    {\displaystyle P_{L}}
   and 
  
    
      
        
          P
          
            R
          
        
      
    
    {\displaystyle P_{R}}
   are the proportions of records in 
  
    
      
        t
      
    
    {\displaystyle t}
   in 
  
    
      
        
          t
          
            L
          
        
      
    
    {\displaystyle t_{L}}
   and 
  
    
      
        
          t
          
            R
          
        
      
    
    {\displaystyle t_{R}}
  , respectively; and 
  
    
      
        P
        (
        j
        
          |
        
        
          t
          
            L
          
        
        )
      
    
    {\displaystyle P(j|t_{L})}
   and 
  
    
      
        P
        (
        j
        
          |
        
        
          t
          
            R
          
        
        )
      
    
    {\displaystyle P(j|t_{R})}
   are the proportions of class 
  
    
      
        j
      
    
    {\displaystyle j}
   records in 
  
    
      
        
          t
          
            L
          
        
      
    
    {\displaystyle t_{L}}
   and 
  
    
      
        
          t
          
            R
          
        
      
    
    {\displaystyle t_{R}}
  , respectively.
Consider an example data set with three attributes: savings(low, medium, high), assets(low, medium, high), income(numerical value), and a binary target variable credit risk(good, bad) and 8 data points. The full data is presented in the table below. To start a decision tree, we will calculate the maximum value of 
  
    
      
        ϕ
        (
        s
        
          |
        
        t
        )
      
    
    {\displaystyle \phi (s|t)}
   using each feature to find which one will split the root node. This process will continue until all children are pure or all 
  
    
      
        ϕ
        (
        s
        
          |
        
        t
        )
      
    
    {\displaystyle \phi (s|t)}
   values are below a set threshold.

To find 
  
    
      
        ϕ
        (
        s
        
          |
        
        t
        )
      
    
    {\displaystyle \phi (s|t)}
   of the feature savings, we need to note the quantity of each value. The original data contained three low's, three medium's, and two high's. Out of the low's, one had a good credit risk while out of the medium's and high's, 4 had a good credit risk. Assume a candidate split 
  
    
      
        s
      
    
    {\displaystyle s}
   such that records with a low savings will be put in the left child and all other records will be put into the right child.

  
    
      
        ϕ
        (
        s
        
          |
        
        r
        o
        o
        t
        )
        =
        2
        
          
            3
            8
          
        
        ∗
        
          
            5
            8
          
        
        ∗
        (
        
          |
        
        (
        
          
            1
            3
          
        
        −
        
          
            4
            5
          
        
        )
        
          |
        
        +
        
          |
        
        (
        
          
            2
            3
          
        
        −
        
          
            1
            5
          
        
        )
        
          |
        
        )
        =
        0.44
      
    
    {\displaystyle \phi (s|root)=2{\frac {3}{8}}*{\frac {5}{8}}*(|({\frac {1}{3}}-{\frac {4}{5}})|+|({\frac {2}{3}}-{\frac {1}{5}})|)=0.44}
  To build the tree, the ""goodness"" of all candidate splits for the root node need to be calculated. The candidate with the maximum value will split the root node, and the process will continue for each impure node until the tree is complete.
Compared to other metrics such as information gain, the measure of ""goodness"" will attempt to create a more balanced tree, leading to more-consistent decision time. However, it sacrifices some priority for creating pure children which can lead to additional splits that are not present with other metrics.

Uses
Advantages
Amongst other data mining methods, decision trees have various advantages:

Simple to understand and interpret. People are able to understand decision tree models after a brief explanation. Trees can also be displayed graphically in a way that is easy for non-experts to interpret.
Able to handle both numerical and categorical data. Other techniques are usually specialized in analyzing datasets that have only one type of variable. (For example, relation rules can be used only with nominal variables while neural networks can be used only with numerical variables or categoricals converted to 0-1 values.) Early decision trees were only capable of handling categorical variables, but more recent versions, such as C4.5, do not have this limitation.
Requires little data preparation. Other techniques often require data normalization. Since trees can handle qualitative predictors, there is no need to create dummy variables.
Uses a white box or open-box model. If a given situation is observable in a model the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model, the explanation for the results is typically difficult to understand, for example with an artificial neural network.
Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.
Non-statistical approach that makes no assumptions of the training data or prediction residuals; e.g., no distributional, independence, or constant variance assumptions
Performs well with large datasets. Large amounts of data can be analyzed using standard computing resources in reasonable time.
Mirrors human decision making more closely than other approaches. This could be useful when modeling human decisions/behavior.
Robust against co-linearity, particularly boosting
In built feature selection. Additional irrelevant feature will be less used so that they can be removed on subsequent runs. The hierarchy of attributes in a decision tree reflects the importance of attributes. It means that the features on top are the most informative.
Decision trees can approximate any Boolean function e.g. XOR.

Limitations
Trees can be very non-robust. A small change in the training data can result in a large change in the tree and consequently the final predictions.
The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristics such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. To reduce the greedy effect of local optimality, some methods such as the dual information distance (DID) tree were proposed.
Decision-tree learners can create over-complex trees that do not generalize well from the training data.  (This is known as overfitting.)  Mechanisms such as pruning are necessary to avoid this problem (with the exception of some algorithms such as the Conditional Inference approach, that does not require pruning).
The average depth of the tree that is defined by the number of nodes or tests till classification is not guaranteed to be minimal or small under various splitting criteria.
For data including categorical variables with different numbers of levels, information gain in decision trees is biased in favor of attributes with more levels. However, the issue of biased predictor selection is avoided by the Conditional Inference approach, a two-stage approach, or adaptive leave-one-out feature selection.

Implementations
Many data mining software packages provide implementations of one or more decision tree algorithms.
Examples include 

Salford Systems CART (which licensed the proprietary code of the original CART authors),
IBM SPSS Modeler,
RapidMiner,
SAS Enterprise Miner,
Matlab,
R (an open-source software environment for statistical computing, which includes several CART implementations such as rpart, party and randomForest packages),
Weka (a free and open-source data-mining suite, contains many decision tree algorithms),
Orange,
KNIME,
Microsoft SQL Server [1], and
scikit-learn (a free and open-source machine learning library for the Python programming language).

Extensions
Decision graphs
In a decision tree, all paths from the root node to the leaf node proceed by way of conjunction, or AND. In a decision graph, it is possible to use disjunctions (ORs) to join two more paths together using minimum message length (MML).  Decision graphs have been further extended to allow for previously unstated new attributes to be learnt dynamically and used at different places within the graph.  The more general coding scheme results in better predictive accuracy and log-loss probabilistic scoring.  In general, decision graphs infer models with fewer leaves than decision trees.

Alternative search methods
Evolutionary algorithms have been used to avoid local optimal decisions and search the decision tree space with little a priori bias.It is also possible for a tree to be sampled using MCMC.The tree can be searched for in a bottom-up fashion. Or several trees can be constructed parallelly to reduce the expected number of tests till classification.

See also
References
Further reading
James, Gareth; Witten, Daniela; Hastie, Trevor; Tibshirani, Robert (2017). ""Tree-Based Methods"" (PDF). An Introduction to Statistical Learning: with Applications in R. New York: Springer. pp. 303–336. ISBN 978-1-4614-7137-0.

External links
Evolutionary Learning of Decision Trees in C++
A very detailed explanation of information gain as splitting criterion",https://en.wikipedia.org/wiki/Decision_tree_learning,"['All articles with unsourced statements', 'Articles with short description', 'Articles with unsourced statements from August 2014', 'Articles with unsourced statements from December 2019', 'Articles with unsourced statements from January 2012', 'Articles with unsourced statements from October 2017', 'CS1 maint: multiple names: authors list', 'Classification algorithms', 'Decision trees', 'Short description matches Wikidata']",Data Science
77,DeepDream,"DeepDream is a computer vision program created by Google engineer Alexander Mordvintsev that uses a convolutional neural network to find and enhance patterns in images via algorithming a dream-like hallucinogenic appearance in the deliberately over-processed images.Google's program popularized the term (deep) ""dreaming"" to refer to the generation of images that produce desired activations in a trained deep network, and the term now refers to a collection of related approaches.

History
The DeepDream software, originated in a deep convolutional network codenamed ""Inception"" after the film of the same name, was developed for the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) in 2014 and released in July 2015.
The dreaming idea and name became popular on the internet in 2015 thanks to Google's DeepDream program.  The idea dates from early in the history of neural networks, and similar methods have been used to synthesize visual textures.
Related visualization ideas were developed (prior to Google's work) by several research groups.After Google published their techniques and made their code open-source, a number of tools in the form of web services, mobile applications, and desktop software appeared on the market to enable users to transform their own photos.

Process
The software is designed to detect faces and other patterns in images, with the aim of automatically classifying images. However, once trained, the network can also be run in reverse, being asked to adjust the original image slightly so that a given output neuron (e.g. the one for faces or certain animals) yields a higher confidence score. This can be used for visualizations to understand the emergent structure of the neural network better, and is the basis for the DeepDream concept. This reversal procedure is never perfectly clear and unambiguous because it utilizes a one-to-many mapping process. However, after enough reiterations, even imagery initially devoid of the sought features will be adjusted enough that a form of pareidolia results, by which psychedelic and surreal images are generated algorithmically. The optimization resembles backpropagation, however instead of adjusting the network weights, the weights are held fixed and the input is adjusted.
For example, an existing image can be altered so that it is ""more cat-like"", and the resulting enhanced image can be again input to the procedure. This usage resembles the activity of looking for animals or other patterns in clouds.
Applying gradient descent independently to each pixel of the input produces images in which
adjacent pixels have little relation and thus the image has too much high frequency information.
The generated images can be greatly improved by including a prior or regularizer that prefers inputs
that have natural image statistics (without a preference for any particular image), or are simply smooth.
For example, Mahendran et al. used the total variation regularizer that prefers images that are piecewise constant. Various regularizers are discussed further in. An in-depth, visual exploration of feature visualization and regularization techniques was published more recently.The cited resemblance of the imagery to LSD- and psilocybin-induced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex.

Usage
The dreaming idea can be applied to hidden (internal) neurons other than those in the output, 
which allows exploration of the roles and representations of various parts of the network.
It is also possible to optimize the input to satisfy either a single neuron (this usage is sometimes called Activity Maximization) or an entire layer of neurons.
While dreaming is most often used for visualizing networks or producing computer art, it has recently been proposed that adding ""dreamed"" inputs to the training set can improve training times for abstractions in Computer Science.The DeepDream model has also been demonstrated to have application in the field of art history.DeepDream was used for Foster the People's music video for the song ""Doing It for the Money"".In 2017, a research group out of the University of Sussex created a Hallucination Machine, applying the DeepDream algorithm to a pre-recorded panoramic video, allowing users to explore virtual reality environments to mimic the experience of psychoactive substances and/or psychopathological conditions.  They were able to demonstrate that the subjective experiences induced by the Hallucination Machine differed significantly from control (non-‘hallucinogenic’) videos, while bearing phenomenological similarities to the psychedelic state (following administration of psilocybin).

See also
Feature detection (computer vision)
Neural Style Transfer
Procedural textures
Texture synthesis

References
External links
Deep Dream, python notebook on GitHub
Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (June 17, 2015). ""Inceptionism: Going Deeper into Neural Networks"". Archived from the original on 2015-07-03.",https://en.wikipedia.org/wiki/DeepDream,"['2015 software', 'Algorithmic art', 'Articles with short description', 'Artificial intelligence', 'Commons category link is on Wikidata', 'Computer art', 'Computer vision software', 'Free software', 'Google software', 'Neural network software', 'Object recognition and categorization', 'Psychedelic art', 'Short description matches Wikidata']",Data Science
78,David Donoho,"David Leigh Donoho (born March 5, 1957) is a professor of statistics at Stanford University, where he is also the Anne T. and Robert M. Bass Professor in the Humanities and Sciences. His work includes the development of effective methods for the construction of low-dimensional representations for high-dimensional data problems (multiscale geometric analysis), development of wavelets for denoising and compressed sensing. He was elected a Member of the American Philosophical Society in 2019.

Academic biography
Donoho did his undergraduate studies at Princeton University, graduating in 1978. His undergraduate thesis advisor was John W. Tukey. Donoho obtained his Ph.D. from Harvard University in 1983, under the supervision of Peter J. Huber. He was on the faculty of the University of California, Berkeley, from 1984 to 1990 before moving to Stanford.
He has been the Ph.D. advisor of at least 20 doctoral students, including Jianqing Fan and Emmanuel Candès.

Awards and honors
In 1991, Donoho was named a MacArthur Fellow. He was elected a Fellow of the American Academy of Arts and Sciences in 1992. He was the winner of the COPSS Presidents' Award in 1994. In 2001, he won the John von Neumann Prize of the Society for Industrial and Applied Mathematics. In 2002, he was appointed to the Bass professorship. He was elected a SIAM Fellow and a foreign associate of the French Académie des sciences in 2009, and in the same year received an honorary doctorate from the University of Chicago. In 2010 he won the Norbert Wiener Prize in Applied Mathematics, given jointly by SIAM and the American Mathematical Society. He is also a member of the United States National Academy of Sciences. In 2012 he became a fellow of the American Mathematical Society. In 2013 he was awarded the Shaw Prize for Mathematics. In 2016, he was awarded an honorary degree at the University of Waterloo. In 2018, he was awarded the Gauss Prize from IMU.

References
External links
David Donoho at the Mathematics Genealogy Project
David Donoho professional home page
Videos on International Congress of Mathematicians 2002, Beijing",https://en.wikipedia.org/wiki/David_Donoho,"['1957 births', 'American statisticians', 'Articles with hCards', 'Fellows of the American Academy of Arts and Sciences', 'Fellows of the American Mathematical Society', 'Harvard University alumni', 'Living people', 'MacArthur Fellows', 'Members of the American Philosophical Society', 'Members of the French Academy of Sciences', 'Members of the United States National Academy of Sciences', 'Princeton University alumni', 'Stanford University Department of Statistics faculty', 'University of California, Berkeley College of Letters and Science faculty', 'Wikipedia articles with DBLP identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MGP identifiers', 'Wikipedia articles with ORCID identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers']",Data Science
79,Dimensionality reduction,"Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable. Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.Methods are commonly divided into linear and non-linear approaches. Approaches can also be divided into feature selection and feature extraction. Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.

Feature selection
Feature selection approaches try to find a subset of the input variables (also called features or attributes). The three strategies are: the filter strategy (e.g. information gain), the wrapper strategy (e.g. search guided by accuracy), and the embedded strategy (selected features add or are removed while building the model based on prediction errors).
Data analysis such as regression or classification can be done in the reduced space more accurately than in the original space.

Feature projection
Feature projection (also called Feature extraction) transforms the data from the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist. For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning.

Principal component analysis (PCA)
The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the covariance (and sometimes the correlation) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system, because they often contribute the vast majority of the system's energy, especially in low-dimensional systems. Still, this must be proven on a case-by-case basis as not all systems exhibit this behavior.   The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors.

Non-negative matrix factorization (NMF)
NMF decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist, such as astronomy. NMF is well known since the multiplicative update rule by Lee & Seung, which has been continuously developed: the inclusion of uncertainties, the consideration of missing data and parallel computation, sequential construction which leads to the stability and linearity of NMF, as well as other updates including handling missing data in digital image processing.With a stable component basis during construction, and a linear modeling process, sequential NMF is able to preserve the flux in direct imaging of circumstellar structures in astromony, as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks. In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA as demonstrated by Ren et al.

Kernel PCA
Principal component analysis can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is entitled kernel PCA.

Graph-based kernel PCA
Other prominent nonlinear techniques include manifold learning techniques such as Isomap, locally linear embedding (LLE), Hessian LLE, Laplacian eigenmaps, and methods based on tangent space analysis. These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA.
More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using semidefinite programming. The most prominent example of such a technique is maximum variance unfolding (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space), while maximizing the distances between points that are not nearest neighbors.
An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical multidimensional scaling, which is identical to PCA; Isomap, which uses geodesic distances in the data space; diffusion maps, which use diffusion distances in the data space; t-distributed stochastic neighbor embedding (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis.
A different approach to nonlinear dimensionality reduction is through the use of autoencoders, a special kind of feed-forward neural networks with a bottle-neck hidden layer. The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of restricted Boltzmann machines) that is followed by a finetuning stage based on backpropagation.

Linear discriminant analysis (LDA)
Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.

Generalized discriminant analysis (GDA)
GDA deals with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the support vector machines (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space. Similar to LDA, the objective of GDA is to find a projection for the features into a lower dimensional space by maximizing the ratio of between-class scatter to within-class scatter.

Autoencoder
Autoencoders can be used to learn non-linear dimension reduction functions and codings together with an inverse function from the coding to the original representation.

t-SNE
T-distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear dimensionality reduction technique useful for visualization of high-dimensional datasets. It is not recommended for use in analysis such as clustering or outlier detection since it does not necessarily preserve densities or distances well.

UMAP
Uniform manifold approximation and projection (UMAP) is a nonlinear dimensionality reduction technique. Visually, it is similar to t-SNE, but it assumes that the data is uniformly distributed on a locally connected Riemannian manifold and that the Riemannian metric is locally constant or approximately locally constant.

Dimension reduction
For high-dimensional datasets (i.e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a K-nearest neighbors algorithm (k-NN) in order to avoid the effects of the curse of dimensionality.Feature extraction and  dimension reduction can be combined in one step using principal component analysis (PCA),  linear discriminant analysis (LDA), canonical correlation analysis (CCA), or non-negative matrix factorization (NMF) techniques as a pre-processing step followed by clustering by K-NN on feature vectors in reduced-dimension space. In machine learning this process is also called low-dimensional embedding.For very-high-dimensional datasets (e.g. when performing similarity search on live video streams, DNA data or high-dimensional time series) running a fast approximate K-NN search using locality sensitive hashing, random projection, ""sketches""  or other high-dimensional similarity search  techniques from the VLDB toolbox might be the only feasible option.

Applications
A dimensionality reduction technique that is sometimes used in neuroscience is maximally informative dimensions, which finds a lower-dimensional representation of a dataset such that as much information as possible about the original data is preserved.

See also
Notes
References
External links
JMLR Special Issue on Variable and Feature Selection
ELastic MAPs
Locally Linear Embedding
Visual Comparison of various dimensionality reduction methods
A Global Geometric Framework for Nonlinear Dimensionality Reduction",https://en.wikipedia.org/wiki/Dimensionality_reduction,"['All articles with unsourced statements', 'Articles with short description', 'Articles with unsourced statements from June 2017', 'Articles with unsourced statements from September 2017', 'Dimension reduction', 'Machine learning', 'Short description matches Wikidata']",Data Science
80,Deep learning,"Deep learning  (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, machine vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.  Specifically, neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analogue.The adjective ""deep"" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, and then that a network with a nonpolynomial activation function with one hidden layer of unbounded width can on the other hand so be. Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, whence the ""structured"" part.

Definition
Deep learning is a class of machine learning algorithms that uses multiple layers to progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.

Overview
Most modern deep learning models are based on artificial neural networks, specifically convolutional neural networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. (Of course, this does not completely eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.)The word ""deep"" in ""deep learning"" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.
Deep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.For supervised learning tasks, deep learning methods eliminate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.
Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are neural history compressors and deep belief networks.

Interpretations
Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as the rectified linear unit.The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; If the width is smaller or equal to the input dimension, then deep neural network is not a universal approximator.
The probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.

History
The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling. Other deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980.The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970, to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.By 1991 such systems were used for recognizing isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng et al. suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron, a method for performing 3-D object recognition in cluttered scenes. Because it directly used natural images, Cresceptron started the beginning of general-purpose visual learning for natural 3D worlds. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron learned an open number of features in each layer without supervision, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization.
In 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton. Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.Since 1997, Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities.
Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural network's (ANN) computational cost and a lack of understanding of how the brain wires its biological networks.
Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power.
Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier, representing the first major industrial application of deep learning.The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.Many aspects of speech recognition were taken over by a deep learning method called long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in 1997. LSTM RNNs avoid the vanishing gradient problem and can learn ""Very Deep Learning"" tasks that require memories of events that happened thousands of discrete time steps before, which is important for speech. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks. Later it was combined with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets.
Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks (CNNs) were superseded for ASR by CTC for LSTM. but are more successful in computer vision.
The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010.
The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition, eventually leading to pervasive and dominant use in that industry. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Advances in hardware have driven renewed interest in deep learning. In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).” That year, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular, GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days. Further, specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models.

Deep learning revolution
In 2012, a team led by George E. Dahl won the ""Merck Molecular Activity Challenge"" using multi-task deep neural networks to predict the biomolecular target of one drug. In 2014, Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the ""Tox21 Data Challenge"" of NIH, FDA and NCATS.Significant additional impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs on GPUs were needed to progress on computer vision. In 2011, this approach achieved for the first time superhuman performance in a visual pattern recognition contest. Also in 2011, it won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest. Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In October 2012, a similar system by Krizhevsky et al. won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. In November 2012, Ciresan et al.'s system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic. In 2013 and 2014, the error rate on the ImageNet task using deep learning was further reduced, following a similar trend in large-scale speech recognition. 
Image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.Some researchers state that the October 2012 ImageNet victory anchored the start of a ""deep learning revolution"" that has transformed the AI industry.In March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.

Neural networks
Artificial neural networks
Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.
An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.
Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.
The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.
Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, playing ""Go"" ).

Deep neural networks
A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components functioning similar to the human brains and can be trained like any other ML algorithm.For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name ""deep"" networks.
DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.
DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or ""weights"", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.
Recurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use.Convolutional deep neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).

Challenges
As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.
DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay (
  
    
      
        
          ℓ
          
            2
          
        
      
    
    {\displaystyle \ell _{2}}
  -regularization) or sparsity (
  
    
      
        
          ℓ
          
            1
          
        
      
    
    {\displaystyle \ell _{1}}
  -regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights for CMAC. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.

Hardware
Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.

Applications
Automatic speech recognition
Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn ""Very Deep Learning"" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.

The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:
Scale-up/out and accelerated DNN training and decoding
Sequence discriminative training
Feature processing by deep models with solid understanding of the underlying mechanisms
Adaptation of DNNs and related deep models
Multi-task and transfer learning by DNNs and related deep models
CNNs and how to design them to best exploit domain knowledge of speech
RNN and its rich LSTM variants
Other types of deep models including tensor-based models and integrated deep generative/discriminative models.All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.

Image recognition
A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.Deep learning-based image recognition has become ""superhuman"", producing more accurate results than human contestants. This first occurred in 2011.Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.

Visual art processing
Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of a) identifying the style period of a given painting, b) Neural Style Transfer –  capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video, and c) generating striking imagery based on random visual input fields.

Natural language processing
Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, Text classification and others.Recent developments generalize word embedding to sentence embedding.
Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system ""learns from millions of examples."" It translates ""whole sentences at a time, rather than pieces. Google Translate supports over one hundred languages. The network encodes the ""semantics of the sentence rather than simply memorizing phrase-to-phrase translations"". GT uses English as an intermediate between most language pairs.

Drug discovery and toxicology
A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets, off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.AtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.

Customer relationship management
Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.

Recommendation systems
Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.

Bioinformatics
An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.In medical informatics, deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data.

Medical Image Analysis
Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement.

Mobile advertising
Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.

Image restoration
Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as ""Shrinkage Fields for Effective Image Restoration"" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.

Financial fraud detection
Deep learning is being successfully applied to financial fraud detection and anti-money laundering. ""Deep anti-money laundering detection system can spot and recognize relationships and similarities between data and, further down the road, learn to detect anomalies or classify and predict specific events"". The solution leverages both supervised learning techniques, such as the classification of suspicious transactions, and unsupervised learning, e.g. anomaly detection.

Military
The United States Department of Defense applied deep learning to train robots in new tasks through observation.

Relation to human cognitive and brain development
Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, ""...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature.""A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality. In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations. Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels.

Commercial activity
Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages.
In 2015, Blippar demonstrated a mobile augmented reality application that uses deep learning to recognize objects in real time.In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as “good job” and “bad job.”

Criticism and comment
Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.

Theory
A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.
Others point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed for realizing this goal entirely. Research psychologist Gary Marcus noted:""Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.""
In further reference to the idea that artistic sensitivity might inhere within relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's website.

Errors
Some deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images and misclassifying minuscule perturbations of correctly classified images. Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI).

Cyber threat
As deep learning moves from the lab into the world, research and experience shows that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such a manipulation is termed an “adversarial attack.”In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.Another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address that would download malware.In “data poisoning,” false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.

Reliance on human microwork
Most Deep Learning systems rely on training and verification data that is generated and/or annotated by humans. It has been argued in media philosophy that not only low-paid clickwork (e.g. on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer Mühlhoff distinguishes five types of ""machinic capture"" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) ""trapping and tracking"" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.Mühlhoff argues that in most commercial end-user applications of Deep Learning such as Facebook's face recognition system, the need for training data does not stop once an ANN is trained. Rather, there is a continued demand for human-generated verification data to constantly calibrate and update the ANN. For this purpose Facebook introduced the feature that once a user is automatically recognized in an image, they receive a notification. They can choose whether of not they like to be publicly labeled on the image, or tell Facebook that it is not them in the picture. This user interface is a mechanism to generate ""a constant stream of  verification data"" to further train the network in real-time. As Mühlhoff argues, involvement of human users to generate training and verification data is so typical for most commercial end-user applications of Deep Learning that such systems may be referred to as ""human-aided artificial intelligence"".

See also
Applications of artificial intelligence
Comparison of deep learning software
Compressed sensing
Differentiable programming
Echo state network
List of artificial intelligence projects
Liquid state machine
List of datasets for machine learning research
Reservoir computing
Sparse coding

References


== Further reading ==",https://en.wikipedia.org/wiki/Deep_learning,"['All articles covered by WikiProject Wikify', 'All articles needing references cleanup', 'All articles that are too technical', 'All articles with unsourced statements', 'Articles covered by WikiProject Wikify from June 2020', 'Articles prone to spam from June 2015', 'Articles with short description', 'Articles with unsourced statements from July 2016', 'Articles with unsourced statements from June 2020', 'Articles with unsourced statements from November 2020', 'Artificial intelligence', 'Artificial neural networks', 'CS1: long volume value', 'CS1 errors: missing periodical', 'CS1 maint: archived copy as title', 'Deep learning', 'Emerging technologies', 'Short description matches Wikidata', 'Wikipedia articles that are too technical from July 2016', 'Wikipedia references cleanup from June 2020']",Data Science
81,Distributed computing,"Distributed computing is a field of computer science that studies distributed systems. A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another from any system. The components interact with one another in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.
A computer program that runs within a distributed system is called  a distributed program (and distributed programming is the process of writing such programs). There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other via message passing.

Introduction
The word distributed in terms such as ""distributed system"", ""distributed programming"", and ""distributed algorithm"" originally referred to computer networks where individual computers were physically distributed within some geographical area. The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.While there is no single definition of a distributed system, the following defining properties are commonly used as:

There are several autonomous computational entities (computers or nodes), each of which has its own local memory.
The entities communicate with each other by message passing.A distributed system may have a common goal, such as solving a large computational problem; the user then perceives the collection of autonomous processors as a unit. Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.Other typical properties of distributed systems include the following:

The system has to tolerate failures in individual computers.
The structure of the system (network topology, network latency, number of computers) is not known in advance, the system may consist of different kinds of computers and network links, and the system may change during the execution of a distributed program.
Each computer has only a limited, incomplete view of the system. Each computer may know only one part of the input.

Parallel and distributed computing
Distributed systems are groups of networked computers which share a common goal for their work.
The terms ""concurrent computing"", ""parallel computing"", and ""distributed computing"" have much overlap, and no clear distinction exists between them. The same system may be characterized both as ""parallel"" and ""distributed""; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particular tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. Nevertheless, it is possible to roughly classify concurrent systems as ""parallel"" or ""distributed"" using the following criteria:

In parallel computing, all processors may have access to a shared memory to exchange information between processors.
In distributed computing, each processor has its own private memory (distributed memory). Information is exchanged by passing messages between the processors.The figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure (c) shows a parallel system in which each processor has a direct access to a shared memory.
The situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.

History
The use of concurrent processes which communicate through message-passing has its roots in operating system architectures studied in the 1960s. The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.ARPANET, one of the predecessors of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET, and it is probably the earliest example of a large-scale distributed application. In addition to ARPANET (and its successor, the global Internet), other early worldwide computer networks included Usenet and FidoNet from the 1980s, both of which were used to support distributed discussion systems.The study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its counterpart International Symposium on Distributed Computing (DISC) was first held in Ottawa in 1985 as the International Workshop on Distributed Algorithms on Graphs.

Architectures
Various hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.Distributed programming typically falls into one of several basic architectures: client–server, three-tier, n-tier, or peer-to-peer; or categories: loose coupling, or tight coupling.
Client–server: architectures where smart clients contact the server for data then format and display it to the users. Input at the client is committed back to the server when it represents a permanent change.
Three-tier: architectures that move the client intelligence to a middle tier so that stateless clients can be used. This simplifies application deployment. Most web applications are three-tier.
n-tier: architectures that refer typically to web applications which further forward their requests to other enterprise services. This type of application is the one most responsible for the success of application servers.
Peer-to-peer: architectures where there are no special machines that provide a service or manage the network resources. Instead all responsibilities are uniformly divided among all machines, known as peers. Peers can serve both as clients and as servers. Examples of this architecture include BitTorrent and the bitcoin network.Another basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a master/slave relationship. Alternatively, a ""database-centric"" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database. Database-centric architecture in particular provides relational processing analytics in a schematic architecture allowing for live environment relay. This enables distributed computing functions both within and beyond the parameters of a networked database.

Applications
Reasons for using distributed systems and distributed computing may include:

The very nature of an application may require the use of a communication network that connects several computers: for example, data produced in one physical location and required in another location.
There are many cases in which the use of a single computer would be possible in principle, but the use of a distributed system is beneficial for practical reasons. For example, it may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer. A distributed system can provide more reliability than a non-distributed system, as there is no single point of failure. Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.

Examples
Examples of distributed systems and applications of distributed computing include the following:
telecommunication networks:
telephone networks and cellular networks,
computer networks such as the Internet,
wireless sensor networks,
routing algorithms;
network applications:
World Wide Web and peer-to-peer networks,
massively multiplayer online games and virtual reality communities,
distributed databases and distributed database management systems,
network file systems,
distributed cache such as burst buffers,
distributed information processing systems such as banking systems and airline reservation systems;
real-time process control:
aircraft control systems,
industrial control systems;
parallel computation:
scientific computing, including cluster computing, grid computing, cloud computing, and various volunteer computing projects (see the list of distributed computing projects),
distributed rendering in computer graphics.

Theoretical foundations
Models
Many tasks that we would like to automate by using a computer are of question–answer type: we would like to ask a question and the computer should produce an answer. In theoretical computer science, such tasks are called computational problems. Formally, a computational problem consists of instances together with a solution for each instance. Instances are questions that we can ask, and solutions are desired answers to these questions.
Theoretical computer science seeks to understand which computational problems can be solved by using a computer (computability theory) and how efficiently (computational complexity theory). Traditionally, it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm.The field of concurrent and distributed computing studies similar questions in the case of either multiple computers, or a computer that executes a network of interacting processes: which computational problems can be solved in such a network and how efficiently? However, it is not at all obvious what is meant by ""solving a problem"" in the case of a concurrent or distributed system: for example, what is the task of the algorithm designer, and what is the concurrent or distributed equivalent of a sequential general-purpose computer?The discussion below focuses on the case of multiple computers, although many of the issues are the same for concurrent processes running on a single computer.
Three viewpoints are commonly used:

Parallel algorithms in shared-memory modelAll processors have access to a shared memory. The algorithm designer chooses the program executed by each processor.
One theoretical model is the parallel random access machines (PRAM) that are used. However, the classical PRAM model assumes synchronous access to the shared memory.
Shared-memory programs can be extended to distributed systems if the underlying operating system encapsulates the communication between nodes and virtually unifies the memory across all individual systems.
A model that is closer to the behavior of real-world multiprocessor machines and takes into account the use of machine instructions, such as Compare-and-swap (CAS), is that of asynchronous shared memory. There is a wide body of work on this model, a summary of which can be found in the literature.Parallel algorithms in message-passing modelThe algorithm designer chooses the structure of the network, as well as the program executed by each computer.
Models such as Boolean circuits and sorting networks are used. A Boolean circuit can be seen as a computer network: each gate is a computer that runs an extremely simple computer program. Similarly, a sorting network can be seen as a computer network: each comparator is a computer.Distributed algorithms in message-passing modelThe algorithm designer only chooses the computer program. All computers run the same program. The system must work correctly regardless of the structure of the network.
A commonly used model is a graph with one finite-state machine per node.In the case of distributed algorithms, computational problems are typically related to graphs. Often the graph that describes the structure of the computer network is the problem instance. This is illustrated in the following example.

An example
Consider the computational problem of finding a coloring of a given graph G. Different fields might take the following approaches:

Centralized algorithmsThe graph G is encoded as a string, and the string is given as input to a computer. The computer program finds a coloring of the graph, encodes the coloring as a string, and outputs the result.Parallel algorithmsAgain, the graph G is encoded as a string. However, multiple computers can access the same string in parallel. Each computer might focus on one part of the graph and produce a coloring for that part.
The main focus is on high-performance computation that exploits the processing power of multiple computers in parallel.Distributed algorithmsThe graph G is the structure of the computer network. There is one computer for each node of G and one communication link for each edge of G. Initially, each computer only knows about its immediate neighbors in the graph G; the computers must exchange messages with each other to discover more about the structure of G. Each computer must produce its own color as output.
The main focus is on coordinating the operation of an arbitrary distributed system.While the field of parallel algorithms has a different focus than the field of distributed algorithms, there is much interaction between the two fields. For example, the Cole–Vishkin algorithm for graph coloring was originally presented as a parallel algorithm, but the same technique can also be used directly as a distributed algorithm.
Moreover, a parallel algorithm can be implemented either in a parallel system (using shared memory) or in a distributed system (using message passing). The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie in the same place as the boundary between parallel and distributed systems (shared memory vs. message passing).

Complexity measures
In parallel algorithms, yet another resource in addition to time and space is the number of computers. Indeed, often there is a trade-off between the running time and the number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors, then the problem is said to be in the class NC. The class NC can be defined equally well by using the PRAM formalism or Boolean circuits—PRAM machines can simulate Boolean circuits efficiently and vice versa.In the analysis of distributed algorithms, more attention is usually paid on communication operations than computational steps. Perhaps the simplest model of distributed computing is a synchronous system where all nodes operate in a lockstep fashion. This model is commonly known as the LOCAL model. During each communication round, all nodes in parallel (1) receive the latest messages from their neighbours, (2) perform arbitrary local computation, and (3) send new messages to their neighbors. In such systems, a central complexity measure is the number of synchronous communication rounds required to complete the task.This complexity measure is closely related to the diameter of the network. Let D be the diameter of the network. On the one hand, any computable problem can be solved trivially in a synchronous distributed system in approximately 2D communication rounds: simply gather all information in one location (D rounds), solve the problem, and inform each node about the solution (D rounds).
On the other hand, if the running time of the algorithm is much smaller than D communication rounds, then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network. In other words, the nodes must make globally consistent decisions based on information that is available in their local D-neighbourhood. Many distributed algorithms are known with the running time much smaller than D rounds, and understanding which problems can be solved by such algorithms is one of the central research questions of the field. Typically an algorithm which solves a problem in polylogarithmic time in the network size is considered efficient in this model.
Another commonly used measure is the total number of bits transmitted in the network (cf. communication complexity). The features of this concept are typically captured with the CONGEST(B) model, which similarly defined as the LOCAL model but where single messages can only contain B bits.

Other problems
Traditional computational problems take the perspective that the user asks a question, a computer (or a distributed system) processes the question, then produces an answer and stops. However, there are also problems where the system is required not to stop, including the dining philosophers problem and other similar mutual exclusion problems. In these problems, the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur.
There are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance. Examples of related problems include consensus problems, Byzantine fault tolerance, and self-stabilisation.Much research is also focused on understanding the asynchronous nature of distributed systems:

Synchronizers can be used to run synchronous algorithms in asynchronous systems.
Logical clocks provide a causal happened-before ordering of events.
Clock synchronization algorithms provide globally consistent physical time stamps.

Election
Coordinator election (or leader election) is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the ""coordinator"" (or leader) of the task, or unable to communicate with the current coordinator. After a coordinator election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task coordinator.The network nodes communicate among themselves in order to decide which of them will get into the ""coordinator"" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the coordinator.The definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost.Coordinator election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira  for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing.
Many other algorithms were suggested for different kind of network graphs, such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by Korach, Kutten, and Moran.In order to perform coordination, distributed systems employ the concept of coordinators. The coordinator election problem is to choose a process from among a group of processes on different processors in a distributed system to act as the central coordinator. Several central coordinator election algorithms exist.

Properties of distributed systems
So far the focus has been on designing a distributed system that solves a given problem. A complementary research problem is studying the properties of a given distributed system.The halting problem is an analogous example from the field of centralised computation: we are given a computer program and the task is to decide whether it halts or runs forever. The halting problem is undecidable in the general case, and naturally understanding the behaviour of a computer network is at least as hard as understanding the behaviour of one computer.However, there are many interesting special cases that are decidable. In particular, it is possible to reason about the behaviour of a network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and non-deterministic) finite-state machines can reach a deadlock. This problem is PSPACE-complete, i.e., it is decidable, but not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the case of large networks.

See also
Notes
References
BooksAndrews, Gregory R. (2000), Foundations of Multithreaded, Parallel, and Distributed Programming, Addison–Wesley, ISBN 978-0-201-35752-3.
Arora, Sanjeev; Barak, Boaz (2009), Computational Complexity – A Modern Approach, Cambridge, ISBN 978-0-521-42426-4.
Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. (1990), Introduction to Algorithms (1st ed.), MIT Press, ISBN 978-0-262-03141-7.
Dolev, Shlomi (2000), Self-Stabilization, MIT Press, ISBN 978-0-262-04178-2.
Elmasri, Ramez; Navathe, Shamkant B. (2000), Fundamentals of Database Systems (3rd ed.), Addison–Wesley, ISBN 978-0-201-54263-9.
Ghosh, Sukumar (2007), Distributed Systems – An Algorithmic Approach, Chapman & Hall/CRC, ISBN 978-1-58488-564-1.
Lynch, Nancy A. (1996), Distributed Algorithms, Morgan Kaufmann, ISBN 978-1-55860-348-6.
Herlihy, Maurice P.; Shavit, Nir N. (2008), The Art of Multiprocessor Programming, Morgan Kaufmann, ISBN 978-0-12-370591-4.
Papadimitriou, Christos H. (1994), Computational Complexity, Addison–Wesley, ISBN 978-0-201-53082-7.
Peleg, David (2000), Distributed Computing: A Locality-Sensitive Approach, SIAM, ISBN 978-0-89871-464-7, archived from the original on 2009-08-06, retrieved 2009-07-16.ArticlesCole, Richard; Vishkin, Uzi (1986), ""Deterministic coin tossing with applications to optimal parallel list ranking"", Information and Control, 70 (1): 32–53, doi:10.1016/S0019-9958(86)80023-7.
Keidar, Idit (2008), ""Distributed computing column 32 – The year in review"", ACM SIGACT News, 39 (4): 53–54, CiteSeerX 10.1.1.116.1285, doi:10.1145/1466390.1466402.
Linial, Nathan (1992), ""Locality in distributed graph algorithms"", SIAM Journal on Computing, 21 (1): 193–201, CiteSeerX 10.1.1.471.6378, doi:10.1137/0221015.
Naor, Moni; Stockmeyer, Larry (1995), ""What can be computed locally?"" (PDF), SIAM Journal on Computing, 24 (6): 1259–1277, CiteSeerX 10.1.1.29.669, doi:10.1137/S0097539793254571.Web sitesGodfrey, Bill (2002). ""A primer on distributed computing"".
Peter, Ian (2004). ""Ian Peter's History of the Internet"". Retrieved 2009-08-04.

Further reading
BooksAttiya, Hagit and Jennifer Welch (2004), Distributed Computing: Fundamentals, Simulations, and Advanced Topics, Wiley-Interscience ISBN 0-471-45324-2.
Christian Cachin; Rachid Guerraoui; Luís Rodrigues (2011), Introduction to Reliable and Secure Distributed Programming (2. ed.), Springer, Bibcode:2011itra.book.....C, ISBN 978-3-642-15259-7
Coulouris, George;  et al. (2011), Distributed Systems: Concepts and Design (5th Edition), Addison-Wesley ISBN 0-132-14301-1.
Faber, Jim (1998), Java Distributed Computing, O'Reilly: Java Distributed Computing by Jim Faber, 1998
Garg, Vijay K. (2002), Elements of Distributed Computing, Wiley-IEEE Press ISBN 0-471-03600-5.
Tel, Gerard (1994), Introduction to Distributed Algorithms, Cambridge University Press
Chandy, Mani;  et al., Parallel Program DesignArticlesKeidar, Idit; Rajsbaum, Sergio, eds. (2000–2009), ""Distributed computing column"", ACM SIGACT News.
Birrell, A. D.; Levin, R.; Schroeder, M. D.; Needham, R. M. (April 1982). ""Grapevine: An exercise in distributed computing"" (PDF). Communications of the ACM. 25 (4): 260–274. doi:10.1145/358468.358487.Conference PapersC. Rodríguez, M. Villagra and B. Barán, Asynchronous team algorithms for Boolean Satisfiability, Bionetics2007, pp. 66–69, 2007.

External links
Distributed computing at Curlie
Distributed computing journals at Curlie",https://en.wikipedia.org/wiki/Distributed_computing,"['All articles with unsourced statements', 'Articles with Curlie links', 'Articles with short description', 'Articles with unsourced statements from October 2016', 'CS1 errors: missing periodical', 'CS1 maint: multiple names: authors list', 'Decentralization', 'Distributed computing', 'Short description matches Wikidata', 'Wikipedia articles with BNE identifiers', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with SUDOC identifiers']",Data Science
82,Doi (identifier),"A digital object identifier (DOI) is a persistent identifier or handle used to identify objects uniquely, standardized by the International Organization for Standardization (ISO). An implementation of the Handle System, DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports, data sets, and official publications. However, they also have been used to identify other types of information resources, such as commercial videos.
A DOI aims to be ""resolvable"", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to identify their referents uniquely. The DOI system uses the indecs Content Model for representing metadata.
The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI is supposed to provide a more stable link than simply using its URL. But every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL. It is the publisher's responsibility to update the DOI database. If they fail to do so, the DOI resolves to a dead link leaving the DOI useless.
The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000. Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs. The DOI system is implemented through a federation of registration agencies coordinated by the IDF. By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations, and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.

Nomenclature and syntax
A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.

prefix/suffixThe prefix identifies the registrant of the identifier and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is at least a four digit number greater than or equal to 1000, whose limit depends only on the total number of registrants. The prefix may be further subdivided with periods, like 10.NNNN.N.For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The ""10"" part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace, and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).
DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works such as licenses, parties to a transaction, etc.
The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.

Display
The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182) This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL – providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyperlinked to its appropriate URL – the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.Since DOI is a namespace within the Handle system, it is semantically correct to represent it as the URI info:doi/10.1000/182.

Content
Major content of the DOI system currently includes:

Scholarly materials (journal articles, books, ebooks, etc.) through CrossRef, a consortium of around 3,000 publishers; Airiti, a leading provider of Chinese and Taiwanese electronic academic journals; and the Japan Link Center (JaLC) an organization providing link management and DOI assignment for electronic academic journals in Japanese.
Research datasets through Datacite, a consortium of leading research libraries, technical information providers, and scientific data centers;
European Union official publications through the EU publications office;
The Chinese National Knowledge Infrastructure project at Tsinghua University and the Institute of Scientific and Technical Information of China (ISTIC), two initiatives sponsored by the Chinese government.
Permanent global identifiers for both commercial and non-commercial audio/visual content titles, edits, and manifestations through the Entertainment ID Registry, commonly known as EIDR.In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.Other registries include Crossref and the multilingual European DOI Registration Agency. Since 2015, RFCs can be referenced as doi:10.17487/rfc….

Features and benefits
The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated (although when the publisher of a journal changes, sometimes all the DOIs will be changed, with the old DOIs no longer working). It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.
The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system. DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request. However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents, that would have been available for no additional fee from alternative locations.The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.
The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.

Comparison with other identifier schemes
A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as ""identifiers"" doesn't mean that they can be compared easily. Other ""identifier systems"" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).
A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.

Resolution
DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.
To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.
Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as https://doi.org/ (preferred) or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.
Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, and https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled (often author archived) version of a title and redirects the user to that instead of the publisher's version. Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016 (later Unpaywall). While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs, which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.

IDF organizational structure
The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system. It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.
The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.
Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.

Standardization
The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9. The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot, which was approved by 100% of those voting in a ballot closing on 15 November 2010. The final standard was published on 23 April 2012.DOI is a registered URI under the info URI scheme specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:

URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.

See also
Notes
References
External links
Official website
Short DOI – DOI Foundation service for converting long DOIs to shorter equivalents
Factsheet: DOI System and Internet Identifier Specifications
CrossRef DOI lookup",https://en.wikipedia.org/wiki/Digital_object_identifier,"['Academic publishing', 'Articles with short description', 'CS1 errors: missing periodical', 'Electronic documents', 'Identifiers', 'Index (publishing)', 'Short description is different from Wikidata', 'Use dmy dates from December 2019', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers']",Data Science
83,Domain knowledge,"Domain knowledge is knowledge of a specific, specialized discipline or field, in contrast to general knowledge, or domain-independent knowledge.  The term is often used in reference to a more general discipline, as, for example, in describing a software engineer who has general knowledge of programming, as well as domain knowledge about the pharmaceutical industry.  People who have domain knowledge, are often considered specialists or experts in the field.

Knowledge capture
In software engineering domain knowledge is knowledge about the environment in which the target system operates, for example, software agents. Domain knowledge usually must be learned from software users in the domain (as domain specialists/experts), rather than from software developers. It may include user workflows, data pipelines, business policies, configurations and constraints and is crucial in the development of a software application. Expert's domain knowledge (frequently informal and ill-structured) is transformed in computer programs and active data, for example in a set of rules in knowledge bases, by knowledge engineers.
Communicating between end-users and software developers is often difficult. They  must find a common language to communicate in. Developing enough shared vocabulary to communicate can often take a while.
The same knowledge can be included in different domain knowledge.
Knowledge which may be applicable across a number of domains is called domain-independent knowledge, for example logics and mathematics.
Operations on domain knowledge are performed by meta-knowledge.

Literature
Hjørland, B. & Albrechtsen, H. (1995). Toward A New Horizon in Information Science: Domain Analysis. Journal of the American Society for Information Science, 1995, 46(6), 400-425.

See also
Domain engineering
Domain of discourse
Knowledge engineering
Problem domain
Subject-matter expert
Artificial Intelligence",https://en.wikipedia.org/wiki/Domain_knowledge,"['All articles lacking sources', 'Articles lacking sources from December 2009', 'Knowledge']",Data Science
84,Electrochemical RAM,"Electrochemical Random-Access Memory (ECRAM) is a type of non-volatile memory (NVM) with multiple levels per cell (MLC) designed for deep learning analog acceleration. An ECRAM cell is a three-terminal device composed of a conductive channel, an insulating electrolyte, an ionic reservoir, and metal contacts. The resistance of the channel is modulated by ionic exchange at the interface between the channel and the electrolyte upon application of an electric field. The charge-transfer process allows both for state retention in the absence of applied power, and for programming of multiple distinct levels, both differentiating ECRAM operation from the one of a field-effect transistor (FET). The write operation is deterministic and can result in symmetrical potentiation and depression, making ECRAM arrays attractive for acting as artificial synaptic weights in physical implementations of artificial neural networks (ANN). The technology challenges include open circuit potential (OCP) and semiconductor foundry compatibility associated with energy materials. Universities, government laboratories, and corporate research teams have contributed to the development of ECRAM for analog computing. Notably, Sandia National Laboratories designed a lithium-based cell inspired by solid-state battery materials, Stanford University built an organic proton-based cell, and International Business Machines (IBM) demonstrated in-memory selector-free parallel programming for a logistic regression task in an array of metal-oxide ECRAM designed for insertion in the back end of line (BEOL).

Operation
Write
Stress to the gate, relative to channel electrodes, can be applied in the form of fixed current or bias, driving ions toward - or away from - the electrolyte/channel interface where charge transfer occurs with free carriers. Upon insertion in the channel, the ionic charge is neutralized and the atomic species intercalate or bind to the conductive host matrix, in some cases yielding strain and localized phase transformation. Such reversible processes are equivalent to anodic/cathodic reactions in battery cells or electrochromic devices. Although in ECRAM, the programming of the memory element is defined not as a change in capacity or opacity, but by a change of channel conductivity associated with atomic species being inserted or removed as a result of the stress signal.

Read
The read operation is decoupled from the write operation thanks to the presence of three electrodes, therefore limiting read disturb. A small bias is applied between the channel electrodes, with the resulting read current being proportional to the channel conductivity, hence sensing the programmed state of the device.

Speed
The programming speed of ECRAM cells is not limited by the bulk diffusion of ions. They indeed only need to cross the interface plane between the electrolyte and the channel to induce a change in conductivity. Nanosecond write pulses can indeed trigger programming. Trade-offs between gate capacitance, electronic conductivity, etc., can yield settling transients, limiting the maximum read-write frequency.

Arrays
ECRAM arrays are integrated in a pseudo-crossbar layout, the gate access line being common to all devices in a row or column. If a change in electrochemical potential, the driving force of a battery, occurs upon ionic exchange between channel and gate electrode, an open circuit potential (OCP) exists at the gate contact and will differ device to device depending on the programmed state. To prevent cross-talk between cells sharing a gate line, an access device to isolate each one is added in series with the memory element. Suppressing OCP in the ECRAM design, minimizes the cell size/complexity, allowing for selector-free parallel read/programming of device arrays.

Synaptic function
Principle
Non-volatile memory (NVM) can be leveraged for in-memory compute, thereby reducing the frequency of data transfer between storage and processing units. This can ultimately improve compute time and energy efficiency over hierarchical system architectures by eliminating the Von Neumann bottleneck. Hence, when using multi-level cells (MLC) at the nodes of cross-bar arrays, one can perform analog operations on time or voltage encoded data such as vector (row input signal) × matrix (memory array) multiply. Following Kirchoff's and Ohm's laws, the resulting vector is then obtained by integrating the current collected at each column. For ECRAM cells, an additional line is added at each row to write the cells during programming cycles, thereby yielding a pseudo-crossbar architecture. In the field of artificial intelligence (A.I.), deep neural networks (DNN) are used for classification and learning tasks, relying on a large number of matrix-multiply operations. Therefore, analog compute with NVM technology for such tasks are extremely attractive. ECRAM cells are uniquely positioned for use in analog deep learning accelerators due to their inherent deterministic and symmetric programming nature when compared to other devices such as resistive RAM (ReRAM or RRAM) and phase-change memory (PCM).

Requirements
Physical implementation of artificial neural networks (ANN) must perform at iso-accuracy when benchmarked against floating point precision weights in software. This sets the boundary for device properties needed for analog deep learning accelerators. In the design of their resisistive processing unit (RPU), IBM Research has published such requirements, a subset of which is listed here. Algorithm and hardware co-design can relax them somewhat but not without other trade-offs.NVM use as synaptic weights in lieu of storage implies significantly different requirements when it comes to target resistance range, number of levels, and  programming speed and symmetry. Because the in-memory computation occurs in parallel through the array, many devices are addressed concurrently and therefore need to have a high average resistance to limit energy dissipation. To perform high-accuracy computation and be resilient to noise, the NVM cell needs a large number of distinct states. The programming time needs only to be fast between levels, not from the highest to the lowest resistance states. During each programming cycle (back-propagation), weight updates can be negative or positive, and the up/down traces therefore need symmetry to allow learning algorithms to converge. All NVM technologies do struggle with these targets. ECRAM individual cells can meet such stringent metrics, but also need to demonstrate high-density array yield and stochasticity.

Demos with ECRAM Synaptic Arrays
Sandia National Laboratories
As reported in a 2019 publication in Science, by Elliot J. Fuller, Alec A. Talin, et al. from Sandia National Laboratories, in collaboration with Stanford University, and the University of Massachusetts Amherst:Using co-planar organic multilevel cells, isolated by conductive bridge memory (CBM) devices, the team demonstrates parallel programming and addressing in up to 3×3 arrays. In particular a 2-layer neural network is mapped to the array by transferring the weights necessary to perform an inference task resulting in a XOR operation on the binary input vector.
Individual cells are shown to have the following properties (not all achieved in the same device configuration); speed = 1 MHz read-write cycles, number of states > 50 (tunable), resistance range = 50-100 nS (tunable), endurance > 108 write ops, size = 50×50 μm2.

IBM Research
As reported in a 2019 proceeding of the IEEE International Electron Device Meeting (IEDM), by Seyoung Kim, John Rozen, et al. from IBM Research:Using metal-oxide ECRAM cells, selector-free, the team demonstrates parallel programming and addressing in 2×2 arrays. In particular, a logistic regression task is performed in-memory with 1,000 2×1 vectors as training set. 2D curve fit is achieved in a dozen epochs.
Individual cells are shown to have the following properties (not all achieved in the same device configuration); speed = 10 ns write pulses, number of states > 1,000 (tunable), resistance range = 0-50 μS (tunable), endurance > 107 write ops, size < 1×1 μm2.

Cell implementations
Various institutions have demonstrated ECRAM cells with vastly different materials, layouts, and performances. 
An example set for discrete cells is listed in the table.

Li-ECRAM
Based on lithium ions, Li-ECRAM devices have demonstrated repeatable and controlled switching by applying known materials from battery technology to the memory design. Consequently, such cells can exhibit an OCP which varies over several volts, depending on the programmed state.

H-ECRAM
Based on hydrogen ions, H-ECRAM devices have proven fast, necessitating small driving forces to induce programming. High diffusion coefficients in various materials can be accompanied by lack of retention within the memory cell, impacting endurance. Most H-ECRAM designs use liquid and/or organic electrolytes.

MO-ECRAM
Metal-oxide based ECRAM, are inspired from OxRam materials and high-k/metal gate technology used in commercial semiconductor offerings. MO-ECRAM do enable negligible OCP and sub-μs write operations.

VLSI
For advanced semiconductor memory or compute applications, a technology needs to be compatible with very large scale integration (VLSI). This puts constraints on materials used, and the techniques employed to fabricate functional devices. The implications for ECRAM are described here.

Semiconductor foundry
A semiconductor foundry can handle several technologies and has strict rules when it comes to materials being introduced in its expensive toolset to avoid cross-contamination and loss of device yield. In particular, metallic mobile ions, if present in active areas, can induce device drift and affect reliability. There are several other considerations for the foundries; including safety, cost, volume, etc. Hence, lithium ion-based Li-ECRAM faces unique challenges beyond the presence of OCP.

Back end of line (BEOL)
Memory arrays require logic periphery to operate and interface with the rest of the compute system. Such periphery is based on field-effect transistors (FETs) built on the surface of silicon wafer substrates with a high thermal budget at the front end of line (FEOL). Memory cells can be inserted between upper metal levels at back end of line (BEOL) but will still need to remain unaffected by temperatures up to ~400°C used in subsequent steps. Together with high density patterning challenges, these restrictions make organic devices unsuitable for such integration.

Heterogeneous integration (HI)
One way to introduce novel memory materials can be to use heterogeneous integration (HI) where the device array is fabricated independently from the logic controls and then bonded to the FET-containing chip to enable its use as high bandwidth memory (HBM). However, the cost and complexity associated with such scheme negatively affects the value proposition for displacing existing memory technologies.

References
External links
spectrum.ieee.org/tech-talk
www.ibm.com/blogs
news.mit.edu
news.stanford.edu
insidehpc.com",https://en.wikipedia.org/wiki/Electrochemical_RAM,"['Articles with short description', 'Non-volatile memory', 'Non-volatile random-access memory', 'Short description matches Wikidata', 'Types of RAM']",Data Science
85,Echo state network,"The echo state network (ESN) is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns. The main interest of this network is that although its behaviour is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.
Alternatively, one may consider a nonparametric Bayesian formulation of the output layer, under which: (i) a prior distribution is imposed over the output weights; and (ii) the output weights are marginalized out in the context of prediction generation, given the training data. This idea has been demonstrated in by using Gaussian priors, whereby a Gaussian process model with ESN-driven kernel function is obtained. Such a solution was shown to outperform ESNs with trainable (finite) sets of weights in several benchmarks.
Some publicly available implementations of ESNs are: (i) aureservoir: an efficient C++ library for various kinds of echo state networks with python/numpy bindings; and (ii) Matlab code: an efficient matlab for an echo state network, (iii) ReservoirComputing.jl: an efficient Julia-based implementation of various types of echo state networks, and (iv) pyESN: simple echo state networks in Python.
The Echo State Network (ESN) belongs to the Recurrent Neural Network (RNN) family and provide their architecture and supervised learning principle. Unlike Feedforward Neural Networks, Recurrent Neural Networks are dynamic systems and not functions. Recurrent Neural Networks are typically used for: Learn dynamical process: signal treatment in engineering and telecommunications, vibration analysis, seismology, control of engines and generators. Signal forecasting and generation: text, music, electric signals. Modeling of biological systems, neurosciences (cognitive neurodynamics), memory modeling, brain-computer Interfaces (BCIs), filtering and Kalman processes, military applications, volatility modeling etc. 
For the training of RNN a number of learning algorithms are available: backpropagation through time, real-time recurrent learning. Convergence is not guaranteed due to instability and bifurcation phenomena.  
The main approach of the ESN is firstly to operate a random, large, fixed, recurring neural network with the input signal, which induces a nonlinear response signal in each neuron within this ""reservoir"" network, and secondly connect a desired output signal by a trainable linear combination of all these response signals.Another feature of the ESN is the autonomous operation in prediction: if the Echo State Network is trained with an input that is a backshifted version of the output, then it can be used for signal generation/prediction by using the previous output as input.The main idea of ESNs is tied to Liquid State Machines (LSM), which were independently and simultaneously developed with ESNs by Wolfgang Maass. LSMs, ESNs and the newly researched Backpropagation Decorrelation learning rule for RNNs are more and more summarized under the name Reservoir Computing.
Schiller and Steil also demonstrated that in conventional training approaches for RNNs, in which all weights (not only output weights) are adapted, the dominant changes are in output weights. In cognitive neuroscience, Peter F. Dominey analysed a related process related to the modelling of sequence processing in the mammalian brain, in particular speech recognition in the human brain. The basic idea also included a model of temporal input discrimination in biological neuronal networks. An early clear formulation of the reservoir computing idea is due to K. Kirby, who disclosed this concept in a largely forgotten conference contribution. The first formulation of the reservoir computing idea known today stems from L. Schomaker, who described how a desired target output can be obtained from an RNN by learning to combine signals from a randomly configured ensemble of spiking neural oscillators.

Variants
Echo state networks can be built in different ways. They can be set up with or without directly trainable input-to-output connections, with or without output reservation feedback, with different neurotypes, different reservoir internal connectivity patterns etc. The output weight can be calculated for linear regression with all algorithms whether they are online or offline. In addition to the solutions for errors with smallest squares, margin maximization criteria, so-called training support vector machines, are used to determine the output values. Other variants of echo state networks seek to change the formulation to better match common models of physical systems, such as those typically those defined by differential equations. Work in this direction includes echo state networks which partially include physical models, hybrid echo state networks, and continuous-time echo state networks.The fixed RNN acts as a random, nonlinear medium whose dynamic response, the ""echo"", is used as a signal base.  The linear combination of this base can be trained to reconstruct the desired output by minimizing some error criteria.

Significance
RNNs were rarely used in practice before the introduction of the ESN. Because these models fit need a version of the gradient descent to adjust the connections. As a result, the algorithms are slow and much worse, making the learning process vulnerable to branching errors.  Convergence cannot therefore be guaranteed. The problem with branching does not have the ESN training and is additionally easy to implement. ESNs outperform all other nonlinear dynamic models.  However, today the problem that RNNs made slow and error-prone has been solved with the advent of Deep Learning and the unique selling point of ESNs has been lost. In addition, the RNNs have proven themselves in several practical areas such as language processing. To cope with tasks of similar complexity using reservoir calculation methods, it would require memory of excessive size. However, they are used in some areas such as many signal processing applications. However, ESNs have been widely used as a computing principle that mixes with non-digital computer substrates. For example: optical microchips, mechanical nanooscillators, polymer mixtures or even artificial soft limbs.

See also
Liquid-state machine: a similar concept with generalized signal and network.
Reservoir computing


== References ==",https://en.wikipedia.org/wiki/Echo_state_network,"['All stub articles', 'Articles with short description', 'Artificial intelligence stubs', 'Artificial neural networks', 'CS1 maint: multiple names: authors list', 'Computer science stubs', 'Short description is different from Wikidata']",Data Science
86,Empirical research,"Empirical research is research using empirical evidence. It is also a way of gaining knowledge by means of direct and indirect observation or experience. Empiricism values some research more than other kinds. Empirical evidence (the record of one's direct observations or experiences) can be analyzed quantitatively or qualitatively. Quantifying the evidence or making sense of it in qualitative form, a researcher can answer empirical questions, which should be clearly defined and answerable with the evidence collected (usually called data). Research design varies by field and by the question being investigated. Many researchers combine qualitative and quantitative forms of analysis to better answer questions which cannot be studied in laboratory settings, particularly in the social sciences and in education.
In some fields, quantitative research may begin with a research question (e.g., ""Does listening to vocal music during the learning of a word list have an effect on later memory for these words?"") which is tested through experimentation. Usually,  the researcher has a certain theory regarding the topic under investigation. Based on this theory, statements or hypotheses will be proposed (e.g., ""Listening to vocal music has a negative effect on learning a word list.""). From these hypotheses, predictions about specific events are derived (e.g., ""People who study a word list while listening to vocal music will remember fewer words on a later memory test than people who study a word list in silence.""). These predictions can then be tested with a suitable experiment. Depending on the outcomes of the experiment, the theory on which the hypotheses and predictions were based will be supported or not, or may need to be modified and then subjected to further testing.

Terminology
The term empirical was originally used to refer to certain ancient Greek practitioners of medicine who rejected adherence to the dogmatic doctrines of the day, preferring instead to rely on the observation of phenomena as perceived in experience. Later empiricism referred to a theory of knowledge in philosophy which adheres to the principle that knowledge arises from experience and evidence gathered specifically using the senses. In scientific use, the term empirical refers to the gathering of data using only evidence that is observable by the senses or in some cases using calibrated scientific instruments. What early philosophers described as empiricist and empirical research have in common is the dependence on observable data to formulate and test theories and come to conclusions.

Usage
The researcher attempts to describe accurately the interaction between the instrument (or the human senses) and the entity being observed. If instrumentation is involved, the researcher is expected to calibrate his/her instrument by applying it to known standard objects and documenting the results before applying it to unknown objects. In other words, it describes the research that has not taken place before and their results.
In practice, the accumulation of evidence for or against any particular theory involves planned research designs for the collection of empirical data, and academic rigor plays a large part of judging the merits of research design. Several typologies for such designs have been suggested, one of the most popular of which comes from Campbell and Stanley. They are responsible for popularizing the widely cited distinction among pre-experimental, experimental, and quasi-experimental designs and are staunch advocates of the central role of randomized experiments in educational research.

Scientific research
Accurate analysis of data using standardized statistical methods in scientific studies is critical to determining the validity of empirical research. Statistical formulas such as regression, uncertainty coefficient, t-test, chi square, and various types of ANOVA (analyses of variance) are fundamental to forming logical, valid conclusions. If empirical data reach significance under the appropriate statistical formula, the research hypothesis is supported. If not, the null hypothesis is supported (or, more accurately, not rejected), meaning no effect of the independent variable(s) was observed on the dependent variable(s).
The outcome of empirical research using statistical hypothesis testing is never proof. It can only support a hypothesis, reject it, or do neither. These methods yield only probabilities. Among scientific researchers, empirical evidence (as distinct from empirical research) refers to objective evidence that appears the same regardless of the observer. For example, a thermometer will not display different temperatures for each individual who observes it. Temperature, as measured by an accurate, well calibrated thermometer, is empirical evidence. By contrast, non-empirical evidence is subjective, depending on the observer. Following the previous example, observer A might truthfully report that a room is warm, while observer B might truthfully report that the same room is cool, though both observe the same reading on the thermometer. The use of empirical evidence negates this effect of personal (i.e., subjective) experience or time.
The varying perception of empiricism and rationalism shows concern with the limit to which there is dependency on experience of sense as an effort of gaining knowledge. According to rationalism, there are a number of different ways in which sense experience is gained independently for the knowledge and concepts. According to empiricism, sense experience is considered as the main source of every piece of knowledge and the concepts. 
In general, rationalists are known for the development of their own views following two different way. First, the key argument can be placed that there are cases in which the content of knowledge or concepts end up outstripping the information. This outstripped information is provided by the sense experience (Hjørland, 2010, 2). Second, there is construction of accounts as to how reasoning helps in the provision of addition knowledge about a specific or broader scope. Empiricists are known to be presenting complementary senses related to thought. 
First there is development of accounts of how there is provision of information by experience that is cited by rationalists. This is insofar for having it in the initial place. At times, empiricists tend to be opting skepticism as an option of rationalism. If experience is not helpful in the provision of knowledge or concept cited by rationalists, then they do not exist (Pearce, 2010, 35). Second, empiricists hold the tendency of attacking the accounts of rationalists while considering reasoning to be an important source of knowledge or concepts. The overall disagreement between empiricists and rationalists show primary concerns in how there is gaining of knowledge with respect to the sources of knowledge and concept. In some of the cases, disagreement at the point of gaining knowledge results in the provision of conflicting responses to other aspects as well. There might be a disagreement in the overall feature of warrant, while limiting the knowledge and thought. Empiricists are known for sharing the view that there is no existence of innate knowledge and rather that is derivation of knowledge out of experience. These experiences are either reasoned using the mind or sensed through the five senses human possess (Bernard, 2011, 5). On the other hand, rationalists are known to be sharing the view that there is existence of innate knowledge and this is different for the objects of innate knowledge being chosen. 
In order to follow rationalism, there must be adoption of one of the three claims related to the theory that are deduction or intuition, innate knowledge, and innate concept. The more there is removal of concept from mental operations and experience, there can be performance over experience with increased plausibility in being innate. Further ahead, empiricism in context with a specific subject provides a rejection of corresponding version related to innate knowledge and deduction or intuition (Weiskopf, 2008, 16). Insofar as there is acknowledgement of concepts and knowledge within the area of subject, the knowledge has major dependence on experience through human senses.

Empirical cycle
A.D. de Groot's empirical cycle:
Observation: The observation of a phenomenon and inquiry concerning its causes.
Induction: The formulation of hypotheses - generalized explanations for the phenomenon.
Deduction: The formulation of experiments that will test the hypotheses (i.e. confirm them if true, refute them if false).
Testing: The procedures by which the hypotheses are tested and data are collected.
Evaluation: The interpretation of the data and the formulation of a theory - an abductive argument that presents the results of the experiment as the most reasonable explanation for the phenomenon.

See also
Case study
Fact
Field research
Scientific method

References
External links
 The dictionary definition of empirical research at Wiktionary
Some Key Concepts for the Design and Review of Empirical Research",https://en.wikipedia.org/wiki/Empirical_research,"['Articles with short description', 'Concepts in epistemology', 'Empiricism', 'Epistemology of science', 'Research', 'Short description matches Wikidata', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers']",Data Science
87,Empirical risk minimization,"Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true ""risk"") because we don't know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the ""empirical"" risk).

Background
Consider the following situation, which is a general setting of many supervised learning problems. We have two spaces of objects 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   and would like to learn a function 
  
    
      
         
        h
        :
        X
        →
        Y
      
    
    {\displaystyle \ h:X\to Y}
   (often called hypothesis) which outputs an object 
  
    
      
        y
        ∈
        Y
      
    
    {\displaystyle y\in Y}
  , given 
  
    
      
        x
        ∈
        X
      
    
    {\displaystyle x\in X}
  . To do so, we have at our disposal a training set of 
  
    
      
        n
      
    
    {\displaystyle n}
   examples 
  
    
      
         
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        ,
        …
        ,
        (
        
          x
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
      
    
    {\displaystyle \ (x_{1},y_{1}),\ldots ,(x_{n},y_{n})}
   where 
  
    
      
        
          x
          
            i
          
        
        ∈
        X
      
    
    {\displaystyle x_{i}\in X}
   is an input and 
  
    
      
        
          y
          
            i
          
        
        ∈
        Y
      
    
    {\displaystyle y_{i}\in Y}
   is the corresponding response that we wish to get from 
  
    
      
         
        h
        (
        
          x
          
            i
          
        
        )
      
    
    {\displaystyle \ h(x_{i})}
  .
To put it more formally, we assume that there is a joint probability distribution 
  
    
      
        P
        (
        x
        ,
        y
        )
      
    
    {\displaystyle P(x,y)}
   over 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
  , and that the training set consists of 
  
    
      
        n
      
    
    {\displaystyle n}
   instances 
  
    
      
         
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        ,
        …
        ,
        (
        
          x
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
      
    
    {\displaystyle \ (x_{1},y_{1}),\ldots ,(x_{n},y_{n})}
   drawn i.i.d. from 
  
    
      
        P
        (
        x
        ,
        y
        )
      
    
    {\displaystyle P(x,y)}
  . Note that the assumption of a joint probability distribution allows us to model uncertainty in predictions (e.g. from noise in data) because 
  
    
      
        y
      
    
    {\displaystyle y}
   is not a deterministic function of 
  
    
      
        x
      
    
    {\displaystyle x}
  , but rather a random variable with conditional distribution 
  
    
      
        P
        (
        y
        
          |
        
        x
        )
      
    
    {\displaystyle P(y|x)}
   for a fixed 
  
    
      
        x
      
    
    {\displaystyle x}
  .
We also assume that we are given a non-negative real-valued loss function 
  
    
      
        L
        (
        
          
            
              y
              ^
            
          
        
        ,
        y
        )
      
    
    {\displaystyle L({\hat {y}},y)}
   which measures how different the prediction 
  
    
      
        
          
            
              y
              ^
            
          
        
      
    
    {\displaystyle {\hat {y}}}
   of a hypothesis is from the true outcome 
  
    
      
        y
        .
      
    
    {\displaystyle y.}
   The risk associated with hypothesis 
  
    
      
        h
        (
        x
        )
      
    
    {\displaystyle h(x)}
   is then defined as the expectation of the loss function:

  
    
      
        R
        (
        h
        )
        =
        
          E
        
        [
        L
        (
        h
        (
        x
        )
        ,
        y
        )
        ]
        =
        ∫
        L
        (
        h
        (
        x
        )
        ,
        y
        )
        
        d
        P
        (
        x
        ,
        y
        )
        .
      
    
    {\displaystyle R(h)=\mathbf {E} [L(h(x),y)]=\int L(h(x),y)\,dP(x,y).}
  A loss function commonly used in theory is the 0-1 loss function: 
  
    
      
        L
        (
        
          
            
              y
              ^
            
          
        
        ,
        y
        )
        =
        
          
            {
            
              
                
                  1
                
                
                  
                    
                       If 
                    
                  
                  
                  
                    
                      
                        y
                        ^
                      
                    
                  
                  ≠
                  y
                
              
              
                
                  0
                
                
                  
                    
                       If 
                    
                  
                  
                  
                    
                      
                        y
                        ^
                      
                    
                  
                  =
                  y
                
              
            
            
          
        
      
    
    {\displaystyle L({\hat {y}},y)={\begin{cases}1&{\mbox{ If }}\quad {\hat {y}}\neq y\\0&{\mbox{ If }}\quad {\hat {y}}=y\end{cases}}}
  .
The ultimate goal of a learning algorithm is to find a hypothesis 
  
    
      
        
          h
          
            ∗
          
        
      
    
    {\displaystyle h^{*}}
   among a fixed class of functions 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
   for which the risk 
  
    
      
        R
        (
        h
        )
      
    
    {\displaystyle R(h)}
   is minimal:

  
    
      
        
          h
          
            ∗
          
        
        =
        arg
        ⁡
        
          min
          
            h
            ∈
            
              
                H
              
            
          
        
        R
        (
        h
        )
        .
      
    
    {\displaystyle h^{*}=\arg \min _{h\in {\mathcal {H}}}R(h).}

Empirical risk minimization
In general, the risk 
  
    
      
        R
        (
        h
        )
      
    
    {\displaystyle R(h)}
   cannot be computed because the distribution 
  
    
      
        P
        (
        x
        ,
        y
        )
      
    
    {\displaystyle P(x,y)}
   is unknown to the learning algorithm (this situation is referred to as agnostic learning). However, we can compute an approximation, called empirical risk, by averaging the loss function on the training set:

  
    
      
        
        
          R
          
            emp
          
        
        (
        h
        )
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        L
        (
        h
        (
        
          x
          
            i
          
        
        )
        ,
        
          y
          
            i
          
        
        )
        .
      
    
    {\displaystyle \!R_{\text{emp}}(h)={\frac {1}{n}}\sum _{i=1}^{n}L(h(x_{i}),y_{i}).}
  The empirical risk minimization principle states that the learning algorithm should choose a hypothesis 
  
    
      
        
          
            
              h
              ^
            
          
        
      
    
    {\displaystyle {\hat {h}}}
   which minimizes the empirical risk:

  
    
      
        
          
            
              h
              ^
            
          
        
        =
        arg
        ⁡
        
          min
          
            h
            ∈
            
              
                H
              
            
          
        
        
          R
          
            emp
          
        
        (
        h
        )
        .
      
    
    {\displaystyle {\hat {h}}=\arg \min _{h\in {\mathcal {H}}}R_{\text{emp}}(h).}
  Thus the learning algorithm defined by the ERM principle consists in solving the above optimization problem.

Properties
Computational complexity
Empirical risk minimization for a classification problem with a 0-1 loss function is known to be an NP-hard problem even for such a relatively simple class of functions as linear classifiers. Though, it can be solved efficiently when the minimal empirical risk is zero, i.e. data is linearly separable.
In practice, machine learning algorithms cope with that either by employing a convex approximation to the 0-1 loss function (like hinge loss for SVM), which is easier to optimize, or by imposing assumptions on the distribution 
  
    
      
        P
        (
        x
        ,
        y
        )
      
    
    {\displaystyle P(x,y)}
   (and thus stop being agnostic learning algorithms to which the above result applies).

See also
Maximum likelihood estimation
M-estimator

References
Further reading
Vapnik, V. (2000). The Nature of Statistical Learning Theory. Information Science and Statistics. Springer-Verlag. ISBN 978-0-387-98780-4.",https://en.wikipedia.org/wiki/Empirical_risk_minimization,"['All articles to be expanded', 'Articles to be expanded from February 2010', 'Articles using small message boxes', 'Machine learning']",Data Science
88,Ensemble learning,"In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.
Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.

Overview
Supervised learning algorithms perform the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem. Even if the hypothesis space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form a (hopefully) better hypothesis. The term ensemble is usually reserved for methods that generate multiple hypotheses using the same base learner.
The broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner.Evaluating the prediction of an ensemble typically requires more computation than evaluating the prediction of a single model. In one sense, ensemble learning may be thought of as a way to compensate for poor learning algorithms by performing a lot of extra computation. On the other hand, the alternative is to do a lot more learning on one non-ensemble system. An ensemble system may be more efficient at improving overall accuracy for the same increase in compute, storage, or communication resources by using that increase on two or more methods, than would have been improved by increasing resource use for a single method.  Fast algorithms such as decision trees are commonly used in ensemble methods (for example, random forests), although slower algorithms can benefit from ensemble techniques as well.
By analogy, ensemble techniques have been used also in unsupervised learning scenarios, for example in consensus clustering or in anomaly detection.

Ensemble theory
An ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. The trained ensemble, therefore, represents a single hypothesis. This hypothesis, however, is not necessarily contained within the hypothesis space of the models from which it is built. Thus, ensembles can be shown to have more flexibility in the functions they can represent. This flexibility can, in theory, enable them to over-fit the training data more than a single model would, but in practice, some ensemble techniques (especially bagging) tend to reduce problems related to over-fitting of the training data.Empirically, ensembles tend to yield better results when there is a significant diversity among the models. Many ensemble methods, therefore, seek to promote diversity among the models they combine. Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees). Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity.

Ensemble size
While the number of component classifiers of an ensemble has a great impact on the accuracy of prediction, there is a limited number of studies addressing this problem. A priori determining of ensemble size and the volume and velocity of big data streams make this even more crucial for online ensemble classifiers. Mostly statistical tests were used for determining the proper number of components. More recently, a theoretical framework suggested that there is an ideal number of component classifiers for an ensemble such that having more or less than this number of classifiers would deteriorate the accuracy. It is called ""the law of diminishing returns in ensemble construction."" Their theoretical framework shows that using the same number of independent component classifiers as class labels gives the highest accuracy.

Common types of ensembles
Bayes optimal classifier
The Bayes optimal classifier is a classification technique. It is an ensemble of all the hypotheses in the hypothesis space. On average, no other ensemble can outperform it. The naive Bayes optimal classifier is a version of this that assumes that the data is conditionally independent on the class and makes the computation more feasible. Each hypothesis is given a vote proportional to the likelihood that the training dataset would be sampled from a system if that hypothesis were true. To facilitate training data of finite size, the vote of each hypothesis is also multiplied by the prior probability of that hypothesis. The Bayes optimal classifier can be expressed with the following equation:

  
    
      
        y
        =
        
          
            
              a
              r
              g
              m
              a
              x
            
            
              
                c
                
                  j
                
              
              ∈
              C
            
          
        
        
          ∑
          
            
              h
              
                i
              
            
            ∈
            H
          
        
        
          P
          (
          
            c
            
              j
            
          
          
            |
          
          
            h
            
              i
            
          
          )
          P
          (
          T
          
            |
          
          
            h
            
              i
            
          
          )
          P
          (
          
            h
            
              i
            
          
          )
        
      
    
    {\displaystyle y={\underset {c_{j}\in C}{\mathrm {argmax} }}\sum _{h_{i}\in H}{P(c_{j}|h_{i})P(T|h_{i})P(h_{i})}}
  where 
  
    
      
        y
      
    
    {\displaystyle y}
   is the predicted class, 
  
    
      
        C
      
    
    {\displaystyle C}
   is the set of all possible classes, 
  
    
      
        H
      
    
    {\displaystyle H}
   is the hypothesis space, 
  
    
      
        P
      
    
    {\displaystyle P}
   refers to a probability, and 
  
    
      
        T
      
    
    {\displaystyle T}
   is the training data. As an ensemble, the Bayes optimal classifier represents a hypothesis that is not necessarily in 
  
    
      
        H
      
    
    {\displaystyle H}
  . The hypothesis represented by the Bayes optimal classifier, however, is the optimal hypothesis in ensemble space (the space of all possible ensembles consisting only of hypotheses in 
  
    
      
        H
      
    
    {\displaystyle H}
  ).
This formula can be restated using Bayes' theorem, which says that the posterior is proportional to the likelihood times the prior:

  
    
      
        P
        (
        
          h
          
            i
          
        
        
          |
        
        T
        )
        ∝
        P
        (
        T
        
          |
        
        
          h
          
            i
          
        
        )
        P
        (
        
          h
          
            i
          
        
        )
      
    
    {\displaystyle P(h_{i}|T)\propto P(T|h_{i})P(h_{i})}
  hence,

  
    
      
        y
        =
        
          
            
              a
              r
              g
              m
              a
              x
            
            
              
                c
                
                  j
                
              
              ∈
              C
            
          
        
        
          ∑
          
            
              h
              
                i
              
            
            ∈
            H
          
        
        
          P
          (
          
            c
            
              j
            
          
          
            |
          
          
            h
            
              i
            
          
          )
          P
          (
          
            h
            
              i
            
          
          
            |
          
          T
          )
        
      
    
    {\displaystyle y={\underset {c_{j}\in C}{\mathrm {argmax} }}\sum _{h_{i}\in H}{P(c_{j}|h_{i})P(h_{i}|T)}}

Bootstrap aggregating (bagging)
Bootstrap aggregating, often abbreviated as bagging, involves having each model in the ensemble vote with equal weight. In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set. As an example, the random forest algorithm combines random decision trees with bagging to achieve very high classification accuracy.In bagging the samples are generated in such a way that the samples are different from each other however replacement is allowed. Replacement means that an instance can occur in multiple samples multiple times or it can not appear in some samples at all. These samples are then given to multiple learners and then the results from each learner are combined in the form of voting.

Boosting
Boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis-classified. In some cases, boosting has been shown to yield better accuracy than bagging, but it also tends to be more likely to over-fit the training data. By far, the most common implementation of boosting is Adaboost, although some newer algorithms are reported to achieve better results.In Boosting, an equal weight (uniform probability distribution) is given to the sample training data (say D1) at the very starting round. This data (D1) is then given to a base learner (say L1). The mis-classified instances by L1 are assigned a weight higher than the correctly classified instances, but keeping in mind that the total probability distribution will be equal to 1. This boosted data (say D2) is then given to second base learner (say L2) and so on. The results are then combined in the form of voting.

Bayesian model averaging
Bayesian model averaging (BMA) makes predictions using an average over several models with weights given by the posterior probability of each model given the data. BMA is known to generally give better answers than a single model, obtained, e.g., via stepwise regression, especially where very different models have nearly identical performance in the training set but may otherwise perform quite differently.
The most obvious question with any technique that uses Bayes' theorem is the prior, i.e., a specification of the probability (subjective, perhaps) that each model is the best to use for a given purpose.  Conceptually, BMA can be used with any prior.  The ensembleBMA and BMA packages for R use the prior implied by the Bayesian information criterion, (BIC), following Raftery (1995). The BAS package for R supports the use of the priors implied by Akaike information criterion (AIC) and other criteria over the alternative models as well as priors over the coefficients.The difference between BIC and AIC is the strength of preference for parsimony.  The penalty for model complexity is 
  
    
      
        ln
        ⁡
        (
        n
        )
        k
      
    
    {\displaystyle \ln(n)k}
   for the BIC and 
  
    
      
        2
        k
      
    
    {\displaystyle 2k}
   for the AIC.  Large sample asymptotic theory has established that if there is a best model then with increasing sample sizes, BIC is strongly consistent, i.e., will almost certainly find it, while AIC may not, because AIC may continue to place excessive posterior probability on models that are more complicated than they need to be.  If on the other hand we are more concerned with efficiency, i.e., minimum mean square prediction error, then asymptotically, AIC and AICc are “efficient” while BIC is not.Burnham and Anderson (1998, 2002) contributed greatly to introducing a wider audience to the basic ideas of Bayesian model averaging and popularizing the methodology. The availability of software, including other free open-source packages for R beyond those mentioned above, helped make the methods accessible to a wider audience.Haussler et al. (1994) showed that when BMA is used for classification, its expected error is at most twice the expected error of the Bayes optimal classifier.

Bayesian model combination
Bayesian model combination (BMC) is an algorithmic correction to Bayesian model averaging (BMA). Instead of sampling each model in the ensemble individually, it samples from the space of possible ensembles (with model weightings drawn randomly from a Dirichlet distribution having uniform parameters). This modification overcomes the tendency of BMA to converge toward giving all of the weight to a single model. Although BMC is somewhat more computationally expensive than BMA, it tends to yield dramatically better results. The results from BMC have been shown to be better on average (with statistical significance) than BMA, and bagging.The use of Bayes' law to compute model weights necessitates computing the probability of the data given each model. Typically, none of the models in the ensemble are exactly the distribution from which the training data were generated, so all of them correctly receive a value close to zero for this term. This would work well if the ensemble were big enough to sample the entire model-space, but such is rarely possible. Consequently, each pattern in the training data will cause the ensemble weight to shift toward the model in the ensemble that is closest to the distribution of the training data. It essentially reduces to an unnecessarily complex method for doing model selection.
The possible weightings for an ensemble can be visualized as lying on a simplex. At each vertex of the simplex, all of the weight is given to a single model in the ensemble. BMA converges toward the vertex that is closest to the distribution of the training data. By contrast, BMC converges toward the point where this distribution projects onto the simplex. In other words, instead of selecting the one model that is closest to the generating distribution, it seeks the combination of models that is closest to the generating distribution.
The results from BMA can often be approximated by using cross-validation to select the best model from a bucket of models. Likewise, the results from BMC may be approximated by using cross-validation to select the best ensemble combination from a random sampling of possible weightings.

Bucket of models
A ""bucket of models"" is an ensemble technique in which a model selection algorithm is used to choose the best model for each problem. When tested with only one problem, a bucket of models can produce no better results than the best model in the set, but when evaluated across many problems, it will typically produce much better results, on average, than any model in the set.
The most common approach used for model-selection is cross-validation selection (sometimes called a ""bake-off contest""). It is described with the following pseudo-code:

For each model m in the bucket:
    Do c times: (where 'c' is some constant)
        Randomly divide the training dataset into two datasets: A, and B.
        Train m with A
        Test m with B
Select the model that obtains the highest average score

Cross-Validation Selection can be summed up as: ""try them all with the training set, and pick the one that works best"".Gating is a generalization of Cross-Validation Selection. It involves training another learning model to decide which of the models in the bucket is best-suited to solve the problem. Often, a perceptron is used for the gating model. It can be used to pick the ""best"" model, or it can be used to give a linear weight to the predictions from each model in the bucket.
When a bucket of models is used with a large set of problems, it may be desirable to avoid training some of the models that take a long time to train. Landmark learning is a meta-learning approach that seeks to solve this problem. It involves training only the fast (but imprecise) algorithms in the bucket, and then using the performance of these algorithms to help determine which slow (but accurate) algorithm is most likely to do best.

Stacking
Stacking (sometimes called stacked generalization) involves training a learning algorithm to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although, in practice, a logistic regression model is often used as the combiner.
Stacking typically yields performance better than any single one of the trained models. 
It has been successfully used on both supervised learning tasks 
(regression, classification and distance learning )
and unsupervised learning (density estimation). It has also been used to
estimate bagging's error rate. It has been reported to out-perform Bayesian model-averaging.
The two top-performers in the Netflix competition utilized blending, which may be considered to be a form of stacking.

Implementations in statistics packages
R: at least three packages offer Bayesian model averaging tools, including the BMS (an acronym for Bayesian Model Selection) package, the BAS (an acronym for Bayesian Adaptive Sampling) package, and the BMA package.
Python: Scikit-learn, a package for machine learning in Python offers packages for ensemble learning including packages for bagging and averaging methods.
MATLAB: classification ensembles are implemented in Statistics and Machine Learning Toolbox.

Ensemble learning applications
In the recent years, due to the growing computational power which allows training large ensemble learning in a reasonable time frame, the number of its applications has grown increasingly. Some of the applications of ensemble classifiers include:

Remote sensing
Land cover mapping
Land cover mapping is one of the major applications of Earth observation satellite sensors, using remote sensing and geospatial data, to identify the materials and objects which are located on the surface of target areas. Generally, the classes of target materials include roads, buildings, rivers, lakes, and vegetation. Some different ensemble learning approaches based on artificial neural networks, kernel principal component analysis (KPCA), decision trees with boosting, random forest and automatic design of multiple classifier systems, are proposed to efficiently identify land cover objects.

Change detection
Change detection is an image analysis problem, consisting of the identification of places where the land cover has changed over time. Change detection is widely used in fields such as urban growth, forest and vegetation dynamics, land use and disaster monitoring.
The earliest applications of ensemble classifiers in change detection are designed with the majority voting, Bayesian average and the maximum posterior probability.

Computer security
Distributed denial of service
Distributed denial of service is one of the most threatening cyber-attacks that may happen to an internet service provider. By combining the output of single classifiers, ensemble classifiers reduce the total error of detecting and discriminating such attacks from legitimate flash crowds.

Malware Detection
Classification of malware codes such as computer viruses, computer worms, trojans, ransomware and spywares with the usage of machine learning techniques, is inspired by the document categorization problem. Ensemble learning systems have shown a proper efficacy in this area.

Intrusion detection
An intrusion detection system monitors computer network or computer systems to identify intruder codes like an anomaly detection process. Ensemble learning successfully aids such monitoring systems to reduce their total error.

Face recognition
Face recognition, which recently has become one of the most popular research areas of pattern recognition, copes with identification or verification of a person by their digital images.Hierarchical ensembles based on Gabor Fisher classifier and independent component analysis preprocessing techniques are some of the earliest ensembles employed in this field.

Emotion recognition
While speech recognition is mainly based on deep learning because most of the industry players in this field like Google, Microsoft and IBM reveal that the core technology of their speech recognition is based on this approach, speech-based emotion recognition can also have a satisfactory performance with ensemble learning.It is also being successfully used in facial emotion recognition.

Fraud detection
Fraud detection deals with the identification of bank fraud, such as money laundering, credit card fraud and telecommunication fraud, which have vast domains of research and applications of machine learning. Because ensemble learning improves the robustness of the normal behavior modelling, it has been proposed as an efficient technique to detect such fraudulent cases and activities in banking and credit card systems.

Financial decision-making
The accuracy of prediction of business failure is a very crucial issue in financial decision-making. Therefore, different ensemble classifiers are proposed to predict financial crises and financial distress. Also, in the trade-based manipulation problem, where traders attempt to manipulate stock prices by buying and selling activities, ensemble classifiers are required to analyze the changes in the stock market data and detect suspicious symptom of stock price manipulation.

Medicine
Ensemble classifiers have been successfully applied in neuroscience, proteomics and medical diagnosis like in neuro-cognitive disorder (i.e. Alzheimer or myotonic dystrophy) detection based on MRI datasets.

See also
Ensemble averaging (machine learning)
Bayesian structural time series (BSTS)

References
Further reading
Zhou Zhihua (2012). Ensemble Methods: Foundations and Algorithms. Chapman and Hall/CRC. ISBN 978-1-439-83003-1.
Robert Schapire; Yoav Freund (2012). Boosting: Foundations and Algorithms. MIT. ISBN 978-0-262-01718-3.

External links
Robi Polikar (ed.). ""Ensemble learning"". Scholarpedia.
The Waffles (machine learning) toolkit contains implementations of Bagging, Boosting, Bayesian Model Averaging, Bayesian Model Combination, Bucket-of-models, and other ensemble techniques",https://en.wikipedia.org/wiki/Ensemble_learning,"['All articles with specifically marked weasel-worded phrases', 'All articles with unsourced statements', 'Articles with specifically marked weasel-worded phrases from December 2017', 'Articles with unsourced statements from December 2017', 'Articles with unsourced statements from January 2012', 'CS1 errors: missing periodical', 'Ensemble learning']",Data Science
89,Expectation–maximization algorithm,"In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.

History
The EM algorithm was explained and given its name in a classic 1977 paper by Arthur Dempster, Nan Laird, and Donald Rubin. They pointed out that the method had been ""proposed many times in special circumstances"" by earlier authors. One of the earliest is the gene-counting method for estimating allele frequencies by Cedric Smith.  A very detailed treatment of the EM method for exponential families was published by Rolf Sundberg in his thesis and several papers following his collaboration with Per Martin-Löf and Anders Martin-Löf. The Dempster–Laird–Rubin paper in 1977 generalized the method and sketched a convergence analysis for a wider class of problems. The Dempster–Laird–Rubin paper established the EM method as an important tool of statistical analysis.
The convergence analysis of the Dempster–Laird–Rubin algorithm was flawed and a correct convergence analysis was published by C. F. Jeff Wu in 1983.
Wu's proof established the EM method's convergence outside of the exponential family, as claimed by Dempster–Laird–Rubin.

Introduction
The EM algorithm is used to find (local) maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly.  Typically these models involve latent variables in addition to unknown parameters and known data observations.  That is, either missing values exist among the data, or the model can be formulated more simply by assuming the existence of further unobserved data points. For example, a mixture model can be described more simply by assuming that each observed data point has a corresponding unobserved data point, or latent variable, specifying the mixture component to which each data point belongs.
Finding a maximum likelihood solution typically requires taking the derivatives of the likelihood function with respect to all the unknown values, the parameters and the latent variables, and simultaneously solving the resulting equations. In statistical models with latent variables, this is usually impossible. Instead, the result is typically a set of interlocking equations in which the solution to the parameters requires the values of the latent variables and vice versa, but substituting one set of equations into the other produces an unsolvable equation.
The EM algorithm proceeds from the observation that there is a way to solve these two sets of equations numerically. One can simply pick arbitrary values for one of the two sets of unknowns, use them to estimate the second set, then use these new values to find a better estimate of the first set, and then keep alternating between the two until the resulting values both converge to fixed points.  It's not obvious that this will work, but it can be proven that in this context it does, and that the derivative of the likelihood is (arbitrarily close to) zero at that point, which in turn means that the point is either a maximum or a saddle point. In general, multiple maxima may occur, with no guarantee that the global maximum will be found.  Some likelihoods also have singularities in them, i.e., nonsensical maxima.  For example, one of the solutions that may be found by EM in a mixture model involves setting one of the components to have zero variance and the mean parameter for the same component to be equal to one of the data points.

Description
Given the statistical model which generates a set 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
   of observed data, a set of unobserved latent data or missing values 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
  , and a vector of unknown parameters 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
  , along with a likelihood function 
  
    
      
        L
        (
        
          θ
        
        ;
        
          X
        
        ,
        
          Z
        
        )
        =
        p
        (
        
          X
        
        ,
        
          Z
        
        ∣
        
          θ
        
        )
      
    
    {\displaystyle L({\boldsymbol {\theta }};\mathbf {X} ,\mathbf {Z} )=p(\mathbf {X} ,\mathbf {Z} \mid {\boldsymbol {\theta }})}
  , the maximum likelihood estimate (MLE) of the unknown parameters is determined by maximizing the marginal likelihood of the observed data

  
    
      
        L
        (
        
          θ
        
        ;
        
          X
        
        )
        =
        p
        (
        
          X
        
        ∣
        
          θ
        
        )
        =
        ∫
        p
        (
        
          X
        
        ,
        
          Z
        
        ∣
        
          θ
        
        )
        
        d
        
          Z
        
      
    
    {\displaystyle L({\boldsymbol {\theta }};\mathbf {X} )=p(\mathbf {X} \mid {\boldsymbol {\theta }})=\int p(\mathbf {X} ,\mathbf {Z} \mid {\boldsymbol {\theta }})\,d\mathbf {Z} }
  However, this quantity is often intractable (e.g. if 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
   is a sequence of events, so that the number of values grows exponentially with the sequence length, the exact calculation of the sum will be extremely difficult).
The EM algorithm seeks to find the MLE of the marginal likelihood by iteratively applying these two steps:

Expectation step (E step): Define 
  
    
      
        Q
        (
        
          θ
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
      
    
    {\displaystyle Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})}
   as the expected value of the log likelihood function of 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
  , with respect to the current conditional distribution of 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
   given 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
   and the current estimates of the parameters 
  
    
      
        
          
            θ
          
          
            (
            t
            )
          
        
      
    
    {\displaystyle {\boldsymbol {\theta }}^{(t)}}
  :

  
    
      
        Q
        (
        
          θ
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
        =
        
          E
          
            
              Z
            
            ∣
            
              X
            
            ,
            
              
                θ
              
              
                (
                t
                )
              
            
          
        
        ⁡
        
          [
          
            log
            ⁡
            L
            (
            
              θ
            
            ;
            
              X
            
            ,
            
              Z
            
            )
          
          ]
        
        
      
    
    {\displaystyle Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})=\operatorname {E} _{\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)}}\left[\log L({\boldsymbol {\theta }};\mathbf {X} ,\mathbf {Z} )\right]\,}
  Maximization step (M step): Find the parameters that maximize this quantity:

  
    
      
        
          
            θ
          
          
            (
            t
            +
            1
            )
          
        
        =
        
          
            
              a
              r
              g
              
              m
              a
              x
            
            θ
          
        
         
        Q
        (
        
          θ
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
        
      
    
    {\displaystyle {\boldsymbol {\theta }}^{(t+1)}={\underset {\boldsymbol {\theta }}{\operatorname {arg\,max} }}\ Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})\,}
  The typical models to which EM is applied use 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
   as a latent variable indicating membership in one of a set of groups:

The observed data points 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
   may be discrete (taking values in a finite or countably infinite set) or continuous (taking values in an uncountably infinite set). Associated with each data point may be a vector of observations.
The missing values (aka latent variables) 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
   are discrete, drawn from a fixed number of values, and with one latent variable per observed unit.
The parameters are continuous, and are of two kinds: Parameters that are associated with all data points, and those associated with a specific value of a latent variable (i.e., associated with all data points which corresponding latent variable has that value).However, it is possible to apply EM to other sorts of models.
The motive is as follows.  If the value of the parameters 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
   is known, usually the value of the latent variables 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
   can be found by maximizing the log-likelihood over all possible values of 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
  , either simply by iterating over 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
   or through an algorithm such as the Baum–Welch algorithm for hidden Markov models.  Conversely, if we know the value of the latent variables 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
  , we can find an estimate of the parameters 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
   fairly easily, typically by simply grouping the observed data points according to the value of the associated latent variable and averaging the values, or some function of the values, of the points in each group.  This suggests an iterative algorithm, in the case where both 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
   and 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
   are unknown:

First, initialize the parameters 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
   to some random values.
Compute the probability of each possible value of 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
   , given 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
  .
Then, use the just-computed values of 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
   to compute a better estimate for the parameters 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
  .
Iterate steps 2 and 3 until convergence.The algorithm as just described monotonically approaches a local minimum of the cost function.

Properties
Speaking of an expectation (E) step is a bit of a misnomer. What are calculated in the first step are the fixed, data-dependent parameters of the function Q. Once the parameters of Q are known, it is fully determined and is maximized in the second (M) step of an EM algorithm.
Although an EM iteration does increase the observed data (i.e., marginal) likelihood function, no guarantee exists that the sequence converges to a maximum likelihood estimator. For multimodal distributions, this means that an EM algorithm may converge to a local maximum of the observed data likelihood function, depending on starting values. A variety of heuristic or metaheuristic approaches exist to escape a local maximum, such as random-restart hill climbing (starting with several different random initial estimates θ(t)), or applying simulated annealing methods.
EM is especially useful when the likelihood is an exponential family: the E step becomes the sum of expectations of sufficient statistics, and the M step involves maximizing a linear function. In such a case, it is usually possible to derive closed-form expression updates for each step, using the Sundberg formula (published by Rolf Sundberg using unpublished results of Per Martin-Löf and Anders Martin-Löf).The EM method was modified to compute maximum a posteriori (MAP) estimates for Bayesian inference in the original paper by Dempster, Laird, and Rubin.
Other methods exist to find maximum likelihood estimates, such as gradient descent, conjugate gradient, or variants of the Gauss–Newton algorithm. Unlike EM, such methods typically require the evaluation of first and/or second derivatives of the likelihood function.

Proof of correctness
Expectation-maximization works to improve 
  
    
      
        Q
        (
        
          θ
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
      
    
    {\displaystyle Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})}
   rather than directly improving 
  
    
      
        log
        ⁡
        p
        (
        
          X
        
        ∣
        
          θ
        
        )
      
    
    {\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})}
  .  Here it is shown that improvements to the former imply improvements to the latter.For any 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
   with non-zero probability 
  
    
      
        p
        (
        
          Z
        
        ∣
        
          X
        
        ,
        
          θ
        
        )
      
    
    {\displaystyle p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }})}
  , we can write

  
    
      
        log
        ⁡
        p
        (
        
          X
        
        ∣
        
          θ
        
        )
        =
        log
        ⁡
        p
        (
        
          X
        
        ,
        
          Z
        
        ∣
        
          θ
        
        )
        −
        log
        ⁡
        p
        (
        
          Z
        
        ∣
        
          X
        
        ,
        
          θ
        
        )
        .
      
    
    {\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})=\log p(\mathbf {X} ,\mathbf {Z} \mid {\boldsymbol {\theta }})-\log p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}).}
  We take the expectation over possible values of the unknown data 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
   under the current parameter estimate 
  
    
      
        
          θ
          
            (
            t
            )
          
        
      
    
    {\displaystyle \theta ^{(t)}}
   by multiplying both sides by 
  
    
      
        p
        (
        
          Z
        
        ∣
        
          X
        
        ,
        
          
            θ
          
          
            (
            t
            )
          
        
        )
      
    
    {\displaystyle p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})}
   and summing (or integrating) over 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbf {Z} }
  .  The left-hand side is the expectation of a constant, so we get:

  
    
      
        
          
            
              
                log
                ⁡
                p
                (
                
                  X
                
                ∣
                
                  θ
                
                )
              
              
                
                =
                
                  ∑
                  
                    
                      Z
                    
                  
                
                p
                (
                
                  Z
                
                ∣
                
                  X
                
                ,
                
                  
                    θ
                  
                  
                    (
                    t
                    )
                  
                
                )
                log
                ⁡
                p
                (
                
                  X
                
                ,
                
                  Z
                
                ∣
                
                  θ
                
                )
                −
                
                  ∑
                  
                    
                      Z
                    
                  
                
                p
                (
                
                  Z
                
                ∣
                
                  X
                
                ,
                
                  
                    θ
                  
                  
                    (
                    t
                    )
                  
                
                )
                log
                ⁡
                p
                (
                
                  Z
                
                ∣
                
                  X
                
                ,
                
                  θ
                
                )
              
            
            
              
              
                
                =
                Q
                (
                
                  θ
                
                ∣
                
                  
                    θ
                  
                  
                    (
                    t
                    )
                  
                
                )
                +
                H
                (
                
                  θ
                
                ∣
                
                  
                    θ
                  
                  
                    (
                    t
                    )
                  
                
                )
                ,
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\log p(\mathbf {X} \mid {\boldsymbol {\theta }})&=\sum _{\mathbf {Z} }p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})\log p(\mathbf {X} ,\mathbf {Z} \mid {\boldsymbol {\theta }})-\sum _{\mathbf {Z} }p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }}^{(t)})\log p(\mathbf {Z} \mid \mathbf {X} ,{\boldsymbol {\theta }})\\&=Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})+H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)}),\end{aligned}}}
  where 
  
    
      
        H
        (
        
          θ
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
      
    
    {\displaystyle H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})}
   is defined by the negated sum it is replacing.
This last equation holds for every value of 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
   including 
  
    
      
        
          θ
        
        =
        
          
            θ
          
          
            (
            t
            )
          
        
      
    
    {\displaystyle {\boldsymbol {\theta }}={\boldsymbol {\theta }}^{(t)}}
  ,

  
    
      
        log
        ⁡
        p
        (
        
          X
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
        =
        Q
        (
        
          
            θ
          
          
            (
            t
            )
          
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
        +
        H
        (
        
          
            θ
          
          
            (
            t
            )
          
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
        ,
      
    
    {\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }}^{(t)})=Q({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)})+H({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)}),}
  and subtracting this last equation from the previous equation gives

  
    
      
        log
        ⁡
        p
        (
        
          X
        
        ∣
        
          θ
        
        )
        −
        log
        ⁡
        p
        (
        
          X
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
        =
        Q
        (
        
          θ
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
        −
        Q
        (
        
          
            θ
          
          
            (
            t
            )
          
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
        +
        H
        (
        
          θ
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
        −
        H
        (
        
          
            θ
          
          
            (
            t
            )
          
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
        ,
      
    
    {\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})-\log p(\mathbf {X} \mid {\boldsymbol {\theta }}^{(t)})=Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})-Q({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)})+H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})-H({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)}),}
  However, Gibbs' inequality tells us that 
  
    
      
        H
        (
        
          θ
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
        ≥
        H
        (
        
          
            θ
          
          
            (
            t
            )
          
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
      
    
    {\displaystyle H({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})\geq H({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)})}
  , so we can conclude that

  
    
      
        log
        ⁡
        p
        (
        
          X
        
        ∣
        
          θ
        
        )
        −
        log
        ⁡
        p
        (
        
          X
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
        ≥
        Q
        (
        
          θ
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
        −
        Q
        (
        
          
            θ
          
          
            (
            t
            )
          
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
        .
      
    
    {\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})-\log p(\mathbf {X} \mid {\boldsymbol {\theta }}^{(t)})\geq Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})-Q({\boldsymbol {\theta }}^{(t)}\mid {\boldsymbol {\theta }}^{(t)}).}
  In words, choosing 
  
    
      
        
          θ
        
      
    
    {\displaystyle {\boldsymbol {\theta }}}
   to improve 
  
    
      
        Q
        (
        
          θ
        
        ∣
        
          
            θ
          
          
            (
            t
            )
          
        
        )
      
    
    {\displaystyle Q({\boldsymbol {\theta }}\mid {\boldsymbol {\theta }}^{(t)})}
   causes 
  
    
      
        log
        ⁡
        p
        (
        
          X
        
        ∣
        
          θ
        
        )
      
    
    {\displaystyle \log p(\mathbf {X} \mid {\boldsymbol {\theta }})}
   to improve at least as much.

As a maximization–maximization procedure
The EM algorithm can be viewed as two alternating maximization steps, that is, as an example of coordinate descent. Consider the function:

  
    
      
        F
        (
        q
        ,
        θ
        )
        :=
        
          E
          
            q
          
        
        ⁡
        [
        log
        ⁡
        L
        (
        θ
        ;
        x
        ,
        Z
        )
        ]
        +
        H
        (
        q
        )
        ,
      
    
    {\displaystyle F(q,\theta ):=\operatorname {E} _{q}[\log L(\theta ;x,Z)]+H(q),}
  where q is an arbitrary probability distribution over the unobserved data z and H(q) is the entropy of the distribution q. This function can be written as

  
    
      
        F
        (
        q
        ,
        θ
        )
        =
        −
        
          D
          
            
              K
              L
            
          
        
        
          
            (
          
        
        q
        ∥
        
          p
          
            Z
            ∣
            X
          
        
        (
        ⋅
        ∣
        x
        ;
        θ
        )
        
          
            )
          
        
        +
        log
        ⁡
        L
        (
        θ
        ;
        x
        )
        ,
      
    
    {\displaystyle F(q,\theta )=-D_{\mathrm {KL} }{\big (}q\parallel p_{Z\mid X}(\cdot \mid x;\theta ){\big )}+\log L(\theta ;x),}
  where  
  
    
      
        
          p
          
            Z
            ∣
            X
          
        
        (
        ⋅
        ∣
        x
        ;
        θ
        )
      
    
    {\displaystyle p_{Z\mid X}(\cdot \mid x;\theta )}
   is the conditional distribution of the unobserved data given the observed data 
  
    
      
        x
      
    
    {\displaystyle x}
   and 
  
    
      
        
          D
          
            K
            L
          
        
      
    
    {\displaystyle D_{KL}}
   is the Kullback–Leibler divergence.
Then the steps in the EM algorithm may be viewed as:

Expectation step: Choose 
  
    
      
        q
      
    
    {\displaystyle q}
   to maximize 
  
    
      
        F
      
    
    {\displaystyle F}
  :

  
    
      
        
          q
          
            (
            t
            )
          
        
        =
        
          
            a
            r
            g
            
            m
            a
            x
          
          
            q
          
        
        ⁡
         
        F
        (
        q
        ,
        
          θ
          
            (
            t
            )
          
        
        )
      
    
    {\displaystyle q^{(t)}=\operatorname {arg\,max} _{q}\ F(q,\theta ^{(t)})}
  
Maximization step: Choose 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   to maximize 
  
    
      
        F
      
    
    {\displaystyle F}
  :

  
    
      
        
          θ
          
            (
            t
            +
            1
            )
          
        
        =
        
          
            a
            r
            g
            
            m
            a
            x
          
          
            θ
          
        
        ⁡
         
        F
        (
        
          q
          
            (
            t
            )
          
        
        ,
        θ
        )
      
    
    {\displaystyle \theta ^{(t+1)}=\operatorname {arg\,max} _{\theta }\ F(q^{(t)},\theta )}

Applications
EM is frequently used for parameter estimation of mixed models, notably in quantitative genetics.In psychometrics, EM is an important tool for estimating item parameters and latent abilities of item response theory models.
With the ability to deal with missing data and observe unidentified variables, EM is becoming a useful tool to price and manage risk of a portfolio.The EM algorithm (and its faster variant ordered subset expectation maximization) is also widely used in medical image reconstruction, especially in positron emission tomography, single photon emission computed tomography, and x-ray computed tomography. See below for other faster variants of EM.
In structural engineering, the Structural Identification using Expectation Maximization (STRIDE) algorithm is an output-only method for identifying natural vibration properties of a structural system using sensor data (see Operational Modal Analysis).
EM is also frequently used for data clustering, computer vision and in machine learning. In natural language processing, two prominent instances of the algorithm are the Baum–Welch algorithm for hidden Markov models, and the inside-outside algorithm for unsupervised induction of probabilistic context-free grammars.

Filtering and smoothing EM algorithms
A Kalman filter is typically used for on-line state estimation and a minimum-variance smoother may be employed for off-line or batch state estimation. However, these minimum-variance solutions require estimates of the state-space model parameters. EM algorithms can be used for solving joint state and parameter estimation problems.
Filtering and smoothing EM algorithms arise by repeating this two-step procedure:

E-step
Operate a Kalman filter or a minimum-variance smoother designed with current parameter estimates to obtain updated state estimates.M-step
Use the filtered or smoothed state estimates within maximum-likelihood calculations to obtain updated parameter estimates.Suppose that a Kalman filter or minimum-variance smoother operates on measurements of a single-input-single-output system that possess additive white noise. An updated measurement noise variance estimate can be obtained from the maximum likelihood calculation

  
    
      
        
          
            
              
                σ
                ^
              
            
          
          
            v
          
          
            2
          
        
        =
        
          
            1
            N
          
        
        
          ∑
          
            k
            =
            1
          
          
            N
          
        
        
          
            (
            
              z
              
                k
              
            
            −
            
              
                
                  
                    x
                    ^
                  
                
              
              
                k
              
            
            )
          
          
            2
          
        
        ,
      
    
    {\displaystyle {\widehat {\sigma }}_{v}^{2}={\frac {1}{N}}\sum _{k=1}^{N}{(z_{k}-{\widehat {x}}_{k})}^{2},}
  where 
  
    
      
        
          
            
              
                x
                ^
              
            
          
          
            k
          
        
      
    
    {\displaystyle {\widehat {x}}_{k}}
   are scalar output estimates calculated by a filter or a smoother from N scalar measurements 
  
    
      
        
          z
          
            k
          
        
      
    
    {\displaystyle z_{k}}
  . The above update can also be applied to updating a Poisson measurement noise intensity. Similarly, for a first-order auto-regressive process, an updated process noise variance estimate can be calculated by

  
    
      
        
          
            
              
                σ
                ^
              
            
          
          
            w
          
          
            2
          
        
        =
        
          
            1
            N
          
        
        
          ∑
          
            k
            =
            1
          
          
            N
          
        
        
          
            (
            
              
                
                  
                    x
                    ^
                  
                
              
              
                k
                +
                1
              
            
            −
            
              
                
                  F
                  ^
                
              
            
            
              
                
                  
                    x
                    ^
                  
                
              
              
                k
              
            
            )
          
          
            2
          
        
        ,
      
    
    {\displaystyle {\widehat {\sigma }}_{w}^{2}={\frac {1}{N}}\sum _{k=1}^{N}{({\widehat {x}}_{k+1}-{\widehat {F}}{\widehat {x}}_{k})}^{2},}
  where 
  
    
      
        
          
            
              
                x
                ^
              
            
          
          
            k
          
        
      
    
    {\displaystyle {\widehat {x}}_{k}}
   and 
  
    
      
        
          
            
              
                x
                ^
              
            
          
          
            k
            +
            1
          
        
      
    
    {\displaystyle {\widehat {x}}_{k+1}}
   are scalar state estimates calculated by a filter or a smoother. The updated model coefficient estimate is obtained via

  
    
      
        
          
            
              F
              ^
            
          
        
        =
        
          
            
              
                ∑
                
                  k
                  =
                  1
                
                
                  N
                
              
              (
              
                
                  
                    
                      x
                      ^
                    
                  
                
                
                  k
                  +
                  1
                
              
              −
              
                
                  
                    F
                    ^
                  
                
              
              
                
                  
                    
                      x
                      ^
                    
                  
                
                
                  k
                
              
              )
            
            
              
                ∑
                
                  k
                  =
                  1
                
                
                  N
                
              
              
                
                  
                    
                      x
                      ^
                    
                  
                
                
                  k
                
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle {\widehat {F}}={\frac {\sum _{k=1}^{N}({\widehat {x}}_{k+1}-{\widehat {F}}{\widehat {x}}_{k})}{\sum _{k=1}^{N}{\widehat {x}}_{k}^{2}}}.}
  The convergence of parameter estimates such as those above are well studied.

Variants
A number of methods have been proposed to accelerate the sometimes slow convergence of the EM algorithm, such as those using conjugate gradient and modified Newton's methods (Newton–Raphson). Also, EM can be used with constrained estimation methods.
Parameter-expanded expectation maximization (PX-EM) algorithm often provides speed up by ""us[ing] a `covariance adjustment' to correct the analysis of the M step, capitalising on extra information captured in the imputed complete data"".Expectation conditional maximization (ECM) replaces each M step with a sequence of conditional maximization (CM) steps in which each parameter θi is maximized individually, conditionally on the other parameters remaining fixed. Itself can be extended into the Expectation conditional maximization either (ECME) algorithm.This idea is further extended in generalized expectation maximization (GEM) algorithm, in which is sought only an increase in the objective function F for both the E step and M step as described in the As a maximization–maximization procedure section. GEM is further developed in a distributed environment and shows promising results.It is also possible to consider the EM algorithm as a subclass of the MM (Majorize/Minimize or Minorize/Maximize, depending on context) algorithm, and therefore use any machinery developed in the more general case.

α-EM algorithm
The Q-function used in the EM algorithm is based on the log likelihood. Therefore, it is regarded as the log-EM algorithm. The use of the log likelihood can be generalized to that of the α-log likelihood ratio. Then, the α-log likelihood ratio of the observed data can be exactly expressed as equality by using the Q-function of the α-log likelihood ratio and the α-divergence. Obtaining this Q-function is a generalized E step. Its maximization is a generalized M step. This pair is called the α-EM algorithm
which contains the log-EM algorithm as its subclass. Thus, the α-EM algorithm by Yasuo Matsuyama is an exact generalization of the log-EM algorithm. No computation of gradient or Hessian matrix is needed. The α-EM shows faster convergence than the log-EM algorithm by choosing an appropriate α. The α-EM algorithm leads to a faster version of the Hidden Markov model estimation algorithm α-HMM.

Relation to variational Bayes methods
EM is a partially non-Bayesian, maximum likelihood method.  Its final result gives a probability distribution over the latent variables (in the Bayesian style) together with a point estimate for θ (either a maximum likelihood estimate or a posterior mode). A fully Bayesian version of this may be wanted, giving a probability distribution over θ and the latent variables.  The Bayesian approach to inference is simply to treat θ as another latent variable.  In this paradigm, the distinction between the E and M steps disappears.  If using the factorized Q approximation as described above (variational Bayes), solving can iterate over each latent variable (now including θ) and optimize them one at a time.  Now, k steps per iteration are needed, where k is the number of latent variables.  For graphical models this is easy to do as each variable's new Q depends only on its Markov blanket, so local message passing can be used for efficient inference.

Geometric interpretation
In information geometry, the E step and the M step are interpreted as projections under dual affine connections, called the e-connection and the m-connection; the Kullback–Leibler divergence can also be understood in these terms.

Examples
Gaussian mixture
Let 
  
    
      
        
          x
        
        =
        (
        
          
            x
          
          
            1
          
        
        ,
        
          
            x
          
          
            2
          
        
        ,
        …
        ,
        
          
            x
          
          
            n
          
        
        )
      
    
    {\displaystyle \mathbf {x} =(\mathbf {x} _{1},\mathbf {x} _{2},\ldots ,\mathbf {x} _{n})}
   be a sample of 
  
    
      
        n
      
    
    {\displaystyle n}
   independent observations from a mixture of two multivariate normal distributions of dimension 
  
    
      
        d
      
    
    {\displaystyle d}
  , and let 
  
    
      
        
          z
        
        =
        (
        
          z
          
            1
          
        
        ,
        
          z
          
            2
          
        
        ,
        …
        ,
        
          z
          
            n
          
        
        )
      
    
    {\displaystyle \mathbf {z} =(z_{1},z_{2},\ldots ,z_{n})}
   be the latent variables that determine the component from which the observation originates.

  
    
      
        
          X
          
            i
          
        
        ∣
        (
        
          Z
          
            i
          
        
        =
        1
        )
        ∼
        
          
            
              N
            
          
          
            d
          
        
        (
        
          
            μ
          
          
            1
          
        
        ,
        
          Σ
          
            1
          
        
        )
      
    
    {\displaystyle X_{i}\mid (Z_{i}=1)\sim {\mathcal {N}}_{d}({\boldsymbol {\mu }}_{1},\Sigma _{1})}
   and 
  
    
      
        
          X
          
            i
          
        
        ∣
        (
        
          Z
          
            i
          
        
        =
        2
        )
        ∼
        
          
            
              N
            
          
          
            d
          
        
        (
        
          
            μ
          
          
            2
          
        
        ,
        
          Σ
          
            2
          
        
        )
        ,
      
    
    {\displaystyle X_{i}\mid (Z_{i}=2)\sim {\mathcal {N}}_{d}({\boldsymbol {\mu }}_{2},\Sigma _{2}),}
  where

  
    
      
        P
        ⁡
        (
        
          Z
          
            i
          
        
        =
        1
        )
        =
        
          τ
          
            1
          
        
        
      
    
    {\displaystyle \operatorname {P} (Z_{i}=1)=\tau _{1}\,}
   and 
  
    
      
        P
        ⁡
        (
        
          Z
          
            i
          
        
        =
        2
        )
        =
        
          τ
          
            2
          
        
        =
        1
        −
        
          τ
          
            1
          
        
        .
      
    
    {\displaystyle \operatorname {P} (Z_{i}=2)=\tau _{2}=1-\tau _{1}.}
  The aim is to estimate the unknown parameters representing the mixing value between the Gaussians and the means and covariances of each:

  
    
      
        θ
        =
        
          
            (
          
        
        
          τ
        
        ,
        
          
            μ
          
          
            1
          
        
        ,
        
          
            μ
          
          
            2
          
        
        ,
        
          Σ
          
            1
          
        
        ,
        
          Σ
          
            2
          
        
        
          
            )
          
        
        ,
      
    
    {\displaystyle \theta ={\big (}{\boldsymbol {\tau }},{\boldsymbol {\mu }}_{1},{\boldsymbol {\mu }}_{2},\Sigma _{1},\Sigma _{2}{\big )},}
  where the incomplete-data likelihood function is

  
    
      
        L
        (
        θ
        ;
        
          x
        
        )
        =
        
          ∏
          
            i
            =
            1
          
          
            n
          
        
        
          ∑
          
            j
            =
            1
          
          
            2
          
        
        
          τ
          
            j
          
        
         
        f
        (
        
          
            x
          
          
            i
          
        
        ;
        
          
            μ
          
          
            j
          
        
        ,
        
          Σ
          
            j
          
        
        )
        ,
      
    
    {\displaystyle L(\theta ;\mathbf {x} )=\prod _{i=1}^{n}\sum _{j=1}^{2}\tau _{j}\ f(\mathbf {x} _{i};{\boldsymbol {\mu }}_{j},\Sigma _{j}),}
  and the complete-data likelihood function is

  
    
      
        L
        (
        θ
        ;
        
          x
        
        ,
        
          z
        
        )
        =
        p
        (
        
          x
        
        ,
        
          z
        
        ∣
        θ
        )
        =
        
          ∏
          
            i
            =
            1
          
          
            n
          
        
        
          ∏
          
            j
            =
            1
          
          
            2
          
        
         
        [
        f
        (
        
          
            x
          
          
            i
          
        
        ;
        
          
            μ
          
          
            j
          
        
        ,
        
          Σ
          
            j
          
        
        )
        
          τ
          
            j
          
        
        
          ]
          
            
              I
            
            (
            
              z
              
                i
              
            
            =
            j
            )
          
        
        ,
      
    
    {\displaystyle L(\theta ;\mathbf {x} ,\mathbf {z} )=p(\mathbf {x} ,\mathbf {z} \mid \theta )=\prod _{i=1}^{n}\prod _{j=1}^{2}\ [f(\mathbf {x} _{i};{\boldsymbol {\mu }}_{j},\Sigma _{j})\tau _{j}]^{\mathbb {I} (z_{i}=j)},}
  or

  
    
      
        L
        (
        θ
        ;
        
          x
        
        ,
        
          z
        
        )
        =
        exp
        ⁡
        
          {
          
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            
              ∑
              
                j
                =
                1
              
              
                2
              
            
            
              I
            
            (
            
              z
              
                i
              
            
            =
            j
            )
            
              
                [
              
            
            log
            ⁡
            
              τ
              
                j
              
            
            −
            
              
                
                  1
                  2
                
              
            
            log
            ⁡
            
              |
            
            
              Σ
              
                j
              
            
            
              |
            
            −
            
              
                
                  1
                  2
                
              
            
            (
            
              
                x
              
              
                i
              
            
            −
            
              
                μ
              
              
                j
              
            
            
              )
              
                ⊤
              
            
            
              Σ
              
                j
              
              
                −
                1
              
            
            (
            
              
                x
              
              
                i
              
            
            −
            
              
                μ
              
              
                j
              
            
            )
            −
            
              
                
                  d
                  2
                
              
            
            log
            ⁡
            (
            2
            π
            )
            
              
                ]
              
            
          
          }
        
        ,
      
    
    {\displaystyle L(\theta ;\mathbf {x} ,\mathbf {z} )=\exp \left\{\sum _{i=1}^{n}\sum _{j=1}^{2}\mathbb {I} (z_{i}=j){\big [}\log \tau _{j}-{\tfrac {1}{2}}\log |\Sigma _{j}|-{\tfrac {1}{2}}(\mathbf {x} _{i}-{\boldsymbol {\mu }}_{j})^{\top }\Sigma _{j}^{-1}(\mathbf {x} _{i}-{\boldsymbol {\mu }}_{j})-{\tfrac {d}{2}}\log(2\pi ){\big ]}\right\},}
  where 
  
    
      
        
          I
        
      
    
    {\displaystyle \mathbb {I} }
   is an indicator function and 
  
    
      
        f
      
    
    {\displaystyle f}
   is the probability density function of a multivariate normal.
In the last equality, for each i, one indicator 
  
    
      
        
          I
        
        (
        
          z
          
            i
          
        
        =
        j
        )
      
    
    {\displaystyle \mathbb {I} (z_{i}=j)}
   is equal to zero, and one indicator is equal to one. The inner sum thus reduces to one term.

E step
Given our current estimate of the parameters θ(t), the conditional distribution of the Zi is determined by Bayes theorem to be the proportional height of the normal density weighted by τ:

  
    
      
        
          T
          
            j
            ,
            i
          
          
            (
            t
            )
          
        
        :=
        P
        ⁡
        (
        
          Z
          
            i
          
        
        =
        j
        ∣
        
          X
          
            i
          
        
        =
        
          
            x
          
          
            i
          
        
        ;
        
          θ
          
            (
            t
            )
          
        
        )
        =
        
          
            
              
                τ
                
                  j
                
                
                  (
                  t
                  )
                
              
               
              f
              (
              
                
                  x
                
                
                  i
                
              
              ;
              
                
                  μ
                
                
                  j
                
                
                  (
                  t
                  )
                
              
              ,
              
                Σ
                
                  j
                
                
                  (
                  t
                  )
                
              
              )
            
            
              
                τ
                
                  1
                
                
                  (
                  t
                  )
                
              
               
              f
              (
              
                
                  x
                
                
                  i
                
              
              ;
              
                
                  μ
                
                
                  1
                
                
                  (
                  t
                  )
                
              
              ,
              
                Σ
                
                  1
                
                
                  (
                  t
                  )
                
              
              )
              +
              
                τ
                
                  2
                
                
                  (
                  t
                  )
                
              
               
              f
              (
              
                
                  x
                
                
                  i
                
              
              ;
              
                
                  μ
                
                
                  2
                
                
                  (
                  t
                  )
                
              
              ,
              
                Σ
                
                  2
                
                
                  (
                  t
                  )
                
              
              )
            
          
        
        .
      
    
    {\displaystyle T_{j,i}^{(t)}:=\operatorname {P} (Z_{i}=j\mid X_{i}=\mathbf {x} _{i};\theta ^{(t)})={\frac {\tau _{j}^{(t)}\ f(\mathbf {x} _{i};{\boldsymbol {\mu }}_{j}^{(t)},\Sigma _{j}^{(t)})}{\tau _{1}^{(t)}\ f(\mathbf {x} _{i};{\boldsymbol {\mu }}_{1}^{(t)},\Sigma _{1}^{(t)})+\tau _{2}^{(t)}\ f(\mathbf {x} _{i};{\boldsymbol {\mu }}_{2}^{(t)},\Sigma _{2}^{(t)})}}.}
  These are called the ""membership probabilities"", which are normally considered the output of the E step (although this is not the Q function of below).
This E step corresponds with setting up this function for Q:

  
    
      
        
          
            
              
                Q
                (
                θ
                ∣
                
                  θ
                  
                    (
                    t
                    )
                  
                
                )
              
              
                
                =
                
                  E
                  
                    
                      Z
                    
                    ∣
                    
                      X
                    
                    ,
                    
                      
                        θ
                      
                      
                        (
                        t
                        )
                      
                    
                  
                
                ⁡
                [
                log
                ⁡
                L
                (
                θ
                ;
                
                  x
                
                ,
                
                  Z
                
                )
                ]
              
            
            
              
              
                
                =
                
                  E
                  
                    
                      Z
                    
                    ∣
                    
                      X
                    
                    ,
                    
                      
                        θ
                      
                      
                        (
                        t
                        )
                      
                    
                  
                
                ⁡
                [
                log
                ⁡
                
                  ∏
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                L
                (
                θ
                ;
                
                  
                    x
                  
                  
                    i
                  
                
                ,
                
                  Z
                  
                    i
                  
                
                )
                ]
              
            
            
              
              
                
                =
                
                  E
                  
                    
                      Z
                    
                    ∣
                    
                      X
                    
                    ,
                    
                      
                        θ
                      
                      
                        (
                        t
                        )
                      
                    
                  
                
                ⁡
                [
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                log
                ⁡
                L
                (
                θ
                ;
                
                  
                    x
                  
                  
                    i
                  
                
                ,
                
                  Z
                  
                    i
                  
                
                )
                ]
              
            
            
              
              
                
                =
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  E
                  
                    
                      Z
                      
                        i
                      
                    
                    ∣
                    
                      X
                    
                    ;
                    
                      
                        θ
                      
                      
                        (
                        t
                        )
                      
                    
                  
                
                ⁡
                [
                log
                ⁡
                L
                (
                θ
                ;
                
                  
                    x
                  
                  
                    i
                  
                
                ,
                
                  Z
                  
                    i
                  
                
                )
                ]
              
            
            
              
              
                
                =
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  ∑
                  
                    j
                    =
                    1
                  
                  
                    2
                  
                
                P
                (
                
                  Z
                  
                    i
                  
                
                =
                j
                ∣
                
                  X
                  
                    i
                  
                
                =
                
                  
                    x
                  
                  
                    i
                  
                
                ;
                
                  θ
                  
                    (
                    t
                    )
                  
                
                )
                log
                ⁡
                L
                (
                
                  θ
                  
                    j
                  
                
                ;
                
                  
                    x
                  
                  
                    i
                  
                
                ,
                j
                )
              
            
            
              
              
                
                =
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  ∑
                  
                    j
                    =
                    1
                  
                  
                    2
                  
                
                
                  T
                  
                    j
                    ,
                    i
                  
                  
                    (
                    t
                    )
                  
                
                
                  
                    [
                  
                
                log
                ⁡
                
                  τ
                  
                    j
                  
                
                −
                
                  
                    
                      1
                      2
                    
                  
                
                log
                ⁡
                
                  |
                
                
                  Σ
                  
                    j
                  
                
                
                  |
                
                −
                
                  
                    
                      1
                      2
                    
                  
                
                (
                
                  
                    x
                  
                  
                    i
                  
                
                −
                
                  
                    μ
                  
                  
                    j
                  
                
                
                  )
                  
                    ⊤
                  
                
                
                  Σ
                  
                    j
                  
                  
                    −
                    1
                  
                
                (
                
                  
                    x
                  
                  
                    i
                  
                
                −
                
                  
                    μ
                  
                  
                    j
                  
                
                )
                −
                
                  
                    
                      d
                      2
                    
                  
                
                log
                ⁡
                (
                2
                π
                )
                
                  
                    ]
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}Q(\theta \mid \theta ^{(t)})&=\operatorname {E} _{\mathbf {Z} \mid \mathbf {X} ,\mathbf {\theta } ^{(t)}}[\log L(\theta ;\mathbf {x} ,\mathbf {Z} )]\\&=\operatorname {E} _{\mathbf {Z} \mid \mathbf {X} ,\mathbf {\theta } ^{(t)}}[\log \prod _{i=1}^{n}L(\theta ;\mathbf {x} _{i},Z_{i})]\\&=\operatorname {E} _{\mathbf {Z} \mid \mathbf {X} ,\mathbf {\theta } ^{(t)}}[\sum _{i=1}^{n}\log L(\theta ;\mathbf {x} _{i},Z_{i})]\\&=\sum _{i=1}^{n}\operatorname {E} _{Z_{i}\mid \mathbf {X} ;\mathbf {\theta } ^{(t)}}[\log L(\theta ;\mathbf {x} _{i},Z_{i})]\\&=\sum _{i=1}^{n}\sum _{j=1}^{2}P(Z_{i}=j\mid X_{i}=\mathbf {x} _{i};\theta ^{(t)})\log L(\theta _{j};\mathbf {x} _{i},j)\\&=\sum _{i=1}^{n}\sum _{j=1}^{2}T_{j,i}^{(t)}{\big [}\log \tau _{j}-{\tfrac {1}{2}}\log |\Sigma _{j}|-{\tfrac {1}{2}}(\mathbf {x} _{i}-{\boldsymbol {\mu }}_{j})^{\top }\Sigma _{j}^{-1}(\mathbf {x} _{i}-{\boldsymbol {\mu }}_{j})-{\tfrac {d}{2}}\log(2\pi ){\big ]}.\end{aligned}}}
  The expectation of 
  
    
      
        log
        ⁡
        L
        (
        θ
        ;
        
          
            x
          
          
            i
          
        
        ,
        
          Z
          
            i
          
        
        )
      
    
    {\displaystyle \log L(\theta ;\mathbf {x} _{i},Z_{i})}
   inside the sum is taken with respect to the probability density function 
  
    
      
        P
        (
        
          Z
          
            i
          
        
        ∣
        
          X
          
            i
          
        
        =
        
          
            x
          
          
            i
          
        
        ;
        
          θ
          
            (
            t
            )
          
        
        )
      
    
    {\displaystyle P(Z_{i}\mid X_{i}=\mathbf {x} _{i};\theta ^{(t)})}
  , which might be different for each  
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   of the training set. Everything in the E step is known before the step is taken except 
  
    
      
        
          T
          
            j
            ,
            i
          
        
      
    
    {\displaystyle T_{j,i}}
  , which is computed according to the equation at the beginning of the E step section.
This full conditional expectation does not need to be calculated in one step, because τ and μ/Σ appear in separate linear terms and can thus be maximized independently.

M step
Q(θ | θ(t)) being quadratic in form means that determining the maximizing values of θ is relatively straightforward. Also, τ, (μ1,Σ1) and (μ2,Σ2) may all be maximized independently since they all appear in separate linear terms.
To begin, consider τ, which has the constraint τ1 + τ2=1:

  
    
      
        
          
            
              
                
                  
                    τ
                  
                  
                    (
                    t
                    +
                    1
                    )
                  
                
              
              
                
                =
                
                  
                    
                      a
                      r
                      g
                      
                      m
                      a
                      x
                    
                    τ
                  
                
                 
                Q
                (
                θ
                ∣
                
                  θ
                  
                    (
                    t
                    )
                  
                
                )
              
            
            
              
              
                
                =
                
                  
                    
                      a
                      r
                      g
                      
                      m
                      a
                      x
                    
                    τ
                  
                
                 
                
                  {
                  
                    
                      [
                      
                        
                          ∑
                          
                            i
                            =
                            1
                          
                          
                            n
                          
                        
                        
                          T
                          
                            1
                            ,
                            i
                          
                          
                            (
                            t
                            )
                          
                        
                      
                      ]
                    
                    log
                    ⁡
                    
                      τ
                      
                        1
                      
                    
                    +
                    
                      [
                      
                        
                          ∑
                          
                            i
                            =
                            1
                          
                          
                            n
                          
                        
                        
                          T
                          
                            2
                            ,
                            i
                          
                          
                            (
                            t
                            )
                          
                        
                      
                      ]
                    
                    log
                    ⁡
                    
                      τ
                      
                        2
                      
                    
                  
                  }
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\boldsymbol {\tau }}^{(t+1)}&={\underset {\boldsymbol {\tau }}{\operatorname {arg\,max} }}\ Q(\theta \mid \theta ^{(t)})\\&={\underset {\boldsymbol {\tau }}{\operatorname {arg\,max} }}\ \left\{\left[\sum _{i=1}^{n}T_{1,i}^{(t)}\right]\log \tau _{1}+\left[\sum _{i=1}^{n}T_{2,i}^{(t)}\right]\log \tau _{2}\right\}.\end{aligned}}}
  This has the same form as the MLE for the binomial distribution, so

  
    
      
        
          τ
          
            j
          
          
            (
            t
            +
            1
            )
          
        
        =
        
          
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                T
                
                  j
                  ,
                  i
                
                
                  (
                  t
                  )
                
              
            
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              (
              
                T
                
                  1
                  ,
                  i
                
                
                  (
                  t
                  )
                
              
              +
              
                T
                
                  2
                  ,
                  i
                
                
                  (
                  t
                  )
                
              
              )
            
          
        
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          T
          
            j
            ,
            i
          
          
            (
            t
            )
          
        
        .
      
    
    {\displaystyle \tau _{j}^{(t+1)}={\frac {\sum _{i=1}^{n}T_{j,i}^{(t)}}{\sum _{i=1}^{n}(T_{1,i}^{(t)}+T_{2,i}^{(t)})}}={\frac {1}{n}}\sum _{i=1}^{n}T_{j,i}^{(t)}.}
  For the next estimates of (μ1,Σ1):

  
    
      
        
          
            
              
                (
                
                  
                    μ
                  
                  
                    1
                  
                  
                    (
                    t
                    +
                    1
                    )
                  
                
                ,
                
                  Σ
                  
                    1
                  
                  
                    (
                    t
                    +
                    1
                    )
                  
                
                )
              
              
                
                =
                
                  
                    
                      a
                      r
                      g
                      
                      m
                      a
                      x
                    
                    
                      
                        
                          μ
                        
                        
                          1
                        
                      
                      ,
                      
                        Σ
                        
                          1
                        
                      
                    
                  
                
                Q
                (
                θ
                ∣
                
                  θ
                  
                    (
                    t
                    )
                  
                
                )
              
            
            
              
              
                
                =
                
                  
                    
                      a
                      r
                      g
                      
                      m
                      a
                      x
                    
                    
                      
                        
                          μ
                        
                        
                          1
                        
                      
                      ,
                      
                        Σ
                        
                          1
                        
                      
                    
                  
                
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  T
                  
                    1
                    ,
                    i
                  
                  
                    (
                    t
                    )
                  
                
                
                  {
                  
                    −
                    
                      
                        
                          1
                          2
                        
                      
                    
                    log
                    ⁡
                    
                      |
                    
                    
                      Σ
                      
                        1
                      
                    
                    
                      |
                    
                    −
                    
                      
                        
                          1
                          2
                        
                      
                    
                    (
                    
                      
                        x
                      
                      
                        i
                      
                    
                    −
                    
                      
                        μ
                      
                      
                        1
                      
                    
                    
                      )
                      
                        ⊤
                      
                    
                    
                      Σ
                      
                        1
                      
                      
                        −
                        1
                      
                    
                    (
                    
                      
                        x
                      
                      
                        i
                      
                    
                    −
                    
                      
                        μ
                      
                      
                        1
                      
                    
                    )
                  
                  }
                
              
            
          
        
        .
      
    
    {\displaystyle {\begin{aligned}({\boldsymbol {\mu }}_{1}^{(t+1)},\Sigma _{1}^{(t+1)})&={\underset {{\boldsymbol {\mu }}_{1},\Sigma _{1}}{\operatorname {arg\,max} }}Q(\theta \mid \theta ^{(t)})\\&={\underset {{\boldsymbol {\mu }}_{1},\Sigma _{1}}{\operatorname {arg\,max} }}\sum _{i=1}^{n}T_{1,i}^{(t)}\left\{-{\tfrac {1}{2}}\log |\Sigma _{1}|-{\tfrac {1}{2}}(\mathbf {x} _{i}-{\boldsymbol {\mu }}_{1})^{\top }\Sigma _{1}^{-1}(\mathbf {x} _{i}-{\boldsymbol {\mu }}_{1})\right\}\end{aligned}}.}
  This has the same form as a weighted MLE for a normal distribution, so

  
    
      
        
          
            μ
          
          
            1
          
          
            (
            t
            +
            1
            )
          
        
        =
        
          
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                T
                
                  1
                  ,
                  i
                
                
                  (
                  t
                  )
                
              
              
                
                  x
                
                
                  i
                
              
            
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                T
                
                  1
                  ,
                  i
                
                
                  (
                  t
                  )
                
              
            
          
        
      
    
    {\displaystyle {\boldsymbol {\mu }}_{1}^{(t+1)}={\frac {\sum _{i=1}^{n}T_{1,i}^{(t)}\mathbf {x} _{i}}{\sum _{i=1}^{n}T_{1,i}^{(t)}}}}
   and 
  
    
      
        
          Σ
          
            1
          
          
            (
            t
            +
            1
            )
          
        
        =
        
          
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                T
                
                  1
                  ,
                  i
                
                
                  (
                  t
                  )
                
              
              (
              
                
                  x
                
                
                  i
                
              
              −
              
                
                  μ
                
                
                  1
                
                
                  (
                  t
                  +
                  1
                  )
                
              
              )
              (
              
                
                  x
                
                
                  i
                
              
              −
              
                
                  μ
                
                
                  1
                
                
                  (
                  t
                  +
                  1
                  )
                
              
              
                )
                
                  ⊤
                
              
            
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                T
                
                  1
                  ,
                  i
                
                
                  (
                  t
                  )
                
              
            
          
        
      
    
    {\displaystyle \Sigma _{1}^{(t+1)}={\frac {\sum _{i=1}^{n}T_{1,i}^{(t)}(\mathbf {x} _{i}-{\boldsymbol {\mu }}_{1}^{(t+1)})(\mathbf {x} _{i}-{\boldsymbol {\mu }}_{1}^{(t+1)})^{\top }}{\sum _{i=1}^{n}T_{1,i}^{(t)}}}}
  and, by symmetry,

  
    
      
        
          
            μ
          
          
            2
          
          
            (
            t
            +
            1
            )
          
        
        =
        
          
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                T
                
                  2
                  ,
                  i
                
                
                  (
                  t
                  )
                
              
              
                
                  x
                
                
                  i
                
              
            
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                T
                
                  2
                  ,
                  i
                
                
                  (
                  t
                  )
                
              
            
          
        
      
    
    {\displaystyle {\boldsymbol {\mu }}_{2}^{(t+1)}={\frac {\sum _{i=1}^{n}T_{2,i}^{(t)}\mathbf {x} _{i}}{\sum _{i=1}^{n}T_{2,i}^{(t)}}}}
   and 
  
    
      
        
          Σ
          
            2
          
          
            (
            t
            +
            1
            )
          
        
        =
        
          
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                T
                
                  2
                  ,
                  i
                
                
                  (
                  t
                  )
                
              
              (
              
                
                  x
                
                
                  i
                
              
              −
              
                
                  μ
                
                
                  2
                
                
                  (
                  t
                  +
                  1
                  )
                
              
              )
              (
              
                
                  x
                
                
                  i
                
              
              −
              
                
                  μ
                
                
                  2
                
                
                  (
                  t
                  +
                  1
                  )
                
              
              
                )
                
                  ⊤
                
              
            
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                T
                
                  2
                  ,
                  i
                
                
                  (
                  t
                  )
                
              
            
          
        
        .
      
    
    {\displaystyle \Sigma _{2}^{(t+1)}={\frac {\sum _{i=1}^{n}T_{2,i}^{(t)}(\mathbf {x} _{i}-{\boldsymbol {\mu }}_{2}^{(t+1)})(\mathbf {x} _{i}-{\boldsymbol {\mu }}_{2}^{(t+1)})^{\top }}{\sum _{i=1}^{n}T_{2,i}^{(t)}}}.}

Termination
Conclude the iterative process if 
  
    
      
        
          E
          
            Z
            ∣
            
              θ
              
                (
                t
                )
              
            
            ,
            
              x
            
          
        
        [
        log
        ⁡
        L
        (
        
          θ
          
            (
            t
            )
          
        
        ;
        
          x
        
        ,
        
          Z
        
        )
        ]
        ≤
        
          E
          
            Z
            ∣
            
              θ
              
                (
                t
                −
                1
                )
              
            
            ,
            
              x
            
          
        
        [
        log
        ⁡
        L
        (
        
          θ
          
            (
            t
            −
            1
            )
          
        
        ;
        
          x
        
        ,
        
          Z
        
        )
        ]
        +
        ε
      
    
    {\displaystyle E_{Z\mid \theta ^{(t)},\mathbf {x} }[\log L(\theta ^{(t)};\mathbf {x} ,\mathbf {Z} )]\leq E_{Z\mid \theta ^{(t-1)},\mathbf {x} }[\log L(\theta ^{(t-1)};\mathbf {x} ,\mathbf {Z} )]+\varepsilon }
   for 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   below some preset threshold.

Generalization
The algorithm illustrated above can be generalized for mixtures of more than two multivariate normal distributions.

Truncated and censored regression
The EM algorithm has been implemented in the case where an underlying linear regression model exists explaining the variation of some quantity, but where the values actually observed are censored or truncated versions of those represented in the model.  Special cases of this model include censored or truncated observations from one normal distribution.

Alternatives
EM typically converges to a local optimum, not necessarily the global optimum, with no bound on the convergence rate in general. It is possible that it can be arbitrarily poor in high dimensions and there can be an exponential number of local optima. Hence, a need exists for alternative methods for guaranteed learning, especially in the high-dimensional setting. Alternatives to EM exist with better guarantees for consistency, which are termed moment-based approaches or the so-called spectral techniques. Moment-based approaches to learning the parameters of a probabilistic model are of increasing interest recently since they enjoy guarantees such as global convergence under certain conditions unlike EM which is often plagued by the issue of getting stuck in local optima. Algorithms with guarantees for learning can be derived for a number of important models such as mixture models, HMMs etc. For these spectral methods, no spurious local optima occur, and the true parameters can be consistently estimated under some regularity conditions.

See also
mixture distribution
compound distribution
density estimation
total absorption spectroscopy
The EM algorithm can be viewed as a special case of the majorize-minimization (MM) algorithm.

References
Further reading
Hogg, Robert; McKean, Joseph; Craig, Allen (2005). Introduction to Mathematical Statistics. Upper Saddle River, NJ: Pearson Prentice Hall. pp. 359–364.
Dellaert, Frank (2002). ""The Expectation Maximization Algorithm"". CiteSeerX 10.1.1.9.9735.  gives an easier explanation of EM algorithm as to lowerbound maximization.
Bishop, Christopher M. (2006). Pattern Recognition and Machine Learning. Springer. ISBN 978-0-387-31073-2.
Gupta, M. R.; Chen, Y. (2010). ""Theory and Use of the EM Algorithm"". Foundations and Trends in Signal Processing. 4 (3): 223–296. CiteSeerX 10.1.1.219.6830. doi:10.1561/2000000034. A well-written short book on EM, including detailed derivation of EM for GMMs, HMMs, and Dirichlet.
Bilmes, Jeff (1998). ""A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models"". CiteSeerX 10.1.1.28.613.  includes a simplified derivation of the EM equations for Gaussian Mixtures and Gaussian Mixture Hidden Markov Models.
McLachlan, Geoffrey J.; Krishnan, Thriyambakam (2008). The EM Algorithm and Extensions (2nd ed.). Hoboken: Wiley. ISBN 978-0-471-20170-0.

External links
Various 1D, 2D and 3D demonstrations of EM together with Mixture Modeling are provided as part of the paired SOCR activities and applets. These applets and activities show empirically the properties of the EM algorithm for parameter estimation in diverse settings.
k-MLE: A fast algorithm for learning statistical mixture models
Class hierarchy in C++ (GPL) including Gaussian Mixtures
The on-line textbook: Information Theory, Inference, and Learning Algorithms, by David J.C. MacKay includes simple examples of the EM algorithm such as clustering using the soft k-means algorithm, and emphasizes the variational view of the EM algorithm, as described in Chapter 33.7 of version 7.2 (fourth edition).
Variational Algorithms for Approximate Bayesian Inference, by M. J. Beal includes comparisons of EM to Variational Bayesian EM and derivations of several models including Variational Bayesian HMMs  (chapters).
The Expectation Maximization Algorithm: A short tutorial, A self-contained derivation of the EM Algorithm by Sean Borman.
The EM Algorithm, by Xiaojin Zhu.
EM algorithm and variants: an informal tutorial by Alexis Roche.  A concise and very clear description of EM and many interesting variants.",https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm,"['All articles with unsourced statements', 'Articles with short description', 'Articles with unsourced statements from April 2019', 'Articles with unsourced statements from November 2017', 'CS1 errors: missing periodical', 'CS1 maint: multiple names: authors list', 'Cluster analysis algorithms', 'Estimation methods', 'Machine learning algorithms', 'Missing data', 'Optimization algorithms and methods', 'Short description matches Wikidata', 'Statistical algorithms']",Data Science
90,"Extract, transform, load","In computing, extract, transform, load (ETL) is the general procedure of copying data from one or more sources into a destination system which represents the data differently from the source(s) or in a different context than the source(s). The ETL process became a popular concept in the 1970s and is often used in data warehousing.Data extraction involves extracting data from homogeneous or heterogeneous sources; data transformation processes data by data cleaning and transforming them into a proper storage format/structure for the purposes of querying and analysis; finally, data loading describes the insertion of data into the final target database such as an operational data store, a data mart, data lake or a data warehouse.A properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms data so that separate sources can be used together, and finally delivers data in a presentation-ready format so that application developers can build applications and end users can make decisions.Since the data extraction takes time, it is common to execute the three phases in pipeline. While the data is being extracted, another transformation process executes while processing the data already received and prepares it for loading while the data loading begins without waiting for the completion of the previous phases.
ETL systems commonly integrate data from multiple applications (systems), typically developed and supported by different vendors or hosted on separate computer hardware. The separate systems containing the original data are frequently managed and operated by different employees. For example, a cost accounting system may combine data from payroll, sales, and purchasing.

Extract
The first part of an ETL process involves extracting the data from the source system(s). In many cases, this represents the most important aspect of ETL, since extracting data correctly sets the stage for the success of subsequent processes. Most data-warehousing projects combine data from different source systems. Each separate system may also use a different data organization and/or format. Common data-source formats include relational databases, XML, JSON and flat files, but may also include non-relational database structures such as Information Management System (IMS) or other data structures such as Virtual Storage Access Method (VSAM) or Indexed Sequential Access Method (ISAM), or even formats fetched from outside sources by means such as web spidering or screen-scraping. The streaming of the extracted data source and loading on-the-fly to the destination database is another way of performing ETL when no intermediate data storage is required. 
An intrinsic part of the extraction involves data validation to confirm whether the data pulled from the sources has the correct/expected values in a given domain (such as a pattern/default or list of values). If the data fails the validation rules, it is rejected entirely or in part. The rejected data is ideally reported back to the source system for further analysis to identify and to rectify the incorrect records.

Transform
In the data transformation stage, a series of rules or functions are applied to the extracted data in order to prepare it for loading into the end target.
An important function of transformation is data cleansing, which aims to pass only ""proper"" data to the target. The challenge when different systems interact is in the relevant systems' interfacing and communicating. Character sets that may be available in one system may not be so in others.
In other cases, one or more of the following transformation types may be required to meet the business and technical needs of the server or data warehouse:

Selecting only certain columns to load: (or selecting null columns not to load). For example, if the source data has three columns (aka ""attributes""), roll_no, age, and salary, then the selection may take only roll_no and salary. Or, the selection mechanism may ignore all those records where salary is not present (salary = null).
Translating coded values: (e.g., if the source system codes male as ""1"" and female as ""2"", but the warehouse codes male as ""M"" and female as ""F"")
Encoding free-form values: (e.g., mapping ""Male"" to ""M"")
Deriving a new calculated value: (e.g., sale_amount = qty * unit_price)
Sorting or ordering the data based on a list of columns to improve search performance
Joining data from multiple sources (e.g., lookup, merge) and deduplicating the data
Aggregating (for example, rollup — summarizing multiple rows of data — total sales for each store, and for each region, etc.)
Generating surrogate-key values
Transposing or pivoting (turning multiple columns into multiple rows or vice versa)
Splitting a column into multiple columns (e.g., converting a comma-separated list, specified as a string in one column, into individual values in different columns)
Disaggregating repeating columns
Looking up and validating the relevant data from tables or referential files
Applying any form of data validation; failed validation may result in a full rejection of the data, partial rejection, or no rejection at all, and thus none, some, or all of the data is handed over to the next step depending on the rule design and exception handling; many of the above transformations may result in exceptions, e.g., when a code translation parses an unknown code in the extracted data

Load
The load phase loads the data into the end target, which can be any data store including a simple delimited flat file or a data warehouse. Depending on the requirements of the organization, this process varies widely. Some data warehouses may overwrite existing information with cumulative information; updating extracted data is frequently done on a daily, weekly, or monthly basis. Other data warehouses (or even other parts of the same data warehouse) may add new data in a historical form at regular intervals — for example, hourly. To understand this, consider a data warehouse that is required to maintain sales records of the last year. This data warehouse overwrites any data older than a year with newer data. However, the entry of data for any one year window is made in a historical manner. The timing and scope to replace or append are strategic design choices dependent on the time available and the business needs. More complex systems can maintain a history and audit trail of all changes to the data loaded in the data warehouse.As the load phase interacts with a database, the constraints defined in the database schema — as well as in triggers activated upon data load — apply (for example, uniqueness, referential integrity, mandatory fields), which also contribute to the overall data quality performance of the ETL process.

For example, a financial institution might have information on a customer in several departments and each department might have that customer's information listed in a different way. The membership department might list the customer by name, whereas the accounting department might list the customer by number. ETL can bundle all of these data elements and consolidate them into a uniform presentation, such as for storing in a database or data warehouse.
Another way that companies use ETL is to move information to another application permanently. For instance, the new application might use another database vendor and most likely a very different database schema. ETL can be used to transform the data into a format suitable for the new application to use.
An example would be an Expense and Cost Recovery System (ECRS) such as used by accountancies, consultancies, and legal firms. The data usually ends up in the time and billing system, although some businesses may also utilize the raw data for employee productivity reports to Human Resources (personnel dept.) or equipment usage reports to Facilities Management.

Real-life ETL cycle
The typical real-life ETL cycle consists of the following execution steps:

Cycle initiation
Build reference data
Extract (from sources)
Validate
Transform (clean, apply business rules, check for data integrity, create aggregates or disaggregates)
Stage (load into staging tables, if used)
Audit reports (for example, on compliance with business rules. Also, in case of failure, helps to diagnose/repair)
Publish (to target tables)
Archive

Challenges
ETL processes can involve considerable complexity, and significant operational problems can occur with improperly designed ETL systems.
The range of data values or data quality in an operational system may exceed the expectations of designers at the time validation and transformation rules are specified. Data profiling of a source during data analysis can identify the data conditions that must be managed by transform rules specifications, leading to an amendment of validation rules explicitly and implicitly implemented in the ETL process.
Data warehouses are typically assembled from a variety of data sources with different formats and purposes. As such, ETL is a key process to bring all the data together in a standard, homogeneous environment.
Design analysis should establish the scalability of an ETL system across the lifetime of its usage — including understanding the volumes of data that must be processed within service level agreements. The time available to extract from source systems may change, which may mean the same amount of data may have to be processed in less time. Some ETL systems have to scale to process terabytes of data to update data warehouses with tens of terabytes of data. Increasing volumes of data may require designs that can scale from daily batch to multiple-day micro batch to integration with message queues or real-time change-data-capture for continuous transformation and update.

Performance
ETL vendors benchmark their record-systems at multiple TB (terabytes) per hour (or ~1 GB per second) using powerful servers with multiple CPUs, multiple hard drives, multiple gigabit-network connections, and much memory.
In real life, the slowest part of an ETL process usually occurs in the database load phase. Databases may perform slowly because they have to take care of concurrency, integrity maintenance, and indices. Thus, for better performance, it may make sense to employ:

Direct path extract method or bulk unload whenever is possible (instead of querying the database) to reduce the load on source system while getting high-speed extract
Most of the transformation processing outside of the database
Bulk load operations whenever possibleStill, even using bulk operations, database access is usually the bottleneck in the ETL process. Some common methods used to increase performance are:

Partition tables (and indices): try to keep partitions similar in size (watch for null values that can skew the partitioning)
Do all validation in the ETL layer before the load: disable integrity checking (disable constraint ...) in the target database tables during the load
Disable triggers (disable trigger ...) in the target database tables during the load: simulate their effect as a separate step
Generate IDs in the ETL layer (not in the database)
Drop the indices (on a table or partition) before the load - and recreate them after the load (SQL: drop index ...; create index ...)
Use parallel bulk load when possible — works well when the table is partitioned or there are no indices (Note: attempting to do parallel loads into the same table (partition) usually causes locks — if not on the data rows, then on indices)
If a requirement exists to do insertions, updates, or deletions, find out which rows should be processed in which way in the ETL layer, and then process these three operations in the database separately; you often can do bulk load for inserts, but updates and deletes commonly go through an API (using SQL)Whether to do certain operations in the database or outside may involve a trade-off. For example, removing duplicates using distinct may be slow in the database; thus, it makes sense to do it outside. On the other side, if using distinct significantly (x100) decreases the number of rows to be extracted, then it makes sense to remove duplications as early as possible in the database before unloading data.
A common source of problems in ETL is a big number of dependencies among ETL jobs. For example, job ""B"" cannot start while job ""A"" is not finished. One can usually achieve better performance by visualizing all processes on a graph, and trying to reduce the graph making maximum use of parallelism, and making ""chains"" of consecutive processing as short as possible. Again, partitioning of big tables and their indices can really help.
Another common issue occurs when the data are spread among several databases, and processing is done in those databases sequentially. Sometimes database replication may be involved as a method of copying data between databases — it can significantly slow down the whole process. The common solution is to reduce the processing graph to only three layers:

Sources
Central ETL layer
TargetsThis approach allows processing to take maximum advantage of parallelism. For example, if you need to load data into two databases, you can run the loads in parallel (instead of loading into the first — and then replicating into the second).
Sometimes processing must take place sequentially. For example, dimensional (reference) data are needed before one can get and validate the rows for main ""fact"" tables.

Parallel processing
A recent development in ETL software is the implementation of parallel processing. It has enabled a number of methods to improve overall performance of ETL when dealing with large volumes of data.
ETL applications implement three main types of parallelism:

Data: By splitting a single sequential file into smaller data files to provide parallel access
Pipeline: allowing the simultaneous running of several components on the same data stream, e.g. looking up a value on record 1 at the same time as adding two fields on record 2
Component: The simultaneous running of multiple processes on different data streams in the same job, e.g. sorting one input file while removing duplicates on another fileAll three types of parallelism usually operate combined in a single job or task.
An additional difficulty comes with making sure that the data being uploaded is relatively consistent. Because multiple source databases may have different update cycles (some may be updated every few minutes, while others may take days or weeks), an ETL system may be required to hold back certain data until all sources are synchronized. Likewise, where a warehouse may have to be reconciled to the contents in a source system or with the general ledger, establishing synchronization and reconciliation points becomes necessary.

Rerunnability, recoverability
Data warehousing procedures usually subdivide a big ETL process into smaller pieces running sequentially or in parallel. To keep track of data flows, it makes sense to tag each data row with ""row_id"", and tag each piece of the process with ""run_id"". In case of a failure, having these IDs help to roll back and rerun the failed piece.
Best practice also calls for checkpoints, which are states when certain phases of the process are completed. Once at a checkpoint, it is a good idea to write everything to disk, clean out some temporary files, log the state, etc.

Virtual ETL
As of 2010, data virtualization had begun to advance ETL processing. The application of data virtualization to ETL allowed solving the most common ETL tasks of data migration and application integration for multiple dispersed data sources. Virtual ETL operates with the abstracted representation of the objects or entities gathered from the variety of relational, semi-structured, and unstructured data sources. ETL tools can leverage object-oriented modeling and work with entities' representations persistently stored in a centrally located hub-and-spoke architecture. Such a collection that contains representations of the entities or objects gathered from the data sources for ETL processing is called a metadata repository and it can reside in memory or be made persistent. By using a persistent metadata repository, ETL tools can transition from one-time projects to persistent middleware, performing data harmonization and data profiling consistently and in near-real time.

Dealing with keys
Unique keys play an important part in all relational databases, as they tie everything together. A unique key is a column that identifies a given entity, whereas a foreign key is a column in another table that refers to a primary key. Keys can comprise several columns, in which case they are composite keys. In many cases, the primary key is an auto-generated integer that has no meaning for the business entity being represented, but solely exists for the purpose of the relational database - commonly referred to as a surrogate key.
As there is usually more than one data source getting loaded into the warehouse, the keys are an important concern to be addressed. For example: customers might be represented in several data sources, with their Social Security Number as the primary key in one source, their phone number in another, and a surrogate in the third. Yet a data warehouse may require the consolidation of all the customer information into one dimension.
A recommended way to deal with the concern involves adding a warehouse surrogate key, which is used as a foreign key from the fact table.Usually, updates occur to a dimension's source data, which obviously must be reflected in the data warehouse.
If the primary key of the source data is required for reporting, the dimension already contains that piece of information for each row. If the source data uses a surrogate key, the warehouse must keep track of it even though it is never used in queries or reports; it is done by creating a lookup table that contains the warehouse surrogate key and the originating key. This way, the dimension is not polluted with surrogates from various source systems, while the ability to update is preserved.
The lookup table is used in different ways depending on the nature of the source data.
There are 5 types to consider; three are included here:

Type 1
The dimension row is simply updated to match the current state of the source system; the warehouse does not capture history; the lookup table is used to identify the dimension row to update or overwrite
Type 2
A new dimension row is added with the new state of the source system; a new surrogate key is assigned; source key is no longer unique in the lookup table
Fully logged
A new dimension row is added with the new state of the source system, while the previous dimension row is updated to reflect it is no longer active and time of deactivation.

Tools
By using an established ETL framework, one may increase one's chances of ending up with better connectivity and scalability. A good ETL tool must be able to communicate with the many different relational databases and read the various file formats used throughout an organization. ETL tools have started to migrate into Enterprise Application Integration, or even Enterprise Service Bus, systems that now cover much more than just the extraction, transformation, and loading of data. Many ETL vendors now have data profiling, data quality, and metadata capabilities. A common use case for ETL tools include converting CSV files to formats readable by relational databases. A typical translation of millions of records is facilitated by ETL tools that enable users to input csv-like data feeds/files and import it into a database with as little code as possible.
ETL tools are typically used by a broad range of professionals — from students in computer science looking to quickly import large data sets to database architects in charge of company account management, ETL tools have become a convenient tool that can be relied on to get maximum performance. ETL tools in most cases contain a GUI that helps users conveniently transform data, using a visual data mapper, as opposed to writing large programs to parse files and modify data types.
While ETL tools have traditionally been for developers and IT staff, the new trend is to provide these capabilities to business users so they can themselves create connections and data integrations when needed, rather than going to the IT staff. Gartner refers to these non-technical users as Citizen Integrators.

ETL Vs. ELT
Extract, load, transform (ELT) is a variant of ETL where the extracted data is loaded into the target system first.
The architecture for the analytics pipeline shall also consider where to cleanse and enrich data as well as how to conform dimensions.Cloud-based data warehouses like Amazon Redshift, Google BigQuery, and Snowflake Computing have been able to provide highly scalable computing power. This lets businesses forgo preload transformations and replicate raw data into their data warehouses, where it can transform them as needed using SQL.
After having used ELT, data may be processed further and stored in a data mart.There are pros and cons to each approach. Most data integration tools skew towards ETL, while ELT is popular in database and data warehouse appliances. Similarly, it is possible to perform TEL (Transform, Extract, Load) where data is first transformed on a blockchain (as a way of recording changes to data, e.g., token burning) before extracting and loading into another data store.

See also
Architecture patterns (EA reference architecture)
Create, read, update and delete (CRUD)
Data cleansing
Data integration
Data mart
Data mediation
Data migration
Electronic data interchange (EDI)
Enterprise architecture
Expense and cost recovery system (ECRS)
Hartmann pipeline
Legal Electronic Data Exchange Standard (LEDES)
Metadata discovery
Online analytical processing
Online transaction processing (OLTP)
Spatial ETL


== References ==","https://en.wikipedia.org/wiki/Extract,_transform,_load","['All articles containing potentially dated statements', 'All articles lacking in-text citations', 'All articles needing additional references', 'All articles that may contain original research', 'All articles with unsourced statements', 'Articles containing potentially dated statements from 2009', 'Articles containing potentially dated statements from 2010', 'Articles lacking in-text citations from November 2011', 'Articles needing additional references from May 2019', 'Articles that may contain original research from December 2011', 'Articles with multiple maintenance issues', 'Articles with unsourced statements from December 2011', 'Data warehousing', 'Extract, transform, load tools', 'Wikipedia articles with MA identifiers']",Data Science
91,Factor analysis,"Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors, plus ""error"" terms.
Simply put, the factor loading of a variable quantifies the extent to which the variable is related with a given factor.A common rationale behind factor analytic methods is that the information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset. Factor analysis is commonly used in biology, psychometrics, personality theories, marketing, product management, operations research, and finance. It may help to deal with data sets where there are large numbers of observed variables that are thought to reflect a smaller number of underlying/latent variables.  It is one of the most commonly used inter-dependency techniques and is used when the relevant set of variables shows a systematic inter-dependence and the objective is to find out the latent factors that create a commonality.

Statistical model
Definition
The model attempts to explain a set of p observations in each of n individuals with a set of k common factors (F) where there are fewer factors per unit than observations per unit (k<p). Each individual has k of their own common factors, and these are related to the observations via factor loading matrix (
  
    
      
        L
        ∈
        
          
            R
          
          
            p
            ×
            k
          
        
      
    
    {\displaystyle L\in \mathbb {R} ^{p\times k}}
  ), for a single observation, according to

  
    
      
        
          x
          
            i
            ,
            m
          
        
        −
        
          μ
          
            i
          
        
        =
        
          l
          
            i
            ,
            1
          
        
        
          f
          
            1
            ,
            m
          
        
        +
        ⋯
        +
        
          l
          
            i
            ,
            k
          
        
        
          f
          
            k
            ,
            m
          
        
        +
        
          ϵ
          
            i
            ,
            m
          
        
      
    
    {\displaystyle x_{i,m}-\mu _{i}=l_{i,1}f_{1,m}+\dots +l_{i,k}f_{k,m}+\epsilon _{i,m}}
  where 
  
    
      
        
          ϵ
          
            i
            ,
            m
          
        
      
    
    {\displaystyle \epsilon _{i,m}}
   is the unobserved stochastic error term with mean zero and finite variance, and 
  
    
      
        
          μ
          
            i
          
        
      
    
    {\displaystyle \mu _{i}}
   is the observation mean for the ith observation.
In matrix notation 

  
    
      
        X
        −
        
          M
        
        =
        L
        F
        +
        ϵ
      
    
    {\displaystyle X-\mathrm {M} =LF+\epsilon }
  where observation matrix 
  
    
      
        X
        ∈
        
          
            R
          
          
            p
            ×
            n
          
        
      
    
    {\displaystyle X\in \mathbb {R} ^{p\times n}}
  , factor matrix 
  
    
      
        F
        ∈
        
          
            R
          
          
            k
            ×
            n
          
        
      
    
    {\displaystyle F\in \mathbb {R} ^{k\times n}}
  , error term matrix 
  
    
      
        ϵ
        ∈
        
          
            R
          
          
            p
            ×
            n
          
        
      
    
    {\displaystyle \epsilon \in \mathbb {R} ^{p\times n}}
   and mean matrix 
  
    
      
        
          M
        
        ∈
        
          
            R
          
          
            p
            ×
            n
          
        
      
    
    {\displaystyle \mathrm {M} \in \mathbb {R} ^{p\times n}}
   where the i, m element is simply 
  
    
      
        
          μ
          
            i
          
        
      
    
    {\displaystyle \mu _{i}}
  .
Also we will impose the following assumptions on 
  
    
      
        F
      
    
    {\displaystyle F}
  :

F and 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
   are independent.

  
    
      
        
          E
        
        (
        F
        )
        =
        0
      
    
    {\displaystyle \mathrm {E} (F)=0}
  ; where E is Expectation

  
    
      
        
          C
          o
          v
        
        (
        F
        )
        =
        I
      
    
    {\displaystyle \mathrm {Cov} (F)=I}
   where Cov is the covariance matrix, to make sure that the factors are uncorrelated, and I is the identity matrix.Suppose 
  
    
      
        
          C
          o
          v
        
        (
        X
        −
        
          M
        
        )
        =
        Σ
      
    
    {\displaystyle \mathrm {Cov} (X-\mathrm {M} )=\Sigma }
  . Then

  
    
      
        Σ
        =
        
          C
          o
          v
        
        (
        X
        −
        
          M
        
        )
        =
        
          C
          o
          v
        
        (
        L
        F
        +
        ϵ
        )
        ,
        
      
    
    {\displaystyle \Sigma =\mathrm {Cov} (X-\mathrm {M} )=\mathrm {Cov} (LF+\epsilon ),\,}
  and therefore, from the conditions imposed on F above,

  
    
      
        Σ
        =
        L
        
          C
          o
          v
        
        (
        F
        )
        
          L
          
            T
          
        
        +
        
          C
          o
          v
        
        (
        ϵ
        )
        ,
        
      
    
    {\displaystyle \Sigma =L\mathrm {Cov} (F)L^{T}+\mathrm {Cov} (\epsilon ),\,}
  or, setting 
  
    
      
        Ψ
        =
        
          C
          o
          v
        
        (
        ϵ
        )
      
    
    {\displaystyle \Psi =\mathrm {Cov} (\epsilon )}
  ,

  
    
      
        Σ
        =
        L
        
          L
          
            T
          
        
        +
        Ψ
        .
        
      
    
    {\displaystyle \Sigma =LL^{T}+\Psi .\,}
  Note that for any orthogonal matrix Q, if we set 
  
    
      
        
          L
          
            ′
          
        
        =
        L
        Q
      
    
    {\displaystyle L^{\prime }=LQ}
   and 
  
    
      
        
          F
          
            ′
          
        
        =
        
          Q
          
            T
          
        
        F
      
    
    {\displaystyle F^{\prime }=Q^{T}F}
  , the criteria for being factors and factor loadings still hold. Hence a set of factors and factor loadings is unique only up to an orthogonal transformation.

Example
Suppose a psychologist has the hypothesis that there are two kinds of intelligence, ""verbal intelligence"" and ""mathematical intelligence"", neither of which is directly observed. Evidence for the hypothesis is sought in the examination scores from each of 10 different academic fields of 1000 students. If each student is chosen randomly from a large population, then each student's 10 scores are random variables. The psychologist's hypothesis may say that for each of the 10 academic fields, the score averaged over the group of all students who share some common pair of values for verbal and mathematical ""intelligences"" is some constant times their level of verbal intelligence plus another constant times their level of mathematical intelligence, i.e., it is a linear combination of those two ""factors"". The numbers for a particular subject, by which the two kinds of intelligence are multiplied to obtain the expected score, are posited by the hypothesis to be the same for all intelligence level pairs, and are called ""factor loading"" for this subject.  For example, the hypothesis may hold that the predicted average student's aptitude in the field of astronomy is

{10 × the student's verbal intelligence} + {6 × the student's mathematical intelligence}.The numbers 10 and 6 are the factor loadings associated with astronomy. Other academic subjects may have different factor loadings.
Two students assumed to have identical degrees of verbal and mathematical intelligence may have different measured aptitudes in astronomy because individual aptitudes differ from average aptitudes (predicted above) and because of measurement error itself. Such differences make up what is collectively called the ""error"" — a statistical term that means the amount by which an individual, as measured, differs from what is average for or predicted by his or her levels of intelligence (see errors and residuals in statistics).
The observable data that go into factor analysis would be 10 scores of each of the 1000 students, a total of 10,000 numbers. The factor loadings and levels of the two kinds of intelligence of each student must be inferred from the data.

Mathematical model of the same example
In the following, matrices will be indicated by indexed variables. ""Subject"" indices will be indicated using letters 
  
    
      
        a
      
    
    {\displaystyle a}
  ,
  
    
      
        b
      
    
    {\displaystyle b}
   and 
  
    
      
        c
      
    
    {\displaystyle c}
  , with values running from 
  
    
      
        1
      
    
    {\displaystyle 1}
   to 
  
    
      
        p
      
    
    {\displaystyle p}
   which is equal to 
  
    
      
        10
      
    
    {\displaystyle 10}
   in the above example.  ""Factor"" indices will be indicated using letters 
  
    
      
        p
      
    
    {\displaystyle p}
  , 
  
    
      
        q
      
    
    {\displaystyle q}
   and 
  
    
      
        r
      
    
    {\displaystyle r}
  , with values running from 
  
    
      
        1
      
    
    {\displaystyle 1}
   to 
  
    
      
        k
      
    
    {\displaystyle k}
   which is equal to 
  
    
      
        2
      
    
    {\displaystyle 2}
   in the above example. ""Instance"" or ""sample"" indices will be indicated using letters 
  
    
      
        i
      
    
    {\displaystyle i}
  ,
  
    
      
        j
      
    
    {\displaystyle j}
   and 
  
    
      
        k
      
    
    {\displaystyle k}
  , with values running from 
  
    
      
        1
      
    
    {\displaystyle 1}
   to 
  
    
      
        N
      
    
    {\displaystyle N}
  . In the example above, if a sample of 
  
    
      
        N
        =
        1000
      
    
    {\displaystyle N=1000}
   students participated in the 
  
    
      
        p
        =
        10
      
    
    {\displaystyle p=10}
   exams, the 
  
    
      
        i
      
    
    {\displaystyle i}
  th student's score for the 
  
    
      
        a
      
    
    {\displaystyle a}
  th exam is given by 
  
    
      
        
          x
          
            a
            i
          
        
      
    
    {\displaystyle x_{ai}}
  . The purpose of factor analysis is to characterize the correlations between the variables 
  
    
      
        
          x
          
            a
          
        
      
    
    {\displaystyle x_{a}}
   of which the 
  
    
      
        
          x
          
            a
            i
          
        
      
    
    {\displaystyle x_{ai}}
   are a particular instance, or set of observations. In order for the variables to be on equal footing, they are normalized into standard scores 
  
    
      
        z
      
    
    {\displaystyle z}
  :

  
    
      
        
          z
          
            a
            i
          
        
        =
        
          
            
              
                x
                
                  a
                  i
                
              
              −
              
                μ
                
                  a
                
              
            
            
              σ
              
                a
              
            
          
        
      
    
    {\displaystyle z_{ai}={\frac {x_{ai}-\mu _{a}}{\sigma _{a}}}}
  where the sample mean is:

  
    
      
        
          μ
          
            a
          
        
        =
        
          
            
              1
              N
            
          
        
        
          ∑
          
            i
          
        
        
          x
          
            a
            i
          
        
      
    
    {\displaystyle \mu _{a}={\tfrac {1}{N}}\sum _{i}x_{ai}}
  and the sample variance is given by:

  
    
      
        
          σ
          
            a
          
          
            2
          
        
        =
        
          
            
              1
              
                N
                −
                1
              
            
          
        
        
          ∑
          
            i
          
        
        (
        
          x
          
            a
            i
          
        
        −
        
          μ
          
            a
          
        
        
          )
          
            2
          
        
      
    
    {\displaystyle \sigma _{a}^{2}={\tfrac {1}{N-1}}\sum _{i}(x_{ai}-\mu _{a})^{2}}
  The factor analysis model for this particular sample is then:

  
    
      
        
          
            
              
                
                  z
                  
                    1
                    ,
                    i
                  
                
              
              
                =
              
              
                
                  ℓ
                  
                    1
                    ,
                    1
                  
                
                
                  F
                  
                    1
                    ,
                    i
                  
                
              
              
                +
              
              
                
                  ℓ
                  
                    1
                    ,
                    2
                  
                
                
                  F
                  
                    2
                    ,
                    i
                  
                
              
              
                +
              
              
                
                  ε
                  
                    1
                    ,
                    i
                  
                
              
            
            
              
                ⋮
              
              
              
                ⋮
              
              
              
                ⋮
              
              
              
                ⋮
              
            
            
              
                
                  z
                  
                    10
                    ,
                    i
                  
                
              
              
                =
              
              
                
                  ℓ
                  
                    10
                    ,
                    1
                  
                
                
                  F
                  
                    1
                    ,
                    i
                  
                
              
              
                +
              
              
                
                  ℓ
                  
                    10
                    ,
                    2
                  
                
                
                  F
                  
                    2
                    ,
                    i
                  
                
              
              
                +
              
              
                
                  ε
                  
                    10
                    ,
                    i
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{matrix}z_{1,i}&=&\ell _{1,1}F_{1,i}&+&\ell _{1,2}F_{2,i}&+&\varepsilon _{1,i}\\\vdots &&\vdots &&\vdots &&\vdots \\z_{10,i}&=&\ell _{10,1}F_{1,i}&+&\ell _{10,2}F_{2,i}&+&\varepsilon _{10,i}\end{matrix}}}
  or, more succinctly:

  
    
      
        
          z
          
            a
            i
          
        
        =
        
          ∑
          
            p
          
        
        
          ℓ
          
            a
            p
          
        
        
          F
          
            p
            i
          
        
        +
        
          ε
          
            a
            i
          
        
      
    
    {\displaystyle z_{ai}=\sum _{p}\ell _{ap}F_{pi}+\varepsilon _{ai}}
  where

  
    
      
        
          F
          
            1
            i
          
        
      
    
    {\displaystyle F_{1i}}
   is the 
  
    
      
        i
      
    
    {\displaystyle i}
  th student's ""verbal intelligence"",

  
    
      
        
          F
          
            2
            i
          
        
      
    
    {\displaystyle F_{2i}}
   is the 
  
    
      
        i
      
    
    {\displaystyle i}
  th student's ""mathematical intelligence"",

  
    
      
        
          ℓ
          
            a
            p
          
        
      
    
    {\displaystyle \ell _{ap}}
   are the factor loadings for the 
  
    
      
        a
      
    
    {\displaystyle a}
  th subject, for 
  
    
      
        p
        =
        1
        ,
        2
      
    
    {\displaystyle p=1,2}
  .In matrix notation, we have

  
    
      
        Z
        =
        L
        F
        +
        ε
      
    
    {\displaystyle Z=LF+\varepsilon }
  Observe that by doubling the scale on which ""verbal intelligence""—the first component in each column of 
  
    
      
        F
      
    
    {\displaystyle F}
  —is measured, and simultaneously halving the factor loadings for verbal intelligence makes no difference to the model. Thus, no generality is lost by assuming that the standard deviation of the factors for verbal intelligence is 
  
    
      
        1
      
    
    {\displaystyle 1}
  . Likewise for mathematical intelligence. Moreover, for similar reasons, no generality is lost by assuming the two factors are uncorrelated with each other. In other words:

  
    
      
        
          ∑
          
            i
          
        
        
          F
          
            p
            i
          
        
        
          F
          
            q
            i
          
        
        =
        
          δ
          
            p
            q
          
        
      
    
    {\displaystyle \sum _{i}F_{pi}F_{qi}=\delta _{pq}}
  where 
  
    
      
        
          δ
          
            p
            q
          
        
      
    
    {\displaystyle \delta _{pq}}
   is the Kronecker delta (
  
    
      
        0
      
    
    {\displaystyle 0}
   when 
  
    
      
        p
        ≠
        q
      
    
    {\displaystyle p\neq q}
   and 
  
    
      
        1
      
    
    {\displaystyle 1}
   when 
  
    
      
        p
        =
        q
      
    
    {\displaystyle p=q}
  ).The errors are assumed to be independent of the factors:

  
    
      
        
          ∑
          
            i
          
        
        
          F
          
            p
            i
          
        
        
          ε
          
            a
            i
          
        
        =
        0
      
    
    {\displaystyle \sum _{i}F_{pi}\varepsilon _{ai}=0}
  Note that, since any rotation of a solution is also a solution, this makes interpreting the factors difficult. See disadvantages below. In this particular example, if we do not know beforehand that the two types of intelligence are uncorrelated, then we cannot interpret the two factors as the two different types of intelligence. Even if they are uncorrelated, we cannot tell which factor corresponds to verbal intelligence and which corresponds to mathematical intelligence without an outside argument.
The values of the loadings 
  
    
      
        L
      
    
    {\displaystyle L}
  , the averages 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  , and the variances of the ""errors"" 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   must be estimated given the observed data 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        F
      
    
    {\displaystyle F}
   (the assumption about the levels of the factors is fixed for a given 
  
    
      
        F
      
    
    {\displaystyle F}
  ). 
The ""fundamental theorem"" may be derived from the above conditions:

  
    
      
        
          ∑
          
            i
          
        
        
          z
          
            a
            i
          
        
        
          z
          
            b
            i
          
        
        =
        
          ∑
          
            j
          
        
        
          ℓ
          
            a
            j
          
        
        
          ℓ
          
            b
            j
          
        
        +
        
          ∑
          
            i
          
        
        
          ε
          
            a
            i
          
        
        
          ε
          
            b
            i
          
        
      
    
    {\displaystyle \sum _{i}z_{ai}z_{bi}=\sum _{j}\ell _{aj}\ell _{bj}+\sum _{i}\varepsilon _{ai}\varepsilon _{bi}}
  The term on the left is the 
  
    
      
        (
        a
        ,
        b
        )
      
    
    {\displaystyle (a,b)}
  -term of the correlation matrix (a 
  
    
      
        p
        ×
        p
      
    
    {\displaystyle p\times p}
   matrix derived as the product of the 
  
    
      
        p
        ×
        N
      
    
    {\displaystyle p\times N}
   matrix of standardized observations with its transpose) of the observed data, and its 
  
    
      
        p
      
    
    {\displaystyle p}
   diagonal elements will be 
  
    
      
        1
      
    
    {\displaystyle 1}
  s. The second term on the right will be a diagonal matrix with terms less than unity. The first term on the right is the ""reduced correlation matrix"" and will be equal to the correlation matrix except for its diagonal values which will be less than unity. These diagonal elements of the reduced correlation matrix are called ""communalities"" (which represent the fraction of the variance in the observed variable that is accounted for by the factors):

  
    
      
        
          h
          
            a
          
          
            2
          
        
        =
        1
        −
        
          ψ
          
            a
          
        
        =
        
          ∑
          
            j
          
        
        
          ℓ
          
            a
            j
          
        
        
          ℓ
          
            a
            j
          
        
      
    
    {\displaystyle h_{a}^{2}=1-\psi _{a}=\sum _{j}\ell _{aj}\ell _{aj}}
  The sample data 
  
    
      
        
          z
          
            a
            i
          
        
      
    
    {\displaystyle z_{ai}}
   will not, of course, exactly obey the fundamental equation given above due to sampling errors, inadequacy of the model, etc. The goal of any analysis of the above model is to find the factors 
  
    
      
        
          F
          
            p
            i
          
        
      
    
    {\displaystyle F_{pi}}
   and loadings 
  
    
      
        
          ℓ
          
            a
            p
          
        
      
    
    {\displaystyle \ell _{ap}}
   which, in some sense, give a ""best fit"" to the data. In factor analysis, the best fit is defined as the minimum of the mean square error in the off-diagonal residuals of the correlation matrix:

  
    
      
        
          ε
          
            2
          
        
        =
        
          ∑
          
            a
            ≠
            b
          
        
        
          
            [
            
              
                ∑
                
                  i
                
              
              
                z
                
                  a
                  i
                
              
              
                z
                
                  b
                  i
                
              
              −
              
                ∑
                
                  j
                
              
              
                ℓ
                
                  a
                  j
                
              
              
                ℓ
                
                  b
                  j
                
              
            
            ]
          
          
            2
          
        
      
    
    {\displaystyle \varepsilon ^{2}=\sum _{a\neq b}\left[\sum _{i}z_{ai}z_{bi}-\sum _{j}\ell _{aj}\ell _{bj}\right]^{2}}
  This is equivalent to minimizing the off-diagonal components of the error covariance which, in the model equations have expected values of zero. This is to be contrasted with principal component analysis which seeks to minimize the mean square error of all residuals. Before the advent of high-speed computers, considerable effort was devoted to finding approximate solutions to the problem, particularly in estimating the communalities by other means, which then simplifies the problem considerably by yielding a known reduced correlation matrix. This was then used to estimate the factors and the loadings. With the advent of high-speed computers, the minimization problem can be solved iteratively with adequate speed, and the communalities are calculated in the process, rather than being needed beforehand. The MinRes algorithm is particularly suited to this problem, but is hardly the only iterative means of finding a solution.
If the solution factors are allowed to be correlated (as in 'oblimin' rotation, for example), then the corresponding mathematical model uses skew coordinates rather than orthogonal coordinates.

Geometric interpretation
The parameters and variables of factor analysis can be given a geometrical interpretation. The data (
  
    
      
        
          z
          
            a
            i
          
        
      
    
    {\displaystyle z_{ai}}
  ), the factors (
  
    
      
        
          F
          
            p
            i
          
        
      
    
    {\displaystyle F_{pi}}
  ) and the errors (
  
    
      
        
          ε
          
            a
            i
          
        
      
    
    {\displaystyle \varepsilon _{ai}}
  ) can be viewed as vectors in an 
  
    
      
        N
      
    
    {\displaystyle N}
  -dimensional Euclidean space (sample space), represented as 
  
    
      
        
          
            z
          
          
            a
          
        
      
    
    {\displaystyle \mathbf {z} _{a}}
  , 
  
    
      
        
          
            F
          
          
            j
          
        
      
    
    {\displaystyle \mathbf {F} _{j}}
   and 
  
    
      
        
          
            ε
          
          
            a
          
        
      
    
    {\displaystyle {\boldsymbol {\varepsilon }}_{a}}
   respectively. Since the data are standardized, the data vectors are of unit length (
  
    
      
        
          |
        
        
          |
        
        
          
            z
          
          
            a
          
        
        
          |
        
        
          |
        
        =
        1
      
    
    {\displaystyle ||\mathbf {z} _{a}||=1}
  ). The factor vectors define an 
  
    
      
        k
      
    
    {\displaystyle k}
  -dimensional linear subspace (i.e. a hyperplane) in this space, upon which the data vectors are projected orthogonally. This follows from the model equation 

  
    
      
        
          
            z
          
          
            a
          
        
        =
        
          ∑
          
            j
          
        
        
          ℓ
          
            a
            j
          
        
        
          
            F
          
          
            j
          
        
        +
        
          
            ε
          
          
            a
          
        
      
    
    {\displaystyle \mathbf {z} _{a}=\sum _{j}\ell _{aj}\mathbf {F} _{j}+{\boldsymbol {\varepsilon }}_{a}}
  and the independence of the factors and the errors: 
  
    
      
        
          
            F
          
          
            j
          
        
        ⋅
        
          
            ε
          
          
            a
          
        
        =
        0
      
    
    {\displaystyle \mathbf {F} _{j}\cdot {\boldsymbol {\varepsilon }}_{a}=0}
  . In the above example, the hyperplane is just a 2-dimensional plane defined by the two factor vectors. The projection of the data vectors onto the hyperplane is given by 

  
    
      
        
          
            
              
                
                  z
                
                ^
              
            
          
          
            a
          
        
        =
        
          ∑
          
            j
          
        
        
          ℓ
          
            a
            j
          
        
        
          
            F
          
          
            j
          
        
      
    
    {\displaystyle {\hat {\mathbf {z} }}_{a}=\sum _{j}\ell _{aj}\mathbf {F} _{j}}
  and the errors are vectors from that projected point to the data point and are perpendicular to the hyperplane. The goal of factor analysis is to find a hyperplane which is a ""best fit"" to the data in some sense, so it doesn't matter how the factor vectors which define this hyperplane are chosen, as long as they are independent and lie in the hyperplane. We are free to specify them as both orthogonal and normal (
  
    
      
        
          
            F
          
          
            j
          
        
        ⋅
        
          
            F
          
          
            q
          
        
        =
        
          δ
          
            p
            q
          
        
      
    
    {\displaystyle \mathbf {F} _{j}\cdot \mathbf {F} _{q}=\delta _{pq}}
  ) with no loss of generality. After a suitable set of factors are found, they may also be arbitrarily rotated within the hyperplane, so that any rotation of the factor vectors will define the same hyperplane, and also be a solution. As a result, in the above example, in which the fitting hyperplane is two dimensional, if we do not know beforehand that the two types of intelligence are uncorrelated, then we cannot interpret the two factors as the two different types of intelligence. Even if they are uncorrelated, we cannot tell which factor corresponds to verbal intelligence and which corresponds to mathematical intelligence, or whether the factors are linear combinations of both, without an outside argument.
The data vectors 
  
    
      
        
          
            z
          
          
            a
          
        
      
    
    {\displaystyle \mathbf {z} _{a}}
   have unit length. The entries of the correlation matrix for the data are given by 
  
    
      
        
          r
          
            a
            b
          
        
        =
        
          
            z
          
          
            a
          
        
        ⋅
        
          
            z
          
          
            b
          
        
      
    
    {\displaystyle r_{ab}=\mathbf {z} _{a}\cdot \mathbf {z} _{b}}
  . The correlation matrix can be geometrically interpreted as the cosine of the angle between the two data vectors 
  
    
      
        
          
            z
          
          
            a
          
        
      
    
    {\displaystyle \mathbf {z} _{a}}
   and 
  
    
      
        
          
            z
          
          
            b
          
        
      
    
    {\displaystyle \mathbf {z} _{b}}
  . The diagonal elements will clearly be 
  
    
      
        1
      
    
    {\displaystyle 1}
  s and the off diagonal elements will have absolute values less than or equal to unity. The ""reduced correlation matrix"" is defined as 

  
    
      
        
          
            
              
                r
                ^
              
            
          
          
            a
            b
          
        
        =
        
          
            
              
                
                  z
                
                ^
              
            
          
          
            a
          
        
        ⋅
        
          
            
              
                
                  z
                
                ^
              
            
          
          
            b
          
        
      
    
    {\displaystyle {\hat {r}}_{ab}={\hat {\mathbf {z} }}_{a}\cdot {\hat {\mathbf {z} }}_{b}}
  .The goal of factor analysis is to choose the fitting hyperplane such that the reduced correlation matrix reproduces the correlation matrix as nearly as possible, except for the diagonal elements of the correlation matrix which are known to have unit value.  In other words, the goal is to reproduce as accurately as possible the cross-correlations in the data. Specifically, for the fitting hyperplane, the mean square error in the off-diagonal components 

  
    
      
        
          ε
          
            2
          
        
        =
        
          ∑
          
            a
            ≠
            b
          
        
        
          
            (
            
              
                r
                
                  a
                  b
                
              
              −
              
                
                  
                    
                      r
                      ^
                    
                  
                
                
                  a
                  b
                
              
            
            )
          
          
            2
          
        
      
    
    {\displaystyle \varepsilon ^{2}=\sum _{a\neq b}\left(r_{ab}-{\hat {r}}_{ab}\right)^{2}}
  is to be minimized, and this is accomplished by minimizing it with respect to a set of orthonormal factor vectors. It can be seen that 

  
    
      
        
          r
          
            a
            b
          
        
        −
        
          
            
              
                r
                ^
              
            
          
          
            a
            b
          
        
        =
        
          
            ε
          
          
            a
          
        
        ⋅
        
          
            ε
          
          
            b
          
        
      
    
    {\displaystyle r_{ab}-{\hat {r}}_{ab}={\boldsymbol {\varepsilon }}_{a}\cdot {\boldsymbol {\varepsilon }}_{b}}
  The term on the right is just the covariance of the errors. In the model, the error covariance is stated to be a diagonal matrix and so the above minimization problem will in fact yield a ""best fit"" to the model: It will yield a sample estimate of the error covariance which has its off-diagonal components minimized in the mean square sense. It can be seen that since the 
  
    
      
        
          
            
              
                z
                ^
              
            
          
          
            a
          
        
      
    
    {\displaystyle {\hat {z}}_{a}}
   are orthogonal projections of the data vectors, their length will be less than or equal to the length of the projected data vector, which is unity. The square of these lengths are just the diagonal elements of the reduced correlation matrix. These diagonal elements of the reduced correlation matrix are known as ""communalities"":

  
    
      
        
          
            
              h
              
                a
              
            
          
          
            2
          
        
        =
        
          |
        
        
          |
        
        
          
            
              
                
                  z
                
                ^
              
            
          
          
            a
          
        
        
          |
        
        
          
            |
          
          
            2
          
        
        =
        
          ∑
          
            j
          
        
        
          
            
              ℓ
              
                a
                j
              
            
          
          
            2
          
        
      
    
    {\displaystyle {h_{a}}^{2}=||{\hat {\mathbf {z} }}_{a}||^{2}=\sum _{j}{\ell _{aj}}^{2}}
  Large values of the communalities will indicate that the fitting hyperplane is rather accurately reproducing the correlation matrix. The mean values of the factors must also be constrained to be zero, from which it follows that the mean values of the errors will also be zero.

Practical implementation
Types of factor analysis
Exploratory factor analysis
Exploratory factor analysis (EFA) is used to identify complex interrelationships among items and group items that are part of unified concepts.  The researcher makes no a priori assumptions about relationships among factors.

Confirmatory factor analysis
Confirmatory factor analysis (CFA) is a more complex approach that tests the hypothesis that the items are associated with specific factors. CFA uses structural equation modeling to test a measurement model whereby loading on the factors allows for evaluation of relationships between observed variables and unobserved variables.  Structural equation modeling approaches can accommodate measurement error, and are less restrictive than least-squares estimation.  Hypothesized models are tested against actual data, and the analysis would demonstrate loadings of observed variables on the latent variables (factors), as well as the correlation between the latent variables.

Types of factor extraction
Principal component analysis (PCA) is a widely used method for factor extraction, which is the first phase of EFA. Factor weights are computed to extract the maximum possible variance, with successive factoring continuing until there is no further meaningful variance left. The factor model must then be rotated for analysis.Canonical factor analysis, also called Rao's canonical factoring, is a different method of computing the same model as PCA, which uses the principal axis method. Canonical factor analysis seeks factors which have the highest canonical correlation with the observed variables. Canonical factor analysis is unaffected by arbitrary rescaling of the data.
Common factor analysis, also called principal factor analysis (PFA) or principal axis factoring (PAF), seeks the fewest factors which can account for the common variance (correlation) of a set of variables.
Image factoring is based on the correlation matrix of predicted variables rather than actual variables, where each variable is predicted from the others using multiple regression.
Alpha factoring is based on maximizing the reliability of factors, assuming variables are randomly sampled from a universe of variables. All other methods assume cases to be sampled and variables fixed.
Factor regression model is a combinatorial model of factor model and regression model; or alternatively, it can be viewed as the hybrid factor model, whose factors are partially known.

Terminology
Factor loadings: Communality is the square of the standardized outer loading of an item. Analogous to Pearson's r-squared, the squared factor loading is the percent of variance in that indicator variable explained by the factor. To get the percent of variance in all the variables accounted for by each factor, add the sum of the squared factor loadings for that factor (column) and divide by the number of variables. (Note the number of variables equals the sum of their variances as the variance of a standardized variable is 1.) This is the same as dividing the factor's eigenvalue by the number of variables.
Interpreting factor loadings: By one rule of thumb in confirmatory factor analysis, loadings should be .7 or higher to confirm that independent variables identified a priori are represented by a particular factor, on the rationale that the .7 level corresponds to about half of the variance in the indicator being explained by the factor. However, the .7 standard is a high one and real-life data may well not meet this criterion, which is why some researchers, particularly for exploratory purposes, will use a lower level such as .4 for the central factor and .25 for other factors. In any event, factor loadings must be interpreted in the light of theory, not by arbitrary cutoff levels.
In oblique rotation, one may examine both a pattern matrix and a structure matrix. The structure matrix is simply the factor loading matrix as in orthogonal rotation, representing the variance in a measured variable explained by a factor on both a unique and common contributions basis. The pattern matrix, in contrast, contains coefficients which just represent unique contributions. The more factors, the lower the pattern coefficients as a rule since there will be more common contributions to variance explained. For oblique rotation, the researcher looks at both the structure and pattern coefficients when attributing a label to a factor. Principles of oblique rotation can be derived from both cross entropy and its dual entropy.Communality: The sum of the squared factor loadings for all factors for a given variable (row) is the variance in that variable accounted for by all the factors. The communality measures the percent of variance in a given variable explained by all the factors jointly and may be interpreted as the reliability of the indicator in the context of the factors being posited.
Spurious solutions: If the communality exceeds 1.0, there is a spurious solution, which may reflect too small a sample or the choice to extract too many or too few factors.
Uniqueness of a variable: The variability of a variable minus its communality.
Eigenvalues/characteristic roots: Eigenvalues measure the amount of variation in the total sample accounted for by each factor. The ratio of eigenvalues is the ratio of explanatory importance of the factors with respect to the variables. If a factor has a low eigenvalue, then it is contributing little to the explanation of variances in the variables and may be ignored as less important than the factors with higher eigenvalues.
Extraction sums of squared loadings: Initial eigenvalues and eigenvalues after extraction (listed by SPSS as ""Extraction Sums of Squared Loadings"") are the same for PCA extraction, but for other extraction methods, eigenvalues after extraction will be lower than their initial counterparts. SPSS also prints ""Rotation Sums of Squared Loadings"" and even for PCA, these eigenvalues will differ from initial and extraction eigenvalues, though their total will be the same.
Factor scores (also called component scores in PCA): are the scores of each case (row) on each factor (column). To compute the factor score for a given case for a given factor, one takes the case's standardized score on each variable, multiplies by the corresponding loadings of the variable for the given factor, and sums these products. Computing factor scores allows one to look for factor outliers. Also, factor scores may be used as variables in subsequent modeling. (Explained from PCA not from Factor Analysis perspective).

Criteria for determining the number of factors
Researchers wish to avoid such subjective or arbitrary criteria for factor retention as ""it made sense to me"". A number of objective methods have been developed to solve this problem, allowing users to determine an appropriate range of solutions to investigate. Methods may not agree. For instance, the parallel analysis may suggest 5 factors while Velicer's MAP suggests 6, so the researcher may request both 5 and 6-factor solutions and discuss each in terms of their relation to external data and theory.

Modern criteria
Horn's parallel analysis (PA): A Monte-Carlo based simulation method that compares the observed eigenvalues with those obtained from uncorrelated normal variables. A factor or component is retained if the associated eigenvalue is bigger than the 95th percentile of the distribution of eigenvalues derived from the random data. PA is among the more commonly recommended rules for determining the number of components to retain, but many programs fail to include this option (a notable exception being R). However, Formann provided both theoretical and empirical evidence that its application might not be appropriate in many cases since its performance is considerably influenced by sample size, item discrimination, and type of correlation coefficient.Velicer's (1976) MAP test as described by Courtney (2013) “involves a complete principal components analysis followed by the examination of a series of matrices of partial correlations” (p. 397 (though note that this quote does not occur in Velicer (1976) and the cited page number is outside the pages of the citation). The squared correlation for Step “0” (see Figure 4) is the average squared off-diagonal correlation for the unpartialed correlation matrix. On Step 1, the first principal component and its associated items are partialed out. Thereafter, the average squared off-diagonal correlation for the subsequent correlation matrix is then computed for Step 1. On Step 2, the first two principal components are partialed out and the resultant average squared off-diagonal correlation is again computed. The computations are carried out for k minus one step (k representing the total number of variables in the matrix). Thereafter, all of the average squared correlations for each step are lined up and the step number in the analyses that resulted in the lowest average squared partial correlation determines the number of components or factors to retain. By this method, components are maintained as long as the variance in the correlation matrix represents systematic variance, as opposed to residual or error variance. Although methodologically akin to principal components analysis, the MAP technique has been shown to perform quite well in determining the number of factors to retain in multiple simulation studies. This procedure is made available through SPSS's user interface, as well as the psych package for the R programming language.

Older methods
Kaiser criterion: The Kaiser rule is to drop all components with eigenvalues under 1.0 – this being the eigenvalue equal to the information accounted for by an average single item. The Kaiser criterion is the default in SPSS and most statistical software but is not recommended when used as the sole cut-off criterion for estimating the number of factors as it tends to over-extract factors. A variation of this method has been created where a researcher calculates confidence intervals for each eigenvalue and retains only factors which have the entire confidence interval greater than 1.0.Scree plot:
The Cattell scree test plots the components as the X-axis and the corresponding eigenvalues as the Y-axis.  As one moves to the right, toward later components, the eigenvalues drop. When the drop ceases and the curve makes an elbow toward less steep decline, Cattell's scree test says to drop all further components after the one starting at the elbow. This rule is sometimes criticised for being amenable to researcher-controlled ""fudging"".  That is, as picking the ""elbow"" can be subjective because the curve has multiple elbows or is a smooth curve, the researcher may be tempted to set the cut-off at the number of factors desired by their research agenda.Variance explained criteria: Some researchers simply use the rule of keeping enough factors to account for 90% (sometimes 80%) of the variation.  Where the researcher's goal emphasizes parsimony (explaining variance with as few factors as possible), the criterion could be as low as 50%.

Bayesian method
A Bayesian approach based on the Indian buffet process returns a probability distribution over the plausible number of latent factors.

Rotation methods
The unrotated output maximizes variance accounted for by the first and subsequent factors, and forces the factors to be orthogonal. This data-compression comes at the cost of having most items load on the early factors, and usually, of having many items load substantially on more than one factor. Rotation serves to make the output more understandable, by seeking so-called ""Simple Structure"": A pattern of loadings where each item loads strongly on only one of the factors, and much more weakly on the other factors. Rotations can be orthogonal or oblique (allowing the factors to correlate).
Varimax rotation is an orthogonal rotation of the factor axes to maximize the variance of the squared loadings of a factor (column) on all the variables (rows) in a factor matrix, which has the effect of differentiating the original variables by extracted factor. Each factor will tend to have either large or small loadings of any particular variable. A varimax solution yields results which make it as easy as possible to identify each variable with a single factor. This is the most common rotation option. However, the orthogonality (i.e., independence) of factors is often an unrealistic assumption. Oblique rotations are inclusive of orthogonal rotation, and for that reason, oblique rotations are a preferred method.  Allowing for factors that are correlated with one another is especially applicable in psychometric research, since attitudes, opinions, and intellectual abilities tend to be correlated, and since it would be unrealistic in many situations to assume otherwise.Quartimax rotation is an orthogonal alternative which minimizes the number of factors needed to explain each variable. This type of rotation often generates a general factor on which most variables are loaded to a high or medium degree. Such a factor structure is usually not helpful to the research purpose.
Equimax rotation is a compromise between varimax and quartimax criteria.
Direct oblimin rotation is the standard method when one wishes a non-orthogonal (oblique) solution – that is, one in which the factors are allowed to be correlated. This will result in higher eigenvalues but diminished interpretability of the factors. See below.Promax rotation is an alternative non-orthogonal (oblique) rotation method which is computationally faster than the direct oblimin method and therefore is sometimes used for very large datasets.

Higher order factor analysis
Higher-order factor analysis is a statistical method consisting of repeating steps factor analysis – oblique rotation – factor analysis of rotated factors.  Its merit is to enable the researcher to see the hierarchical structure of studied phenomena. To interpret the results, one proceeds either by post-multiplying the primary factor pattern matrix by the higher-order factor pattern matrices (Gorsuch, 1983) and perhaps applying a Varimax rotation to the result (Thompson, 1990) or by using a Schmid-Leiman solution (SLS, Schmid & Leiman, 1957, also known as Schmid-Leiman transformation) which attributes the variation from the primary factors to the second-order factors.

In psychometrics
History
Charles Spearman was the first psychologist to discuss common factor analysis and did so in his 1904 paper. It provided few details about his methods and was concerned with single-factor models. He discovered that school children's scores on a wide variety of seemingly unrelated subjects were positively correlated, which led him to postulate that a single general mental ability, or g, underlies and shapes human cognitive performance.
The initial development of common factor analysis with multiple factors was given by Louis Thurstone in two papers in the early 1930s, summarized in his 1935 book, The Vector of Mind. Thurstone introduced several important factor analysis concepts, including communality, uniqueness, and rotation. He advocated for ""simple structure"", and developed methods of rotation that could be used as a way to achieve such structure.In Q methodology, Stephenson, a student of Spearman, distinguish between R factor analysis, oriented toward the study of inter-individual differences, and Q factor analysis oriented toward subjective intra-individual differences.Raymond Cattell was a strong advocate of factor analysis and psychometrics and used Thurstone's multi-factor theory to explain intelligence. Cattell also developed the ""scree"" test and similarity coefficients.

Applications in psychology
Factor analysis is used to identify ""factors"" that explain a variety of results on different tests. For example, intelligence research found that people who get a high score on a test of verbal ability are also good on other tests that require verbal abilities. Researchers explained this by using factor analysis to isolate one factor, often called verbal intelligence, which represents the degree to which someone is able to solve problems involving verbal skills.
Factor analysis in psychology is most often associated with intelligence research. However, it also has been used to find factors in a broad range of domains such as personality, attitudes, beliefs, etc. It is linked to psychometrics, as it can assess the validity of an instrument by finding if the instrument indeed measures the postulated factors.
Factor analysis is a frequently used technique in cross-cultural research. It serves the purpose of extracting cultural dimensions. The best known cultural dimensions models are those elaborated by Geert Hofstede, Ronald Inglehart, Christian Welzel, Shalom Schwartz and Michael Minkov.

Advantages
Reduction of number of variables, by combining two or more variables into a single factor. For example, performance at running, ball throwing, batting, jumping and weight lifting could be combined into a single factor such as general athletic ability. Usually, in an item by people matrix, factors are selected by grouping related items. In the Q factor analysis technique the matrix is transposed and factors are created by grouping related people. For example, liberals, libertarians, conservatives, and socialists might form into separate groups.
Identification of groups of inter-related variables, to see how they are related to each other. For example, Carroll used factor analysis to build his Three Stratum Theory. He found that a factor called ""broad visual perception"" relates to how good an individual is at visual tasks. He also found a ""broad auditory perception"" factor, relating to auditory task capability. Furthermore, he found a global factor, called ""g"" or general intelligence, that relates to both ""broad visual perception"" and ""broad auditory perception"". This means someone with a high ""g"" is likely to have both a high ""visual perception"" capability and a high ""auditory perception"" capability, and that ""g"" therefore explains a good part of why someone is good or bad in both of those domains.

Disadvantages
""...each orientation is equally acceptable mathematically. But different factorial theories proved to differ as much in terms of the orientations of factorial axes for a given solution as in terms of anything else, so that model fitting did not prove to be useful in distinguishing among theories."" (Sternberg, 1977). This means all rotations represent different underlying processes, but all rotations are equally valid outcomes of standard factor analysis optimization. Therefore, it is impossible to pick the proper rotation using factor analysis alone.
Factor analysis can be only as good as the data allows. In psychology, where researchers often have to rely on less valid and reliable measures such as self-reports, this can be problematic.
Interpreting factor analysis is based on using a ""heuristic"", which is a solution that is ""convenient even if not absolutely true"". More than one interpretation can be made of the same data factored the same way, and factor analysis cannot identify causality.

Exploratory factor analysis (EFA) versus principal components analysis (PCA)
Factor analysis is related to principal component analysis (PCA), but the two are not identical. There has been significant controversy in the field over differences between the two techniques. PCA can be considered as a more basic version of exploratory factor analysis (EFA) that was developed in the early days prior to the advent of high-speed computers. Both PCA and factor analysis aim to reduce the dimensionality of a set of data, but the approaches taken to do so are different for the two techniques. Factor analysis is clearly designed with the objective to identify certain unobservable factors from the observed variables, whereas PCA does not directly address this objective; at best, PCA provides an approximation to the required factors. From the point of view of exploratory analysis, the eigenvalues of PCA are inflated component loadings, i.e., contaminated with error variance.Whilst EFA and PCA are treated as synonymous techniques in some fields of statistics, this has been criticised. Factor analysis ""deals with the assumption of an underlying causal structure: [it] assumes that the covariation in the observed variables is due to the presence of one or more latent variables (factors) that exert causal influence on these observed variables"". In contrast, PCA neither assumes nor depends on such an underlying causal relationship. Researchers have argued that the distinctions between the two techniques may mean that there are objective benefits for preferring one over the other based on the analytic goal. If the factor model is incorrectly formulated or the assumptions are not met, then factor analysis will give erroneous results. Factor analysis has been used successfully where adequate understanding of the system permits good initial model formulations. PCA employs a mathematical transformation to the original data with no assumptions about the form of the covariance matrix. The objective of PCA is to determine linear combinations of the original variables and select a few that can be used to summarize the data set without losing much information.

Arguments contrasting PCA and EFA
Fabrigar et al. (1999) address a number of reasons used to suggest that PCA is not equivalent to factor analysis:

It is sometimes suggested that PCA is computationally quicker and requires fewer resources than factor analysis. Fabrigar et al. suggest that readily available computer resources have rendered this practical concern irrelevant.
PCA and factor analysis can produce similar results. This point is also addressed by Fabrigar et al.; in certain cases, whereby the communalities are low (e.g. 0.4), the two techniques produce divergent results. In fact, Fabrigar et al. argue that in cases where the data correspond to assumptions of the common factor model, the results of PCA are inaccurate results.
There are certain cases where factor analysis leads to 'Heywood cases'. These encompass situations whereby 100% or more of the variance in a measured variable is estimated to be accounted for by the model. Fabrigar et al. suggest that these cases are actually informative to the researcher, indicating an incorrectly specified model or a violation of the common factor model. The lack of Heywood cases in the PCA approach may mean that such issues pass unnoticed.
Researchers gain extra information from a PCA approach, such as an individual's score on a certain component; such information is not yielded from factor analysis. However, as Fabrigar et al. contend, the typical aim of factor analysis – i.e. to determine the factors accounting for the structure of the correlations between measured variables – does not require knowledge of factor scores and thus this advantage is negated. It is also possible to compute factor scores from a factor analysis.

Variance versus covariance
Factor analysis takes into account the random error that is inherent in measurement, whereas PCA fails to do so. This point is exemplified by Brown (2009), who indicated that, in respect to the correlation matrices involved in the calculations:

""In PCA, 1.00s are put in the diagonal meaning that all of the variance in the matrix is to be accounted for (including variance unique to each variable, variance common among variables, and error variance). That would, therefore, by definition, include all of the variance in the variables. In contrast, in EFA, the communalities are put in the diagonal meaning that only the variance shared with other variables is to be accounted for (excluding variance unique to each variable and error variance). That would, therefore, by definition, include only variance that is common among the variables.""
For this reason, Brown (2009) recommends using factor analysis when theoretical ideas about relationships between variables exist, whereas PCA should be used if the goal of the researcher is to explore patterns in their data.

Differences in procedure and results
The differences between PCA and factor analysis (FA) are further illustrated by Suhr (2009):
PCA results in principal components that account for a maximal amount of variance for observed variables; FA accounts for common variance in the data.
PCA inserts ones on the diagonals of the correlation matrix; FA adjusts the diagonals of the correlation matrix with the unique factors.
PCA minimizes the sum of squared perpendicular distance to the component axis; FA estimates factors which influence responses on observed variables.
The component scores in PCA represent a linear combination of the observed variables weighted by eigenvectors; the observed variables in FA are linear combinations of the underlying and unique factors.
In PCA, the components yielded are uninterpretable, i.e. they do not represent underlying ‘constructs’; in FA, the underlying constructs can be labelled and readily interpreted, given an accurate model specification.

In marketing
The basic steps are:

Identify the salient attributes consumers use to evaluate products in this category.
Use quantitative marketing research techniques (such as surveys) to collect data from a sample of potential customers concerning their ratings of all the product attributes.
Input the data into a statistical program and run the factor analysis procedure. The computer will yield a set of underlying attributes (or factors).
Use these factors to construct perceptual maps and other product positioning devices.

Information collection
The data collection stage is usually done by marketing research professionals. Survey questions ask the respondent to rate a product sample or descriptions of product concepts on a range of attributes. Anywhere from five to twenty attributes are chosen. They could include things like: ease of use, weight, accuracy, durability, colourfulness, price, or size. The attributes chosen will vary depending on the product being studied. The same question is asked about all the products in the study. The data for multiple products is coded and input into a statistical program such as R, SPSS, SAS, Stata, STATISTICA, JMP, and SYSTAT.

Analysis
The analysis will isolate the underlying factors that explain the data using a matrix of associations. Factor analysis is an interdependence technique. The complete set of interdependent relationships is examined. There is no specification of dependent variables, independent variables, or causality. Factor analysis assumes that all the rating data on different attributes can be reduced down to a few important dimensions. This reduction is possible because some attributes may be related to each other. The rating given to any one attribute is partially the result of the influence of other attributes. The statistical algorithm deconstructs the rating (called a raw score) into its various components, and reconstructs the partial scores into underlying factor scores. The degree of correlation between the initial raw score and the final factor score is called a factor loading.

Advantages
Both objective and subjective attributes can be used provided the subjective attributes can be converted into scores.
Factor analysis can identify latent dimensions or constructs that direct analysis may not.
It is easy and inexpensive.

Disadvantages
Usefulness depends on the researchers' ability to collect a sufficient set of product attributes.  If important attributes are excluded or neglected, the value of the procedure is reduced.
If sets of observed variables are highly similar to each other and distinct from other items, factor analysis will assign a single factor to them.  This may obscure factors that represent more interesting relationships.
Naming factors may require knowledge of theory because seemingly dissimilar attributes can correlate strongly for unknown reasons.

In physical and biological sciences
Factor analysis has also been widely used in physical sciences such as geochemistry,  hydrochemistry, astrophysics and cosmology, as well as biological sciences, such as ecology, molecular biology, neuroscience and biochemistry.
In groundwater quality management, it is important to relate the spatial distribution of different chemical
parameters to different possible sources, which have different chemical signatures. For example, a sulfide mine is likely to be associated with high levels of acidity, dissolved sulfates and transition metals. These signatures can be identified as factors through R-mode factor analysis, and the location of possible sources can be suggested by contouring the factor scores.In geochemistry, different factors can correspond to different mineral associations, and thus to mineralisation.

In microarray analysis
Factor analysis can be used for summarizing high-density oligonucleotide DNA microarrays data at probe level for Affymetrix GeneChips. In this case, the latent variable corresponds to the RNA concentration in a sample.

Implementation
Factor analysis has been implemented in several statistical analysis programs since the 1980s:

BMDP
JMP (statistical software)
Mplus (statistical software)]
Python: module Scikit-learn
R (with the base function factanal or fa function in package psych). Rotations are implemented in the GPArotation R package.
SAS (using PROC FACTOR or PROC CALIS)
SPSS
Stata

See also
References
Further reading
Child, Dennis (2006), The Essentials of Factor Analysis (3rd ed.), Continuum International, ISBN 978-0-8264-8000-2.
Fabrigar, L.R.; Wegener, D.T.; MacCallum, R.C.; Strahan, E.J. (September 1999). ""Evaluating the use of exploratory factor analysis in psychological research"". Psychological Methods. 4 (3): 272–299. doi:10.1037/1082-989X.4.3.272.
B.T. Gray (1997) Higher-Order Factor Analysis (Conference paper)
Jennrich, Robert I., ""Rotation to Simple Loadings Using Component Loss Function: The Oblique Case,"" Psychometrika, Vol. 71, No. 1, pp. 173–191, March 2006.
Katz, Jeffrey Owen, and Rohlf, F. James. Primary product functionplane: An oblique rotation to simple structure. Multivariate Behavioral Research, April 1975, Vol. 10, pp. 219–232.
Katz, Jeffrey Owen, and Rohlf, F. James. Functionplane: A new approach to simple structure rotation. Psychometrika, March 1974, Vol. 39, No. 1, pp. 37–51.
Katz, Jeffrey Owen, and Rohlf, F. James. Function-point cluster analysis. Systematic Zoology, September 1973, Vol. 22, No. 3, pp. 295–301.
Mulaik, S. A. (2010), Foundations of Factor Analysis, Chapman & Hall.
Preacher, K.J.; MacCallum, R.C. (2003). ""Repairing Tom Swift's Electric Factor Analysis Machine"" (PDF). Understanding Statistics. 2 (1): 13–43. doi:10.1207/S15328031US0201_02. hdl:1808/1492.
J.Schmid and J. M. Leiman (1957). The development of hierarchical factor solutions. Psychometrika, 22(1), 53–61.
Thompson, B. (2004), Exploratory and Confirmatory Factor Analysis: Understanding concepts and applications, Washington DC: American Psychological Association, ISBN 978-1591470939.Hans-Georg Wolff, Katja Preising (2005)Exploring item and higher order factor structure with the schmid-leiman solution : Syntax codes for SPSS and SASBehavior research methods, instruments & computers, 37 (1), 48-58

External links
A Beginner's Guide to Factor Analysis
Exploratory Factor Analysis. A Book Manuscript by Tucker, L. & MacCallum R. (1993). Retrieved June 8, 2006, from: [1]
Garson, G. David, ""Factor Analysis,"" from Statnotes: Topics in Multivariate Analysis. Retrieved on April 13, 2009 from StatNotes: Topics in Multivariate Analysis, from G. David Garson at North Carolina State University, Public Administration Program
Factor Analysis at 100 — conference material
FARMS — Factor Analysis for Robust Microarray Summarization, an R package",https://en.wikipedia.org/wiki/Factor_analysis,"['All Wikipedia articles needing clarification', 'All articles needing additional references', 'All articles with unsourced statements', 'All pages needing factual verification', 'Articles needing additional references from April 2012', 'Articles with unsourced statements from March 2016', 'CS1 errors: missing periodical', 'Commons category link is on Wikidata', 'Educational psychology', 'Factor analysis', 'Latent variable models', 'Market research', 'Market segmentation', 'Product management', 'Psychometrics', 'Quantitative marketing research', 'Wikipedia articles needing clarification from July 2019', 'Wikipedia articles needing clarification from March 2010', 'Wikipedia articles needing clarification from May 2012', 'Wikipedia articles needing factual verification from November 2013']",Data Science
92,Feature engineering,"Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself.

Features
A feature is an attribute or property shared by all of the independent units on which analysis or prediction is to be done. Any attribute could be a feature, as long as it is useful to the model.
The purpose of a feature, other than being an attribute, would be much easier to understand in the context of a problem. A feature is a characteristic that might help when solving the problem.

Importance
Features are important to predictive models and influence results.It is asserted that feature engineering plays an important part of Kaggle competitions  and machine learning projects' success or failure.

Process
The feature engineering process is:
Brainstorming or testing features;
Deciding what features to create;
Creating features;
Checking how the features work with your model;
Improving your features if needed;
Go back to brainstorming/creating more features until the work is done.

Relevance
A feature could be strongly relevant (i.e., the feature has information that doesn't exist in any other feature), relevant, weakly relevant (some information that other features include) or irrelevant. Even if some features are irrelevant, having too many is better than missing those that are important. Feature selection can be used to prevent overfitting.

Feature explosion
Feature explosion can be caused by feature combination or feature templates, both leading to a quick growth in the total number of features.

Feature templates - implementing feature templates instead of coding new features
Feature combinations - combinations that cannot be represented by the linear systemFeature explosion can be limited via techniques such as: regularization, kernel method, feature selection.

Automation
Automation of feature engineering is a research topic that dates back to at least the late 1990s and machine learning software that incorporates automated feature engineering has been commercially available since 2016.  The academic literature on the topic can be roughly separated into two strings: First, Multi-relational decision tree learning (MRDTL), which uses a supervised algorithm that is similar to a decision tree. Second, more recent approaches, like Deep Feature Synthesis, which use simpler methods.Multi-relational decision tree learning (MRDTL) generates features in the form of SQL queries by successively adding new clauses to the queries. For instance, the algorithm might start out with 

The query can then successively be refined by adding conditions, such as ""WHERE t1.charge <= -0.392"".However, most of the academic studies on MRDTL use implementations based on existing relational databases, which results in many redundant operations. These redundancies can be reduced by using tricks such as tuple id propagation. More recently, it has been demonstrated that the efficiency can be increased further by using incremental updates, which completely eliminates redundancies.In 2015, researchers at MIT presented the Deep Feature Synthesis algorithm and demonstrated its effectiveness in online data science competitions where it beat 615 of 906 human teams. Deep Feature Synthesis is available as an open source library called Featuretools. That work was followed by other researchers including IBM's OneBM and Berkeley's ExploreKit. The researchers at IBM stated that feature engineering automation ""helps data scientists reduce data exploration time allowing them to try and error many ideas in short time. On the other hand, it enables non-experts, who are not familiar with data science, to quickly extract value from their data with a little effort, time, and cost.""

See also
Covariate
Data transformation
Feature extraction
Feature learning
Hashing trick
Kernel method
List of datasets for machine learning research
Space mapping

References
Further reading
Boehmke, Bradley; Greenwell, Brandon (2019). ""Feature & Target Engineering"". Hands-On Machine Learning with R. Chapman & Hall. pp. 41–75. ISBN 978-1-138-49568-5.
Zheng, Alice; Casari, Amanda (2018). Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists. O'Reilly. ISBN 978-1-4919-5324-2.
Zumel, Nina; Mount, John (2020). ""Data Engineering and Data Shaping"". Practical Data Science with R (2nd ed.). Manning. pp. 113–160. ISBN 978-1-61729-587-4.",https://en.wikipedia.org/wiki/Feature_engineering,"['All articles lacking reliable references', 'All articles with unsourced statements', 'Articles lacking reliable references from January 2020', 'Articles with unsourced statements from January 2020', 'CS1 errors: missing periodical', 'Data analysis', 'Machine learning']",Data Science
93,Feature learning,"In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features  and use them to perform  a specific task.
Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.
Feature learning can be either supervised or unsupervised.

In supervised feature learning, features are learned using labeled input data. Examples include supervised neural networks, multilayer perceptron and (supervised) dictionary learning.
In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.

Supervised
Supervised feature learning is learning features from labeled data. The data label allows the system to compute an error term, the degree to which the system fails to produce the label, which can then be used as feedback to correct the learning process (reduce/minimize the error). Approaches include:

Supervised dictionary learning
Dictionary learning develops a set (dictionary) of representative elements from the input data such that each data point can be represented as a weighted sum of the representative elements. The dictionary elements and the weights may be found by minimizing the average representation error  (over the input data), together with L1 regularization on the weights to enable sparsity (i.e., the representation of each data point has only a few nonzero weights).
Supervised dictionary learning exploits both the structure underlying the input data and the labels for optimizing the dictionary elements. For example, this supervised dictionary learning technique applies dictionary learning on classification problems by jointly optimizing the dictionary elements, weights for representing data points, and parameters of the classifier based on the input data. In particular, a minimization problem is formulated, where the objective function consists of the classification error, the representation error, an L1 regularization on the representing weights for each data point (to enable sparse representation of data), and an L2 regularization on the parameters of the classifier.

Neural networks
Neural networks are a family of learning algorithms that use a ""network"" consisting of multiple layers of inter-connected nodes. It is inspired by the animal nervous system, where the nodes are viewed as neurons and edges are viewed as synapses. Each edge has an associated weight, and the network defines computational rules for passing input data from the network's input layer to the output layer. A network function associated with a neural network characterizes the relationship between input and output layers, which is parameterized by the weights. With appropriately defined network functions, various learning tasks can be performed by minimizing a cost function over the network function (weights).
Multilayer neural networks can be used to perform feature learning, since they learn a representation of their input at the hidden layer(s) which is subsequently used for classification or regression at the output layer. The most popular network architecture of this type is Siamese networks.

Unsupervised
Unsupervised feature learning is learning features from unlabeled data. The goal of unsupervised feature learning is often to discover low-dimensional features that capture some structure underlying the high-dimensional input data. When the feature learning is performed in an unsupervised way, it enables a form of semisupervised learning where features learned from an unlabeled dataset are then employed to improve performance in a supervised setting with labeled data. Several approaches are introduced in the following.

K-means clustering
K-means clustering is an approach for vector quantization. In particular, given a set of n vectors, k-means clustering groups them into k clusters (i.e., subsets) in such a way that each vector belongs to the cluster with the closest mean. The problem is computationally NP-hard, although suboptimal greedy algorithms have been developed.
K-means clustering can be used to group an unlabeled set of inputs into k clusters, and then use the centroids of these clusters to produce features. These features can be produced in several ways. The simplest is to add k binary features to each sample, where each feature j has value one iff the jth centroid learned by k-means is the closest to the sample under consideration. It is also possible to use the distances to the clusters as features, perhaps after transforming them through a radial basis function (a technique that has been used to train RBF networks). Coates and Ng note that certain variants of k-means behave similarly to sparse coding algorithms.In a comparative evaluation of unsupervised feature learning methods, Coates, Lee and Ng found that k-means clustering with an appropriate transformation outperforms the more recently invented auto-encoders and RBMs on an image classification task. K-means also improves performance in the domain of NLP, specifically for named-entity recognition; there, it competes with Brown clustering, as well as with distributed word representations (also known as neural word embeddings).

Principal component analysis
Principal component analysis (PCA) is often used for dimension reduction. Given an unlabeled set of n input data vectors, PCA generates p (which is much smaller than the dimension of the input data) right singular vectors corresponding to the p largest singular values of the data matrix, where the kth row of the data matrix is the kth input data vector shifted by the sample mean of the input (i.e., subtracting the sample mean from the data vector). Equivalently, these singular vectors are the eigenvectors corresponding to the p largest eigenvalues of the sample covariance matrix of the input vectors. These p singular vectors are the feature vectors learned from the input data, and they represent directions along which the data has the largest variations.
PCA is a linear feature learning approach since the p singular vectors are linear functions of the data matrix. The singular vectors can be generated via a simple algorithm with p iterations. In the ith iteration, the projection of the data matrix on the (i-1)th eigenvector is subtracted, and the ith singular vector is found as the right singular vector corresponding to the largest singular of the residual data matrix.
PCA has several limitations. First, it assumes that the directions with large variance are of most interest, which may not be the case. PCA only relies on orthogonal transformations of the original data, and it exploits only the first- and second-order moments of the data, which may not well characterize the data distribution. Furthermore, PCA can effectively reduce dimension only when the input data vectors are correlated (which results in a few dominant eigenvalues).

Local linear embedding
Local linear embedding (LLE) is a nonlinear learning approach for generating low-dimensional neighbor-preserving representations from (unlabeled) high-dimension input. The approach was proposed by Roweis and Saul (2000). The general idea of LLE is to reconstruct the original high-dimensional data using lower-dimensional points while maintaining some geometric properties of the neighborhoods in the original data set.
LLE consists of two major steps. The first step is for ""neighbor-preserving"", where each input data point Xi is reconstructed as a weighted sum of K nearest neighbor data points, and the optimal weights are found by minimizing the average squared reconstruction error (i.e., difference between an input point and its reconstruction) under the constraint that the weights associated with each point sum up to one. The second step is for ""dimension reduction,"" by looking for vectors in a lower-dimensional space that minimizes the representation error using the optimized weights in the first step. Note that in the first step, the weights are optimized with fixed data, which can be solved as a least squares problem. In the second step, lower-dimensional points are optimized with fixed weights, which can be solved via sparse eigenvalue decomposition.
The reconstruction weights obtained in the first step capture the ""intrinsic geometric properties"" of a neighborhood in the input data. It is assumed that original data lie on a smooth lower-dimensional manifold, and the ""intrinsic geometric properties"" captured by the weights of the original data are also expected to be on the manifold. This is why the same weights are used in the second step of LLE. Compared with PCA, LLE is more powerful in exploiting the underlying data structure.

Independent component analysis
Independent component analysis (ICA) is a technique for forming a data representation using a weighted sum of independent non-Gaussian components. The assumption of non-Gaussian is imposed since the weights cannot be uniquely determined when all the components follow Gaussian distribution.

Unsupervised dictionary learning
Unsupervised dictionary learning does not utilize data labels and exploits the structure underlying the data for optimizing dictionary elements. An example of unsupervised dictionary learning is sparse coding, which aims to learn basis functions (dictionary elements) for data representation from unlabeled input data. Sparse coding can be applied to learn overcomplete dictionaries, where the number of dictionary elements is larger than the dimension of the input data. Aharon et al. proposed algorithm K-SVD for learning a dictionary of elements that enables sparse representation.

Multilayer/deep architectures
The hierarchical architecture of the biological neural system inspires deep learning architectures for feature learning by stacking multiple layers of learning nodes. These architectures are often designed based on the assumption of distributed representation: observed data is generated by the interactions of many different factors on multiple levels. In a deep learning architecture, the output of each intermediate layer can be viewed as a representation of the original input data. Each level uses the representation produced by previous level as input, and produces new representations as output, which is then fed to higher levels. The input at the bottom layer is raw data, and the output of the final layer is the final low-dimensional feature or representation.

Restricted Boltzmann machine
Restricted Boltzmann machines (RBMs) are often used as a building block for multilayer learning architectures. An RBM can be represented by an undirected bipartite graph consisting of a group of binary hidden variables, a group of visible variables, and edges connecting the hidden and visible nodes. It is a special case of the more general Boltzmann machines with the constraint of no intra-node connections. Each edge in an RBM is associated with a weight. The weights together with the connections define an energy function, based on which a joint distribution of visible and hidden nodes can be devised. Based on the topology of the RBM, the hidden (visible) variables are independent, conditioned on the visible (hidden) variables. Such conditional independence facilitates computations.
An RBM can be viewed as a single layer architecture for unsupervised feature learning. In particular, the visible variables correspond to input data, and the hidden variables correspond to feature detectors. The weights can be trained by maximizing the probability of visible variables using Hinton's contrastive divergence (CD) algorithm.In general training RBM by solving the maximization problem tends to result in non-sparse representations. Sparse RBM was proposed to enable sparse representations. The idea is to add a regularization term in the objective function of data likelihood, which penalizes the deviation of the expected hidden variables from a small constant 
  
    
      
        p
      
    
    {\displaystyle p}
  .

Autoencoder
An autoencoder consisting of an encoder and a decoder is a paradigm for deep learning architectures. An example is provided by Hinton and Salakhutdinov where the encoder uses raw data (e.g., image) as input and produces feature or representation as output and the decoder uses the extracted feature from the encoder as input and reconstructs the original input raw data as output. The encoder and decoder are constructed by stacking multiple layers of RBMs. The parameters involved in the architecture were originally trained in a greedy layer-by-layer manner: after one layer of feature detectors is learned, they are fed up as visible variables for training the corresponding RBM. Current approaches typically apply end-to-end training with stochastic gradient descent methods. Training can be repeated until some stopping criteria are satisfied.

See also
Automated machine learning (AutoML)
Basis function
Deep learning
Feature detection (computer vision)
Feature extraction
Kernel trick
Vector quantization


== References ==",https://en.wikipedia.org/wiki/Feature_learning,"['CS1 errors: missing periodical', 'Machine learning', 'Wikipedia articles needing clarification from June 2017']",Data Science
94,Gated recurrent unit,"Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate. 
GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs have been shown to exhibit better performance on certain smaller and less frequent datasets.

Architecture
There are several variations on the full gated unit, with gating done using the previous hidden state and the bias in various combinations, and a simplified form called minimal gated unit.The operator 
  
    
      
        ⊙
      
    
    {\displaystyle \odot }
   denotes the Hadamard product in the following.

Fully gated unit
Initially, for 
  
    
      
        t
        =
        0
      
    
    {\displaystyle t=0}
  , the output vector is 
  
    
      
        
          h
          
            0
          
        
        =
        0
      
    
    {\displaystyle h_{0}=0}
  . 

  
    
      
        
          
            
              
                
                  z
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  W
                  
                    z
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    z
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    z
                  
                
                )
              
            
            
              
                
                  r
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  W
                  
                    r
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    r
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    r
                  
                
                )
              
            
            
              
                
                  
                    
                      
                        h
                        ^
                      
                    
                  
                  
                    t
                  
                
              
              
                
                =
                
                  ϕ
                  
                    h
                  
                
                (
                
                  W
                  
                    h
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    h
                  
                
                (
                
                  r
                  
                    t
                  
                
                ⊙
                
                  h
                  
                    t
                    −
                    1
                  
                
                )
                +
                
                  b
                  
                    h
                  
                
                )
              
            
            
              
                
                  h
                  
                    t
                  
                
              
              
                
                =
                (
                1
                −
                
                  z
                  
                    t
                  
                
                )
                ⊙
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  z
                  
                    t
                  
                
                ⊙
                
                  
                    
                      
                        h
                        ^
                      
                    
                  
                  
                    t
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}z_{t}&=\sigma _{g}(W_{z}x_{t}+U_{z}h_{t-1}+b_{z})\\r_{t}&=\sigma _{g}(W_{r}x_{t}+U_{r}h_{t-1}+b_{r})\\{\hat {h}}_{t}&=\phi _{h}(W_{h}x_{t}+U_{h}(r_{t}\odot h_{t-1})+b_{h})\\h_{t}&=(1-z_{t})\odot h_{t-1}+z_{t}\odot {\hat {h}}_{t}\end{aligned}}}
  Variables

  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  : input vector

  
    
      
        
          h
          
            t
          
        
      
    
    {\displaystyle h_{t}}
  : output vector

  
    
      
        
          
            
              
                h
                ^
              
            
          
          
            t
          
        
      
    
    {\displaystyle {\hat {h}}_{t}}
  : candidate activation vector

  
    
      
        
          z
          
            t
          
        
      
    
    {\displaystyle z_{t}}
  : update gate vector

  
    
      
        
          r
          
            t
          
        
      
    
    {\displaystyle r_{t}}
  : reset gate vector

  
    
      
        W
      
    
    {\displaystyle W}
  , 
  
    
      
        U
      
    
    {\displaystyle U}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
  : parameter matrices and vectorActivation functions

  
    
      
        
          σ
          
            g
          
        
      
    
    {\displaystyle \sigma _{g}}
  : The original is a sigmoid function.

  
    
      
        
          ϕ
          
            h
          
        
      
    
    {\displaystyle \phi _{h}}
  : The original is a hyperbolic tangent.Alternative activation functions are possible, provided that 
  
    
      
        
          σ
          
            g
          
        
        (
        x
        )
        ∈
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle \sigma _{g}(x)\in [0,1]}
  .

Alternate forms can be created by changing 
  
    
      
        
          z
          
            t
          
        
      
    
    {\displaystyle z_{t}}
   and 
  
    
      
        
          r
          
            t
          
        
      
    
    {\displaystyle r_{t}}
  
Type 1, each gate depends only on the previous hidden state and the bias.

  
    
      
        
          
            
              
                
                  z
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  U
                  
                    z
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    z
                  
                
                )
              
            
            
              
                
                  r
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  U
                  
                    r
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    r
                  
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}z_{t}&=\sigma _{g}(U_{z}h_{t-1}+b_{z})\\r_{t}&=\sigma _{g}(U_{r}h_{t-1}+b_{r})\\\end{aligned}}}
  
Type 2, each gate depends only on  the previous hidden state.

  
    
      
        
          
            
              
                
                  z
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  U
                  
                    z
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                )
              
            
            
              
                
                  r
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  U
                  
                    r
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}z_{t}&=\sigma _{g}(U_{z}h_{t-1})\\r_{t}&=\sigma _{g}(U_{r}h_{t-1})\\\end{aligned}}}
  
Type 3, each gate is computed using only the bias.

  
    
      
        
          
            
              
                
                  z
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  b
                  
                    z
                  
                
                )
              
            
            
              
                
                  r
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  b
                  
                    r
                  
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}z_{t}&=\sigma _{g}(b_{z})\\r_{t}&=\sigma _{g}(b_{r})\\\end{aligned}}}

Minimal gated unit
The minimal gated unit is similar to the fully gated unit, except the update and reset gate vector is merged into a forget gate. This also implies that the equation for the output vector must be changed:

  
    
      
        
          
            
              
                
                  f
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  W
                  
                    f
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    f
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    f
                  
                
                )
              
            
            
              
                
                  
                    
                      
                        h
                        ^
                      
                    
                  
                  
                    t
                  
                
              
              
                
                =
                
                  ϕ
                  
                    h
                  
                
                (
                
                  W
                  
                    h
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    h
                  
                
                (
                
                  f
                  
                    t
                  
                
                ⊙
                
                  h
                  
                    t
                    −
                    1
                  
                
                )
                +
                
                  b
                  
                    h
                  
                
                )
              
            
            
              
                
                  h
                  
                    t
                  
                
              
              
                
                =
                (
                1
                −
                
                  f
                  
                    t
                  
                
                )
                ⊙
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  f
                  
                    t
                  
                
                ⊙
                
                  
                    
                      
                        h
                        ^
                      
                    
                  
                  
                    t
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})\\{\hat {h}}_{t}&=\phi _{h}(W_{h}x_{t}+U_{h}(f_{t}\odot h_{t-1})+b_{h})\\h_{t}&=(1-f_{t})\odot h_{t-1}+f_{t}\odot {\hat {h}}_{t}\end{aligned}}}
  Variables

  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  : input vector

  
    
      
        
          h
          
            t
          
        
      
    
    {\displaystyle h_{t}}
  : output vector

  
    
      
        
          
            
              
                h
                ^
              
            
          
          
            t
          
        
      
    
    {\displaystyle {\hat {h}}_{t}}
  : candidate activation vector

  
    
      
        
          f
          
            t
          
        
      
    
    {\displaystyle f_{t}}
  : forget vector

  
    
      
        W
      
    
    {\displaystyle W}
  , 
  
    
      
        U
      
    
    {\displaystyle U}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
  : parameter matrices and vector


== References ==",https://en.wikipedia.org/wiki/Gated_recurrent_unit,"['Articles with long short description', 'Articles with short description', 'Artificial neural networks', 'CS1 errors: missing periodical', 'Short description with empty Wikidata description']",Data Science
95,Generative adversarial network,"A generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the form of a zero-sum game, where one agent's gain is another agent's loss).
Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proven useful for semi-supervised learning, fully supervised learning, and reinforcement learning.The core idea of a GAN is based on the ""indirect"" training through the discriminator, which itself is also being updated dynamically. This basically means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.

Method
The generative network generates candidates while the discriminative network evaluates them. The contest operates in terms of data distributions. Typically, the generative network learns to map from a latent space to a data distribution of interest, while the discriminative network distinguishes candidates produced by the generator from the true data distribution. The generative network's training objective is to increase the error rate of the discriminative network (i.e., ""fool"" the discriminator network by producing novel candidates that the discriminator thinks are not synthesized (are part of the true data distribution)).A known dataset serves as the initial training data for the discriminator. Training it involves presenting it with samples from the training dataset, until it achieves acceptable accuracy. The generator trains based on whether it succeeds in fooling the discriminator. Typically the generator is seeded with randomized input that is sampled from a predefined latent space (e.g. a multivariate normal distribution). Thereafter, candidates synthesized by the generator are evaluated by the discriminator. Independent backpropagation procedures are applied to both networks so that the generator produces better samples, while the discriminator becomes more skilled at flagging synthetic samples. When used for image generation, the generator is typically a deconvolutional neural network, and the discriminator is a convolutional neural network.
GANs often suffer from a ""mode collapse"" where they fail to generalize properly, missing entire modes from the input data. For example, a GAN trained on the MNIST dataset containing many samples of each digit, might nevertheless timidly omit a subset of the digits from its output. Some researchers perceive the root problem to be a weak discriminative network that fails to notice the pattern of omission, while others assign blame to a bad choice of objective function. Many solutions have been proposed.

Applications
GAN applications have increased rapidly.

Fashion, art and advertising
GANs can be used to generate art; The Verge wrote in March 2019 that ""The images created by GANs have become the defining look of contemporary AI art."" GANs can also be used to inpaint photographs or create photos of imaginary fashion models, with no need to hire a model, photographer or makeup artist, or pay for a studio and transportation.

Science
GANs can improve astronomical images and simulate gravitational lensing for dark matter research. They were used in 2019 to successfully model the distribution of dark matter in a particular direction in space and to predict the gravitational lensing that will occur.GANs have been proposed as a fast and accurate way of modeling high energy jet formation and modeling showers through calorimeters of high-energy physics experiments. GANs have also been trained to accurately approximate bottlenecks in computationally expensive simulations of particle physics experiments. Applications in the context of present and proposed CERN experiments have demonstrated the potential of these methods for accelerating simulation and/or improving simulation fidelity.

Video games
In 2018, GANs reached the video game modding community, as a method of up-scaling low-resolution 2D textures in old video games by recreating them in 4k or higher resolutions via image training, and then down-sampling them to fit the game's native resolution (with results resembling the supersampling method of anti-aliasing). With proper training, GANs provide a clearer and sharper 2D texture image magnitudes higher in quality than the original, while fully retaining the original's level of details, colors, etc. Known examples of extensive GAN usage include Final Fantasy VIII, Final Fantasy IX, Resident Evil REmake HD Remaster, and Max Payne.

Concerns about malicious applications
Concerns have been raised about the potential use of GAN-based human image synthesis for sinister purposes, e.g., to produce fake, possibly incriminating, photographs and videos.
GANs can be used to generate unique, realistic profile photos of people who do not exist, in order to automate creation of fake social media profiles.In 2019 the state of California considered and passed on October 3, 2019 the bill AB-602, which bans the use of human image synthesis technologies to make fake pornography without the consent of the people depicted, and bill AB-730, which prohibits distribution of manipulated videos of a political candidate within 60 days of an election. Both bills were authored by Assembly member Marc Berman and signed by Governor Gavin Newsom. The laws will come into effect in 2020.DARPA's Media Forensics program studies ways to counteract fake media, including fake media produced using GANs.

Miscellaneous applications
GAN can be used to detect glaucomatous images helping the early diagnosis which is essential to avoid partial or total loss
of vision.GANs that produce photorealistic images can be used to visualize interior design, industrial design, shoes, bags, and clothing items or items for computer games' scenes. Such networks were reported to be used by Facebook.GANs can reconstruct 3D models of objects from images, and model patterns of motion in video.GANs can be used to age face photographs to show how an individual's appearance might change with age.GANs can also be used to transfer map styles in cartography or augment street view imagery.Relevance feedback on GANs can be used to generate images and replace image search systems.A variation of the GANs is used in training a network to generate optimal control inputs to nonlinear dynamical systems. Where the discriminatory network is known as a critic that checks the optimality of the solution and the generative network is known as an Adaptive network that generates the optimal control. The critic and adaptive network train each other to approximate a nonlinear optimal control.GANs have been used to visualize the effect that climate change will have on specific houses.A GAN model called Speech2Face can reconstruct an image of a person's face after listening to their voice.In 2016 GANs were used to generate new molecules for a variety of protein targets implicated in cancer, inflammation, and fibrosis. In 2019 GAN-generated molecules were validated experimentally all the way into mice.Whereas the majority of GAN applications are in image processing, the work has also been done with time-series data. For example, recurrent GANs (R-GANs) have been used to generate energy data for machine learning.

History
The most direct inspiration for GANs was noise-contrastive estimation, which uses the same loss function as GANs and which Goodfellow studied during his PhD in 2010–2014.
Other people had similar ideas but did not develop them similarly. An idea involving adversarial networks was published in a 2010 blog post by Olli Niemitalo. This idea was never implemented and did not involve stochasticity in the generator and thus was not a generative model. It is now known as a conditional GAN or cGAN. An idea similar to GANs was used to model animal behavior by Li, Gauci and Gross in 2013.Adversarial machine learning has other uses besides generative modeling and can be applied to models other than neural networks. In control theory, adversarial learning based on neural networks was used in 2006 to train robust controllers in a game theoretic sense, by alternating the iterations between a minimizer policy, the controller, and a maximizer policy, the disturbance.In 2017, a GAN was used for image enhancement focusing on realistic textures rather than pixel-accuracy, producing a higher image quality at high magnification. In 2017, the first faces were generated. These were exhibited in February 2018 at the Grand Palais. Faces generated by StyleGAN in 2019 drew comparisons with deepfakes.Beginning in 2017, GAN technology began to make its presence felt in the fine arts arena with the appearance of a newly developed implementation which was said to have crossed the threshold of being able to generate unique and appealing abstract paintings, and thus dubbed a ""CAN"", for ""creative adversarial network"". A GAN system was used to create the 2018 painting Edmond de Belamy, which sold for US$432,500. An early 2019 article by members of the original CAN team discussed further progress with that system, and gave consideration as well to the overall prospects for an AI-enabled art.In May 2019, researchers at Samsung demonstrated a GAN-based system that produces videos of a person speaking, given only a single photo of that person.In August 2019, a large dataset consisting of 12,197 MIDI songs each with paired lyrics and melody alignment was created for neural melody generation from lyrics using conditional GAN-LSTM (refer to sources at GitHub AI Melody Generation from Lyrics).In May 2020, Nvidia researchers taught an AI system (termed ""GameGAN"") to recreate the game of Pac-Man simply by watching it being played.

Classification
Bidirectional GAN
While the standard GAN model learns a mapping from a latent space to the data distribution, inverse models such as Bidirectional GAN (BiGAN)

and Adversarial Autoencoders

also learn a mapping from data to the latent space.
This inverse mapping allows real or generated data examples to be projected back into the latent space, similar to the encoder of a variational autoencoder.
Applications of bidirectional models include semi-supervised learning, interpretable machine learning, and neural machine translation.

References
External links

Knight, Will. ""5 Big Predictions for Artificial Intelligence in 2017"". MIT Technology Review. Retrieved 2017-01-05.
A Style-Based Generator Architecture for Generative Adversarial Networks
This Person Does Not Exist –  photorealistic images of people who do not exist, generated by StyleGAN
This Cat Does Not Exist –  photorealistic images of cats who do not exist, generated by StyleGAN
""Generative Adversarial Networks: A Survey and Taxonomy"", recent review by Zhengwei Wang, Qi She, Tomas E. Ward",https://en.wikipedia.org/wiki/Generative_adversarial_network,"['All articles with unsourced statements', 'Articles with short description', 'Articles with unsourced statements from February 2018', 'Articles with unsourced statements from January 2020', 'Artificial neural networks', 'CS1 errors: missing periodical', 'Cognitive science', 'Short description matches Wikidata', 'Unsupervised learning', 'Wikipedia articles needing clarification from March 2021']",Data Science
96,Glossary of artificial intelligence,"This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence, its sub-disciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, and Glossary of machine vision.

A
abductive logic programming (ALP)
A high-level knowledge-representation framework that can be used to solve problems declaratively based on abductive reasoning. It extends normal logic programming by allowing some predicates to be incompletely defined, declared as abducible predicates.

abductive reasoning
Also abduction.
A form of logical inference which starts with an observation or set of observations then seeks to find the simplest and most likely explanation. This process, unlike deductive reasoning, yields a plausible conclusion but does not positively verify it. abductive inference, or retroduction

abstract data type
A mathematical model for data types, where a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations.

abstraction
The process of removing physical, spatial, or temporal details or attributes in the study of objects or systems in order to more closely attend to other details of interest

accelerating change
A perceived increase in the rate of technological change throughout history, which may suggest faster and more profound change in the future and may or may not be accompanied by equally profound social and cultural change.

action language
A language for specifying state transition systems, and is commonly used to create formal models of the effects of actions on the world. Action languages are commonly used in the artificial intelligence and robotics domains, where they describe how actions affect the states of systems over time, and may be used for automated planning.

action model learning
An area of machine learning concerned with creation and modification of software agent's knowledge about effects and preconditions of the actions that can be executed within its environment. This knowledge is usually represented in logic-based action description language and used as the input for automated planners.

action selection
A way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, ""the action selection problem"" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment.

activation function
In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs.

adaptive algorithm
An algorithm that changes its behavior at the time it is run, based on a priori defined reward mechanism or criterion.

adaptive neuro fuzzy inference system (ANFIS)
Also adaptive network-based fuzzy inference system.
A kind of artificial neural network that is based on Takagi–Sugeno fuzzy inference system. The technique was developed in the early 1990s. Since it integrates both neural networks and fuzzy logic principles, it has potential to capture the benefits of both in a single framework. Its inference system corresponds to a set of fuzzy IF–THEN rules that have learning capability to approximate nonlinear functions. Hence, ANFIS is considered to be a universal estimator. For using the ANFIS in a more efficient and optimal way, one can use the best parameters obtained by genetic algorithm.

admissible heuristic
In computer science, specifically in algorithms related to pathfinding, a heuristic function is said to be admissible if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path.

affective computing
Also artificial emotional intelligence or emotion AI.
The study and development of systems and devices that can recognize, interpret, process, and simulate human affects. Affective computing is an interdisciplinary field spanning computer science, psychology, and cognitive science.

agent architecture
A blueprint for software agents and intelligent control systems, depicting the arrangement of components. The architectures implemented by intelligent agents are referred to as cognitive architectures.

AI accelerator
A class of microprocessor or computer system designed as hardware acceleration for artificial intelligence applications, especially artificial neural networks, machine vision, and machine learning.

AI-complete
In the field of artificial intelligence, the most difficult problems are informally known as AI-complete or AI-hard, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence problem—making computers as intelligent as people, or strong AI. To call a problem AI-complete reflects an attitude that it would not be solved by a simple specific algorithm.

algorithm
An unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing, and automated reasoning tasks.

algorithmic efficiency
A property of an algorithm which relates to the number of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.

algorithmic probability
In algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation. It was invented by Ray Solomonoff in the 1960s.

AlphaGo
A computer program that plays the board game Go. It was developed by Alphabet Inc.'s Google DeepMind in London. AlphaGo has several versions including AlphaGo Zero, AlphaGo Master, AlphaGo Lee, etc. In October 2015, AlphaGo became the first computer Go program to beat a human professional Go player without handicaps on a full-sized 19×19 board.

ambient intelligence (AmI)
Electronic environments that are sensitive and responsive to the presence of people.

analysis of algorithms
The determination of the computational complexity of algorithms, that is the amount of time, storage and/or other resources necessary to execute them. Usually, this involves determining a function that relates the length of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity).

analytics
The discovery, interpretation, and communication of meaningful patterns in data.

answer set programming (ASP)
A form of declarative programming oriented towards difficult (primarily NP-hard) search problems. It is based on the stable model (answer set) semantics of logic programming.  In ASP, search problems are reduced to computing stable models, and answer set solvers—programs for generating stable models—are used to perform search.

anytime algorithm
An algorithm that can return a valid solution to a problem even if it is interrupted before it ends.

application programming interface (API)
A set of subroutine definitions, communication protocols, and tools for building software. In general terms, it is a set of clearly defined methods of communication among various components. A good API makes it easier to develop a computer program by providing all the building blocks, which are then put together by the programmer. An API may be for a web-based system, operating system, database system, computer hardware, or software library.

approximate string matching
Also fuzzy string searching.
The technique of finding strings that match a pattern approximately (rather than exactly). The problem of approximate string matching is typically divided into two sub-problems: finding approximate substring matches inside a given string and finding dictionary strings that match the pattern approximately.

approximation error
The discrepancy between an exact value and some approximation to it.

argumentation framework
Also argumentation system.
A way to deal with contentious information and draw conclusions from it. In an abstract argumentation framework, entry-level information is a set of abstract arguments that, for instance, represent data or a proposition. Conflicts between arguments are represented by a binary relation on the set of arguments. In concrete terms, you represent an argumentation framework with a directed graph such that the nodes are the arguments, and the arrows represent the attack relation.  There exist some extensions of the Dung's framework, like the logic-based argumentation frameworks or the value-based argumentation frameworks.

artificial general intelligence (AGI)

artificial immune system (AIS)
A class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. The algorithms are typically modeled after the immune system's characteristics of learning and memory for use in problem-solving.

artificial intelligence (AI)
Also machine intelligence.
Any intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals. In computer science, AI research is defined as the study of ""intelligent agents"": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term ""artificial intelligence"" is applied when a machine mimics ""cognitive"" functions that humans associate with other human minds, such as ""learning"" and ""problem solving"".

Artificial Intelligence Markup Language
An XML dialect for creating natural language software agents.

artificial neural network (ANN)
Also connectionist system.
Any computing system vaguely inspired by the biological neural networks that constitute animal brains.

Association for the Advancement of Artificial Intelligence (AAAI)
An international, nonprofit, scientific society devoted to promote research in, and responsible use of, artificial intelligence. AAAI also aims to increase public understanding of artificial intelligence (AI), improve the teaching and training of AI practitioners, and provide guidance for research planners and funders concerning the importance and potential of current AI developments and future directions.

asymptotic computational complexity
In computational complexity theory, asymptotic computational complexity is the usage of asymptotic analysis for the estimation of computational complexity of algorithms and computational problems, commonly associated with the usage of the big O notation.

attributional calculus
A logic and representation system defined by Ryszard S. Michalski. It combines elements of predicate logic, propositional calculus, and multi-valued logic. Attributional calculus provides a formal language for natural induction, an inductive learning process whose results are in forms natural to people.

augmented reality (AR)
An interactive experience of a real-world environment where the objects that reside in the real-world are ""augmented"" by computer-generated perceptual information, sometimes across multiple sensory modalities, including visual, auditory, haptic, somatosensory, and olfactory.

automata theory
The study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science and discrete mathematics (a subject of study in both mathematics and computer science).

automated planning and scheduling
Also simply AI planning.
A branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.

automated reasoning
An area of computer science and mathematical logic dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science, and even philosophy.

autonomic computing (AC)
The self-managing characteristics of distributed computing resources, adapting to unpredictable changes while hiding intrinsic complexity to operators and users. Initiated by IBM in 2001, this initiative ultimately aimed to develop computer systems capable of self-management, to overcome the rapidly growing complexity of computing systems management, and to reduce the barrier that complexity poses to further growth.

autonomous car
Also self-driving car, robot car, and driverless car.
A vehicle that is capable of sensing its environment and moving with little or no human input.

autonomous robot
A robot that performs behaviors or tasks with a high degree of autonomy. Autonomous robotics is usually considered to be a subfield of artificial intelligence, robotics, and information engineering.

B
backpropagation
A method used in artificial neural networks to calculate a gradient that is needed in the calculation of the weights to be used in the network. Backpropagation is shorthand for ""the backward propagation of errors"", since an error is computed at the output and distributed backwards throughout the network's layers. It is commonly used to train deep neural networks, a term referring to neural networks with more than one hidden layer.

backpropagation through time (BPTT)
A gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. The algorithm was independently derived by numerous researchers

backward chaining
Also backward reasoning.
An inference method described colloquially as working backward from the goal. It is used in automated theorem provers, inference engines, proof assistants, and other artificial intelligence applications.

bag-of-words model
A simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision. The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.

bag-of-words model in computer vision
In computer vision, the bag-of-words model (BoW model) can be applied to image classification, by treating image features as words.  In document classification, a bag of words is a sparse vector of occurrence counts of words; that is, a sparse histogram over the vocabulary. In computer vision, a bag of visual words is a vector of occurrence counts of a vocabulary of local image features.

batch normalization
A technique for improving the performance and stability of artificial neural networks. It is a technique to provide any layer in a neural network with inputs that are zero mean/unit variance. Batch normalization was introduced in a 2015 paper. It is used to normalize the input layer by adjusting and scaling the activations.

Bayesian programming
A formalism and a methodology for having a technique to specify probabilistic models and solve problems when less than the necessary information is available.

bees algorithm
A population-based search algorithm which was developed by Pham, Ghanbarzadeh and et al. in 2005. It mimics the food foraging behaviour of honey bee colonies. In its basic version the algorithm performs a kind of neighbourhood search combined with global search, and can be used for both combinatorial optimization and continuous optimization. The only condition for the application of the bees algorithm is that some measure of distance between the solutions is defined. The effectiveness and specific abilities of the bees algorithm have been proven in a number of studies.

behavior informatics (BI)
The informatics of behaviors so as to obtain behavior intelligence and behavior insights.

behavior tree (BT)
A mathematical model of plan execution used in computer science, robotics, control systems and video games. They describe switchings between a finite set of tasks in a modular fashion. Their strength comes from their ability to create very complex tasks composed of simple tasks, without worrying how the simple tasks are implemented. BTs present some similarities to hierarchical state machines with the key difference that the main building block of a behavior is a task rather than a state. Its ease of human understanding make BTs less error-prone and very popular in the game developer community. BTs have shown to generalize several other control architectures.

belief-desire-intention software model (BDI)
A software model developed for programming intelligent agents. Superficially characterized by the implementation of an agent's beliefs, desires and intentions, it actually uses these concepts to solve a particular problem in agent programming. In essence, it provides a mechanism for separating the activity of selecting a plan (from a plan library or an external planner application) from the execution of currently active plans. Consequently, BDI agents are able to balance the time spent on deliberating about plans (choosing what to do) and executing those plans (doing it). A third activity, creating the plans in the first place (planning), is not within the scope of the model, and is left to the system designer and programmer.

bias–variance tradeoff
In statistics and machine learning, the bias–variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa.

big data
A term used to refer to data sets that are too large or complex for traditional data-processing application software to adequately deal with. Data with many cases (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.

Big O notation
A mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity.  It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann–Landau notation or asymptotic notation.

binary tree
A tree data structure in which each node has at most two children, which are referred to as the left child and the right child. A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set. Some authors allow the binary tree to be the empty set as well.

blackboard system
An artificial intelligence approach based on the blackboard architectural model, where a common knowledge base, the ""blackboard"", is iteratively updated by a diverse group of specialist knowledge sources, starting with a problem specification and ending with a solution.  Each knowledge source updates the blackboard with a partial solution when its internal constraints match the blackboard state.  In this way, the specialists work together to solve the problem.

Boltzmann machine
Also stochastic Hopfield network with hidden units.
A type of stochastic recurrent neural network and Markov random field.  Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield networks.

Boolean satisfiability problem
Also propositional satisfiability problem; abbreviated SATISFIABILITY or SAT.
{{{content}}}

brain technology
Also self-learning know-how system.
A technology that employs the latest findings in neuroscience. The term was first introduced by the Artificial Intelligence Laboratory in Zurich, Switzerland, in the context of the ROBOY project. Brain Technology can be employed in robots, know-how management systems and any other application with self-learning capabilities. In particular, Brain Technology applications allow the visualization of the underlying learning architecture often coined as ""know-how maps"".

branching factor
In computing, tree data structures, and game theory, the number of children at each node, the outdegree. If this value is not uniform, an average branching factor can be calculated.

brute-force search
Also exhaustive search or generate and test.
A very general problem-solving technique and algorithmic paradigm that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem's statement.

C
capsule neural network (CapsNet)
A machine learning system that is a type of artificial neural network (ANN) that can be used to better model hierarchical relationships. The approach is an attempt to more closely mimic biological neural organization.

case-based reasoning (CBR)
Broadly construed, the process of solving new problems based on the solutions of similar past problems.

chatbot
Also smartbot, talkbot, chatterbot, bot, IM bot, interactive agent, conversational interface, or artificial conversational entity.
A computer program or an artificial intelligence which conducts a conversation via auditory or textual methods.

cloud robotics
A field of robotics that attempts to invoke cloud technologies such as cloud computing, cloud storage, and other Internet technologies centred on the benefits of converged infrastructure and shared services for robotics. When connected to the cloud, robots can benefit from the powerful computation, storage, and communication resources of modern data center in the cloud, which can process and share information from various robots or agent (other machines, smart objects, humans, etc.). Humans can also delegate tasks to robots remotely through networks. Cloud computing technologies enable robot systems to be endowed with powerful capability whilst reducing costs through cloud technologies. Thus, it is possible to build lightweight, low cost, smarter robots have intelligent ""brain"" in the cloud. The ""brain"" consists of data center, knowledge base, task planners, deep learning, information processing, environment models, communication support, etc.

cluster analysis
Also clustering.
The task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics.

Cobweb
An incremental system for hierarchical conceptual clustering. COBWEB was invented by Professor Douglas H. Fisher, currently at Vanderbilt University. COBWEB incrementally organizes observations into a classification tree. Each node in a classification tree represents a class (concept) and is labeled by a probabilistic concept that summarizes the attribute-value distributions of objects classified under the node. This classification tree can be used to predict missing attributes or the class of a new object.

cognitive architecture
The Institute of Creative Technologies defines cognitive architecture as: ""hypothesis about the fixed structures that provide a mind, whether in natural or artificial systems, and how they work together – in conjunction with knowledge and skills embodied within the architecture – to yield intelligent behavior in a diversity of complex environments.""

cognitive computing
In general, the term cognitive computing has been used to refer to new hardware and/or software that mimics the functioning of the human brain and helps to improve human decision-making. In this sense, CC is a new type of computing with the goal of more accurate models of how the human brain/mind senses, reasons, and responds to stimulus.

cognitive science
The interdisciplinary scientific study of the mind and its processes.

combinatorial optimization
In Operations Research, applied mathematics and theoretical computer science, combinatorial optimization  is a topic that consists of finding an optimal object from a finite set of objects.

committee machine
A type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks (experts) are combined into a single response.  The combined response of the committee machine is supposed to be superior to those of its constituent experts. Compare ensembles of classifiers.

commonsense knowledge
In artificial intelligence research, commonsense knowledge consists of facts about the everyday world, such as ""Lemons are sour"", that all humans are expected to know.  The first AI program to address common sense knowledge was Advice Taker in 1959 by John McCarthy.

commonsense reasoning
A branch of artificial intelligence concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day.

computational chemistry
A branch of chemistry that uses computer simulation to assist in solving chemical problems.

computational complexity theory
Focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.

computational creativity
Also artificial creativity, mechanical creativity, creative computing, or creative computation.
A multidisciplinary endeavour that includes the fields of artificial intelligence, cognitive psychology, philosophy, and the arts.

computational cybernetics
The integration of cybernetics and computational intelligence techniques.

computational humor
A branch of computational linguistics and artificial intelligence which uses computers in humor research.

computational intelligence (CI)
Usually refers to the ability of a computer to learn a specific task from data or experimental observation.

computational learning theory
In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.

computational linguistics
An interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective, as well as the study of appropriate computational approaches to linguistic questions.

computational mathematics
The mathematical research in areas of science where computing plays an essential role.

computational neuroscience
Also theoretical neuroscience or mathematical neuroscience. 
A branch of neuroscience which employs mathematical models, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, physiology, and cognitive abilities of the nervous system.

computational number theory
Also algorithmic number theory.
The study of algorithms for performing number theoretic computations.

computational problem
In theoretical computer science, a computational problem is a mathematical object representing a collection of questions that computers might be able to solve.

computational statistics
Also statistical computing.
The interface between statistics and computer science.

computer-automated design (CAutoD)
Design automation usually refers to electronic design automation, or Design Automation which is a Product Configurator. Extending Computer-Aided Design (CAD), automated design and computer-automated design are concerned with a broader range of applications, such as automotive engineering, civil engineering, composite material design, control engineering, dynamic system identification and optimization, financial systems, industrial equipment, mechatronic systems, steel construction, structural optimisation, and the invention of novel systems. More recently, traditional CAD simulation is seen to be transformed to CAutoD by biologically inspired machine learning, including heuristic search techniques such as evolutionary computation, and swarm intelligence algorithms.

computer audition (CA)
See machine listening.

computer science
The theory, experimentation, and engineering that form the basis for the design and use of computers. It involves the study of algorithms that process, store, and communicate digital information. A computer scientist specializes in the theory of computation and the design of computational systems.

computer vision
An interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.

concept drift
In predictive analytics and machine learning, the concept drift means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.

connectionism
An approach in the fields of cognitive science, that hopes to explain mental phenomena using artificial neural networks.

consistent heuristic
In the study of path-finding problems in artificial intelligence, a heuristic function is said to be consistent, or monotone, if its estimate is always less than or equal to the estimated distance from any neighboring vertex to the goal, plus the cost of reaching that neighbor.

constrained conditional model (CCM)
A machine learning and inference framework that augments the learning of conditional (probabilistic or discriminative) models with declarative constraints.

constraint logic programming
A form of constraint programming, in which logic programming is extended to include concepts from constraint satisfaction. A constraint logic program is a logic program that contains constraints in the body of clauses. An example of a clause including a constraint is A(X,Y) :- X+Y>0, B(X), C(Y). In this clause, X+Y>0 is a constraint; A(X,Y), B(X), and C(Y) are literals as in regular logic programming. This clause states one condition under which the statement A(X,Y) holds: X+Y is greater than zero and both B(X) and C(Y) are true.

constraint programming
A programming paradigm wherein relations between variables are stated in the form of constraints. Constraints differ from the common primitives of imperative programming languages in that they do not specify a step or sequence of steps to execute, but rather the properties of a solution to be found.

constructed language
Also conlang. 
A language whose phonology, grammar, and vocabulary are consciously devised, instead of having developed naturally. Constructed languages may also be referred to as artificial, planned, or invented languages.

control theory
In control systems engineering is a subfield of mathematics that deals with the control of continuously operating dynamical systems in engineered processes and machines. The objective is to develop a control model for controlling such systems using a control action in an optimum manner without delay or overshoot and ensuring control stability.

convolutional neural network
In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.

crossover
Also recombination.
In genetic algorithms and evolutionary computation, a genetic operator used to combine the genetic information of two parents to generate new offspring. It is one way to stochastically generate new solutions from an existing population, and analogous to the crossover that happens during sexual reproduction in biological organisms. Solutions can also be generated by cloning an existing solution, which is analogous to asexual reproduction. Newly generated solutions are typically mutated before being added to the population.

D
Darkforest
A computer go program developed by Facebook, based on deep learning techniques using a convolutional neural network. Its updated version Darkfores2 combines the techniques of its predecessor with Monte Carlo tree search. The MCTS effectively takes tree search methods commonly seen in computer chess programs and randomizes them. With the update, the system is known as Darkfmcts3.

Dartmouth workshop
The Dartmouth Summer Research Project on Artificial Intelligence was the name of a 1956 summer workshop now considered by many (though not all) to be the seminal event for artificial intelligence as a field.

data augmentation
Data augmentation in data analysis are techniques used to increase the amount of data. It helps reduce overfitting when training a machine learning.

data fusion
The process of integrating multiple data sources to produce more consistent, accurate, and useful information than that provided by any individual data source.

data integration
The process of combining data residing in different sources and providing users with a unified view of them. This process becomes significant in a variety of situations, which include both commercial (such as when two similar companies need to merge their databases) and scientific (combining research results from different bioinformatics repositories, for example) domains. Data integration appears with increasing frequency as the volume (that is, big data) and the need to share existing data explodes. It has become the focus of extensive theoretical work, and numerous open problems remain unsolved.

data mining
The process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.

data science
An interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining. Data science is a ""concept to unify statistics, data analysis, machine learning and their related methods"" in order to ""understand and analyze actual phenomena"" with data. It employs techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science.

data set
Also dataset.
A collection of data. Most commonly a data set corresponds to the contents of a single database table, or a single statistical data matrix, where every column of the table represents a particular variable, and each row corresponds to a given member of the data set in question. The data set lists values for each of the variables, such as height and weight of an object, for each member of the data set.  Each value is known as a datum. The data set may comprise data for one or more members, corresponding to the number of rows.

data warehouse (DW or DWH)
Also enterprise data warehouse (EDW).
A system used for reporting and data analysis. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place

Datalog
A declarative logic programming language that syntactically is a subset of Prolog. It is often used as a query language for deductive databases. In recent years, Datalog has found new application in data integration, information extraction, networking, program analysis, security, and cloud computing.

decision boundary
In the case of backpropagation-based artificial neural networks or perceptrons, the type of decision boundary that the network can learn is determined by the number of hidden layers the network has. If it has no hidden layers, then it can only learn linear problems. If it has one hidden layer, then it can learn any continuous function on compact subsets of Rn as shown by the Universal approximation theorem, thus it can have an arbitrary decision boundary.

decision support system (DSS)
Aan information system that supports business or organizational decision-making activities. DSSs serve the management, operations and planning levels of an organization (usually mid and higher management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance—i.e. unstructured and semi-structured decision problems. Decision support systems can be either fully computerized or human-powered, or a combination of both.

decision theory
Also theory of choice.
The study of the reasoning underlying an agent's choices. Decision theory can be broken into two branches: normative decision theory, which gives advice on how to make the best decisions given a set of uncertain beliefs and a set of values, and descriptive decision theory which analyzes how existing, possibly irrational agents actually make decisions.

decision tree learning
Uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining and machine learning.

declarative programming
A programming paradigm—a style of building the structure and elements of computer programs—that expresses the logic of a computation without describing its control flow.

deductive classifier
A type of artificial intelligence inference engine. It takes as input a set of declarations in a frame language about a domain such as medical research or molecular biology. For example, the names of classes, sub-classes, properties, and restrictions on allowable values.

Deep Blue
was a chess-playing computer developed by IBM. It is known for being the first computer chess-playing system to win both a chess game and a chess match against a reigning world champion under regular time controls.

deep learning
Also deep structured learning or hierarchical learning.
Part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, semi-supervised, or unsupervised.

DeepMind Technologies
A British artificial intelligence company founded in September 2010, currently owned by Alphabet Inc. The company is based in London, with research centres in Canada, France, and the United States. Acquired by Google in 2014, the company has created a neural network that learns how to play video games in a fashion similar to that of humans, as well as a neural Turing machine, or a neural network that may be able to access an external memory like a conventional Turing machine, resulting in a computer that mimics the short-term memory of the human brain. The company made headlines in 2016 after its AlphaGo program beat human professional Go player Lee Sedol, the world champion, in a five-game match, which was the subject of a documentary film. A more general program, AlphaZero, beat the most powerful programs playing Go, chess, and shogi (Japanese chess) after a few days of play against itself using reinforcement learning.

default logic
A non-monotonic logic proposed by Raymond Reiter to formalize reasoning with default assumptions.

description logic (DL)
A family of formal knowledge representation languages. Many DLs are more expressive than propositional logic but less expressive than first-order logic. In contrast to the latter, the core reasoning problems for DLs are (usually) decidable, and efficient decision procedures have been designed and implemented for these problems. There are general, spatial, temporal, spatiotemporal, and fuzzy descriptions logics, and each description logic features a different balance between DL expressivity and reasoning complexity by supporting different sets of mathematical constructors.

developmental robotics (DevRob)
Also epigenetic robotics.
A scientific field which aims at studying the developmental mechanisms, architectures, and constraints that allow lifelong and open-ended learning of new skills and new knowledge in embodied machines.

diagnosis
Concerned with the development of algorithms and techniques that are able to determine whether the behaviour of a system is correct. If the system is not functioning correctly, the algorithm should be able to determine, as accurately as possible, which part of the system is failing, and which kind of fault it is facing. The computation is based on observations, which provide information on the current behaviour.

dialogue system
Also conversational agent (CA).
A computer system intended to converse with a human with a coherent structure. Dialogue systems have employed text, speech, graphics, haptics, gestures, and other modes for communication on both the input and output channel.

dimensionality reduction
Also dimension reduction. 
The process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into feature selection and feature extraction.

discrete system
Any system with a countable number of states. Discrete systems may be contrasted with continuous systems, which may also be called analog systems. A final discrete system is often modeled with a directed graph and is analyzed for correctness and complexity according to computational theory. Because discrete systems have a countable number of states, they may be described in precise mathematical models. A computer is a finite state machine that may be viewed as a discrete system. Because computers are often used to model not only other discrete systems but continuous systems as well, methods have been developed to represent real-world continuous systems as discrete systems. One such method involves sampling a continuous signal at discrete time intervals.

distributed artificial intelligence (DAI)
Also decentralized artificial intelligence.
A subfield of artificial intelligence research dedicated to the development of distributed solutions for problems. DAI is closely related to and a predecessor of the field of multi-agent systems.

dynamic epistemic logic (DEL)
A logical framework dealing with knowledge and information change. Typically, DEL focuses on situations involving multiple agents and studies how their knowledge changes when events occur.

E
eager learning
A learning method in which the system tries to construct a general, input-independent target function during training of the system, as opposed to lazy learning, where generalization beyond the training data is delayed until a query is made to the system.

Ebert test
A test which gauges whether a computer-based synthesized voice can tell a joke with sufficient skill to cause people to laugh. It was proposed by film critic Roger Ebert at the 2011 TED conference as a challenge to software developers to have a computerized voice master the inflections, delivery, timing, and intonations of a speaking human. The test is similar to the Turing test proposed by Alan Turing in 1950 as a way to gauge a computer's ability to exhibit intelligent behavior by generating performance indistinguishable from a human being.

echo state network (ESN)
A recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can (re)produce specific temporal patterns. The main interest of this network is that although its behaviour is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.

embodied agent
Also interface agent.
An intelligent agent that interacts with the environment through a physical body within that environment. Agents that are represented graphically with a body, for example a human or a cartoon animal, are also called embodied agents, although they have only virtual, not physical, embodiment.

embodied cognitive science
An interdisciplinary field of research, the aim of which is to explain the mechanisms underlying intelligent behavior. It comprises three main methodologies: 1) the modeling of psychological and biological systems in a holistic manner that considers the mind and body as a single entity, 2) the formation of a common set of general principles of intelligent behavior, and 3) the experimental use of robotic agents in controlled environments.

error-driven learning
A sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to minimize some error feedback. It is a type of reinforcement learning.

ensemble averaging
In machine learning, particularly in the creation of artificial neural networks, ensemble averaging is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model.

ethics of artificial intelligence
The part of the ethics of technology specific to artificial intelligence.

evolutionary algorithm (EA)
A subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.

evolutionary computation
A family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.

evolving classification function (ECF)
Evolving classifier functions or evolving classifiers are used for classifying and clustering in the field of machine learning and artificial intelligence, typically employed for data stream mining tasks in dynamic and changing environments.

existential risk
The hypothesis that substantial progress in artificial general intelligence (AGI) could someday result in human extinction or some other unrecoverable global catastrophe.

expert system
A computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code.

F
fast-and-frugal trees
A type of classification tree. Fast-and-frugal trees can be used as decision-making tools which operate as lexicographic classifiers, and, if required, associate an action (decision) to each class or category.

feature extraction
In machine learning, pattern recognition, and image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations.

feature learning
In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task.

feature selection
In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction.

federated learning
A type of machine learning that allows for training on multiple devices with decentralized data, thus helping preserve the privacy of individual users and their data.

first-order logic
Also known as first-order predicate calculus and predicate logic.
A collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects and allows the use of sentences that contain variables, so that rather than propositions such as Socrates is a man one can have expressions in the form ""there exists X such that X is Socrates and X is a man"" and there exists is a quantifier while X is a variable. This distinguishes it from propositional logic, which does not use quantifiers or relations.

fluent
A condition that can change over time. In logical approaches to reasoning about actions, fluents can be represented in first-order logic by predicates having an argument that depends on time.

formal language
A set of words whose letters are taken from an alphabet and are well-formed according to a specific set of rules.

forward chaining
Also forward reasoning. 
One of the two main methods of reasoning when using an inference engine and can be described logically as repeated application of modus ponens. Forward chaining is a popular implementation strategy for expert systems, business and production rule systems. The opposite of forward chaining is backward chaining.  Forward chaining starts with the available data and uses inference rules to extract more data (from an end user, for example) until a goal is reached. An inference engine using forward chaining searches the inference rules until it finds one where the antecedent (If clause) is known to be true. When such a rule is found, the engine can conclude, or infer, the consequent (Then clause), resulting in the addition of new information to its data.

frame
An artificial intelligence data structure used to divide knowledge into substructures by representing ""stereotyped situations"". Frames are the primary data structure used in artificial intelligence frame language.

frame language
A technology used for knowledge representation in artificial intelligence. Frames are stored as ontologies of sets and subsets of the frame concepts. They are similar to class hierarchies in object-oriented languages although their fundamental design goals are different. Frames are focused on explicit and intuitive representation of knowledge whereas objects focus on encapsulation and information hiding. Frames originated in AI research and objects primarily in software engineering. However, in practice the techniques and capabilities of frame and object-oriented languages overlap significantly.

frame problem
The problem of finding adequate collections of axioms for a viable description of a robot environment.

friendly artificial intelligence
Also friendly AI or FAI.
A hypothetical artificial general intelligence (AGI) that would have a positive effect on humanity. It is a part of the ethics of artificial intelligence and is closely related to machine ethics. While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behaviour and ensuring it is adequately constrained.

futures studies
The study of postulating possible, probable, and preferable futures and the worldviews and myths that underlie them.

fuzzy control system
A control system based on fuzzy logic—a mathematical system that analyzes analog input values in terms of logical variables that take on continuous values between 0 and 1, in contrast to classical or digital logic, which operates on discrete values of either 1 or 0 (true or false, respectively).

fuzzy logic
A simple form for the many-valued logic, in which the truth values of variables may have any degree of ""Truthfulness"" that can be represented by any real number in the range between 0 (as in Completely False) and 1 (as in Completely True) inclusive. Consequently, It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false. In contrast to Boolean logic, where the truth values of variables may have the integer values 0 or 1 only.

fuzzy rule
A rule used within fuzzy logic systems to infer an output based on input variables.

fuzzy set
In classical set theory, the membership of elements in a set is assessed in binary terms according to a bivalent condition — an element either belongs or does not belong to the set. By contrast, fuzzy set theory permits the gradual assessment of the membership of elements in a set; this is described with the aid of a membership function valued in the real unit interval [0, 1]. Fuzzy sets generalize classical sets, since the indicator functions (aka characteristic functions) of classical sets are special cases of the membership functions of fuzzy sets, if the latter only take values 0 or 1. In fuzzy set theory, classical bivalent sets are usually called crisp sets. The fuzzy set theory can be used in a wide range of domains in which information is incomplete or imprecise, such as bioinformatics.

G
game theory
The study of mathematical models of strategic interaction between rational decision-makers.

general game playing (GGP)
General game playing is the design of artificial intelligence programs to be able to run and play more than one game successfully.

generative adversarial network (GAN)
A class of machine learning systems. Two neural networks contest with each other in a zero-sum game framework.

genetic algorithm (GA)
A metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on bio-inspired operators such as mutation, crossover and selection.

genetic operator
An operator used in genetic algorithms to guide the algorithm towards a solution to a given problem. There are three main types of operators (mutation, crossover and selection), which must work in conjunction with one another in order for the algorithm to be successful.

glowworm swarm optimization
A swarm intelligence optimization algorithm based on the behaviour of glowworms (also known as fireflies or lightning bugs).

graph (abstract data type)
In computer science, a graph is an abstract data type that is meant to implement the undirected graph and directed graph concepts from mathematics; specifically, the field of graph theory.

graph (discrete mathematics)
In mathematics, and more specifically in graph theory, a graph is a structure amounting to a set of objects in which some pairs of the objects are in some sense ""related"". The objects correspond to mathematical abstractions called vertices (also called nodes or points) and each of the related pairs of vertices is called an edge (also called an arc or line).

graph database (GDB)
A database that uses graph structures for semantic queries with nodes, edges, and properties to represent and store data. A key concept of the system is the graph (or edge or relationship), which directly relates data items in the store a collection of nodes of data and edges representing the relationships between the nodes. The relationships allow data in the store to be linked together directly, and in many cases retrieved with one operation. Graph databases hold the relationships between data as a priority. Querying relationships within a graph database is fast because they are perpetually stored within the database itself. Relationships can be intuitively visualized using graph databases, making it useful for heavily inter-connected data.

graph theory
The study of graphs, which are mathematical structures used to model pairwise relations between objects.

graph traversal
Also graph search.
The process of visiting (checking and/or updating) each vertex in a graph. Such traversals are classified by the order in which the vertices are visited. Tree traversal is a special case of graph traversal.

H
halting problem

heuristic
A technique designed for solving a problem more quickly when classic methods are too slow, or for finding an approximate solution when classic methods fail to find any exact solution.  This is achieved by trading optimality, completeness, accuracy, or precision for speed.  In a way, it can be considered a shortcut.  A heuristic function, also called simply a heuristic, is a function that ranks alternatives in search algorithms at each branching step based on available information to decide which branch to follow. For example, it may approximate the exact solution.

hidden layer
An internal layer of neurons in an artificial neural network, not dedicated to input or output.

hidden unit
A neuron in a hidden layer in an artificial neural network.

hyper-heuristic
A heuristic search method that seeks to automate the process of selecting, combining, generating, or adapting several simpler heuristics (or components of such heuristics) to efficiently solve computational search problems, often by the incorporation of machine learning techniques. One of the motivations for studying hyper-heuristics is to build systems which can handle classes of problems rather than solving just one problem.

I
IEEE Computational Intelligence Society
A professional society of the Institute of Electrical and Electronics Engineers (IEEE) focussing on ""the theory, design, application, and development of biologically and linguistically motivated computational paradigms emphasizing neural networks, connectionist systems, genetic algorithms, evolutionary programming, fuzzy systems, and hybrid intelligent systems in which these paradigms are contained"".

incremental learning
A method of machine learning, in which input data is continuously used to extend the existing model's knowledge i.e. to further train the model. It represents a dynamic technique of supervised learning and unsupervised learning that can be applied when training data becomes available gradually over time or its size is out of system memory limits. Algorithms that can facilitate incremental learning are known as incremental machine learning algorithms.

inference engine
A component of the system that applies logical rules to the knowledge base to deduce new information.

information integration (II)
The merging of information from heterogeneous sources with differing conceptual, contextual and typographical representations. It is used in data mining and consolidation of data from unstructured or semi-structured resources. Typically, information integration refers to textual representations of knowledge but is sometimes applied to rich-media content. Information fusion, which is a related term, involves the combination of information into a new set of information towards reducing redundancy and uncertainty.

Information Processing Language (IPL)
A programming language that includes features intended to help with programs that perform simple problem solving actions such as lists, dynamic memory allocation, data types, recursion, functions as arguments, generators, and cooperative multitasking.  IPL invented the concept of list processing, albeit in an assembly-language style.

intelligence amplification (IA)
Also cognitive augmentation, machine augmented intelligence, and enhanced intelligence.
The effective use of information technology in augmenting human intelligence.

intelligence explosion
A possible outcome of humanity building artificial general intelligence (AGI). AGI would be capable of recursive self-improvement leading to rapid emergence of ASI (artificial superintelligence), the limits of which are unknown, at the time of the technological singularity.

intelligent agent (IA)
An autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent). Intelligent agents may also learn or use knowledge to achieve their goals. They may be very simple or very complex.

intelligent control
A class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, reinforcement learning, evolutionary computation and genetic algorithms.

intelligent personal assistant
Also virtual assistant or personal digital assistant.
A software agent that can perform tasks or services for an individual based on verbal commands. Sometimes the term ""chatbot"" is used to refer to virtual assistants generally or specifically accessed by online chat (or in some cases online chat programs that are exclusively for entertainment purposes).  Some virtual assistants are able to interpret human speech and respond via synthesized voices. Users can ask their assistants questions, control home automation devices and media playback via voice, and manage other basic tasks such as email, to-do lists, and calendars with verbal commands.

interpretation
An assignment of meaning to the symbols of a formal language. Many formal languages used in mathematics, logic, and theoretical computer science are defined in solely syntactic terms, and as such do not have any meaning until they are given some interpretation. The general study of interpretations of formal languages is called formal semantics.

intrinsic motivation
An intelligent agent is intrinsically motivated to act if the information content alone, of the experience resulting from the action, is the motivating factor. Information content in this context is measured in the information theory sense as quantifying uncertainty. A typical intrinsic motivation is to search for unusual (surprising) situations, in contrast to a typical extrinsic motivation such as the search for food. Intrinsically motivated artificial agents display behaviours akin to exploration and curiosity.

issue tree
Also logic tree.
A graphical breakdown of a question that dissects it into its different components vertically and that progresses into details as it reads to the right.  Issue trees are useful in problem solving to identify the root causes of a problem as well as to identify its potential solutions. They also provide a reference point to see how each piece fits into the whole picture of a problem.

J
junction tree algorithm
Also Clique Tree.
A method used in machine learning to extract marginalization in general graphs. In essence, it entails performing belief propagation on a modified graph called a junction tree. The graph is called a tree because it branches into different sections of data; nodes of variables are the branches.

K
kernel method
In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets.

KL-ONE
A well-known knowledge representation system in the tradition of semantic networks and frames; that is, it is a frame language. The system is an attempt to overcome semantic indistinctness in semantic network representations and to explicitly represent conceptual information as a structured inheritance network.

knowledge acquisition
The process used to define the rules and ontologies required for a knowledge-based system. The phrase was first used in conjunction with expert systems to describe the initial tasks associated with developing an expert system, namely finding and interviewing domain experts and capturing their knowledge via rules, objects, and frame-based ontologies.

knowledge-based system (KBS)
A computer program that reasons and uses a knowledge base to solve complex problems. The term is broad and refers to many different kinds of systems. The one common theme that unites all knowledge based systems is an attempt to represent knowledge explicitly and a reasoning system that allows it to derive new knowledge. Thus, a knowledge-based system has two distinguishing features: a knowledge base and an inference engine.

knowledge engineering (KE)
All technical, scientific, and social aspects involved in building, maintaining, and using knowledge-based systems.

knowledge extraction
The creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criteria is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.

knowledge Interchange Format (KIF)
A computer language designed to enable systems to share and re-use information from knowledge-based systems. KIF is similar to frame languages such as KL-ONE and LOOM but unlike such language its primary role is not intended as a framework for the expression or use of knowledge but rather for the interchange of knowledge between systems. The designers of KIF likened it to PostScript. PostScript was not designed primarily as a language to store and manipulate documents but rather as an interchange format for systems and devices to share documents. In the same way KIF is meant to facilitate sharing of knowledge across different systems that use different languages, formalisms, platforms, etc.

knowledge representation and reasoning (KR² or KR&R)
The field of artificial intelligence dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning, such as the application of rules or the relations of sets and subsets. Examples of knowledge representation formalisms include semantic nets, systems architecture, frames, rules, and ontologies. Examples of automated reasoning engines include inference engines, theorem provers, and classifiers.

L
lazy learning
In machine learning, lazy learning is a learning method in which generalization of the training data is, in theory, delayed until a query is made to the system, as opposed to in eager learning, where the system tries to generalize the training data before receiving queries.

Lisp (programming language) (LISP)
A family of programming languages with a long history and a distinctive, fully parenthesized prefix notation.

logic programming
A type of programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain. Major logic programming language families include Prolog, answer set programming (ASP), and Datalog.

long short-term memory (LSTM)
An artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections that make it a ""general purpose computer"" (that is, it can compute anything that a Turing machine can). It can not only process single data points (such as images), but also entire sequences of data (such as speech or video).

M
machine vision (MV)
The technology and methods used to provide imaging-based automatic inspection and analysis for such applications as automatic inspection, process control, and robot guidance, usually in industry.  Machine vision is a term encompassing a large number of technologies, software and hardware products, integrated systems, actions, methods and expertise. Machine vision as a systems engineering discipline can be considered distinct from computer vision, a form of computer science.  It attempts to integrate existing technologies in new ways and apply them to solve real world problems. The term is the prevalent one for these functions in industrial automation environments but is also used for these functions in other environments such as security and vehicle guidance.

Markov chain
A stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.

Markov decision process (MDP)
A discrete time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming and reinforcement learning.

mathematical optimization
Also mathematical programming.
In mathematics, computer science, and operations research, the selection of a best element (with regard to some criterion) from some set of available alternatives.

machine learning (ML)
The scientific study of algorithms and statistical models that computer systems use in order to perform a specific task effectively without using explicit instructions, relying on patterns and inference instead.

machine listening
Also computer audition (CA).
A general field of study of algorithms and systems for audio understanding by machine.

machine perception
The capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them.

mechanism design
A field in economics and game theory that takes an engineering approach to designing economic mechanisms or incentives, toward desired objectives, in strategic settings, where players act rationally. Because it starts at the end of the game, then goes backwards, it is also called reverse game theory. It has broad applications, from economics and politics (markets, auctions, voting procedures) to networked-systems (internet interdomain routing, sponsored search auctions).

mechatronics
Also mechatronic engineering.
A multidisciplinary branch of engineering that focuses on the engineering of both electrical and mechanical systems, and also includes a combination of robotics, electronics, computer, telecommunications, systems, control, and product engineering.

metabolic network reconstruction and simulation
Allows for an in-depth insight into the molecular mechanisms of a particular organism. In particular, these models correlate the genome with molecular physiology.

metaheuristic
In computer science and mathematical optimization, a metaheuristic is a higher-level procedure or heuristic designed to find, generate, or select a heuristic (partial search algorithm) that may provide a sufficiently good solution to an optimization problem, especially with incomplete or imperfect information or limited computation capacity. Metaheuristics sample a set of solutions which is too large to be completely sampled.

model checking
In computer science, model checking or property checking is, for a given model of a system, exhaustively and automatically checking whether this model meets a given specification. Typically, one has hardware or software systems in mind, whereas the specification contains safety requirements such as the absence of deadlocks and similar critical states that can cause the system to crash. Model checking is a technique for automatically verifying correctness properties of finite-state systems.

modus ponens
In propositional logic, modus ponens is a rule of inference. It can be summarized as ""P implies Q and P is asserted to be true, therefore Q must be true.""

modus tollens
In propositional logic, modus tollens is a valid argument form and a rule of inference.  It is an application of the general truth that if a statement is true, then so is its contrapositive.  The inference rule modus tollens asserts that the inference from P implies Q to the negation of Q implies the negation of P is valid.

Monte Carlo tree search
In computer science, Monte Carlo tree search (MCTS) is a heuristic search algorithm for some kinds of decision processes.

multi-agent system (MAS)
Also self-organized system.
A computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve. Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning.

multi-swarm optimization
A variant of particle swarm optimization (PSO) based on the use of multiple sub-swarms instead of one (standard) swarm. The general approach in multi-swarm optimization is that each sub-swarm focuses on a specific region while a specific diversification method decides where and when to launch the sub-swarms. The multi-swarm framework is especially fitted for the optimization on multi-modal problems, where multiple (local) optima exist.

mutation
A genetic operator used to maintain genetic diversity from one generation of a population of genetic algorithm chromosomes to the next. It is analogous to biological mutation. Mutation alters one or more gene values in a chromosome from its initial state. In mutation, the solution may change entirely from the previous solution. Hence GA can come to a better solution by using mutation. Mutation occurs during evolution according to a user-definable mutation probability. This probability should be set low. If it is set too high, the search will turn into a primitive random search.

Mycin
An early backward chaining expert system that used artificial intelligence to identify bacteria causing severe infections, such as bacteremia and meningitis, and to recommend antibiotics, with the dosage adjusted for patient's body weight – the name derived from the antibiotics themselves, as many antibiotics have the suffix ""-mycin"". The MYCIN system was also used for the diagnosis of blood clotting diseases.

N
naive Bayes classifier
In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.

naive semantics
An approach used in computer science for representing basic knowledge about a specific domain, and has been used in applications such as the representation of the meaning of natural language sentences in artificial intelligence applications. In a general setting the term has been used to refer to the use of a limited store of generally understood knowledge about a specific domain in the world, and has been applied to fields such as the knowledge based design of data schemas.

name binding
In programming languages, name binding is the association of entities (data and/or code) with identifiers. An identifier bound to an object is said to reference that object. Machine languages have no built-in notion of identifiers, but name-object bindings as a service and notation for the programmer is implemented by programming languages. Binding is intimately connected with scoping, as scope determines which names bind to which objects – at which locations in the program code (lexically) and in which one of the possible execution paths (temporally).  Use of an identifier id in a context that establishes a binding for id is called a binding (or defining) occurrence. In all other occurrences (e.g., in expressions, assignments, and subprogram calls), an identifier stands for what it is bound to; such occurrences are called applied occurrences.

named-entity recognition (NER)
Also entity identification, entity chunking, and entity extraction.
A subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.

named graph
A key concept of Semantic Web architecture in which a set of Resource Description Framework statements (a graph) are identified using a URI, allowing descriptions to be made of that set of statements such as context, provenance information or other such metadata.  Named graphs are a simple extension of the RDF data model through which graphs can be created but the model lacks an effective means of distinguishing between them once published on the Web at large.

natural language generation (NLG)
A software process that transforms structured data into plain-English content.  It can be used to produce long-form content for organizations to automate custom reports, as well as produce custom content for a web or mobile application. It can also be used to generate short blurbs of text in interactive conversations (a chatbot) which might even be read out loud by a text-to-speech system.

natural language processing (NLP)
A subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.

natural language programming
An ontology-assisted way of programming in terms of natural-language sentences, e.g. English.

network motif
All networks, including biological networks, social networks, technological networks (e.g., computer networks and electrical circuits) and more, can be represented as graphs, which include a wide variety of subgraphs. One important local property of networks are so-called network motifs, which are defined as recurrent and statistically significant sub-graphs or patterns.

neural machine translation (NMT)
An approach to machine translation that uses a large artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.

neural Turing machine (NTM)
A recurrent neural network model. NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers. An NTM has a neural network controller coupled to external memory resources, which it interacts with through attentional mechanisms. The memory interactions are differentiable end-to-end, making it possible to optimize them using gradient descent. An NTM with a long short-term memory (LSTM) network controller can infer simple algorithms such as copying, sorting, and associative recall from examples alone.

neuro-fuzzy
Combinations of artificial neural networks and fuzzy logic.

neurocybernetics
Also brain–computer interface (BCI), neural-control interface (NCI), mind-machine interface (MMI), direct neural interface (DNI), or brain–machine interface (BMI).
A direct communication pathway between an enhanced or wired brain and an external device. BCI differs from neuromodulation in that it allows for bidirectional information flow. BCIs are often directed at researching, mapping, assisting, augmenting, or repairing human cognitive or sensory-motor functions.

neuromorphic engineering
Also neuromorphic computing.
A concept describing the use of very-large-scale integration (VLSI) systems containing electronic analog circuits to mimic neuro-biological architectures present in the nervous system. In recent times, the term neuromorphic has been used to describe analog, digital, mixed-mode analog/digital VLSI, and software systems that implement models of neural systems (for perception, motor control, or multisensory integration). The implementation of neuromorphic computing on the hardware level can be realized by oxide-based memristors, spintronic memories, threshold switches, and transistors.

node
A basic unit of a data structure, such as a linked list or tree data structure. Nodes contain data and also may link to other nodes. Links between nodes are often implemented by pointers.

nondeterministic algorithm
An algorithm that, even for the same input, can exhibit different behaviors on different runs, as opposed to a deterministic algorithm.

nouvelle AI
Nouvelle AI differs from classical AI by aiming to produce robots with intelligence levels similar to insects. Researchers believe that intelligence can emerge organically from simple behaviors as these intelligences interacted with the ""real world"", instead of using the constructed worlds which symbolic AIs typically needed to have programmed into them.

NP
In computational complexity theory, NP (nondeterministic polynomial time) is a complexity class used to classify decision problems.  NP is the set of decision problems for which the problem instances, where the answer is ""yes"", have proofs verifiable in polynomial time.

NP-completeness
In computational complexity theory, a problem is NP-complete when it can be solved by a restricted class of brute force search algorithms and it can be used to simulate any other problem with a similar algorithm. More precisely, each input to the problem should be associated with a set of solutions of polynomial length, whose validity can be tested quickly (in polynomial time), such that the output for any input is ""yes"" if the solution set is non-empty and ""no"" if it is empty.

NP-hardness
Also non-deterministic polynomial-time hardness.
In computational complexity theory, the defining property of a class of problems that are, informally, ""at least as hard as the hardest problems in NP"". A simple example of an NP-hard problem is the subset sum problem.

O
Occam's razor
Also Ockham's razor or Ocham's razor.
The problem-solving principle that states that when presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions; the principle is not meant to filter out hypotheses that make different predictions. The idea is attributed to the English Franciscan friar William of Ockham (c. 1287–1347), a scholastic philosopher and theologian.

offline learning

online machine learning
A method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time.

ontology learning
Also ontology extraction, ontology generation, or ontology acquisition.
The automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval.

OpenAI
The for-profit corporation OpenAI LP, whose parent organization is the non-profit organization OpenAI Inc that conducts research in the field of artificial intelligence (AI) with the stated aim to promote and develop friendly AI in such a way as to benefit humanity as a whole.

OpenCog
A project that aims to build an open-source artificial intelligence framework. OpenCog Prime is an architecture for robot and virtual embodied cognition that defines a set of interacting components designed to give rise to human-equivalent artificial general intelligence (AGI) as an emergent phenomenon of the whole system.

Open Mind Common Sense
An artificial intelligence project based at the Massachusetts Institute of Technology (MIT) Media Lab whose goal is to build and utilize a large commonsense knowledge base from the contributions of many thousands of people across the Web.

open-source software (OSS)
A type of computer software in which source code is released under a license in which the copyright holder grants users the rights to study, change, and distribute the software to anyone and for any purpose. Open-source software may be developed in a collaborative public manner. Open-source software is a prominent example of open collaboration.

P
partial order reduction
A technique for reducing the size of the state-space to be searched by a model checking or automated planning and scheduling algorithm. It exploits the commutativity of concurrently executed transitions, which result in the same state when executed in different orders.

partially observable Markov decision process (POMDP)
A generalization of a Markov decision process (MDP). A POMDP models an agent decision process in which it is assumed that the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state. Instead, it must maintain a probability distribution over the set of possible states, based on a set of observations and observation probabilities, and the underlying MDP.

particle swarm optimization (PSO)
A computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. It solves a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity. Each particle's movement is influenced by its local best known position, but is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions.

pathfinding
Also pathing.
The plotting, by a computer application, of the shortest route between two points. It is a more practical variant on solving mazes. This field of research is based heavily on Dijkstra's algorithm for finding a shortest path on a weighted graph.

pattern recognition
Concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.

predicate logic
Also first-order logic, predicate logic, and first-order predicate calculus.
A collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects and allows the use of sentences that contain variables, so that rather than propositions such as Socrates is a man one can have expressions in the form ""there exists x such that x is Socrates and x is a man"" and there exists is a quantifier while x is a variable. This distinguishes it from propositional logic, which does not use quantifiers or relations; in this sense, propositional logic is the foundation of first-order logic.

predictive analytics
A variety of statistical techniques from data mining, predictive modelling, and machine learning, that analyze current and historical facts to make predictions about future or otherwise unknown events.

principal component analysis (PCA)
A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component, in turn, has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.

principle of rationality
Also rationality principle.
A principle coined by Karl R. Popper in his Harvard Lecture of 1963, and published in his book Myth of Framework. It is related to what he called the 'logic of the situation' in an Economica article of 1944/1945, published later in his book The Poverty of Historicism. According to Popper's rationality principle, agents act in the most adequate way according to the objective situation. It is an idealized conception of human behavior which he used to drive his model of situational analysis.

probabilistic programming (PP)
A programming paradigm in which probabilistic models are specified and inference for these models is performed automatically. It represents an attempt to unify probabilistic modeling and traditional general-purpose programming in order to make the former easier and more widely applicable. It can be used to create systems that help make decisions in the face of uncertainty.  Programming languages used for probabilistic programming are referred to as ""Probabilistic programming languages"" (PPLs).

production system

programming language
A formal language, which comprises a set of instructions that produce various kinds of output. Programming languages are used in computer programming to implement algorithms.

Prolog
A logic programming language associated with artificial intelligence and computational linguistics.  Prolog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is intended primarily as a declarative programming language: the program logic is expressed in terms of relations, represented as facts and rules.  A computation is initiated by running a query over these relations.

propositional calculus
Also propositional logic, statement logic, sentential calculus, sentential logic, and zeroth-order logic.
A branch of logic which deals with propositions (which can be true or false) and argument flow. Compound propositions are formed by connecting propositions by logical connectives. The propositions without logical connectives are called atomic propositions. Unlike first-order logic, propositional logic does not deal with non-logical objects, predicates about them, or quantifiers. However, all the machinery of propositional logic is included in first-order logic and higher-order logics. In this sense, propositional logic is the foundation of first-order logic and higher-order logic.

Python
An interpreted, high-level, general-purpose programming language created by Guido van Rossum and first released in 1991. Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.

Q
qualification problem
In philosophy and artificial intelligence (especially knowledge-based systems), the qualification problem is concerned with the impossibility of listing all of the preconditions required for a real-world action to have its intended effect. It might be posed as how to deal with the things that prevent me from achieving my intended result. It is strongly connected to, and opposite the ramification side of, the frame problem.

quantifier
In logic, quantification specifies the quantity of specimens in the domain of discourse that satisfy an open formula. The two most common quantifiers mean ""for all"" and ""there exists"". For example, in arithmetic, quantifiers allow one to say that the natural numbers go on forever, by writing that for all n (where n is a natural number), there is another number (say, the successor of n) which is one bigger than n.

quantum computing
The use of quantum-mechanical phenomena such as superposition and entanglement to perform computation. A quantum computer is used to perform such computation, which can be implemented theoretically or physically.

query language
Query languages or data query languages (DQLs) are computer languages used to make queries in databases and information systems.  Broadly, query languages can be classified according to whether they are database query languages or information retrieval query languages. The difference is that a database query language attempts to give factual answers to factual questions, while an information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry.

R
R programming language
A programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.

radial basis function network
In the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters. Radial basis function networks have many uses, including function approximation, time series prediction, classification, and system control. They were first formulated in a 1988 paper by Broomhead and Lowe, both researchers at the Royal Signals and Radar Establishment.

random forest
Also random decision forest.
An ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.

reasoning system
In information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems.

recurrent neural network (RNN)
A class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.

region connection calculus

reinforcement learning (RL)
An area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.  It differs from supervised learning in that labelled input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).

reservoir computing
A framework for computation that may be viewed as an extension of neural networks. Typically an input signal is fed into a fixed (random) dynamical system called a reservoir and the dynamics of the reservoir map the input to a higher dimension. Then a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output. The main benefit is that training is performed only at the readout stage and the reservoir is fixed. Liquid-state machines and echo state networks are two major types of reservoir computing.

Resource Description Framework (RDF)
A family of World Wide Web Consortium (W3C) specifications originally designed as a metadata data model. It has come to be used as a general method for conceptual description or modeling of information that is implemented in web resources, using a variety of syntax notations and data serialization formats. It is also used in knowledge management applications.

restricted Boltzmann machine (RBM)
A generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.

Rete algorithm
A pattern matching algorithm for implementing rule-based systems. The algorithm was developed to efficiently apply many rules or patterns to many objects, or facts, in a knowledge base. It is used to determine which of the system's rules should fire based on its data store, its facts.

robotics
An interdisciplinary branch of science and engineering that includes mechanical engineering, electronic engineering, information engineering, computer science, and others. Robotics deals with the design, construction, operation, and use of robots, as well as computer systems for their control, sensory feedback, and information processing.

rule-based system
In computer science, a rule-based system is used to store and manipulate knowledge to interpret information in a useful way. It is often used in artificial intelligence applications and research.  Normally, the term rule-based system is applied to systems involving human-crafted or curated rule sets.  Rule-based systems constructed using automatic rule inference, such as rule-based machine learning, are normally excluded from this system type.

S
satisfiability
In mathematical logic, satisfiability and validity are elementary concepts of semantics. A formula is satisfiable if it is possible to find an interpretation (model) that makes the formula true.  A formula is valid if all interpretations make the formula true. The opposites of these concepts are unsatisfiability and invalidity, that is, a formula is unsatisfiable if none of the interpretations make the formula true, and invalid if some such interpretation makes the formula false. These four concepts are related to each other in a manner exactly analogous to Aristotle's square of opposition.

search algorithm
Any algorithm which solves the search problem, namely, to retrieve information stored within some data structure, or calculated in the search space of a problem domain, either with discrete or continuous values.

selection
The stage of a genetic algorithm in which individual genomes are chosen from a population for later breeding (using the crossover operator).

self-management
The process by which computer systems manage their own operation without human intervention.

semantic network
Also frame network.
A knowledge base that represents semantic relations between concepts in a network. This is often used as a form of knowledge representation. It is a directed or undirected graph consisting of vertices, which represent concepts, and edges, which represent semantic relations between concepts, mapping or connecting semantic fields.

semantic reasoner
Also reasoning engine, rules engine, or simply reasoner.
A piece of software able to infer logical consequences from a set of asserted facts or axioms. The notion of a semantic reasoner generalizes that of an inference engine, by providing a richer set of mechanisms to work with. The inference rules are commonly specified by means of an ontology language, and often a description logic language. Many reasoners use first-order predicate logic to perform reasoning; inference commonly proceeds by forward chaining and backward chaining.

semantic query
Allows for queries and analytics of associative and contextual nature. Semantic queries enable the retrieval of both explicitly and implicitly derived information based on syntactic, semantic and structural information contained in data. They are designed to deliver precise results (possibly the distinctive selection of one single piece of information) or to answer more fuzzy and wide-open questions through pattern matching and digital reasoning.

semantics
In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages. It does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.

sensor fusion
The combining of sensory data or data derived from disparate sources such that the resulting information has less uncertainty than would be possible when these sources were used individually.

separation logic
An extension of Hoare logic, a way of reasoning about programs. The assertion language of separation logic is a special case of the logic of bunched implications (BI).

similarity learning
An area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn from a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.

simulated annealing (SA)
A probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space for an optimization problem.

situated approach
In artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment. This requires designing AI ""from the bottom-up"" by focussing on the basic perceptual and motor skills required to survive. The situated approach gives a much lower priority to abstract reasoning or problem-solving skills.

situation calculus
A logic formalism designed for representing and reasoning about dynamical domains.

Selective Linear Definite clause resolution
Also simply SLD resolution.
The basic inference rule used in logic programming. It is a refinement of resolution, which is both sound and refutation complete for Horn clauses.

software
A collection of data or computer instructions that tell the computer how to work. This is in contrast to physical hardware, from which the system is built and actually performs the work. In computer science and software engineering, computer software is all information processed by computer systems, programs and data. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media.

software engineering
The application of engineering to the development of software in a systematic method.

spatial-temporal reasoning
An area of artificial intelligence which draws from the fields of computer science, cognitive science, and cognitive psychology. The theoretic goal—on the cognitive side—involves representing and reasoning spatial-temporal knowledge in mind. The applied goal—on the computing side—involves developing high-level control systems of automata for navigating and understanding time and space.

SPARQL
An RDF query language—that is, a semantic query language for databases—able to retrieve and manipulate data stored in Resource Description Framework (RDF) format.

speech recognition
An interdisciplinary subfield of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields.

spiking neural network (SNN)
An artificial neural network that more closely mimics a natural neural network. In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their Operating Model.

state
In information technology and computer science, a program is described as stateful if it is designed to remember preceding events or user interactions; the remembered information is called the state of the system.

statistical classification
In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.  Examples are assigning a given email to the ""spam"" or ""non-spam"" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).  Classification is an example of pattern recognition.

statistical relational learning (SRL)
A subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. Note that SRL is sometimes called Relational Machine Learning (RML) in the literature. Typically, the knowledge representation formalisms developed in SRL use (a subset of) first-order logic to describe relational properties of a domain in a general manner (universal quantification) and draw upon probabilistic graphical models (such as Bayesian networks or Markov networks) to model the uncertainty; some also build upon the methods of inductive logic programming.

stochastic optimization (SO)
Any optimization method that generates and uses random variables. For stochastic problems, the random variables appear in the formulation of the optimization problem itself, which involves random objective functions or random constraints. Stochastic optimization methods also include methods with random iterates. Some stochastic optimization methods use random iterates to solve stochastic problems, combining both meanings of stochastic optimization. Stochastic optimization methods generalize deterministic methods for deterministic problems.

stochastic semantic analysis
An approach used in computer science as a semantic component of natural language understanding. Stochastic models generally use the definition of segments of words as basic semantic units for the semantic models, and in some cases involve a two layered approach.

Stanford Research Institute Problem Solver (STRIPS)

subject-matter expert

superintelligence
A hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds. Superintelligence may also refer to a property of problem-solving systems (e.g., superintelligent language translators or engineering assistants) whether or not these high-level intellectual competencies are embodied in agents that act within the physical world. A superintelligence may or may not be created by an intelligence explosion and be associated with a technological singularity.

supervised learning
The machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples.  In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal).  A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias).

support-vector machines
In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.

swarm intelligence (SI)
The collective behavior of decentralized, self-organized systems, either natural or artificial. The expression was introduced in the context of cellular robotic systems.

symbolic artificial intelligence
The term for the collection of all methods in artificial intelligence research that are based on high-level ""symbolic"" (human-readable) representations of problems, logic, and search.

synthetic intelligence (SI)
An alternative term for artificial intelligence which emphasizes that the intelligence of machines need not be an imitation or in any way artificial; it can be a genuine form of intelligence.

systems neuroscience
A subdiscipline of neuroscience and systems biology that studies the structure and function of neural circuits and systems.  It is an umbrella term, encompassing a number of areas of study concerned with how nerve cells behave when connected together to form neural pathways, neural circuits, and larger brain networks.

T
technological singularity
Also simply the singularity.
A hypothetical point in the future when technological growth becomes uncontrollable and irreversible, resulting in unfathomable changes to human civilization.

temporal difference learning
A class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.

tensor network theory
A theory of brain function (particularly that of the cerebellum) that provides a mathematical model of the transformation of sensory space-time coordinates into motor coordinates and vice versa by cerebellar neuronal networks. The theory was developed as a geometrization of brain function (especially of the central nervous system) using tensors.

TensorFlow
A free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks.

theoretical computer science (TCS)
A subset of general computer science and mathematics that focuses on more mathematical topics of computing and includes the theory of computation.

theory of computation
In theoretical computer science and mathematics, the theory of computation is the branch that deals with how efficiently problems can be solved on a model of computation, using an algorithm. The field is divided into three major branches: automata theory and languages, computability theory, and computational complexity theory, which are linked by the question: ""What are the fundamental capabilities and limitations of computers?"".

Thompson sampling
A heuristic for choosing actions that addresses the exploration-exploitation dilemma in the multi-armed bandit problem. It consists in choosing the action that maximizes the expected reward with respect to a randomly drawn belief.

time complexity
The computational complexity that describes the amount of time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor.

transhumanism
Abbreviated H+ or h+.
An international philosophical movement that advocates for the transformation of the human condition by developing and making widely available sophisticated technologies to greatly enhance human intellect and physiology.

transition system
In theoretical computer science, a transition system is a concept used in the study of computation. It is used to describe the potential behavior of discrete systems. It consists of states and transitions between states, which may be labeled with labels chosen from a set; the same label may appear on more than one transition. If the label set is a singleton, the system is essentially unlabeled, and a simpler definition that omits the labels is possible.

tree traversal
Also tree search.
A form of graph traversal and refers to the process of visiting (checking and/or updating) each node in a tree data structure, exactly once. Such traversals are classified by the order in which the nodes are visited.

true quantified Boolean formula
In computational complexity theory, the language TQBF is a formal language consisting of the true quantified Boolean formulas.  A (fully) quantified Boolean formula is a formula in quantified propositional logic where every variable is quantified (or bound), using either existential or universal quantifiers, at the beginning of the sentence. Such a formula is equivalent to either true or false (since there are no free variables). If such a formula evaluates to true, then that formula is in the language TQBF. It is also known as QSAT (Quantified SAT).

Turing machine

Turing test
A test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human, developed by Alan Turing in 1950. Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses. The evaluator would be aware that one of the two partners in conversation is a machine, and all participants would be separated from one another. The conversation would be limited to a text-only channel such as a computer keyboard and screen so the result would not depend on the machine's ability to render words as speech. If the evaluator cannot reliably tell the machine from the human, the machine is said to have passed the test. The test results do not depend on the machine's ability to give correct answers to questions, only how closely its answers resemble those a human would give.

type system
In programming languages, a set of rules that assigns a property called type to the various constructs of a computer program, such as variables, expressions, functions, or modules. These types formalize and enforce the otherwise implicit categories the programmer uses for algebraic data types, data structures, or other components (e.g. ""string"", ""array of float"", ""function returning boolean""). The main purpose of a type system is to reduce possibilities for bugs in computer programs by defining interfaces between different parts of a computer program, and then checking that the parts have been connected in a consistent way. This checking can happen statically (at compile time), dynamically (at run time), or as a combination of static and dynamic checking. Type systems have other purposes as well, such as expressing business rules, enabling certain compiler optimizations, allowing for multiple dispatch, providing a form of documentation, etc.

U
unsupervised learning
A type of self-organized Hebbian learning that helps find previously unknown patterns in data set without pre-existing labels. It is also known as self-organization and allows modeling probability densities of given inputs. It is one of the main three categories of machine learning, along with supervised and reinforcement learning. Semi-supervised learning has also been described and is a hybridization of supervised and unsupervised techniques.

V
vision processing unit (VPU)
A type of microprocessor designed to accelerate machine vision tasks.Value-alignment complete – Analogous to an AI-complete problem, a value-alignment complete problem is a problem where the AI control problem needs to be fully solved to solve it.

W
Watson
A question-answering computer system capable of answering questions posed in natural language, developed in IBM's DeepQA project by a research team led by principal investigator David Ferrucci. Watson was named after IBM's first CEO, industrialist Thomas J. Watson.

weak AI
Also narrow AI.
Artificial intelligence that is focused on one narrow task.

World Wide Web Consortium (W3C)
The main international standards organization for the World Wide Web (abbreviated WWW or W3).

See also
Artificial intelligence

References


== Notes ==",https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence,"['All articles with unsourced statements', 'Articles with short description', 'Articles with unsourced statements from January 2019', 'Artificial intelligence', 'CS1: long volume value', 'CS1 German-language sources (de)', 'CS1 errors: external links', 'CS1 errors: missing periodical', 'CS1 maint: archived copy as title', 'CS1 maint: bot: original URL status unknown', 'Glossaries of science', 'Glossaries of technology', 'Harv and Sfn no-target errors', 'Machine learning', 'Short description is different from Wikidata', 'Use dmy dates from September 2017', 'Webarchive template wayback links']",Data Science
97,Grammar induction,"Grammar induction (or grammatical inference) is the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.

Grammar classes
Grammatical inference has often been very focused on the problem of learning finite state machines of various types (see the article Induction of regular languages for details on these approaches), since there have been efficient algorithms for this problem since the 1980s.
Since the beginning of the century, these approaches have been extended to the problem of inference of context-free grammars and richer formalisms, such as multiple context-free grammars and parallel multiple context-free grammars.
Other classes of grammars for which grammatical inference has been studied are combinatory categorial grammars, stochastic context-free grammars, contextual grammars and pattern languages.

Learning models
The simplest form of learning is where the learning algorithm merely receives a set of examples drawn from the language in question: the aim is to learn the language from examples of it (and, rarely, from counter-examples, that is, example that do not belong to the language).
However, other learning models have been studied. One frequently studied alternative is the case where the learner can ask membership queries as in the exact query learning model or minimally adequate teacher model introduced by Angluin.

Methodologies
There is a wide variety of methods for grammatical inference.  Two of the classic sources are Fu (1977) and Fu (1982). Duda, Hart & Stork (2001) also devote a brief section to the problem, and cite a number of references.  The basic trial-and-error method they present is discussed below. For approaches to infer subclasses of regular languages in particular, see Induction of regular languages. A more recent textbook is de la Higuera (2010), which covers the theory of grammatical inference of regular languages and finite state automata. D'Ulizia, Ferri and Grifoni provide a survey that explores grammatical inference methods for natural languages.

Grammatical inference by trial-and-error
The method proposed in Section 8.7 of Duda, Hart & Stork (2001) suggests successively guessing grammar rules (productions) and testing them against positive and negative observations.  The rule set is expanded so as to be able to generate each positive example, but if a given rule set also generates a negative example, it must be discarded.  This particular approach can be characterized as ""hypothesis testing"" and bears some similarity to Mitchel's version space algorithm. The Duda, Hart & Stork (2001) text provide a simple example which nicely illustrates the process, but the feasibility of such an unguided trial-and-error approach for more substantial problems is dubious.

Grammatical inference by genetic algorithms
Grammatical induction using evolutionary algorithms is the process of evolving a representation of the grammar of a target language through some evolutionary process. Formal grammars can easily be represented as tree structures of production rules that can be subjected to evolutionary operators. Algorithms of this sort stem from the genetic programming paradigm pioneered by John Koza. Other early work on simple formal languages used the binary string representation of genetic algorithms, but the inherently hierarchical structure of grammars couched in the EBNF language made trees a more flexible approach.
Koza represented Lisp programs as trees. He was able to find analogues to the genetic operators within the standard set of tree operators. For example, swapping sub-trees is equivalent to the corresponding process of genetic crossover, where sub-strings of a genetic code are transplanted into an individual of the next generation. Fitness is measured by scoring the output from the functions of the Lisp code. Similar analogues between the tree structured lisp representation and the representation of grammars as trees, made the application of genetic programming techniques possible for grammar induction.
In the case of grammar induction, the transplantation of sub-trees corresponds to the swapping of production rules that enable the parsing of phrases from some language. The fitness operator for the grammar is based upon some measure of how well it performed in parsing some group of sentences from the target language. In a tree representation of a grammar, a terminal symbol of a production rule corresponds to a leaf node of the tree. Its parent nodes corresponds to a non-terminal symbol (e.g. a noun phrase or a verb phrase) in the rule set. Ultimately, the root node might correspond to a sentence non-terminal.

Grammatical inference by greedy algorithms
Like all greedy algorithms, greedy grammar inference algorithms make, in iterative manner, decisions that seem to be the best at that stage.
The decisions made usually deal with things like the creation of new rules, the removal of existing rules, the choice of a rule to be applied or the merging of some existing rules.
Because there are several ways to define 'the stage' and 'the best', there are also several greedy grammar inference algorithms.
These context-free grammar generating algorithms make the decision after every read symbol:

Lempel-Ziv-Welch algorithm creates a context-free grammar in a deterministic way such that it is necessary to store only the start rule of the generated grammar.
Sequitur and its modifications.These context-free grammar generating algorithms first read the whole given symbol-sequence and then start to make decisions:

Byte pair encoding and its optimizations.

Distributional learning
A more recent approach is based on distributional learning. Algorithms using these approaches have been applied to learning context-free grammars and mildly context-sensitive languages and have been proven to be correct and efficient for large subclasses of these grammars.

Learning of pattern languages
Angluin defines a pattern to be ""a string of constant symbols from Σ and variable symbols from a disjoint set"".
The language of such a pattern is the set of all its nonempty ground instances  i.e. all strings resulting from consistent replacement of its variable symbols by nonempty strings of constant symbols.
A pattern is called descriptive for a finite input set of strings if its language is minimal (with respect to set inclusion) among all pattern languages subsuming the input set.
Angluin gives a polynomial algorithm to compute, for a given input string set, all descriptive patterns in one variable x.
To this end, she builds an automaton representing all possibly relevant patterns; using sophisticated arguments about word lengths, which rely on x being the only variable, the state count can be drastically reduced.Erlebach et al. give a more efficient version of Angluin's pattern learning algorithm, as well as a parallelized version.Arimura et al. show that a language class  obtained from limited unions of patterns can be learned in polynomial time.

Pattern theory
Pattern theory, formulated by Ulf Grenander, is a mathematical formalism to describe knowledge of the world as patterns. It differs from other approaches to artificial intelligence in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns; rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language.
In addition to the new algebraic vocabulary, its statistical approach was novel in its aim to:

Identify the hidden variables of a data set using real world data rather than artificial stimuli, which was commonplace at the time.
Formulate prior distributions for hidden variables and models for the observed variables that form the vertices of a Gibbs-like graph.
Study the randomness and variability of these graphs.
Create the basic classes of stochastic models applied by listing the deformations of the patterns.
Synthesize (sample) from the models, not just analyze signals with it.Broad in its mathematical coverage, pattern theory spans algebra and statistics, as well as local topological and global entropic properties.

Applications
The principle of grammar induction has been applied to other aspects of natural language processing, and has been applied (among many other problems) to semantic parsing, natural language understanding, example-based translation, morpheme analysis, and place name derivations. Grammar induction has also been used for lossless data compression and statistical inference via minimum message length (MML) and minimum description length (MDL) principles. Grammar induction has also been used in some probabilistic models of language acquisition.

See also
Artificial grammar learning#Artificial intelligence
Example-based machine translation
Inductive programming
Kolmogorov complexity
Language identification in the limit
Straight-line grammar
Syntactic pattern recognition

Notes
References
Sources
Duda, Richard O.; Hart, Peter E.; Stork, David G. (2001), Pattern Classification (2 ed.), New York: John Wiley & Sons
Fu, King Sun (1982), Syntactic Pattern Recognition and Applications, Englewood Cliffs, NJ: Prentice-Hall
Fu, King Sun (1977), Syntactic Pattern Recognition, Applications, Berlin: Springer-Verlag
Horning, James Jay (1969), A Study of Grammatical Inference (Ph.D. Thesis ed.), Stanford: Stanford University Computer Science Department, ProQuest 302483145
Gold, E. Mark (1967), Language Identification in the Limit, 10, Information and Control, pp. 447–474, archived from the original on 2016-08-28, retrieved 2016-09-04
Gold, E. Mark (1967), Language Identification in the Limit (PDF), 10, Information and Control, pp. 447–474",https://en.wikipedia.org/wiki/Grammar_induction,"['All articles with dead external links', 'All articles with unsourced statements', 'Articles with dead external links from February 2018', 'Articles with dead external links from March 2018', 'Articles with dead external links from September 2018', 'Articles with permanently dead external links', 'Articles with unsourced statements from August 2007', 'Articles with unsourced statements from August 2017', 'Articles with unsourced statements from February 2018', 'Computational linguistics', 'Genetic programming', 'Grammar', 'Inference', 'Machine learning', 'Natural language processing']",Data Science
98,Graphical model,"A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics—particularly Bayesian statistics—and machine learning.

Types of graphical models
Generally, probabilistic graphical models use a graph-based representation as the foundation for encoding a  distribution over a multi-dimensional space and a graph that is a compact or factorized representation of a set of independences that hold in the specific distribution. Two branches of graphical representations of distributions are commonly used, namely, Bayesian networks and Markov random fields. Both families encompass the properties of factorization and independences, but they differ in the set of independences they can encode and the factorization of the distribution that they induce.

Bayesian network
If the network structure of the model is a directed acyclic graph, the model represents a factorization of the joint probability of all random variables.  More precisely, if the events are 
  
    
      
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{1},\ldots ,X_{n}}
   then the joint probability satisfies

  
    
      
        P
        [
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        ]
        =
        
          ∏
          
            i
            =
            1
          
          
            n
          
        
        P
        [
        
          X
          
            i
          
        
        
          |
        
        
          pa
        
        (
        
          X
          
            i
          
        
        )
        ]
      
    
    {\displaystyle P[X_{1},\ldots ,X_{n}]=\prod _{i=1}^{n}P[X_{i}|{\text{pa}}(X_{i})]}
  where 
  
    
      
        
          pa
        
        (
        
          X
          
            i
          
        
        )
      
    
    {\displaystyle {\text{pa}}(X_{i})}
   is the set of parents of node 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
   (nodes with edges directed towards 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
  ).  In other words, the joint distribution factors into a product of conditional distributions. For example, the graphical model in the Figure shown above (which is actually not a directed acyclic graph, but an ancestral graph) consists of the random variables 
  
    
      
        A
        ,
        B
        ,
        C
        ,
        D
      
    
    {\displaystyle A,B,C,D}
  
with a joint probability density that factors as

  
    
      
        P
        [
        A
        ,
        B
        ,
        C
        ,
        D
        ]
        =
        P
        [
        A
        ]
        ⋅
        P
        [
        B
        ]
        ⋅
        P
        [
        C
        ,
        D
        
          |
        
        A
        ,
        B
        ]
      
    
    {\displaystyle P[A,B,C,D]=P[A]\cdot P[B]\cdot P[C,D|A,B]}
  Any two nodes are conditionally independent given the values of their parents.  In general, any two sets of nodes are conditionally independent given a third set if a criterion called d-separation holds in the graph.  Local independences and global independences are equivalent in Bayesian networks.
This type of graphical model is known as a directed graphical model, Bayesian network, or belief network. Classic machine learning models like hidden Markov models, neural networks and newer models such as variable-order Markov models can be considered special cases of Bayesian networks.

Other types
Naive Bayes classifier where we use a tree with a single root
Dependency network  where cycles are allowed
Tree-augmented classifier or TAN model
A factor graph is an undirected bipartite graph connecting variables and factors. Each factor represents a function over the variables it is connected to. This is a helpful representation for understanding and implementing belief propagation.
A clique tree or junction tree is a tree of cliques, used in the junction tree algorithm.
A chain graph is a graph which may have both directed and undirected edges, but without any directed cycles (i.e. if we start at any vertex and move along the graph respecting the directions of any arrows, we cannot return to the vertex we started from if we have passed an arrow). Both directed acyclic graphs and undirected graphs are special cases of chain graphs, which can therefore provide a way of unifying and generalizing Bayesian and Markov networks.
An ancestral graph is a further extension, having directed, bidirected and undirected edges.
Random field techniques
A Markov random field, also known as a Markov network, is a model over an undirected graph. A graphical model with many repeated subunits can be represented with plate notation.
A conditional random field is a discriminative model specified over an undirected graph.
A restricted Boltzmann machine is a bipartite generative model specified over an undirected graph.

Applications
The framework of the models, which provides algorithms for discovering and analyzing structure in complex distributions  to describe them succinctly and extract the unstructured information, allows them to be constructed and utilized effectively. Applications of graphical models include causal inference, information extraction, speech recognition, computer vision, decoding of low-density parity-check codes, modeling of gene regulatory networks, gene finding and diagnosis of diseases, and graphical models for protein structure.

See also
Belief propagation
Structural equation model

Notes
Further reading
Books and book chapters
Barber, David (2012). Bayesian Reasoning and Machine Learning. Cambridge University Press. ISBN 978-0-521-51814-7.Bishop, Christopher M. (2006). ""Chapter 8. Graphical Models"" (PDF). Pattern Recognition and Machine Learning. Springer. pp. 359–422. ISBN 978-0-387-31073-2. MR 2247587.
Cowell, Robert G.; Dawid, A. Philip; Lauritzen, Steffen L.; Spiegelhalter, David J. (1999). Probabilistic networks and expert systems. Berlin: Springer. ISBN 978-0-387-98767-5. MR 1697175. A more advanced and statistically oriented book
Jensen, Finn (1996). An introduction to Bayesian networks. Berlin: Springer. ISBN 978-0-387-91502-9.
Pearl, Judea (1988). Probabilistic Reasoning in Intelligent Systems (2nd revised ed.). San Mateo, CA: Morgan Kaufmann. ISBN 978-1-55860-479-7. MR 0965765. A computational reasoning approach, where the relationships between graphs and probabilities were formally introduced.

Journal articles
Edoardo M. Airoldi (2007). ""Getting Started in Probabilistic Graphical Models"". PLOS Computational Biology. 3 (12): e252. doi:10.1371/journal.pcbi.0030252. PMC 2134967. PMID 18069887.
Jordan, M. I. (2004). ""Graphical Models"". Statistical Science. 19: 140–155. doi:10.1214/088342304000000026.
Ghahramani, Zoubin (May 2015). ""Probabilistic machine learning and artificial intelligence"". Nature. 521 (7553): 452–459. doi:10.1038/nature14541. PMID 26017444. S2CID 216356.

Other
Heckerman's Bayes Net Learning Tutorial
A Brief Introduction to Graphical Models and Bayesian Networks
Sargur Srihari's lecture slides on probabilistic graphical models

External links
Graphical models and Conditional Random Fields
Probabilistic Graphical Models taught by Eric Xing at CMU",https://en.wikipedia.org/wiki/Graphical_model,"['All articles lacking in-text citations', 'Articles lacking in-text citations from May 2017', 'Bayesian statistics', 'Graphical models']",Data Science
99,Hierarchical clustering,"In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:
Agglomerative: This is a ""bottom-up"" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.
Divisive: This is a ""top-down"" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.
The standard algorithm for hierarchical agglomerative clustering (HAC) has a time complexity of 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{3})}
   and requires 
  
    
      
        Ω
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle \Omega (n^{2})}
   memory, which makes it too slow for even medium data sets. However, for some special cases, optimal efficient agglomerative methods (of complexity 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{2})}
  ) are known: SLINK for single-linkage and CLINK for complete-linkage clustering. With a heap, the runtime of the general case can be reduced to 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            2
          
        
        log
        ⁡
        n
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{2}\log n)}
  , an improvement on the aforementioned bound of 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{3})}
  , at the cost of further increasing the memory requirements. In many cases, the memory overheads of this approach are too large to make it practically usable.
Except for the special case of single-linkage, none of the algorithms (except exhaustive search in 
  
    
      
        
          
            O
          
        
        (
        
          2
          
            n
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(2^{n})}
  ) can be guaranteed to find the optimum solution.
Divisive clustering with an exhaustive search is 
  
    
      
        
          
            O
          
        
        (
        
          2
          
            n
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(2^{n})}
  , but it is common to use faster heuristics to choose splits, such as k-means.

Cluster dissimilarity
In order to decide which clusters should be combined (for agglomerative), or where a cluster should be split (for divisive), a measure of dissimilarity between sets of observations is required. In most methods of hierarchical clustering, this is achieved by use of an appropriate metric (a measure of distance between pairs of observations), and a linkage criterion which specifies the dissimilarity of sets as a function of the pairwise distances of observations in the sets.

Metric
The choice of an appropriate metric will influence the shape of the clusters, as some elements may be relatively closer to one another under one metric than another. For example, in two dimensions, under the Manhattan distance metric, the distance between the origin (0,0) and (0.5, 0.5) is the same as the distance between the origin and (0, 1), while under the Euclidean distance metric the latter is strictly greater.
Some commonly used metrics for hierarchical clustering are:
For text or other non-numeric data, metrics such as the Hamming distance or Levenshtein distance are often used.
A review of cluster analysis in health psychology research found that the most common distance measure in published studies in that research area is the Euclidean distance or the squared Euclidean distance.

Linkage criteria
The linkage criterion determines the distance between sets of observations as a function of the pairwise distances between observations.
Some commonly used linkage criteria between two sets of observations A and B are:
where d is the chosen metric.  Other linkage criteria include:

The sum of all intra-cluster variance.
The increase in variance for the cluster being merged (Ward's criterion).
The probability that candidate clusters spawn from the same distribution function (V-linkage).
The product of in-degree and out-degree on a k-nearest-neighbour graph (graph degree linkage).
The increment of some cluster descriptor (i.e., a quantity defined for measuring the quality of a cluster) after merging two clusters.

Discussion
Hierarchical clustering has the distinct advantage that any valid measure of distance can be used. In fact, the observations themselves are not required: all that is used is a matrix of distances.

Agglomerative clustering example
For example, suppose this data is to be clustered, and the Euclidean distance is the distance metric.
The hierarchical clustering dendrogram would be as such:

Cutting the tree at a given height will give a partitioning clustering at a selected precision. In this example, cutting after the second row (from the top) of the dendrogram will yield clusters {a} {b c} {d e} {f}. Cutting after the third row will yield clusters {a} {b c} {d e f}, which is a coarser clustering, with a smaller number but larger clusters.
This method builds the hierarchy from the individual elements by progressively merging clusters. In our example, we have six elements {a} {b} {c} {d} {e} and {f}. The first step is to determine which elements to merge in a cluster. Usually, we want to take the two closest elements, according to the chosen distance.
Optionally, one can also construct a distance matrix at this stage, where the number in the i-th row j-th column is the distance between the i-th and j-th elements. Then, as clustering progresses, rows and columns are merged as the clusters are merged and the distances updated. This is a common way to implement this type of clustering, and has the benefit of caching distances between clusters. A simple agglomerative clustering algorithm is described in the single-linkage clustering page; it can easily be adapted to different types of linkage (see below).
Suppose we have merged the two closest elements b and c, we now have the following clusters {a}, {b, c}, {d}, {e} and {f}, and want to merge them further. To do that, we need to take the distance between {a} and {b c}, and therefore define the distance between two clusters.
Usually the distance between two clusters 
  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\mathcal {A}}}
   and 
  
    
      
        
          
            B
          
        
      
    
    {\displaystyle {\mathcal {B}}}
   is one of the following:

The maximum distance between elements of each cluster (also called complete-linkage clustering):
  
    
      
        max
        {
        
        d
        (
        x
        ,
        y
        )
        :
        x
        ∈
        
          
            A
          
        
        ,
        
        y
        ∈
        
          
            B
          
        
        
        }
        .
      
    
    {\displaystyle \max\{\,d(x,y):x\in {\mathcal {A}},\,y\in {\mathcal {B}}\,\}.}
  The minimum distance between elements of each cluster (also called single-linkage clustering):
  
    
      
        min
        {
        
        d
        (
        x
        ,
        y
        )
        :
        x
        ∈
        
          
            A
          
        
        ,
        
        y
        ∈
        
          
            B
          
        
        
        }
        .
      
    
    {\displaystyle \min\{\,d(x,y):x\in {\mathcal {A}},\,y\in {\mathcal {B}}\,\}.}
  The mean distance between elements of each cluster (also called average linkage clustering, used e.g. in UPGMA):
  
    
      
        
          
            1
            
              
                |
              
              
                
                  A
                
              
              
                |
              
              ⋅
              
                |
              
              
                
                  B
                
              
              
                |
              
            
          
        
        
          ∑
          
            x
            ∈
            
              
                A
              
            
          
        
        
          ∑
          
            y
            ∈
            
              
                B
              
            
          
        
        d
        (
        x
        ,
        y
        )
        .
      
    
    {\displaystyle {1 \over {|{\mathcal {A}}|\cdot |{\mathcal {B}}|}}\sum _{x\in {\mathcal {A}}}\sum _{y\in {\mathcal {B}}}d(x,y).}
  The sum of all intra-cluster variance.
The increase in variance for the cluster being merged (Ward's method)
The probability that candidate clusters spawn from the same distribution function (V-linkage).In case of tied minimum distances, a pair is randomly chosen, thus being able to generate several structurally different dendrograms. Alternatively, all tied pairs may be joined at the same time, generating a unique dendrogram.One can always decide to stop clustering when there is a sufficiently small number of clusters (number criterion). Some linkages may also guarantee that agglomeration occurs at a greater distance between clusters than the previous agglomeration, and then one can stop clustering when the clusters are too far apart to be merged (distance criterion). However, this is not the case of, e.g., the centroid linkage where the so-called reversals (inversions, departures from ultrametricity) may occur.

Divisive clustering
The basic principle of divisive clustering was published as the DIANA (DIvisive ANAlysis Clustering) algorithm. Initially, all data is in the same cluster, and the largest cluster is split until every object is separate.
Because there exist 
  
    
      
        O
        (
        
          2
          
            n
          
        
        )
      
    
    {\displaystyle O(2^{n})}
   ways of splitting each cluster, heuristics are needed. DIANA chooses the object with the maximum average dissimilarity and then moves all objects to this cluster that are more similar to the new cluster than to the remainder.

Software
Open source implementations
ALGLIB implements several hierarchical clustering algorithms (single-link, complete-link, Ward) in C++ and C# with O(n²) memory and O(n³) run time.
ELKI includes multiple hierarchical clustering algorithms, various linkage strategies and also includes the efficient SLINK, CLINK and Anderberg algorithms, flexible cluster extraction from dendrograms and various other cluster analysis algorithms.
Octave, the GNU analog to MATLAB implements hierarchical clustering in function ""linkage"".
Orange, a data mining software suite, includes hierarchical clustering with interactive dendrogram visualisation.
R has many packages that provide functions for hierarchical clustering.
SciPy implements hierarchical clustering in Python, including the efficient SLINK algorithm.
scikit-learn also implements hierarchical clustering in Python.
Weka includes hierarchical cluster analysis.

Commercial implementations
MATLAB includes hierarchical cluster analysis.
SAS includes hierarchical cluster analysis in PROC CLUSTER.
Mathematica includes a Hierarchical Clustering Package.
NCSS includes hierarchical cluster analysis.
SPSS includes hierarchical cluster analysis.
Qlucore Omics Explorer includes hierarchical cluster analysis.
Stata includes hierarchical cluster analysis.
CrimeStat includes a nearest neighbor hierarchical cluster algorithm with a graphical output for a Geographic Information System.

See also
References
Further reading
Kaufman, L.; Rousseeuw, P.J. (1990). Finding Groups in Data: An Introduction to Cluster Analysis (1 ed.). New York: John Wiley. ISBN 0-471-87876-6.
Hastie, Trevor; Tibshirani, Robert; Friedman, Jerome (2009). ""14.3.12 Hierarchical clustering"". The Elements of Statistical Learning (2nd ed.). New York: Springer. pp. 520–528. ISBN 978-0-387-84857-0. Archived from the original (PDF) on 2009-11-10. Retrieved 2009-10-20.",https://en.wikipedia.org/wiki/Hierarchical_clustering,"['All articles with unsourced statements', 'Articles with short description', 'Articles with unsourced statements from April 2009', 'Cluster analysis algorithms', 'Network analysis', 'Short description is different from Wikidata', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers']",Data Science
100,Hidden Markov model,"Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process – call it 
  
    
      
        X
      
    
    {\displaystyle X}
   – with unobservable (""hidden"") states. HMM assumes that there is another process 
  
    
      
        Y
      
    
    {\displaystyle Y}
   whose behavior ""depends"" on 
  
    
      
        X
      
    
    {\displaystyle X}
  . The goal is to learn about 
  
    
      
        X
      
    
    {\displaystyle X}
   by observing 
  
    
      
        Y
      
    
    {\displaystyle Y}
  . HMM stipulates that, for each time instance 
  
    
      
        
          n
          
            0
          
        
      
    
    {\displaystyle n_{0}}
  , the conditional probability distribution of 
  
    
      
        
          Y
          
            
              n
              
                0
              
            
          
        
      
    
    {\displaystyle Y_{n_{0}}}
   given the history 
  
    
      
        {
        
          X
          
            n
          
        
        =
        
          x
          
            n
          
        
        
          }
          
            n
            ≤
            
              n
              
                0
              
            
          
        
      
    
    {\displaystyle \{X_{n}=x_{n}\}_{n\leq n_{0}}}
   must not depend on 
  
    
      
        {
        
          x
          
            n
          
        
        
          }
          
            n
            <
            
              n
              
                0
              
            
          
        
      
    
    {\displaystyle \{x_{n}\}_{n<n_{0}}}
  .
Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics.

Definition
Let 
  
    
      
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{n}}
   and 
  
    
      
        
          Y
          
            n
          
        
      
    
    {\displaystyle Y_{n}}
   be discrete-time stochastic processes and 
  
    
      
        n
        ≥
        1
      
    
    {\displaystyle n\geq 1}
  . The pair 
  
    
      
        (
        
          X
          
            n
          
        
        ,
        
          Y
          
            n
          
        
        )
      
    
    {\displaystyle (X_{n},Y_{n})}
   is a hidden markov model if

  
    
      
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{n}}
   is a Markov process whose behavior is not directly observable (""hidden"");

  
    
      
        
          
            P
          
        
        ⁡
        
          
            (
          
        
        
          Y
          
            n
          
        
        ∈
        A
         
        
          
            |
          
        
         
        
          X
          
            1
          
        
        =
        
          x
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        =
        
          x
          
            n
          
        
        
          
            )
          
        
        =
        
          
            P
          
        
        ⁡
        
          
            (
          
        
        
          Y
          
            n
          
        
        ∈
        A
         
        
          
            |
          
        
         
        
          X
          
            n
          
        
        =
        
          x
          
            n
          
        
        
          
            )
          
        
        ,
      
    
    {\displaystyle \operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\ {\bigl |}\ X_{1}=x_{1},\ldots ,X_{n}=x_{n}{\bigr )}=\operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\ {\bigl |}\ X_{n}=x_{n}{\bigr )},}
  for every 
  
    
      
        n
        ≥
        1
        ,
      
    
    {\displaystyle n\geq 1,}
   
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        ,
      
    
    {\displaystyle x_{1},\ldots ,x_{n},}
   and an arbitrary (measurable) set 
  
    
      
        A
      
    
    {\displaystyle A}
  .

Terminology
The states of the process 
  
    
      
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{n}}
   are called hidden states, and 
  
    
      
        
          
            P
          
        
        ⁡
        
          
            (
          
        
        
          Y
          
            n
          
        
        ∈
        A
         
        
          
            |
          
        
         
        
          X
          
            n
          
        
        =
        
          x
          
            n
          
        
        
          
            )
          
        
      
    
    {\displaystyle \operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\ {\bigl |}\ X_{n}=x_{n}{\bigr )}}
   is called emission probability or output probability.

Examples
Drawing balls from hidden urns
In its discrete form, a hidden Markov process can be visualized as a generalization of the urn problem with replacement (where each item from the urn is returned to the original urn before the next step). Consider this example: in a room that is not visible to an observer there is a genie. The room contains urns X1, X2, X3, ... each of which contains a known mix of balls, each ball labeled y1, y2, y3, ... .  The genie chooses an urn in that room and randomly draws a ball from that urn.  It then puts the ball onto a conveyor belt, where the observer can observe the sequence of the balls but not the sequence of urns from which they were drawn. The genie has some procedure to choose urns; the choice of the urn for the n-th ball depends only upon a random number and the choice of the urn for the (n − 1)-th ball.  The choice of urn does not directly depend on the urns chosen before this single previous urn; therefore, this is called a Markov process. It can be described by the upper part of Figure 1.
The Markov process itself cannot be observed, only the sequence of labeled balls, thus this arrangement is called a ""hidden Markov process"". This is illustrated by the lower part of the diagram shown in Figure 1, where one can see that balls y1, y2, y3, y4 can be drawn at each state. Even if the observer knows the composition of the urns and has just observed a sequence of three balls, e.g. y1, y2 and y3 on the conveyor belt, the observer still cannot be sure which urn (i.e., at which state) the genie has drawn the third ball from. However, the observer can work out other information, such as the likelihood that the third ball came from each of the urns.

Weather guessing game
Consider two friends, Alice and Bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no definite information about the weather, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been like.
Alice believes that the weather operates as a discrete Markov chain. There are two states, ""Rainy"" and ""Sunny"", but she cannot observe them directly, that is, they are hidden from her. On each day, there is a certain chance that Bob will perform one of the following activities, depending on the weather: ""walk"", ""shop"", or ""clean"". Since Bob tells Alice about his activities, those are the observations. The entire system is that of a hidden Markov model (HMM).
Alice knows the general weather trends in the area, and what Bob likes to do on average. In other words, the parameters of the HMM are known. They can be represented as follows in Python:

In this piece of code, start_probability represents Alice's belief about which state the HMM is in when Bob first calls her (all she knows is that it tends to be rainy on average). The particular probability distribution used here is not the equilibrium one, which is (given the transition probabilities) approximately {'Rainy': 0.57, 'Sunny': 0.43}. The transition_probability represents the change of the weather in the underlying Markov chain. In this example, there is only a 30% chance that tomorrow will be sunny if today is rainy. The emission_probability represents how likely Bob is to perform a certain activity on each day. If it is rainy, there is a 50% chance that he is cleaning his apartment; if it is sunny, there is a 60% chance that he is outside for a walk.

A similar example is further elaborated in the Viterbi algorithm page.

Structural architecture
The diagram below shows the general architecture of an instantiated HMM. Each oval shape represents a random variable that can adopt any of a number of values. The random variable x(t) is the hidden state at time t (with the model from the above diagram, x(t) ∈ { x1, x2, x3 }). The random variable y(t) is the observation at time t (with y(t) ∈ { y1, y2, y3, y4 }). The arrows in the diagram (often called a trellis diagram) denote conditional dependencies.
From the diagram, it is clear that the conditional probability distribution of the hidden variable x(t) at time t, given the values of the hidden variable x at all times, depends only on the value of the hidden variable x(t − 1); the values at time t − 2 and before have no influence. This is called the Markov property. Similarly, the value of the observed variable y(t) only depends on the value of the hidden variable x(t) (both at time t).
In the standard type of hidden Markov model considered here, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution).  The parameters of a hidden Markov model are of two types, transition probabilities and emission probabilities (also known as output probabilities).  The transition probabilities control the way the hidden state at time t is chosen given the hidden state at time 
  
    
      
        t
        −
        1
      
    
    {\displaystyle t-1}
  .
The hidden state space is assumed to consist of one of N possible values, modelled as a categorical distribution. (See the section below on extensions for other possibilities.) This means that for each of the N possible states that a hidden variable at time t can be in, there is a transition probability from this state to each of the N possible states of the hidden variable at time 
  
    
      
        t
        +
        1
      
    
    {\displaystyle t+1}
  , for a total of 
  
    
      
        
          N
          
            2
          
        
      
    
    {\displaystyle N^{2}}
   transition probabilities. Note that the set of transition probabilities for transitions from any given state must sum to 1. Thus, the 
  
    
      
        N
        ×
        N
      
    
    {\displaystyle N\times N}
   matrix of transition probabilities is a Markov matrix. Because any one transition probability can be determined once the others are known, there are a total of 
  
    
      
        N
        (
        N
        −
        1
        )
      
    
    {\displaystyle N(N-1)}
   transition parameters.
In addition, for each of the N possible states, there is a set of emission probabilities governing the distribution of the observed variable at a particular time given the state of the hidden variable at that time.  The size of this set depends on the nature of the observed variable.  For example, if the observed variable is discrete with M possible values, governed by a categorical distribution, there will be 
  
    
      
        M
        −
        1
      
    
    {\displaystyle M-1}
   separate parameters, for a total of 
  
    
      
        N
        (
        M
        −
        1
        )
      
    
    {\displaystyle N(M-1)}
   emission parameters over all hidden states.  On the other hand, if the observed variable is an M-dimensional vector distributed according to an arbitrary multivariate Gaussian distribution, there will be M parameters controlling the means and 
  
    
      
        
          
            
              M
              (
              M
              +
              1
              )
            
            2
          
        
      
    
    {\displaystyle {\frac {M(M+1)}{2}}}
   parameters controlling the covariance matrix, for a total of 
  
    
      
        N
        
          (
          
            M
            +
            
              
                
                  M
                  (
                  M
                  +
                  1
                  )
                
                2
              
            
          
          )
        
        =
        
          
            
              N
              M
              (
              M
              +
              3
              )
            
            2
          
        
        =
        O
        (
        N
        
          M
          
            2
          
        
        )
      
    
    {\displaystyle N\left(M+{\frac {M(M+1)}{2}}\right)={\frac {NM(M+3)}{2}}=O(NM^{2})}
   emission parameters. (In such a case, unless the value of M is small, it may be more practical to restrict the nature of the covariances between individual elements of the observation vector, e.g. by assuming that the elements are independent of each other, or less restrictively, are independent of all but a fixed number of adjacent elements.)

Inference
Several inference problems are associated with hidden Markov models, as outlined below.

Probability of an observed sequence
The task is to compute in a best way, given the parameters of the model, the probability of a particular output sequence.  This requires summation over all possible state sequences:
The probability of observing a sequence

  
    
      
        Y
        =
        y
        (
        0
        )
        ,
        y
        (
        1
        )
        ,
        …
        ,
        y
        (
        L
        −
        1
        )
        
      
    
    {\displaystyle Y=y(0),y(1),\dots ,y(L-1)\,}
  of length L is given by

  
    
      
        P
        (
        Y
        )
        =
        
          ∑
          
            X
          
        
        P
        (
        Y
        ∣
        X
        )
        P
        (
        X
        )
        ,
        
      
    
    {\displaystyle P(Y)=\sum _{X}P(Y\mid X)P(X),\,}
  where the sum runs over all possible hidden-node sequences

  
    
      
        X
        =
        x
        (
        0
        )
        ,
        x
        (
        1
        )
        ,
        …
        ,
        x
        (
        L
        −
        1
        )
        .
        
      
    
    {\displaystyle X=x(0),x(1),\dots ,x(L-1).\,}
  Applying the principle of dynamic programming, this problem, too, can be handled efficiently using the forward algorithm.

Probability of the latent variables
A number of related tasks ask about the probability of one or more of the latent variables, given the model's parameters and a sequence of observations 
  
    
      
        y
        (
        1
        )
        ,
        …
        ,
        y
        (
        t
        )
        .
      
    
    {\displaystyle y(1),\dots ,y(t).}

Filtering
The task is to compute, given the model's parameters and a sequence of observations, the distribution over hidden states of the last latent variable at the end of the sequence, i.e. to compute 
  
    
      
        P
        (
        x
        (
        t
        )
         
        
          |
        
         
        y
        (
        1
        )
        ,
        …
        ,
        y
        (
        t
        )
        )
      
    
    {\displaystyle P(x(t)\ |\ y(1),\dots ,y(t))}
  .  This task is normally used when the sequence of latent variables is thought of as the underlying states that a process moves through at a sequence of points of time, with corresponding observations at each point in time.  Then, it is natural to ask about the state of the process at the end.
This problem can be handled efficiently using the forward algorithm.

Smoothing
This is similar to filtering but asks about the distribution of a latent variable somewhere in the middle of a sequence, i.e. to compute 
  
    
      
        P
        (
        x
        (
        k
        )
         
        
          |
        
         
        y
        (
        1
        )
        ,
        …
        ,
        y
        (
        t
        )
        )
      
    
    {\displaystyle P(x(k)\ |\ y(1),\dots ,y(t))}
   for some 
  
    
      
        k
        <
        t
      
    
    {\displaystyle k<t}
  .  From the perspective described above, this can be thought of as the probability distribution over hidden states for a point in time k in the past, relative to time t.
The forward-backward algorithm is a good method for computing the smoothed values for all hidden state variables.

Most likely explanation
The task, unlike the previous two, asks about the joint probability of the entire sequence of hidden states that generated a particular sequence of observations (see illustration on the right).  This task is generally applicable when HMM's are applied to different sorts of problems from those for which the tasks of filtering and smoothing are applicable.  An example is part-of-speech tagging, where the hidden states represent the underlying parts of speech corresponding to an observed sequence of words.  In this case, what is of interest is the entire sequence of parts of speech, rather than simply the part of speech for a single word, as filtering or smoothing would compute.
This task requires finding a maximum over all possible state sequences, and can be solved efficiently by the Viterbi algorithm.

Statistical significance
For some of the above problems, it may also be interesting to ask about statistical significance.  What is the probability that a sequence drawn from some null distribution will have an HMM probability (in the case of the forward algorithm) or a maximum state sequence probability (in the case of the Viterbi algorithm) at least as large as that of a particular output sequence?  When an HMM is used to evaluate the relevance of a hypothesis for a particular output sequence, the statistical significance indicates the false positive rate associated with failing to reject the hypothesis for the output sequence.

Learning
The parameter learning task in HMMs is to find, given an output sequence or a set of such sequences, the best set of state transition and emission probabilities. The task is usually to derive the maximum likelihood estimate of the parameters of the HMM given the set of output sequences. No tractable algorithm is known for solving this problem exactly, but a local maximum likelihood can be derived efficiently using the Baum–Welch algorithm or the Baldi–Chauvin algorithm.  The Baum–Welch algorithm is a special case of the expectation-maximization algorithm. If the HMMs are used for time series prediction, more sophisticated Bayesian inference methods, like Markov chain Monte Carlo (MCMC) sampling are proven to be favorable over finding a single maximum likelihood model both in terms of accuracy and stability. Since MCMC imposes significant computational burden, in cases where computational scalability is also of interest, one may alternatively resort to variational approximations to Bayesian inference, e.g. Indeed, approximate variational inference offers computational efficiency comparable to expectation-maximization, while yielding an accuracy profile only slightly inferior to exact MCMC-type Bayesian inference.

Applications
HMMs can be applied in many fields where the goal is to recover a data sequence that is not immediately observable (but other data that depend on the sequence are). Applications include:

Computational finance
Single-molecule kinetic analysis
Cryptanalysis
Speech recognition, including Siri
Speech synthesis
Part-of-speech tagging
Document separation in scanning solutions
Machine translation
Partial discharge
Gene prediction
Handwriting recognition
Alignment of bio-sequences
Time series analysis
Activity recognition
Protein folding
Sequence classification
Metamorphic virus detection
DNA motif discovery
DNA hybridization kinetics
Chromatin state discovery
Transportation forecasting
Solar irradiance variability

History
The Hidden Markov Models were described in a series of statistical papers by Leonard E. Baum and other authors in the second half of the 1960s. One of the first applications of HMMs was speech recognition, starting in the mid-1970s.In the second half of the 1980s, HMMs began to be applied to the analysis of biological sequences, in particular DNA. Since then, they have become ubiquitous in the field of bioinformatics.

Extensions
In the hidden Markov models considered above, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution). Hidden Markov models can also be generalized to allow continuous state spaces. Examples of such models are those where the Markov process over hidden variables is a linear dynamical system, with a linear relationship among related variables and where all hidden and observed variables follow a Gaussian distribution. In simple cases, such as the linear dynamical system just mentioned, exact inference is tractable (in this case, using the Kalman filter); however, in general, exact inference in HMMs with continuous latent variables is infeasible, and approximate methods must be used, such as the extended Kalman filter or the particle filter.
Hidden Markov models are generative models, in which the joint distribution of observations and hidden states, or equivalently both the prior distribution of hidden states (the transition probabilities) and conditional distribution of observations given states (the emission probabilities), is modeled.  The above algorithms implicitly assume a uniform prior distribution over the transition probabilities. However, it is also possible to create hidden Markov models with other types of prior distributions. An obvious candidate, given the categorical distribution of the transition probabilities, is the Dirichlet distribution, which is the conjugate prior distribution of the categorical distribution.  Typically, a symmetric Dirichlet distribution is chosen, reflecting ignorance about which states are inherently more likely than others.  The single parameter of this distribution (termed the concentration parameter) controls the relative density or sparseness of the resulting transition matrix.  A choice of 1 yields a uniform distribution.  Values greater than 1 produce a dense matrix, in which the transition probabilities between pairs of states are likely to be nearly equal.  Values less than 1 result in a sparse matrix in which, for each given source state, only a small number of destination states have non-negligible transition probabilities.  It is also possible to use a two-level prior Dirichlet distribution, in which one Dirichlet distribution (the upper distribution) governs the parameters of another Dirichlet distribution (the lower distribution), which in turn governs the transition probabilities.  The upper distribution governs the overall distribution of states, determining how likely each state is to occur; its concentration parameter determines the density or sparseness of states.  Such a two-level prior distribution, where both concentration parameters are set to produce sparse distributions, might be useful for example in unsupervised part-of-speech tagging, where some parts of speech occur much more commonly than others; learning algorithms that assume a uniform prior distribution generally perform poorly on this task.  The parameters of models of this sort, with non-uniform prior distributions, can be learned using Gibbs sampling or extended versions of the expectation-maximization algorithm.
An extension of the previously described hidden Markov models with Dirichlet priors uses a Dirichlet process in place of a Dirichlet distribution.  This type of model allows for an unknown and potentially infinite number of states.  It is common to use a two-level Dirichlet process, similar to the previously described model with two levels of Dirichlet distributions.  Such a model is called a hierarchical Dirichlet process hidden Markov model, or HDP-HMM for short. It was originally described under the name ""Infinite Hidden Markov Model""[3] and was further formalized in[4].
A different type of extension uses a discriminative model in place of the generative model of standard HMMs.  This type of model directly models the conditional distribution of the hidden states given the observations, rather than modeling the joint distribution.  An example of this model is the so-called maximum entropy Markov model (MEMM), which models the conditional distribution of the states using logistic regression (also known as a ""maximum entropy model"").  The advantage of this type of model is that arbitrary features (i.e. functions) of the observations can be modeled, allowing domain-specific knowledge of the problem at hand to be injected into the model.  Models of this sort are not limited to modeling direct dependencies between a hidden state and its associated observation; rather, features of nearby observations, of combinations of the associated observation and nearby observations, or in fact of arbitrary observations at any distance from a given hidden state can be included in the process used to determine the value of a hidden state.  Furthermore, there is no need for these features to be statistically independent of each other, as would be the case if such features were used in a generative model.  Finally, arbitrary features over pairs of adjacent hidden states can be used rather than simple transition probabilities.  The disadvantages of such models are: (1) The types of prior distributions that can be placed on hidden states are severely limited; (2) It is not possible to predict the probability of seeing an arbitrary observation.  This second limitation is often not an issue in practice, since many common usages of HMM's do not require such predictive probabilities.
A variant of the previously described discriminative model is the linear-chain conditional random field.  This uses an undirected graphical model (aka Markov random field) rather than the directed graphical models of MEMM's and similar models.  The advantage of this type of model is that it does not suffer from the so-called label bias problem of MEMM's, and thus may make more accurate predictions.  The disadvantage is that training can be slower than for MEMM's.
Yet another variant is the factorial hidden Markov model, which allows for a single observation to be conditioned on the corresponding hidden variables of a set of 
  
    
      
        K
      
    
    {\displaystyle K}
   independent Markov chains, rather than a single Markov chain. It is equivalent to a single HMM, with 
  
    
      
        
          N
          
            K
          
        
      
    
    {\displaystyle N^{K}}
   states (assuming there are 
  
    
      
        N
      
    
    {\displaystyle N}
   states for each chain), and therefore, learning in such a model is difficult: for a sequence of length 
  
    
      
        T
      
    
    {\displaystyle T}
  , a straightforward Viterbi algorithm has complexity 
  
    
      
        O
        (
        
          N
          
            2
            K
          
        
        
        T
        )
      
    
    {\displaystyle O(N^{2K}\,T)}
  . To find an exact solution, a junction tree algorithm could be used, but it results in an 
  
    
      
        O
        (
        
          N
          
            K
            +
            1
          
        
        
        K
        
        T
        )
      
    
    {\displaystyle O(N^{K+1}\,K\,T)}
   complexity. In practice, approximate techniques, such as variational approaches, could be used.All of the above models can be extended to allow for more distant dependencies among hidden states, e.g. allowing for a given state to be dependent on the previous two or three states rather than a single previous state; i.e. the transition probabilities are extended to encompass sets of three or four adjacent states (or in general 
  
    
      
        K
      
    
    {\displaystyle K}
   adjacent states).  The disadvantage of such models is that dynamic-programming algorithms for training them have an 
  
    
      
        O
        (
        
          N
          
            K
          
        
        
        T
        )
      
    
    {\displaystyle O(N^{K}\,T)}
   running time, for 
  
    
      
        K
      
    
    {\displaystyle K}
   adjacent states and 
  
    
      
        T
      
    
    {\displaystyle T}
   total observations (i.e. a length-
  
    
      
        T
      
    
    {\displaystyle T}
   Markov chain).
Another recent extension is the triplet Markov model, in which an auxiliary underlying process is added to model some data specificities. Many variants of this model have been proposed. One should also mention the interesting link that has been established between the theory of evidence and the triplet Markov models and which allows to fuse data in Markovian context and to model nonstationary data. Note that alternative multi-stream data fusion strategies have also been proposed in the recent literature, e.g.Finally, a different rationale towards addressing the problem of modeling nonstationary data by means of hidden Markov models was suggested in 2012. It consists in employing a small recurrent neural network (RNN), specifically a reservoir network, to capture the evolution of the temporal dynamics in the observed data. This information, encoded in the form of a high-dimensional vector, is used as a conditioning variable of the HMM state transition probabilities. Under such a setup, we eventually obtain a nonstationary HMM the transition probabilities of which evolve over time in a manner that is inferred from the data itself, as opposed to some unrealistic ad-hoc model of temporal evolution.
The model suitable in the context of longitudinal data is named latent Markov model. The basic version of this model has been extended to include individual covariates, random effects and to model more complex data structures such as multilevel data. A complete overview of the latent Markov models, with special attention to the model assumptions and  to their practical use is provided in

See also
References
External links
Concepts
Teif, V. B.; Rippe, K. (2010). ""Statistical–mechanical lattice models for protein–DNA binding in chromatin"". J. Phys.: Condens. Matter. 22 (41): 414105. arXiv:1004.5514. Bibcode:2010JPCM...22O4105T. doi:10.1088/0953-8984/22/41/414105. PMID 21386588. S2CID 103345.
A Revealing Introduction to Hidden Markov Models by Mark Stamp, San Jose State University.
Fitting HMM's with expectation-maximization – complete derivation
A step-by-step tutorial on HMMs (University of Leeds)
Hidden Markov Models (an exposition using basic mathematics)
Hidden Markov Models (by Narada Warakagoda)
Hidden Markov Models: Fundamentals and Applications Part 1, Part 2 (by V. Petrushin)
Lecture on a Spreadsheet by Jason Eisner, Video and interactive spreadsheet",https://en.wikipedia.org/wiki/Hidden_Markov_model,"['Articles with example Python (programming language) code', 'Articles with short description', 'Bioinformatics', 'Commons category link is on Wikidata', 'Good articles', 'Hidden Markov models', 'Markov models', 'Pages containing links to subscription-only content', 'Short description matches Wikidata', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with multiple identifiers']",Data Science
101,Human–computer interaction,"Human–computer interaction (HCI) studies the design and use of computer technology, focused on the interfaces between people (users) and computers. Researchers in the field of HCI observe the ways in which humans interact with computers and design technologies that let humans interact with computers in novel ways. 
As a field of research, human-computer interaction is situated at the intersection of computer science, behavioural sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their seminal 1983 book, The Psychology of Human–Computer Interaction, although the authors first used the term in 1980 and the first known use was in 1975. The term connotes that, unlike other tools with only limited uses (such as a wooden mallet, useful for hitting things, but not much else), a computer has many uses and this takes place as an open-ended dialog between the user and the computer. The notion of dialog likens human–computer interaction to human-to-human interaction, an analogy which is crucial to theoretical considerations in the field.

Introduction
Humans interact with computers in many ways; the interface between humans and computers is crucial to facilitate this interaction. Desktop applications, internet browsers, handheld computers, ERP, and computer kiosks make use of the prevalent graphical user interfaces (GUI) of today. Voice user interfaces (VUI) are used for speech recognition and synthesizing systems, and the emerging multi-modal and Graphical user interfaces (GUI) allow humans to engage with embodied character agents in a way that cannot be achieved with other interface paradigms. The growth in human–computer interaction field has been in quality of interaction, and in different branching in its history. Instead of designing regular interfaces, the different research branches have had a different focus on the concepts of multimodality rather than unimodality, intelligent adaptive interfaces rather than command/action based ones, and finally active rather than passive interfaces.The Association for Computing Machinery (ACM) defines human-computer interaction as ""a discipline concerned with the design, evaluation and implementation of interactive computing systems for human use and with the study of major phenomena surrounding them"". An important facet of HCI is user satisfaction (or simply End User Computing Satisfaction).
""Because human–computer interaction studies a human and a machine in communication, it draws from supporting knowledge on both the machine and the human side. On the machine side, techniques in computer graphics, operating systems, programming languages, and development environments are relevant. On the human side, communication theory, graphic and industrial design disciplines, linguistics, social sciences, cognitive psychology, social psychology, and human factors such as computer user satisfaction are relevant. And, of course, engineering and design methods are relevant."" Due to the multidisciplinary nature of HCI, people with different backgrounds contribute to its success. HCI is also sometimes termed human–machine interaction (HMI), man-machine interaction (MMI) or computer-human interaction (CHI).
Poorly designed human-machine interfaces can lead to many unexpected problems. A classic example is the Three Mile Island accident, a nuclear meltdown accident, where investigations concluded that the design of the human-machine interface was at least partly responsible for the disaster. Similarly, accidents in aviation have resulted from manufacturers' decisions to use non-standard flight instruments or throttle quadrant layouts: even though the new designs were proposed to be superior in basic human-machine interaction, pilots had already ingrained the ""standard"" layout and thus the conceptually good idea actually had undesirable results.

Human–computer interface
The human–computer interface can be described as the point of communication between the human user and the computer. The flow of information between the human and computer is defined as the loop of interaction. The loop of interaction has several aspects to it, including:

Visual Based :The visual based human computer interaction is probably the most widespread area in Human Computer Interaction (HCI) research.
Audio Based : The audio based interaction between a computer and a human is another important area of in HCI systems. This area deals with information acquired by different audio signals.
Task environment: The conditions and goals set upon the user.
Machine environment: The environment that the computer is connected to, e.g. a laptop in a college student's dorm room.
Areas of the interface: Non-overlapping areas involve processes of the human and computer not pertaining to their interaction. Meanwhile, the overlapping areas only concern themselves with the processes pertaining to their interaction.
Input flow: The flow of information that begins in the task environment, when the user has some task that requires using their computer.
Output: The flow of information that originates in the machine environment.
Feedback: Loops through the interface that evaluate, moderate, and confirm processes as they pass from the human through the interface to the computer and back.
Fit: This is the match between the computer design, the user and the task to optimize the human resources needed to accomplish the task.

Goals for computers
Human–computer interaction studies the ways in which humans make—or do not make—use of computational artifacts, systems and infrastructures. Much of the research in the field seeks to improve human–computer interaction by improving the usability of computer interfaces. How usability is to be precisely understood, how it relates to other social and cultural values and when it is, and when it may not be a desirable property of computer interfaces is increasingly debated.Much of the research in the field of human–computer interaction takes an interest in:

Methods for designing new computer interfaces, thereby optimizing a design for a desired property such as learnability, findability,  efficiency of use.
Methods for implementing interfaces, e.g., by means of software libraries.
Methods for evaluating and comparing interfaces with respect to their usability and other desirable properties.
Methods for studying human-computer use and its sociocultural implications more broadly.
Methods for determining whether or not the user is human or computer.
Models and theories of human computer use as well as conceptual frameworks for the design of computer interfaces, such as cognitivist user models, Activity Theory or ethnomethodological accounts of human computer use.
Perspectives that critically reflect upon the values that underlie computational design, computer use and HCI research practice.Visions of what researchers in the field seek to achieve vary. When pursuing a cognitivist perspective, researchers of HCI may seek to align computer interfaces with the mental model that humans have of their activities. When pursuing a post-cognitivist perspective, researchers of HCI may seek to align computer interfaces with existing social practices or existing sociocultural values.
Researchers in HCI are interested in developing design methodologies, experimenting with devices, prototyping software and hardware systems, exploring interaction paradigms, and developing models and theories of interaction.

Differences with related fields
HCI differs from human factors and ergonomics as HCI focuses more on users working specifically with computers, rather than other kinds of machines or designed artifacts. There is also a focus in HCI on how to implement the computer software and hardware mechanisms to support human–computer interaction. Thus, human factors is a broader term. HCI could be described as the human factors of computers – although some experts try to differentiate these areas.
HCI also differs from human factors in that there is less of a focus on repetitive work-oriented tasks and procedures, and much less emphasis on physical stress and the physical form or industrial design of the user interface, such as keyboards and mouse devices.
Three areas of study have substantial overlap with HCI even as the focus of inquiry shifts. Personal information management (PIM) studies how people acquire and use personal information (computer based and other) to complete tasks. In computer-supported cooperative work (CSCW), emphasis is placed on the use of computing systems in support of the collaborative work. The principles of human interaction management (HIM) extend the scope of CSCW to an organizational level and can be implemented without use of computers.

Design
Principles
The following experimental design principles are considered, when evaluating a current user interface, or designing a new user interface:

Early focus is placed on user(s) and task(s): How many users are needed to perform the task(s) is established and who the appropriate users should be is determined (someone who has never used the interface, and will not use the interface in the future, is most likely not a valid user). In addition, the task(s) the users will be performing and how often the task(s) need to be performed is defined.
Empirical measurement: the interface is tested with real users who come in contact with the interface on a daily basis. The results can vary with the performance level of the user and the typical human–computer interaction may not always be represented. Quantitative usability specifics, such as the number of users performing the task(s), the time to complete the task(s), and the number of errors made during the task(s) are determined.
Iterative design: After determining what users, tasks, and empirical measurements to include, the following iterative design steps are performed:
Design the user interface
Test
Analyze results
RepeatThe iterative design process is repeated until a sensible, user-friendly interface is created.

Methodologies
Various different strategies delineating methods for human–PC interaction design have developed since the ascent of the field during the 1980s. Most plan philosophies come from a model for how clients, originators, and specialized frameworks interface. Early techniques treated clients' psychological procedures as unsurprising and quantifiable and urged plan specialists to look at subjective science to establish zones, (for example, memory and consideration) when structuring UIs. Present day models, in general, center around a steady input and discussion between clients, creators, and specialists and push for specialized frameworks to be folded with the sorts of encounters clients need to have, as opposed to wrapping user experience around a finished framework.

Activity theory: utilized in HCI to characterize and consider the setting where human cooperations with PCs occur. Action hypothesis gives a structure for reasoning about activities in these specific circumstances, and illuminates design of interactions from an action driven perspective.
User-focused design: client focused structure (UCD) is a cutting edge, broadly rehearsed plan theory established on the possibility that clients must become the overwhelming focus in the plan of any PC framework. Clients, architects and specialized experts cooperate to determine the requirements and restrictions of the client and make a framework to support these components. Frequently, client focused plans are informed by ethnographic investigations of situations in which clients will associate with the framework. This training is like participatory design, which underscores the likelihood for end-clients to contribute effectively through shared plan sessions and workshops.
Principles of UI design: these standards may be considered during the design of a client interface: resistance, effortlessness, perceivability, affordance, consistency, structure and feedback.
Value delicate design (VSD): a technique for building innovation that accounts for the individuals who utilize the design straightforwardly, and just as well for those who the design influences, either directly or indirectly. VSD utilizes an iterative plan process that includes three kinds of examinations: theoretical, exact and specialized. Applied examinations target the understanding and articulation of the different parts of the design, and its qualities or any clashes that may emerge for the users of the design. Exact examinations are subjective or quantitative plan explore thinks about used to advise the creators' understanding regarding the clients' qualities, needs, and practices. Specialized examinations can include either investigation of how individuals use related advances, or the framework plans.

Display designs
Displays are human-made artifacts designed to support the perception of relevant system variables and to facilitate further processing of that information. Before a display is designed, the task that the display is intended to support must be defined (e.g. navigating, controlling, decision making, learning, entertaining, etc.). A user or operator must be able to process whatever information that a system generates and displays; therefore, the information must be displayed according to principles in a manner that will support perception, situation awareness, and understanding.

Thirteen principles of display design
Christopher Wickens et al. defined 13 principles of display design in their book An Introduction to Human Factors Engineering.These principles of human perception and information processing can be utilized to create an effective display design. A reduction in errors, a reduction in required training time, an increase in efficiency, and an increase in user satisfaction are a few of the many potential benefits that can be achieved through utilization of these principles.
Certain principles may not be applicable to different displays or situations. Some principles may seem to be conflicting, and there is no simple solution to say that one principle is more important than another. The principles may be tailored to a specific design or situation. Striking a functional balance among the principles is critical for an effective design.

Perceptual principles
1.	Make displays legible (or audible). A display's legibility is critical and necessary for designing a usable display. If the characters or objects being displayed cannot be discernible, then the operator cannot effectively make use of them.
2.	Avoid absolute judgment limits. Do not ask the user to determine the level of a variable on the basis of a single sensory variable (e.g. color, size, loudness). These sensory variables can contain many possible levels.
3.	Top-down processing. Signals are likely perceived and interpreted in accordance with what is expected based on a user's experience. If a signal is presented contrary to the user's expectation, more physical evidence of that signal may need to be presented to assure that it is understood correctly.
4.	Redundancy gain. If a signal is presented more than once, it is more likely that it will be understood correctly. This can be done by presenting the signal in alternative physical forms (e.g. color and shape, voice and print, etc.), as redundancy does not imply repetition. A traffic light is a good example of redundancy, as color and position are redundant.
5.	Similarity causes confusion: Use distinguishable elements. Signals that appear to be similar will likely be confused. The ratio of similar features to different features causes signals to be similar. For example, A423B9 is more similar to A423B8 than 92 is to 93. Unnecessarily similar features should be removed and dissimilar features should be highlighted.

Mental model principles
6.	Principle of pictorial realism. A display should look like the variable that it represents (e.g. high temperature on a thermometer shown as a higher vertical level). If there are multiple elements, they can be configured in a manner that looks like it would in the represented environment.
7.	Principle of the moving part. Moving elements should move in a pattern and direction compatible with the user's mental model of how it actually moves in the system. For example, the moving element on an altimeter should move upward with increasing altitude.

Principles based on attention
8.	Minimizing information access cost or interaction cost. When the user's attention is diverted from one location to another to access necessary information, there is an associated cost in time or effort. A display design should minimize this cost by allowing for frequently accessed sources to be located at the nearest possible position. However, adequate legibility should not be sacrificed to reduce this cost.
9.	Proximity compatibility principle. Divided attention between two information sources may be necessary for the completion of one task. These sources must be mentally integrated and are defined to have close mental proximity. Information access costs should be low, which can be achieved in many ways (e.g. proximity, linkage by common colours, patterns, shapes, etc.). However, close display proximity can be harmful by causing too much clutter.
10.	Principle of multiple resources. A user can more easily process information across different resources. For example, visual and auditory information can be presented simultaneously rather than presenting all visual or all auditory information.

Memory principles
11.	Replace memory with visual information: knowledge in the world. A user should not need to retain important information solely in working memory or retrieve it from long-term memory. A menu, checklist, or another display can aid the user by easing the use of their memory. However, the use of memory may sometimes benefit the user by eliminating the need to reference some type of knowledge in the world (e.g., an expert computer operator would rather use direct commands from memory than refer to a manual). The use of knowledge in a user's head and knowledge in the world must be balanced for an effective design.
12.	Principle of predictive aiding. Proactive actions are usually more effective than reactive actions. A display should attempt to eliminate resource-demanding cognitive tasks and replace them with simpler perceptual tasks to reduce the use of the user's mental resources. This will allow the user to focus on current conditions, and to consider possible future conditions. An example of a predictive aid is a road sign displaying the distance to a certain destination.
13.	Principle of consistency. Old habits from other displays will easily transfer to support processing of new displays if they are designed consistently. A user's long-term memory will trigger actions that are expected to be appropriate. A design must accept this fact and utilize consistency among different displays.

Current research
Topics in human-computer interaction include the following:

User customization
End-user development studies have shown how ordinary users could routinely tailor applications to their own needs and to invent new applications based on their understanding of their own domains. With their deeper knowledge, users could increasingly be important sources of new applications at the expense of generic programmers with systems expertise but low domain expertise.

Embedded computation
Computation is passing beyond computers into every object for which uses can be found. Embedded systems make the environment alive with little computations and automated processes, from computerized cooking appliances to lighting and plumbing fixtures to window blinds to automobile braking systems to greeting cards. The expected difference in the future is the addition of networked communications that will allow many of these embedded computations to coordinate with each other and with the user. Human interfaces to these embedded devices will in many cases be disparate from those appropriate to workstations.

Augmented reality
Augmented reality refers to the notion of layering relevant information into our vision of the world. Existing projects show real-time statistics to users performing difficult tasks, such as manufacturing. Future work might include augmenting our social interactions by providing additional information about those we converse with.

Social computing
In recent years, there has been an explosion of social science research focusing on interactions as the unit of analysis. Much of this research draws from psychology, social psychology, and sociology. For example, one study found out that people expected a computer with a man's name to cost more than a machine with a woman's name. Other research finds that individuals perceive their interactions with computers more positively than humans, despite behaving the same way towards these machines.

Knowledge-driven human–computer interaction
In human and computer interactions, a semantic gap usually exists between human and computer's understandings towards mutual behaviors. Ontology, as a formal representation of domain-specific knowledge, can be used to address this problem, through solving the semantic ambiguities between the two parties.

Emotions and human-computer interaction
In the interaction of humans and computers, research has studied how computers can detect, process and react to human emotions to develop emotionally intelligent information systems. Researchers have suggested several 'affect-detection channels'. The potential of telling human emotions in an automated and digital fashion lies in improvements to the effectiveness of human-computer interaction. The influence of emotions in human-computer interaction has been studied in fields such as financial decision making using ECG and organisational knowledge sharing using eye tracking and face readers as affect-detection channels. In these fields it has been shown that affect-detection channels have the potential to detect human emotions and that information systems can incorporate the data obtained from affect-detection channels to improve decision models.

Brain–computer interfaces
A brain–computer interface (BCI), is a direct communication pathway between an enhanced or wired brain and an external device. BCI differs from neuromodulation in that it allows for bidirectional information flow. BCIs are often directed at researching, mapping, assisting, augmenting, or repairing human cognitive or sensory-motor functions.

Factors of change
Traditionally, computer use was modeled as a human–computer dyad in which the two were connected by a narrow explicit communication channel, such as text-based terminals. Much work has been done to make the interaction between a computing system and a human more reflective of the multidimensional nature of everyday communication. Because of potential issues, human–computer interaction shifted focus beyond the interface to respond to observations as articulated by D. Engelbart: ""If ease of use was the only valid criterion, people would stick to tricycles and never try bicycles.""The means by which humans interact with computers continues to evolve rapidly. Human–computer interaction is affected by developments in computing. These forces include:

Decreasing hardware costs leading to larger memory and faster systems
Miniaturization of hardware leading to portability
Reduction in power requirements leading to portability
New display technologies leading to the packaging of computational devices in new forms
Specialized hardware leading to new functions
Increased development of network communication and distributed computing
Increasingly widespread use of computers, especially by people who are outside of the computing profession
Increasing innovation in input techniques (e.g., voice, gesture, pen), combined with lowering cost, leading to rapid computerization by people formerly left out of the computer revolution.
Wider social concerns leading to improved access to computers by currently disadvantaged groupsAs of 2010 the future for HCI is expected to include the following characteristics:

Ubiquitous computing and communication. Computers are expected to communicate through high speed local networks, nationally over wide-area networks, and portably via infrared, ultrasonic, cellular, and other technologies. Data and computational services will be portably accessible from many if not most locations to which a user travels.
High-functionality systems. Systems can have large numbers of functions associated with them. There are so many systems that most users, technical or non-technical, do not have time to learn about in the traditional way (e.g., through thick user manuals).
Mass availability of computer graphics. Computer graphics capabilities such as image processing, graphics transformations, rendering, and interactive animation are becoming widespread as inexpensive chips become available for inclusion in general workstations and mobile devices.
Mixed media. Commercial systems can handle images, voice, sounds, video, text, formatted data. These are exchangeable over communication links among users. The separate fields of consumer electronics (e.g., stereo sets, DVD players, televisions) and computers are beginning to merge. Computer and print fields are expected to cross-assimilate.
High-bandwidth interaction. The rate at which humans and machines interact is expected to increase substantially due to the changes in speed, computer graphics, new media, and new input/output devices. This can lead to some qualitatively different interfaces, such as virtual reality or computational video.
Large and thin displays. New display technologies are maturing, enabling very large displays and displays that are thin, lightweight, and low in power use. This is having large effects on portability and will likely enable developing paper-like, pen-based computer interaction systems very different in feel from present desktop workstations.
Information utilities. Public information utilities (such as home banking and shopping) and specialized industry services (e.g., weather for pilots) are expected to proliferate. The rate of proliferation can accelerate with the introduction of high-bandwidth interaction and the improvement in quality of interfaces.

Scientific conferences
One of the main conferences for new research in human–computer interaction is the annually held Association for Computing Machinery's (ACM) Conference on Human Factors in Computing Systems, usually referred to by its short name CHI (pronounced kai, or khai). CHI is organized by ACM Special Interest Group on Computer–Human Interaction (SIGCHI). CHI is a large conference, with thousands of attendants, and is quite broad in scope. It is attended by academics, practitioners and industry people, with company sponsors such as Google, Microsoft, and PayPal.
There are also dozens of other smaller, regional or specialized HCI-related conferences held around the world each year, including:

See also
Human–computer interaction portal
Outline of human–computer interaction
Information design
Information architecture
Physiological interaction
User experience design
Mindfulness and technology
CAPTCHA
Turing test
HCI Bibliography, a web-based project to provide a bibliography of Human Computer Interaction literature

Footnotes
Further reading
Academic overviews of the fieldJulie A. Jacko (Ed.). (2012). Human–Computer Interaction Handbook (3rd Edition). CRC Press. ISBN 1-4398-2943-8
Andrew Sears and Julie A. Jacko (Eds.). (2007). Human–Computer Interaction Handbook (2nd Edition). CRC Press. ISBN 0-8058-5870-9
Julie A. Jacko and Andrew Sears (Eds.). (2003). Human–Computer Interaction Handbook. Mahwah: Lawrence Erlbaum & Associates. ISBN 0-8058-4468-6Historically important classicStuart K. Card, Thomas P. Moran, Allen Newell (1983): The Psychology of Human–Computer Interaction. Erlbaum, Hillsdale 1983 ISBN 0-89859-243-7Overviews of history of the fieldJonathan Grudin: A moving target: The evolution of human–computer interaction. In Andrew Sears and Julie A. Jacko (Eds.). (2007). Human–Computer Interaction Handbook (2nd Edition). CRC Press. ISBN 0-8058-5870-9
Myers, Brad (1998). ""A brief history of human–computer interaction technology"". Interactions. 5 (2): 44–54. CiteSeerX 10.1.1.23.2422. doi:10.1145/274430.274436. S2CID 8278771.
John M. Carroll: Human Computer Interaction: History and Status. Encyclopedia Entry at Interaction-Design.org
Carroll, John M. (2010). ""Conceptualizing a possible discipline of human–computer interaction"". Interacting with Computers. 22 (1): 3–12. doi:10.1016/j.intcom.2009.11.008.
Sara Candeias, S. and A. Veiga The dialogue between man and machine: the role of language theory and technology, Sandra M. Aluísio & Stella E. O. Tagnin, New Language Technologies and Linguistic Research, A Two-Way Road: cap. 11. Cambridge Scholars Publishing. (ISBN 978-1-4438-5377-4)Social science and HCINass, Clifford; Fogg, B. J.; Moon, Youngme (1996). ""Can computers be teammates?"". International Journal of Human-Computer Studies. 45 (6): 669–678. doi:10.1006/ijhc.1996.0073.
Nass, Clifford; Moon, Youngme (2000). ""Machines and mindlessness: Social responses to computers"". Journal of Social Issues. 56 (1): 81–103. doi:10.1111/0022-4537.00153. S2CID 15851410.
Posard, Marek N (2014). ""Status processes in human–computer interactions: Does gender matter?"". Computers in Human Behavior. 37: 189–195. doi:10.1016/j.chb.2014.04.025.
Posard, Marek N.; Rinderknecht, R. Gordon (2015). ""Do people like working with computers more than human beings?"". Computers in Human Behavior. 51: 232–238. doi:10.1016/j.chb.2015.04.057.Academic journalsACM Transactions on Computer-Human Interaction
Behaviour & Information Technology [1]
Interacting with Computers
International Journal of Human–Computer Interaction
International Journal of Human–Computer Studies
Human–Computer Interaction [2] [3]Collection of papersRonald M. Baecker, Jonathan Grudin, William A. S. Buxton, Saul Greenberg (Eds.) (1995): Readings in human–computer interaction. Toward the Year 2000. 2. ed. Morgan Kaufmann, San Francisco 1995 ISBN 1-55860-246-1
Mithun Ahamed, Developing a Message Interface Architecture for Android Operating Systems, (2015). [4]Treatments by one or few authors, often aimed at a more general audienceJakob Nielsen: Usability Engineering. Academic Press, Boston 1993 ISBN 0-12-518405-0
Donald A. Norman: The Psychology of Everyday Things. Basic Books, New York 1988 ISBN 0-465-06709-3
Jef Raskin: The Humane Interface. New directions for designing interactive systems. Addison-Wesley, Boston 2000 ISBN 0-201-37937-6
Bruce Tognazzini: Tog on Interface. Addison-Wesley, Reading 1991 ISBN 0-201-60842-1TextbooksAlan Dix, Janet Finlay, Gregory Abowd, and Russell Beale (2003): Human–Computer Interaction. 3rd Edition. Prentice Hall, 2003. http://hcibook.com/e3/ ISBN 0-13-046109-1
Yvonne Rogers, Helen Sharp & Jenny Preece: Interaction Design: Beyond Human–Computer Interaction, 3rd ed. John Wiley & Sons Ltd., 2011 ISBN 0-470-66576-9
Helen Sharp, Yvonne Rogers & Jenny Preece: Interaction Design: Beyond Human–Computer Interaction, 2nd ed. John Wiley & Sons Ltd., 2007 ISBN 0-470-01866-6
Matt Jones (interaction designer) and Gary Marsden (2006). Mobile Interaction Design, John Wiley and Sons Ltd.

External links
Bad Human Factors Designs
The HCI Wiki Bibliography with over 100,000 publications.
The HCI Bibliography Over 100,000 publications about HCI.
Human–Centered Computing Education Digital Library
HCI Webliography",https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction,"['All articles containing potentially dated statements', 'All articles needing additional references', 'All articles with unsourced statements', 'Articles containing potentially dated statements from 2010', 'Articles needing additional references from February 2013', 'Articles needing additional references from October 2010', 'Articles with short description', 'Articles with unsourced statements from February 2013', 'Articles with unsourced statements from October 2015', 'CS1 maint: multiple names: authors list', 'Commons category link is on Wikidata', 'Human communication', 'Human–computer interaction', 'Human–machine interaction', 'Short description is different from Wikidata', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers']",Data Science
102,ISSN (identifier),"An International Standard Serial Number (ISSN) is an eight-digit serial number used to uniquely identify a serial publication, such as a magazine. The ISSN is especially helpful in distinguishing between serials with the same title. ISSNs are used in ordering, cataloging, interlibrary loans, and other practices in connection with serial literature.The ISSN system was first drafted as an International Organization for Standardization (ISO) international standard in 1971 and published as ISO 3297 in 1975. ISO subcommittee TC 46/SC 9 is responsible for maintaining the standard.
When a serial with the same content is published in more than one media type, a different ISSN is assigned to each media type. For example, many serials are published both in print and electronic media. The ISSN system refers to these types as print ISSN (p-ISSN) and electronic ISSN (e-ISSN), respectively. Consequently, as defined in ISO 3297:2007, every serial in the ISSN system is also assigned a linking ISSN (ISSN-L), typically the same as the ISSN assigned to the serial in its first published medium, which links together all ISSNs assigned to the serial in every medium.

Code format
The format of the ISSN is an eight-digit code, divided by a hyphen into two four-digit numbers. As an integer number, it can be represented by the first seven digits. The last code digit, which may be 0-9 or an X, is a check digit. Formally, the general form of the ISSN code (also named ""ISSN structure"" or ""ISSN syntax"")  can be expressed as follows:
NNNN-NNNC
where N is in the set {0,1,2,...,9}, a digit character, and C is in {0,1,2,...,9,X};or by a Perl Compatible Regular Expressions (PCRE) regular expression:
^[0-9]{4}-[0-9]{3}[0-9xX]$.The ISSN of the journal Hearing Research, for example, is 0378-5955, where the final 5 is the check digit, that is C=5. To calculate the check digit, the following algorithm may be used:

Calculate the sum of the first seven digits of the ISSN multiplied by its position in the number, counting from the right—that is, 8, 7, 6, 5, 4, 3, and 2, respectively:

  
    
      
        0
        ⋅
        8
        +
        3
        ⋅
        7
        +
        7
        ⋅
        6
        +
        8
        ⋅
        5
        +
        5
        ⋅
        4
        +
        9
        ⋅
        3
        +
        5
        ⋅
        2
      
    
    {\displaystyle 0\cdot 8+3\cdot 7+7\cdot 6+8\cdot 5+5\cdot 4+9\cdot 3+5\cdot 2}
  

  
    
      
        =
        0
        +
        21
        +
        42
        +
        40
        +
        20
        +
        27
        +
        10
      
    
    {\displaystyle =0+21+42+40+20+27+10}
  

  
    
      
        =
        160
      
    
    {\displaystyle =160}
  
The modulus 11 of this sum is then calculated; divide the sum by 11 and determine the remainder:

  
    
      
        
          
            160
            11
          
        
        =
        14
        
          
             remainder 
          
        
        6
        =
        14
        +
        
          
            6
            11
          
        
      
    
    {\displaystyle {\frac {160}{11}}=14{\mbox{ remainder }}6=14+{\frac {6}{11}}}
  If there is no remainder the check digit is 0, otherwise the remainder value is subtracted from 11 to give the check digit:

  
    
      
        11
        −
        6
        =
        5
      
    
    {\displaystyle 11-6=5}
  
5 is the check digit, C.For calculations, an upper case X in the check digit position indicates a check digit of 10 (like a Roman ten).To confirm the check digit, calculate the sum of all eight digits of the ISSN multiplied by its position in the number, counting from the right (if the check digit is X, then add 10 to the sum). The modulus 11 of the sum must be 0. There is an online ISSN checker that can validate an ISSN, based on the above algorithm.

In EANs
ISSNs can be encoded in EAN-13 bar codes with a 977 ""country code"" (compare the 978 country code (""bookland"") for ISBNs), followed by the 7 main digits of the ISSN (the check digit is not included), followed by 2 publisher-defined digits, followed by the EAN check digit (which need not match the ISSN check digit).

Code assignment, maintenance and look-up
ISSN codes are assigned by a network of ISSN National Centres, usually located at national libraries and coordinated by the ISSN International Centre based in Paris. The International Centre is an intergovernmental organization created in 1974 through an agreement between UNESCO and the French government.

Linking ISSN
ISSN-L is a unique identifier for all versions of the serial containing the same content across different media. As defined by ISO 3297:2007, the ""linking ISSN (ISSN-L)"" provides a mechanism for collocation or linking among the different media versions of the same continuing resource. The ISSN-L is one of a serial's existing ISSNs, so does not change the use or assignment of ""ordinary"" ISSNs; it is based on the ISSN of the first published medium version of the publication. If the print and online versions of the publication are published at the same time, the ISSN of the print version is chosen as the basis of the ISSN-L. 
With ISSN-L is possible to designate one single ISSN for all those media versions of the title. The use of ISSN-L facilitates search, retrieval and delivery across all media versions for services like OpenURL, library catalogues, search engines or knowledge bases.

Register
The International Centre maintains a database of all ISSNs assigned worldwide, the ISDS Register (International Serials Data System), otherwise known as the ISSN Register. At the end of 2016, the ISSN Register contained records for 1,943,572 items. The Register is not freely available for interrogation on the web, but is available by subscription. 

The print version of a serial typically will include the ISSN code as part of the publication information.
Most serial websites contain ISSN code information.
Derivative lists of publications will often contain ISSN codes; these can be found through on-line searches with the ISSN code itself or serial title.
WorldCat permits searching its catalog by ISSN, by entering ""issn:"" before the code in the query field. One can also go directly to an ISSN's record by appending it to ""https://www.worldcat.org/ISSN/"", e.g. https://www.worldcat.org/ISSN/1021-9749. This does not query the ISSN Register itself, but rather shows whether any Worldcat library holds an item with the given ISSN.

Comparison with other identifiers
ISSN and ISBN codes are similar in concept, where ISBNs are assigned to individual books. An ISBN might be assigned for particular issues of a serial, in addition to the ISSN code for the serial as a whole. An ISSN, unlike the ISBN code, is an anonymous identifier associated with a serial title, containing no information as to the publisher or its location. For this reason a new ISSN is assigned to a serial each time it undergoes a major title change.

Extensions
Since the ISSN applies to an entire serial a new identifier, other identifiers have been built on top of it to allow references to specific volumes, articles, or other identifiable components (like the table of contents): the Publisher Item Identifier (PII) and the Serial Item and Contribution Identifier (SICI).

Media versus content
Separate ISSNs are needed for serials in different media (except reproduction microforms). Thus, the print and electronic media versions of a serial need separate ISSNs, and CD-ROM versions and web versions require different ISSNs. However, the same ISSN can be used for different file formats (e.g. PDF and HTML) of the same online serial.
This ""media-oriented identification"" of serials made sense in the 1970s. In the 1990s and onward, with personal computers, better screens, and the Web, it makes sense to consider only content, independent of media. This ""content-oriented identification"" of serials was a repressed demand during a decade, but no ISSN update or initiative occurred. A natural extension for ISSN, the unique-identification of the articles in the serials, was the main demand application. An alternative serials' contents model arrived with the indecs Content Model and its application, the digital object identifier (DOI), an ISSN-independent initiative, consolidated in the 2000s.
Only later, in 2007, ISSN-L was defined in the new ISSN standard (ISO 3297:2007) as an ""ISSN designated by the ISSN Network to enable collocation or versions of a continuing resource linking among the different media"".

Use in URNs
An ISSN can be encoded as a uniform resource name (URN) by prefixing it with ""urn:ISSN:"". For example, Rail could be referred to as ""urn:ISSN:0953-4563"". URN namespaces are case-sensitive, and the ISSN namespace is all caps. If the checksum digit is ""X"" then it is always encoded in uppercase in a URN.

Problems
The URNs are content-oriented, but ISSN is media-oriented:

ISSN is not unique when the concept is ""a journal is a set of contents, generally copyrighted content"": the same journal (same contents and same copyrights) may have two or more ISSN codes. A URN needs to point to ""unique content"" (a ""unique journal"" as a ""set of contents"" reference).Example: Nature has an ISSN for print, 0028-0836, and another for the same content on the Web, 1476-4687; only the oldest (0028-0836) is used as a unique identifier. As the ISSN is not unique, the U.S. National Library of Medicine needed to create, prior to 2007, the NLM Unique ID (JID).ISSN does not offer resolution mechanisms like a digital object identifier (DOI) or a URN does, so the DOI is used as a URN for articles, with (for historical reasons) no need for an ISSN's existence.Example: the DOI name ""10.1038/nature13777"" can be represented as an HTTP string by https://doi.org/10.1038/nature13777, and is redirected (resolved) to the current article's page; but there is no ISSN online service, like http://dx.issn.org/, to resolve the ISSN of the journal (in this sample 1476-4687).A unique URN for serials simplifies the search, recovery and delivery of data for various services including, in particular, search systems and knowledge databases. ISSN-L (see Linking ISSN above) was created to fill this gap.

Media category labels
The two standard categories of media in which serials are most available are print and electronic. In metadata contexts (e.g., JATS), these may have standard labels.

Print ISSN
p-ISSN is a standard label for ""Print ISSN"", the ISSN for the print media (paper) version of a serial. Usually it is the ""default media"" and so the ""default ISSN"".

Electronic ISSN
e-ISSN (or eISSN) is a standard label for ""Electronic ISSN"", the ISSN for the electronic media (online) version of a serial.

ROAD
ROAD: Directory of Open Access Scholarly Resources (est. 2013), produced by the ISSN International Centre and UNESCO

See also
CODEN
WorldCat—an ISSN-resolve service

References
External links
ISSN International Centre
ISSN Portal
List of 63800 ISSN numbers and titles
ISSN InterNational Centre (January 2015), ISSN Manual (PDF) (2015 ed.), Paris: ISSN InterNational Centre.
How U.S. publishers can obtain an ISSN, United States: Library of Congress.
ISSN Canada, Ottawa: Library and Archives Canada, 8 January 2020, retrieved 3 April 2020..
Getting an ISSN in the UK, British Library.
Getting an ISSN in France (in French), Bibliothèque nationale de France
Getting an ISSN in Germany (in German), Deutsche Nationalbibliothek
Getting an ISSN in South Africa, National Library of South Africa, archived from the original on 24 December 2017, retrieved 7 January 2015",https://en.wikipedia.org/wiki/International_Standard_Serial_Number,"['All articles containing potentially dated statements', 'Articles containing potentially dated statements from December 2016', 'Articles with short description', 'CS1 French-language sources (fr)', 'CS1 German-language sources (de)', 'CS1 maint: archived copy as title', 'CS1 maint: multiple names: authors list', 'Checksum algorithms', 'ISO standards', 'Library science', 'Serial numbers', 'Short description is different from Wikidata', 'Unique identifiers', 'Use dmy dates from August 2017', 'Wikipedia articles with BNE identifiers', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with NDL identifiers', 'Wikipedia articles with NKC identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WorldCat-VIAF identifiers']",Data Science
103,ISBN (identifier),"The International Standard Book Number (ISBN) is a numeric commercial book identifier which is intended to be unique. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.An ISBN is assigned to each separate edition and variation (except reprintings) of a publication. For example, an e-book, a paperback and a hardcover edition of the same book will each have a different ISBN. The ISBN is ten digits long if assigned before 2007, and thirteen digits long if assigned on or after 1 January 2007. The method of assigning an ISBN is nation-specific and varies between countries, often depending on how large the publishing industry is within a country.
The initial ISBN identification format was devised in 1967, based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the 9-digit SBN code can be converted to a 10-digit ISBN by prefixing it with a zero digit '0').
Privately published books sometimes appear without an ISBN. The International ISBN Agency sometimes assigns such books ISBNs on its own initiative.Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines and newspapers. The International Standard Music Number (ISMN) covers musical scores.

History
The Standard Book Number (SBN) is a commercial system using nine-digit code numbers to identify books. It was created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin, for the booksellers and stationers WHSmith and others in 1965. The ISBN identification format was conceived in 1967 in the United Kingdom by David Whitaker (regarded as the ""Father of the ISBN"") and in 1968 in the United States by Emery Koltay (who later became director of the U.S. ISBN agency R. R. Bowker).The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108. The United Kingdom continued to use the nine-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.An SBN may be converted to an ISBN by prefixing the digit ""0"". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has ""SBN 340 01381 8"", where ""340"" indicates the publisher, ""01381"" is the serial number assigned by the publisher, and ""8"" is the check digit. By prefixing a zero, this can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated. Some publishers, such as Ballantine Books, would sometimes use 12-digit SBNs where the last three digits indicated the price of the book; for example, Woodstock Handmade Houses had a 12-digit Standard Book Number of 345-24223-8-595 (valid SBN: 345-24223-8, ISBN: 0-345-24223-8), and it cost US$5.95.Since 1 January 2007, ISBNs have contained thirteen digits, a format that is compatible with ""Bookland"" European Article Numbers, which have 13 digits.

Overview
A separate ISBN is assigned to each edition and variation (except reprintings) of a publication. For example, an ebook, audiobook, paperback, and hardcover edition of the same book will each have a different ISBN assigned to it. The ISBN is thirteen digits long if assigned on or after 1 January 2007, and ten digits long if assigned before 2007. An International Standard Book Number consists of four parts (if it is a 10-digit ISBN) or five parts (for a 13-digit ISBN).
Section 5 of the International ISBN Agency's official user manual describes the structure of the 13-digit ISBN, as follows:

for a 13-digit ISBN, a prefix element – a GS1 prefix: so far 978 or 979 have been made available by GS1,
the registration group element (language-sharing country group, individual country or territory),
the registrant element,
the publication element, and
a checksum character or check digit.A 13-digit ISBN can be separated into its parts (prefix element, registration group, registrant, publication and check digit), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts (registration group, registrant, publication and check digit) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.

How ISBNs are issued
ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.A full directory of ISBN agencies is available on the International ISBN Agency website. List for a few countries is given below:

Australia – Thorpe-Bowker
Brazil – The National Library of Brazil; (Up to 28 February 2020)
Brazil – Câmara Brasileira do Livro (From 1 March 2020)
Canada – English Library and Archives Canada, a government agency; French Bibliothèque et Archives nationales du Québec;
Colombia – Cámara Colombiana del Libro, an NGO
Hong Kong – Books Registration Office (BRO), under the Hong Kong Public Libraries
India – The Raja Rammohun Roy National Agency for ISBN (Book Promotion and Copyright Division), under Department of Higher Education, a constituent of the Ministry of Human Resource Development
Iceland – Landsbókasafn (National and University Library of Iceland)
Israel – The Israel Center for Libraries
Italy – EDISER srl, owned by Associazione Italiana Editori (Italian Publishers Association)
Maldives – The National Bureau of Classification (NBC)
Malta – The National Book Council (Maltese: Il-Kunsill Nazzjonali tal-Ktieb)
Morocco – The National Library of Morocco
New Zealand – The National Library of New Zealand
Pakistan – National Library of Pakistan
Philippines – National Library of the Philippines
South Africa – National Library of South Africa
Spain – Spanish ISBN Agency – Agencia del ISBN
Turkey – General Directorate of Libraries and Publications, a branch of the Ministry of Culture
United Kingdom and Republic of Ireland – Nielsen Book Services Ltd, part of Nielsen Holdings N.V.
United States – R. R. Bowker

Registration group element
The ISBN registration group element is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979), and can be separated between hyphens, such as ""978-1-..."". Registration groups have primarily been allocated within the 978 prefix element. The single-digit registration groups within the 978-prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit registration group is 99936, for Bhutan. The allocated registration groups are: 0–5, 600–625, 65, 7, 80–94, 950–989, 9917–9989, and 99901–99983. Books published in rare languages typically have longer group elements.Within the 979 prefix element, the registration group 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN. The registration groups within prefix element 979 that have been assigned are 8 for the United States of America, 10 for France, 11 for the Republic of Korea, and 12 for Italy.The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.

Registrant element
The national ISBN agency assigns the registrant element (cf. Category:ISBN agencies) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not legally required to assign an ISBN, although most large bookstores only handle publications that have ISBNs assigned to them.A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form. The web site of the ISBN agency does not offer any free method of looking up publisher codes. Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.
Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.
By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements. Here are some sample ISBN-10 codes, illustrating block length variations.

Pattern for English language ISBNs
English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:

Check digits
A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the 10-digit ISBN is an extension of that for SBNs, so the two systems are compatible; an SBN prefixed with a zero (the 10-digit ISBN) will give the same check digit as the SBN without the zero. The check digit is base eleven, and can be an integer between 0 and 9, or an 'X'. The system for 13-digit ISBNs is not compatible with SBNs and will, in general, give a different check digit from the corresponding 10-digit ISBN, so does not provide the same protection against transposition. This is because the 13-digit code was required to be compatible with the EAN format, and hence could not contain an 'X'.

ISBN-10 check digits
According to the 2001 edition of the International ISBN Agency's official user manual, the ISBN-10 check digit (which is the last digit of the 10-digit ISBN) must range from 0 to 10 (the symbol 'X' is used for 10), and must be such that the sum of the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11. That is, if xi is the ith digit, then x10 must be chosen such that:

For example, for an ISBN-10 of 0-306-40615-2:

Formally, using modular arithmetic, this is rendered:

It is also true for ISBN-10s that the sum of all ten digits, each multiplied by its weight in ascending order from 1 to 10, is a multiple of 11. For this example:

Formally, this is rendered:

The two most common errors in handling an ISBN (e.g. when typing it or writing it down) are a single altered digit or the transposition of adjacent digits. It can be proven mathematically that all pairs of valid ISBN-10s differ in at least two digits. It can also be proven that there are no pairs of valid ISBN-10s with eight identical digits and two transposed digits. (These proofs are true because the ISBN is less than eleven digits long and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e., if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error were to occur in the publishing house and remain undetected, the book would be issued with an invalid ISBN.In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).

ISBN-10 check digit calculation
Each of the first nine digits of the 10-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.
For example, the check digit for an ISBN-10 of 0-306-40615-? is calculated as follows:

Adding 2 to 130 gives a multiple of 11 (because 132 = 12×11) – this is the only number between 0 and 10 which does so. Therefore, the check digit has to be 2, and the complete sequence is ISBN 0-306-40615-2. If the value of 
  
    
      
        
          x
          
            10
          
        
      
    
    {\displaystyle x_{10}}
   required to satisfy this condition is 10, then an 'X' should be used.
Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation, the calculation could result in a check digit value of 11−0 = 11, which is invalid. (Strictly speaking, the first ""modulo 11"" is not needed, but it may be considered to simplify the calculation.)
For example, the check digit for the ISBN-10 of 0-306-40615-? is calculated as follows:

Thus the check digit is 2.
It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding t into s computes the necessary multiples:

The modular reduction can be done once at the end, as shown above (in which case s could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or s and t could be reduced by a conditional subtract after each addition.

ISBN-13 check digit calculation
Appendix 1 of the International ISBN Agency's official user manual describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10. As ISBN-13 is a subset of EAN-13, the algorithm for calculating the check digit is exactly the same for both.
Formally, using modular arithmetic, this is rendered:

The calculation of an ISBN-13 check digit begins with the first twelve digits of the 13-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.
For example, the ISBN-13 check digit of 978-0-306-40615-? is calculated as follows:

s = 9×1 + 7×3 + 8×1 + 0×3 + 3×1 + 0×3 + 6×1 + 4×3 + 0×1 + 6×3 + 1×1 + 5×3
  =   9 +  21 +   8 +   0 +   3 +   0 +   6 +  12 +   0 +  18 +   1 +  15
  = 93
93 / 10 = 9 remainder 3
10 –  3 = 7

Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7.
In general, the ISBN-13 check digit is calculated as follows.
Let

Then

This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0–9 to express the check digit.
Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).

ISBN-10 to ISBN-13 conversion
An ISBN-10 is converted to ISBN-13 by prepending ""978"" to the  ISBN-10 and recalculating the final checksum digit using the ISBN-13 algorithm. The reverse process can also be performed, but not for numbers commencing with a prefix other than 978, which have no 10-digit equivalent.

Errors in usage
Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers. For example, ISBN 0-590-76484-5 is shared by two books – Ninja gaiden®: a novel based on the best-selling game by Tecmo (1990) and Wacky laws (1997), both published by Scholastic.
Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase ""Cancelled ISBN"". However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine. OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.

eISBN
Only the term ""ISBN"" should be used; the terms ""eISBN"" and ""e-ISBN"" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic ""eISBN"" which encompasses all the e-book formats for a title.

EAN format used in barcodes, and upgrading
Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits called an EAN-5 for the currency and the recommended retail price. For 10-digit ISBNs, the number ""978"", the Bookland ""country code"", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN-13 formula (modulo 10, 1x and 3x weighting on alternating digits).
Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a 13-digit ISBN (ISBN-13). The process began on 1 January 2005 and was planned to conclude on 1 January 2007. As of 2011, all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. The 10-digit ISMN codes differed visually as they began with an ""M"" letter; the bar code represents the ""M"" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now thirteen digits commencing 979-0; 979-1 to 979-9 will be used by ISBN.
Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the 10-digit ISBN check digit generally is not the same as the 13-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the ISBN-13 in North America.

See also
ASIN (Amazon Standard Identification Number)
BICI (Book Item and Component Identifier)
CODEN (serial publication identifier currently used by libraries; replaced by the ISSN for new works)
DOI (Digital Object Identifier)
ESTC (English Short Title Catalogue)
ETTN (Electronic Textbook Track Number)
ISAN (International Standard Audiovisual Number)
ISMN (International Standard Music Number)
ISRC (International Standard Recording Code)
ISSN (International Standard Serial Number)
ISTC (International Standard Text Code)
ISWC (International Standard Musical Work Code)
ISWN (International Standard Wine Number)
LCCN (Library of Congress Control Number)
License number (East German books) (Book identification system used between 1951 and 1990 in the former GDR)
List of group-0 ISBN publisher codes
List of group-1 ISBN publisher codes
List of ISBN identifier groups
OCLC number (Online Computer Library Center number)
Registration authority
SICI (Serial Item and Contribution Identifier)
VD 16 (Verzeichnis der im deutschen Sprachbereich erschienenen Drucke des 16. Jahrhunderts, ""Bibliography of Books Printed in the German Speaking Countries of the Sixteenth Century"")
VD 17 (Verzeichnis der im deutschen Sprachraum erschienenen Drucke des 17. Jahrhunderts, ""Bibliography of Books Printed in the German Speaking Countries of the Seventeenth Century"")

Notes
References
External links

ISO 2108:2017 – International Standard Book Number (ISBN)
International ISBN Agency—coordinates and supervises the worldwide use of the ISBN system
Numerical List of Group Identifiers—List of language/region prefixes
Free conversion tool: ISBN-10 to ISBN-13 & ISBN-13 to ISBN-10 from the ISBN agency. Also shows correct hyphenation & verifies if ISBNs are valid or not.
""Guidelines for the Implementation of 13-Digit ISBNs"" (PDF). Archived from the original (PDF) on 12 September 2004
""Are You Ready for ISBN-13?"". R. R. Bowker LLC.
RFC 3187—Using International Standard Book Numbers as Uniform Resource Names (URN)
Book sources search — allows search by ISBN",https://en.wikipedia.org/wiki/International_Standard_Book_Number,"['All articles containing potentially dated statements', 'All articles that may contain original research', 'All articles with unsourced statements', 'Articles containing German-language text', 'Articles containing Maltese-language text', 'Articles containing potentially dated statements from 2011', 'Articles that may contain original research from May 2019', 'Articles with short description', 'Articles with unsourced statements from March 2012', 'Book publishing', 'Book terminology', 'Bookselling', 'CS1 Italian-language sources (it)', 'CS1 Maltese-language sources (mt)', 'CS1 Portuguese-language sources (pt)', 'CS1 errors: missing periodical', 'Checksum algorithms', 'ISO standards', 'Identifiers', 'International Standard Book Number', 'Pages using Sister project links with hidden wikidata', 'Short description matches Wikidata', 'Unique identifiers', 'Use dmy dates from May 2020', 'Webarchive template wayback links', 'Wikipedia articles with BNE identifiers', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with NDL identifiers', 'Wikipedia pages semi-protected against vandalism']",Data Science
104,Independent component analysis,"In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that the subcomponents are non-Gaussian signals and that they are statistically independent from each other. ICA is a special case of blind source separation. A common example application is the ""cocktail party problem"" of listening in on one person's speech in a noisy room.

Introduction
Independent component analysis attempts to decompose a multivariate signal into independent non-Gaussian signals. As an example, sound is usually a signal that is composed of the numerical addition, at each time t, of signals from several sources. The question then is whether it is possible to separate these contributing sources from the observed total signal. When the statistical independence assumption is correct, blind ICA separation of a mixed signal gives very good results. It is also used for signals that are not supposed to be generated by mixing for analysis purposes.
A simple application of ICA is the ""cocktail party problem"", where the underlying speech signals are separated from a sample data consisting of people talking simultaneously in a room. Usually the problem is simplified by assuming no time delays or echoes. Note that a filtered and delayed signal is a copy of a dependent component, and thus the statistical independence assumption is not violated.
Mixing weights for constructing the 
  
    
      
        M
      
    
    {\textstyle M}
   observed signals from the 
  
    
      
        N
      
    
    {\textstyle N}
   components can be placed in an 
  
    
      
        M
        ×
        N
      
    
    {\textstyle M\times N}
   matrix. An important thing to consider is that if 
  
    
      
        N
      
    
    {\textstyle N}
   sources are present, at least 
  
    
      
        N
      
    
    {\textstyle N}
   observations (e.g. microphones if the observed signal is audio) are needed to recover the original signals. When there are an equal number of observations and source signals, the mixing matrix is square (
  
    
      
        M
        =
        N
      
    
    {\textstyle M=N}
  ). Other cases of underdetermined (
  
    
      
        M
        <
        N
      
    
    {\textstyle M<N}
  ) and overdetermined (
  
    
      
        M
        >
        N
      
    
    {\textstyle M>N}
  ) have been investigated.
That the ICA separation of mixed signals gives very good results is based on two assumptions and three effects of mixing source signals. Two assumptions:

The source signals are independent of each other.
The values in each source signal have non-Gaussian distributions.Three effects of mixing source signals:

Independence: As per assumption 1, the source signals are independent; however, their signal mixtures are not. This is because the signal mixtures share the same source signals.
Normality: According to the Central Limit Theorem, the distribution of a sum of independent random variables with finite variance tends towards a Gaussian distribution.Loosely speaking, a sum of two independent random variables usually has a distribution that is closer to Gaussian than any of the two original variables. Here we consider the value of each signal as the random variable.
Complexity: The temporal complexity of any signal mixture is greater than that of its simplest constituent source signal.Those principles contribute to the basic establishment of ICA. If the signals extracted from a set of mixtures are independent, and have non-Gaussian histograms or have low complexity, then they must be source signals.

Defining component independence
ICA finds the independent components (also called factors, latent variables or sources) by maximizing the statistical independence of the estimated components. We may choose one of many ways to define a proxy for independence, and this choice governs the form of the ICA algorithm. The two broadest definitions of independence for ICA are

Minimization of mutual information
Maximization of non-GaussianityThe Minimization-of-Mutual information (MMI) family of ICA algorithms uses measures like Kullback-Leibler Divergence and maximum entropy.  The non-Gaussianity family of ICA algorithms, motivated by the central limit theorem, uses kurtosis and negentropy.
Typical algorithms for ICA use centering (subtract the mean to create a zero mean signal), whitening (usually with the eigenvalue decomposition), and dimensionality reduction as preprocessing steps in order to simplify and reduce the complexity of the problem for the actual iterative algorithm. Whitening and dimension reduction can be achieved with principal component analysis or singular value decomposition. Whitening ensures that all dimensions are treated equally a priori before the algorithm is run. Well-known algorithms for ICA include infomax, FastICA, JADE, and kernel-independent component analysis, among others. In general, ICA cannot identify the actual number of source signals, a uniquely correct ordering of the source signals, nor the proper scaling (including sign) of the source signals.
ICA is important to blind signal separation and has many practical applications. It is closely related to (or even a special case of) the search for a factorial code of the data, i.e., a new vector-valued representation of each data vector such that it gets uniquely encoded by the resulting code vector (loss-free coding), but the code components are statistically independent.

Mathematical definitions
Linear independent component analysis can be divided into noiseless and noisy cases, where noiseless ICA is a special case of noisy ICA. Nonlinear ICA should be considered as a separate case.

General definition
The data are represented by the observed random vector 
  
    
      
        
          x
        
        =
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            m
          
        
        
          )
          
            T
          
        
      
    
    {\displaystyle {\boldsymbol {x}}=(x_{1},\ldots ,x_{m})^{T}}
   and the hidden components as the random vector 
  
    
      
        
          s
        
        =
        (
        
          s
          
            1
          
        
        ,
        …
        ,
        
          s
          
            n
          
        
        
          )
          
            T
          
        
        .
      
    
    {\displaystyle {\boldsymbol {s}}=(s_{1},\ldots ,s_{n})^{T}.}
   The task is to transform the observed data 
  
    
      
        
          x
        
        ,
      
    
    {\displaystyle {\boldsymbol {x}},}
   using a linear static transformation 
  
    
      
        
          W
        
      
    
    {\displaystyle {\boldsymbol {W}}}
   as 
  
    
      
        
          s
        
        =
        
          W
        
        
          x
        
        ,
      
    
    {\displaystyle {\boldsymbol {s}}={\boldsymbol {W}}{\boldsymbol {x}},}
   into a vector of maximally independent components 
  
    
      
        
          s
        
      
    
    {\displaystyle {\boldsymbol {s}}}
   measured by some function 
  
    
      
        F
        (
        
          s
          
            1
          
        
        ,
        …
        ,
        
          s
          
            n
          
        
        )
      
    
    {\displaystyle F(s_{1},\ldots ,s_{n})}
   of independence.

Generative model
Linear noiseless ICA
The components 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   of the observed random vector 
  
    
      
        
          x
        
        =
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            m
          
        
        
          )
          
            T
          
        
      
    
    {\displaystyle {\boldsymbol {x}}=(x_{1},\ldots ,x_{m})^{T}}
   are generated as a sum of the independent components 
  
    
      
        
          s
          
            k
          
        
      
    
    {\displaystyle s_{k}}
  , 
  
    
      
        k
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle k=1,\ldots ,n}
  :

  
    
      
        
          x
          
            i
          
        
        =
        
          a
          
            i
            ,
            1
          
        
        
          s
          
            1
          
        
        +
        ⋯
        +
        
          a
          
            i
            ,
            k
          
        
        
          s
          
            k
          
        
        +
        ⋯
        +
        
          a
          
            i
            ,
            n
          
        
        
          s
          
            n
          
        
      
    
    {\displaystyle x_{i}=a_{i,1}s_{1}+\cdots +a_{i,k}s_{k}+\cdots +a_{i,n}s_{n}}
  
weighted by the mixing weights 
  
    
      
        
          a
          
            i
            ,
            k
          
        
      
    
    {\displaystyle a_{i,k}}
  .
The same generative model can be written in vector form as 
  
    
      
        
          x
        
        =
        
          ∑
          
            k
            =
            1
          
          
            n
          
        
        
          s
          
            k
          
        
        
          
            a
          
          
            k
          
        
      
    
    {\displaystyle {\boldsymbol {x}}=\sum _{k=1}^{n}s_{k}{\boldsymbol {a}}_{k}}
  , where the observed random vector 
  
    
      
        
          x
        
      
    
    {\displaystyle {\boldsymbol {x}}}
   is represented by the basis vectors 
  
    
      
        
          
            a
          
          
            k
          
        
        =
        (
        
          
            a
          
          
            1
            ,
            k
          
        
        ,
        …
        ,
        
          
            a
          
          
            m
            ,
            k
          
        
        
          )
          
            T
          
        
      
    
    {\displaystyle {\boldsymbol {a}}_{k}=({\boldsymbol {a}}_{1,k},\ldots ,{\boldsymbol {a}}_{m,k})^{T}}
  . The basis vectors 
  
    
      
        
          
            a
          
          
            k
          
        
      
    
    {\displaystyle {\boldsymbol {a}}_{k}}
   form the columns of the mixing matrix 
  
    
      
        
          A
        
        =
        (
        
          
            a
          
          
            1
          
        
        ,
        …
        ,
        
          
            a
          
          
            n
          
        
        )
      
    
    {\displaystyle {\boldsymbol {A}}=({\boldsymbol {a}}_{1},\ldots ,{\boldsymbol {a}}_{n})}
   and the generative formula can be written as 
  
    
      
        
          x
        
        =
        
          A
        
        
          s
        
      
    
    {\displaystyle {\boldsymbol {x}}={\boldsymbol {A}}{\boldsymbol {s}}}
  , where 
  
    
      
        
          s
        
        =
        (
        
          s
          
            1
          
        
        ,
        …
        ,
        
          s
          
            n
          
        
        
          )
          
            T
          
        
      
    
    {\displaystyle {\boldsymbol {s}}=(s_{1},\ldots ,s_{n})^{T}}
  .
Given the model and realizations (samples) 
  
    
      
        
          
            x
          
          
            1
          
        
        ,
        …
        ,
        
          
            x
          
          
            N
          
        
      
    
    {\displaystyle {\boldsymbol {x}}_{1},\ldots ,{\boldsymbol {x}}_{N}}
   of the random vector 
  
    
      
        
          x
        
      
    
    {\displaystyle {\boldsymbol {x}}}
  , the task is to estimate both the mixing matrix 
  
    
      
        
          A
        
      
    
    {\displaystyle {\boldsymbol {A}}}
   and the sources 
  
    
      
        
          s
        
      
    
    {\displaystyle {\boldsymbol {s}}}
  . This is done by adaptively calculating the 
  
    
      
        
          w
        
      
    
    {\displaystyle {\boldsymbol {w}}}
   vectors and setting up a cost function which either maximizes the non-gaussianity of the calculated 
  
    
      
        
          s
          
            k
          
        
        =
        
          
            w
          
          
            T
          
        
        
          x
        
      
    
    {\displaystyle s_{k}={\boldsymbol {w}}^{T}{\boldsymbol {x}}}
   or minimizes the mutual information. In some cases, a priori knowledge of the probability distributions of the sources can be used in the cost function.
The original sources 
  
    
      
        
          s
        
      
    
    {\displaystyle {\boldsymbol {s}}}
   can be recovered by multiplying the observed signals 
  
    
      
        
          x
        
      
    
    {\displaystyle {\boldsymbol {x}}}
   with the inverse of the mixing matrix 
  
    
      
        
          W
        
        =
        
          
            A
          
          
            −
            1
          
        
      
    
    {\displaystyle {\boldsymbol {W}}={\boldsymbol {A}}^{-1}}
  , also known as the unmixing matrix. Here it is assumed that the mixing matrix is square (
  
    
      
        n
        =
        m
      
    
    {\displaystyle n=m}
  ). If the number of basis vectors is greater than the dimensionality of the observed vectors, 
  
    
      
        n
        >
        m
      
    
    {\displaystyle n>m}
  , the task is overcomplete but is still solvable with the pseudo inverse.

Linear noisy ICA
With the added assumption of zero-mean and uncorrelated Gaussian noise 
  
    
      
        n
        ∼
        N
        (
        0
        ,
        diag
        ⁡
        (
        Σ
        )
        )
      
    
    {\displaystyle n\sim N(0,\operatorname {diag} (\Sigma ))}
  , the ICA model takes the form 
  
    
      
        
          x
        
        =
        
          A
        
        
          s
        
        +
        n
      
    
    {\displaystyle {\boldsymbol {x}}={\boldsymbol {A}}{\boldsymbol {s}}+n}
  .

Nonlinear ICA
The mixing of the sources does not need to be linear. Using a nonlinear mixing function 
  
    
      
        f
        (
        ⋅
        
          |
        
        θ
        )
      
    
    {\displaystyle f(\cdot |\theta )}
   with parameters 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   the nonlinear ICA model is 
  
    
      
        x
        =
        f
        (
        s
        
          |
        
        θ
        )
        +
        n
      
    
    {\displaystyle x=f(s|\theta )+n}
  .

Identifiability
The independent components are identifiable up to a permutation and scaling of the sources. This identifiability requires that:

At most one of the sources 
  
    
      
        
          s
          
            k
          
        
      
    
    {\displaystyle s_{k}}
   is Gaussian,
The number of observed mixtures, 
  
    
      
        m
      
    
    {\displaystyle m}
  , must be at least as large as the number of estimated components 
  
    
      
        n
      
    
    {\displaystyle n}
  : 
  
    
      
        m
        ≥
        n
      
    
    {\displaystyle m\geq n}
  . It is equivalent to say that the mixing matrix 
  
    
      
        
          A
        
      
    
    {\displaystyle {\boldsymbol {A}}}
   must be of full rank for its inverse to exist.

Binary ICA
A special variant of ICA is binary ICA in which both signal sources and monitors are in binary form and observations from monitors are disjunctive mixtures of binary independent sources. The problem was shown to have applications in many domains including medical diagnosis, multi-cluster assignment, network tomography and internet resource management.
Let 
  
    
      
        
          
            x
            
              1
            
          
          ,
          
            x
            
              2
            
          
          ,
          …
          ,
          
            x
            
              m
            
          
        
      
    
    {\displaystyle {x_{1},x_{2},\ldots ,x_{m}}}
   be the set of binary variables from 
  
    
      
        m
      
    
    {\displaystyle m}
   monitors and 
  
    
      
        
          
            y
            
              1
            
          
          ,
          
            y
            
              2
            
          
          ,
          …
          ,
          
            y
            
              n
            
          
        
      
    
    {\displaystyle {y_{1},y_{2},\ldots ,y_{n}}}
   be the set of binary variables from 
  
    
      
        n
      
    
    {\displaystyle n}
   sources. Source-monitor connections are represented by the (unknown) mixing matrix 
  
    
      
        
          G
        
      
    
    {\textstyle {\boldsymbol {G}}}
  , where 
  
    
      
        
          g
          
            i
            j
          
        
        =
        1
      
    
    {\displaystyle g_{ij}=1}
   indicates that signal from the i-th source can be observed by the j-th monitor. The system works as follows: at any time, if a source 
  
    
      
        i
      
    
    {\displaystyle i}
   is active (
  
    
      
        
          y
          
            i
          
        
        =
        1
      
    
    {\displaystyle y_{i}=1}
  ) and it is connected to the monitor 
  
    
      
        j
      
    
    {\displaystyle j}
   (
  
    
      
        
          g
          
            i
            j
          
        
        =
        1
      
    
    {\displaystyle g_{ij}=1}
  ) then the monitor 
  
    
      
        j
      
    
    {\displaystyle j}
   will observe some activity (
  
    
      
        
          x
          
            j
          
        
        =
        1
      
    
    {\displaystyle x_{j}=1}
  ). Formally we have:

  
    
      
        
          x
          
            i
          
        
        =
        
          ⋁
          
            j
            =
            1
          
          
            n
          
        
        (
        
          g
          
            i
            j
          
        
        ∧
        
          y
          
            j
          
        
        )
        ,
        i
        =
        1
        ,
        2
        ,
        …
        ,
        m
        ,
      
    
    {\displaystyle x_{i}=\bigvee _{j=1}^{n}(g_{ij}\wedge y_{j}),i=1,2,\ldots ,m,}
  where 
  
    
      
        ∧
      
    
    {\displaystyle \wedge }
   is Boolean AND and 
  
    
      
        ∨
      
    
    {\displaystyle \vee }
   is Boolean OR. Note that noise is not explicitly modelled, rather, can be treated as independent sources.
The above problem can be heuristically solved  by assuming variables are continuous and running FastICA on binary observation data to get the mixing matrix 
  
    
      
        
          G
        
      
    
    {\textstyle {\boldsymbol {G}}}
   (real values), then apply round number techniques on 
  
    
      
        
          G
        
      
    
    {\textstyle {\boldsymbol {G}}}
   to obtain the binary values. This approach has been shown to produce a highly inaccurate result.Another method is to use dynamic programming: recursively breaking the observation matrix 
  
    
      
        
          X
        
      
    
    {\textstyle {\boldsymbol {X}}}
   into its sub-matrices and run the inference algorithm on these sub-matrices. The key observation which leads to this algorithm is the sub-matrix 
  
    
      
        
          
            X
          
          
            0
          
        
      
    
    {\textstyle {\boldsymbol {X}}^{0}}
   of 
  
    
      
        
          X
        
      
    
    {\textstyle {\boldsymbol {X}}}
   where 
  
    
      
        
          x
          
            i
            j
          
        
        =
        0
        ,
        ∀
        j
      
    
    {\textstyle x_{ij}=0,\forall j}
   corresponds to the unbiased observation matrix of hidden components that do not have connection to the 
  
    
      
        i
      
    
    {\displaystyle i}
  -th monitor. Experimental results from  show that this approach is accurate under moderate noise levels.
The Generalized Binary ICA framework  introduces a broader problem formulation which does not necessitate any knowledge on the generative model. In other words, this method attempts to decompose a source into its independent components (as much as possible, and without losing any information) with no prior assumption on the way it was generated. Although this problem appears quite complex, it can be accurately solved with a branch and bound search tree algorithm or tightly upper bounded with a single multiplication of a matrix with a vector.

Methods for blind source separation
Projection pursuit
Signal mixtures tend to have Gaussian probability density functions, and source signals tend to have non-Gaussian probability density functions.  Each source signal can be extracted from a set of signal mixtures by taking the inner product of a weight vector and those signal mixtures where this inner product provides an orthogonal projection of the signal mixtures.  The remaining challenge is finding such a weight vector. One type of method for doing so is projection pursuit.Projection pursuit seeks one projection at a time such that the extracted signal is as non-Gaussian as possible. This contrasts with ICA, which typically extracts M signals simultaneously from M signal mixtures, which requires estimating a M × M unmixing matrix. One practical advantage of projection pursuit over ICA is that fewer than M signals can be extracted if required, where each source signal is extracted from M signal mixtures using an M-element weight vector.
We can use kurtosis to recover the multiple source signal by finding the correct weight vectors with the use of projection pursuit.
The kurtosis of the probability density function of a signal, for a finite sample, is computed as

  
    
      
        K
        =
        
          
            
              E
              ⁡
              [
              (
              
                y
              
              −
              
                
                  y
                  ¯
                
              
              
                )
                
                  4
                
              
              ]
            
            
              (
              E
              ⁡
              [
              (
              
                y
              
              −
              
                
                  y
                  ¯
                
              
              
                )
                
                  2
                
              
              ]
              
                )
                
                  2
                
              
            
          
        
        −
        3
      
    
    {\displaystyle K={\frac {\operatorname {E} [(\mathbf {y} -\mathbf {\overline {y}} )^{4}]}{(\operatorname {E} [(\mathbf {y} -\mathbf {\overline {y}} )^{2}])^{2}}}-3}
  where 
  
    
      
        
          
            y
            ¯
          
        
      
    
    {\displaystyle \mathbf {\overline {y}} }
   is the sample mean of 
  
    
      
        
          y
        
      
    
    {\displaystyle \mathbf {y} }
  , the extracted signals. The constant 3 ensures that Gaussian signals have zero kurtosis, Super-Gaussian signals have positive kurtosis, and Sub-Gaussian signals have negative kurtosis. The denominator is the variance of 
  
    
      
        
          y
        
      
    
    {\displaystyle \mathbf {y} }
  , and ensures that the measured kurtosis takes account of signal variance. The goal of projection pursuit is to maximize the kurtosis, and make the extracted signal as non-normal as possible.
Using kurtosis as a measure of non-normality, we can now examine how the kurtosis of a signal 
  
    
      
        
          y
        
        =
        
          
            w
          
          
            T
          
        
        
          x
        
      
    
    {\displaystyle \mathbf {y} =\mathbf {w} ^{T}\mathbf {x} }
   extracted from a set of M mixtures 
  
    
      
        
          x
        
        =
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            M
          
        
        
          )
          
            T
          
        
      
    
    {\displaystyle \mathbf {x} =(x_{1},x_{2},\ldots ,x_{M})^{T}}
   varies as the weight vector 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   is rotated around the origin. Given our assumption that each source signal 
  
    
      
        
          s
        
      
    
    {\displaystyle \mathbf {s} }
   is super-gaussian we would expect:

the kurtosis of the extracted signal 
  
    
      
        
          y
        
      
    
    {\displaystyle \mathbf {y} }
   to be maximal precisely when 
  
    
      
        
          y
        
        =
        
          s
        
      
    
    {\displaystyle \mathbf {y} =\mathbf {s} }
  .
the kurtosis of the extracted signal 
  
    
      
        
          y
        
      
    
    {\displaystyle \mathbf {y} }
   to be maximal when 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   is orthogonal to the projected axes 
  
    
      
        
          S
          
            1
          
        
      
    
    {\displaystyle S_{1}}
   or 
  
    
      
        
          S
          
            2
          
        
      
    
    {\displaystyle S_{2}}
  , because we know the optimal weight vector should be orthogonal to a transformed axis 
  
    
      
        
          S
          
            1
          
        
      
    
    {\displaystyle S_{1}}
   or 
  
    
      
        
          S
          
            2
          
        
      
    
    {\displaystyle S_{2}}
  .For multiple source mixture signals, we can use kurtosis and Gram-Schmidt Orthogonalization (GSO) to recover the signals. Given M signal mixtures in an M-dimensional space, GSO project these data points onto an (M-1)-dimensional space by using the weight vector. We can guarantee the independence of the extracted signals with the use of GSO.
In order to find the correct value of 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
  , we can use gradient descent method.  We first of all whiten the data, and transform 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   into a new mixture 
  
    
      
        
          z
        
      
    
    {\displaystyle \mathbf {z} }
  , which has unit variance, and 
  
    
      
        
          z
        
        =
        (
        
          z
          
            1
          
        
        ,
        
          z
          
            2
          
        
        ,
        …
        ,
        
          z
          
            M
          
        
        
          )
          
            T
          
        
      
    
    {\displaystyle \mathbf {z} =(z_{1},z_{2},\ldots ,z_{M})^{T}}
  . This process can be achieved by applying Singular value decomposition to 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
  ,

  
    
      
        
          x
        
        =
        
          U
        
        
          D
        
        
          
            V
          
          
            T
          
        
      
    
    {\displaystyle \mathbf {x} =\mathbf {U} \mathbf {D} \mathbf {V} ^{T}}
  Rescaling each vector 
  
    
      
        
          U
          
            i
          
        
        =
        
          U
          
            i
          
        
        
          /
        
        E
        ⁡
        (
        
          U
          
            i
          
          
            2
          
        
        )
      
    
    {\displaystyle U_{i}=U_{i}/\operatorname {E} (U_{i}^{2})}
  , and let 
  
    
      
        
          z
        
        =
        
          U
        
      
    
    {\displaystyle \mathbf {z} =\mathbf {U} }
  . The signal extracted by a weighted vector 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   is 
  
    
      
        
          y
        
        =
        
          
            w
          
          
            T
          
        
        
          z
        
      
    
    {\displaystyle \mathbf {y} =\mathbf {w} ^{T}\mathbf {z} }
  . If the weight vector w has unit length, that is 
  
    
      
        E
        ⁡
        [
        (
        
          
            w
          
          
            T
          
        
        
          z
        
        
          )
          
            2
          
        
        ]
        =
        1
      
    
    {\displaystyle \operatorname {E} [(\mathbf {w} ^{T}\mathbf {z} )^{2}]=1}
  , then the kurtosis can be written as:

  
    
      
        K
        =
        
          
            
              E
              ⁡
              [
              
                
                  y
                
                
                  4
                
              
              ]
            
            
              (
              E
              ⁡
              [
              
                
                  y
                
                
                  2
                
              
              ]
              
                )
                
                  2
                
              
            
          
        
        −
        3
        =
        E
        ⁡
        [
        (
        
          
            w
          
          
            T
          
        
        
          z
        
        
          )
          
            4
          
        
        ]
        −
        3.
      
    
    {\displaystyle K={\frac {\operatorname {E} [\mathbf {y} ^{4}]}{(\operatorname {E} [\mathbf {y} ^{2}])^{2}}}-3=\operatorname {E} [(\mathbf {w} ^{T}\mathbf {z} )^{4}]-3.}
  The updating process for 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   is:

  
    
      
        
          
            w
          
          
            n
            e
            w
          
        
        =
        
          
            w
          
          
            o
            l
            d
          
        
        −
        η
        E
        ⁡
        [
        
          z
        
        (
        
          
            w
          
          
            o
            l
            d
          
          
            T
          
        
        
          z
        
        
          )
          
            3
          
        
        ]
        .
      
    
    {\displaystyle \mathbf {w} _{new}=\mathbf {w} _{old}-\eta \operatorname {E} [\mathbf {z} (\mathbf {w} _{old}^{T}\mathbf {z} )^{3}].}
  where 
  
    
      
        η
      
    
    {\displaystyle \eta }
   is a small constant to guarantee that 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   converges to the optimal solution. After each update, we normalize 
  
    
      
        
          
            w
          
          
            n
            e
            w
          
        
        =
        
          
            
              
                w
              
              
                n
                e
                w
              
            
            
              
                |
              
              
                
                  w
                
                
                  n
                  e
                  w
                
              
              
                |
              
            
          
        
      
    
    {\displaystyle \mathbf {w} _{new}={\frac {\mathbf {w} _{new}}{|\mathbf {w} _{new}|}}}
  , and set 
  
    
      
        
          
            w
          
          
            o
            l
            d
          
        
        =
        
          
            w
          
          
            n
            e
            w
          
        
      
    
    {\displaystyle \mathbf {w} _{old}=\mathbf {w} _{new}}
  , and repeat the updating process until convergence. We can also use another algorithm to update the weight vector 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
  .
Another approach is using negentropy instead of kurtosis. Using negentropy is a more robust method than kurtosis, as kurtosis is very sensitive to outliers. The negentropy methods are based on an important property of Gaussian distribution: a Gaussian variable has the largest entropy among all continuous random variables of equal variance. This is also the reason why  we want to find the most nongaussian variables. A simple proof can be found in Differential entropy.

  
    
      
        J
        (
        x
        )
        =
        S
        (
        y
        )
        −
        S
        (
        x
        )
        
      
    
    {\displaystyle J(x)=S(y)-S(x)\,}
  y is a Gaussian random variable of the same covariance matrix as x

  
    
      
        S
        (
        x
        )
        =
        −
        ∫
        
          p
          
            x
          
        
        (
        u
        )
        log
        ⁡
        
          p
          
            x
          
        
        (
        u
        )
        d
        u
      
    
    {\displaystyle S(x)=-\int p_{x}(u)\log p_{x}(u)du}
  An approximation for negentropy is

  
    
      
        J
        (
        x
        )
        =
        
          
            1
            12
          
        
        (
        E
        (
        
          x
          
            3
          
        
        )
        
          )
          
            2
          
        
        +
        
          
            1
            48
          
        
        (
        k
        u
        r
        t
        (
        x
        )
        
          )
          
            2
          
        
      
    
    {\displaystyle J(x)={\frac {1}{12}}(E(x^{3}))^{2}+{\frac {1}{48}}(kurt(x))^{2}}
  A proof can be found in the original papers of Comon; it has been reproduced in the book Independent Component Analysis by Aapo Hyvärinen, Juha Karhunen, and Erkki Oja This approximation also suffers from the same problem as kurtosis (sensitivity to outliers). Other approaches have been developed.

  
    
      
        J
        (
        y
        )
        =
        
          k
          
            1
          
        
        (
        E
        (
        
          G
          
            1
          
        
        (
        y
        )
        )
        
          )
          
            2
          
        
        +
        
          k
          
            2
          
        
        (
        E
        (
        
          G
          
            2
          
        
        (
        y
        )
        )
        −
        E
        (
        
          G
          
            2
          
        
        (
        v
        )
        
          )
          
            2
          
        
      
    
    {\displaystyle J(y)=k_{1}(E(G_{1}(y)))^{2}+k_{2}(E(G_{2}(y))-E(G_{2}(v))^{2}}
  A choice of 
  
    
      
        
          G
          
            1
          
        
      
    
    {\displaystyle G_{1}}
   and 
  
    
      
        
          G
          
            2
          
        
      
    
    {\displaystyle G_{2}}
   are 

  
    
      
        
          G
          
            1
          
        
        =
        
          
            1
            
              a
              
                1
              
            
          
        
        log
        ⁡
        (
        cosh
        ⁡
        (
        
          a
          
            1
          
        
        u
        )
        )
      
    
    {\displaystyle G_{1}={\frac {1}{a_{1}}}\log(\cosh(a_{1}u))}
   and 
  
    
      
        
          G
          
            2
          
        
        =
        −
        exp
        ⁡
        (
        −
        
          
            
              u
              
                2
              
            
            2
          
        
        )
      
    
    {\displaystyle G_{2}=-\exp(-{\frac {u^{2}}{2}})}

Based on infomax
Infomax ICA is essentially a multivariate, parallel version of projection pursuit. Whereas projection pursuit extracts a series of signals one at a time from a set of M signal mixtures, ICA extracts M signals in parallel. This tends to make ICA more robust than projection pursuit.The projection pursuit method uses Gram-Schmidt orthogonalization to ensure the independence of the extracted signal, while ICA use infomax and maximum likelihood estimate to ensure the independence of the extracted signal. The Non-Normality of the extracted signal is achieved by assigning an appropriate model, or prior, for the signal.
The process of ICA based on infomax in short is: given a set of signal mixtures 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   and a set of identical independent model cumulative distribution functions(cdfs) 
  
    
      
        g
      
    
    {\displaystyle g}
  , we seek the unmixing matrix 
  
    
      
        
          W
        
      
    
    {\displaystyle \mathbf {W} }
   which maximizes the joint entropy of the signals 
  
    
      
        
          Y
        
        =
        g
        (
        
          y
        
        )
      
    
    {\displaystyle \mathbf {Y} =g(\mathbf {y} )}
  , where 
  
    
      
        
          y
        
        =
        
          W
          x
        
      
    
    {\displaystyle \mathbf {y} =\mathbf {Wx} }
   are the signals extracted by 
  
    
      
        
          W
        
      
    
    {\displaystyle \mathbf {W} }
  . Given the optimal 
  
    
      
        
          W
        
      
    
    {\displaystyle \mathbf {W} }
  , the signals 
  
    
      
        
          Y
        
      
    
    {\displaystyle \mathbf {Y} }
   have maximum entropy and are therefore independent, which ensures that the extracted signals 
  
    
      
        
          y
        
        =
        
          g
          
            −
            1
          
        
        (
        
          Y
        
        )
      
    
    {\displaystyle \mathbf {y} =g^{-1}(\mathbf {Y} )}
   are also independent. 
  
    
      
        g
      
    
    {\displaystyle g}
   is an invertible function, and is the signal model. Note that if the source signal model probability density function 
  
    
      
        
          p
          
            s
          
        
      
    
    {\displaystyle p_{s}}
   matches the probability density function of the extracted signal 
  
    
      
        
          p
          
            
              y
            
          
        
      
    
    {\displaystyle p_{\mathbf {y} }}
  , then maximizing the joint entropy of 
  
    
      
        Y
      
    
    {\displaystyle Y}
   also maximizes the amount of mutual information between 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   and 
  
    
      
        
          Y
        
      
    
    {\displaystyle \mathbf {Y} }
  . For this reason, using entropy to extract independent signals is known as infomax.
Consider the entropy of the vector variable 
  
    
      
        
          Y
        
        =
        g
        (
        
          y
        
        )
      
    
    {\displaystyle \mathbf {Y} =g(\mathbf {y} )}
  , where 
  
    
      
        
          y
        
        =
        
          W
          x
        
      
    
    {\displaystyle \mathbf {y} =\mathbf {Wx} }
   is the set of signals extracted by the unmixing matrix 
  
    
      
        
          W
        
      
    
    {\displaystyle \mathbf {W} }
  . For a finite set of values sampled from a distribution with pdf 
  
    
      
        
          p
          
            
              y
            
          
        
      
    
    {\displaystyle p_{\mathbf {y} }}
  , the entropy of 
  
    
      
        
          Y
        
      
    
    {\displaystyle \mathbf {Y} }
   can be estimated as:

  
    
      
        H
        (
        
          Y
        
        )
        =
        −
        
          
            1
            N
          
        
        
          ∑
          
            t
            =
            1
          
          
            N
          
        
        ln
        ⁡
        
          p
          
            
              Y
            
          
        
        (
        
          
            Y
          
          
            t
          
        
        )
      
    
    {\displaystyle H(\mathbf {Y} )=-{\frac {1}{N}}\sum _{t=1}^{N}\ln p_{\mathbf {Y} }(\mathbf {Y} ^{t})}
  The joint pdf 
  
    
      
        
          p
          
            
              Y
            
          
        
      
    
    {\displaystyle p_{\mathbf {Y} }}
   can be shown to be related to the joint pdf 
  
    
      
        
          p
          
            
              y
            
          
        
      
    
    {\displaystyle p_{\mathbf {y} }}
   of the extracted signals by the multivariate form:

  
    
      
        
          p
          
            
              Y
            
          
        
        (
        Y
        )
        =
        
          
            
              
                p
                
                  
                    y
                  
                
              
              (
              
                y
              
              )
            
            
              
                |
              
              
                
                  
                    ∂
                    
                      Y
                    
                  
                  
                    ∂
                    
                      y
                    
                  
                
              
              
                |
              
            
          
        
      
    
    {\displaystyle p_{\mathbf {Y} }(Y)={\frac {p_{\mathbf {y} }(\mathbf {y} )}{|{\frac {\partial \mathbf {Y} }{\partial \mathbf {y} }}|}}}
  where 
  
    
      
        
          J
        
        =
        
          
            
              ∂
              
                Y
              
            
            
              ∂
              
                y
              
            
          
        
      
    
    {\displaystyle \mathbf {J} ={\frac {\partial \mathbf {Y} }{\partial \mathbf {y} }}}
   is the Jacobian matrix. We have 
  
    
      
        
          |
        
        
          J
        
        
          |
        
        =
        
          g
          ′
        
        (
        
          y
        
        )
      
    
    {\displaystyle |\mathbf {J} |=g'(\mathbf {y} )}
  , and 
  
    
      
        
          g
          ′
        
      
    
    {\displaystyle g'}
   is the pdf assumed for source signals 
  
    
      
        
          g
          ′
        
        =
        
          p
          
            s
          
        
      
    
    {\displaystyle g'=p_{s}}
  , therefore,

  
    
      
        
          p
          
            
              Y
            
          
        
        (
        Y
        )
        =
        
          
            
              
                p
                
                  
                    y
                  
                
              
              (
              
                y
              
              )
            
            
              
                |
              
              
                
                  
                    ∂
                    
                      Y
                    
                  
                  
                    ∂
                    
                      y
                    
                  
                
              
              
                |
              
            
          
        
        =
        
          
            
              
                p
                
                  
                    y
                  
                
              
              (
              
                y
              
              )
            
            
              
                p
                
                  
                    s
                  
                
              
              (
              
                y
              
              )
            
          
        
      
    
    {\displaystyle p_{\mathbf {Y} }(Y)={\frac {p_{\mathbf {y} }(\mathbf {y} )}{|{\frac {\partial \mathbf {Y} }{\partial \mathbf {y} }}|}}={\frac {p_{\mathbf {y} }(\mathbf {y} )}{p_{\mathbf {s} }(\mathbf {y} )}}}
  therefore,

  
    
      
        H
        (
        
          Y
        
        )
        =
        −
        
          
            1
            N
          
        
        
          ∑
          
            t
            =
            1
          
          
            N
          
        
        ln
        ⁡
        
          
            
              
                p
                
                  
                    y
                  
                
              
              (
              
                y
              
              )
            
            
              
                p
                
                  
                    s
                  
                
              
              (
              
                y
              
              )
            
          
        
      
    
    {\displaystyle H(\mathbf {Y} )=-{\frac {1}{N}}\sum _{t=1}^{N}\ln {\frac {p_{\mathbf {y} }(\mathbf {y} )}{p_{\mathbf {s} }(\mathbf {y} )}}}
  We know that when 
  
    
      
        
          p
          
            
              y
            
          
        
        =
        
          p
          
            s
          
        
      
    
    {\displaystyle p_{\mathbf {y} }=p_{s}}
  , 
  
    
      
        
          p
          
            
              Y
            
          
        
      
    
    {\displaystyle p_{\mathbf {Y} }}
   is of uniform distribution, and 
  
    
      
        H
        (
        
          
            Y
          
        
        )
      
    
    {\displaystyle H({\mathbf {Y} })}
   is maximized. Since

  
    
      
        
          p
          
            
              y
            
          
        
        (
        
          y
        
        )
        =
        
          
            
              
                p
                
                  
                    x
                  
                
              
              (
              
                x
              
              )
            
            
              
                |
              
              
                
                  
                    ∂
                    
                      y
                    
                  
                  
                    ∂
                    
                      x
                    
                  
                
              
              
                |
              
            
          
        
        =
        
          
            
              
                p
                
                  
                    x
                  
                
              
              (
              
                x
              
              )
            
            
              
                |
              
              
                W
              
              
                |
              
            
          
        
      
    
    {\displaystyle p_{\mathbf {y} }(\mathbf {y} )={\frac {p_{\mathbf {x} }(\mathbf {x} )}{|{\frac {\partial \mathbf {y} }{\partial \mathbf {x} }}|}}={\frac {p_{\mathbf {x} }(\mathbf {x} )}{|\mathbf {W} |}}}
  where 
  
    
      
        
          |
        
        
          W
        
        
          |
        
      
    
    {\displaystyle |\mathbf {W} |}
   is the absolute value of the determinant of the unmixing matix 
  
    
      
        
          W
        
      
    
    {\displaystyle \mathbf {W} }
  . Therefore,

  
    
      
        H
        (
        
          Y
        
        )
        =
        −
        
          
            1
            N
          
        
        
          ∑
          
            t
            =
            1
          
          
            N
          
        
        ln
        ⁡
        
          
            
              
                p
                
                  
                    x
                  
                
              
              (
              
                
                  x
                
                
                  t
                
              
              )
            
            
              
                |
              
              
                W
              
              
                |
              
              
                p
                
                  
                    s
                  
                
              
              (
              
                
                  y
                
                
                  t
                
              
              )
            
          
        
      
    
    {\displaystyle H(\mathbf {Y} )=-{\frac {1}{N}}\sum _{t=1}^{N}\ln {\frac {p_{\mathbf {x} }(\mathbf {x} ^{t})}{|\mathbf {W} |p_{\mathbf {s} }(\mathbf {y} ^{t})}}}
  so,

  
    
      
        H
        (
        
          Y
        
        )
        =
        
          
            1
            N
          
        
        
          ∑
          
            t
            =
            1
          
          
            N
          
        
        ln
        ⁡
        
          p
          
            
              s
            
          
        
        (
        
          
            y
          
          
            t
          
        
        )
        +
        ln
        ⁡
        
          |
        
        
          W
        
        
          |
        
        +
        H
        (
        
          x
        
        )
      
    
    {\displaystyle H(\mathbf {Y} )={\frac {1}{N}}\sum _{t=1}^{N}\ln p_{\mathbf {s} }(\mathbf {y} ^{t})+\ln |\mathbf {W} |+H(\mathbf {x} )}
  since 
  
    
      
        H
        (
        
          x
        
        )
        =
        −
        
          
            1
            N
          
        
        
          ∑
          
            t
            =
            1
          
          
            N
          
        
        ln
        ⁡
        
          p
          
            
              x
            
          
        
        (
        
          
            x
          
          
            t
          
        
        )
      
    
    {\displaystyle H(\mathbf {x} )=-{\frac {1}{N}}\sum _{t=1}^{N}\ln p_{\mathbf {x} }(\mathbf {x} ^{t})}
  , and maximizing 
  
    
      
        
          W
        
      
    
    {\displaystyle \mathbf {W} }
   does not affect 
  
    
      
        
          H
          
            
              x
            
          
        
      
    
    {\displaystyle H_{\mathbf {x} }}
  , so we can maximize the function

  
    
      
        h
        (
        
          Y
        
        )
        =
        
          
            1
            N
          
        
        
          ∑
          
            t
            =
            1
          
          
            N
          
        
        ln
        ⁡
        
          p
          
            
              s
            
          
        
        (
        
          
            y
          
          
            t
          
        
        )
        +
        ln
        ⁡
        
          |
        
        
          W
        
        
          |
        
      
    
    {\displaystyle h(\mathbf {Y} )={\frac {1}{N}}\sum _{t=1}^{N}\ln p_{\mathbf {s} }(\mathbf {y} ^{t})+\ln |\mathbf {W} |}
  to achieve the independence of extracted signal.
If there are M marginal pdfs of the model joint pdf 
  
    
      
        
          p
          
            
              s
            
          
        
      
    
    {\displaystyle p_{\mathbf {s} }}
   are independent and use the commonly super-gaussian model pdf for the source signals 
  
    
      
        
          p
          
            
              s
            
          
        
        =
        (
        1
        −
        tanh
        ⁡
        (
        
          s
        
        
          )
          
            2
          
        
        )
      
    
    {\displaystyle p_{\mathbf {s} }=(1-\tanh(\mathbf {s} )^{2})}
  , then we have

  
    
      
        h
        (
        
          Y
        
        )
        =
        
          
            1
            N
          
        
        
          ∑
          
            i
            =
            1
          
          
            M
          
        
        
          ∑
          
            t
            =
            1
          
          
            N
          
        
        ln
        ⁡
        (
        1
        −
        tanh
        ⁡
        (
        
          
            w
            
              i
            
            
              T
            
          
          
            x
            
              t
            
          
        
        
          )
          
            2
          
        
        )
        +
        ln
        ⁡
        
          |
        
        
          W
        
        
          |
        
      
    
    {\displaystyle h(\mathbf {Y} )={\frac {1}{N}}\sum _{i=1}^{M}\sum _{t=1}^{N}\ln(1-\tanh(\mathbf {w_{i}^{T}x^{t}} )^{2})+\ln |\mathbf {W} |}
  In the sum, given an observed signal mixture  
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
  , the corresponding set of extracted signals  
  
    
      
        
          y
        
      
    
    {\displaystyle \mathbf {y} }
    and source signal model 
  
    
      
        
          p
          
            
              s
            
          
        
        =
        
          g
          ′
        
      
    
    {\displaystyle p_{\mathbf {s} }=g'}
  ,  we can find the optimal unmixing matrix 
  
    
      
        
          W
        
      
    
    {\displaystyle \mathbf {W} }
  , and make the extracted signals independent and non-gaussian. Like the projection pursuit situation, we can use gradient descent method to find the optimal solution of the unmixing matrix.

Based on maximum likelihood estimation
Maximum likelihood estimation (MLE) is a standard statistical tool for finding parameter values (e.g. the unmixing matrix 
  
    
      
        
          W
        
      
    
    {\displaystyle \mathbf {W} }
  ) that provide the best fit of some data (e.g., the extracted signals 
  
    
      
        y
      
    
    {\displaystyle y}
  ) to a given a model (e.g., the assumed joint probability density function (pdf) 
  
    
      
        
          p
          
            s
          
        
      
    
    {\displaystyle p_{s}}
   of source signals).The ML ""model"" includes a specification of a pdf, which in this case is the pdf 
  
    
      
        
          p
          
            s
          
        
      
    
    {\displaystyle p_{s}}
   of the unknown source signals 
  
    
      
        s
      
    
    {\displaystyle s}
  . Using ML ICA, the objective is to find an unmixing matrix that yields extracted signals 
  
    
      
        y
        =
        
          W
        
        x
      
    
    {\displaystyle y=\mathbf {W} x}
   with a joint pdf as similar as possible to the joint pdf 
  
    
      
        
          p
          
            s
          
        
      
    
    {\displaystyle p_{s}}
   of the unknown source signals 
  
    
      
        s
      
    
    {\displaystyle s}
  .
MLE is thus based on the assumption that if the model pdf 
  
    
      
        
          p
          
            s
          
        
      
    
    {\displaystyle p_{s}}
   and the model parameters 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbf {A} }
   are correct then a high probability should be obtained for the data 
  
    
      
        x
      
    
    {\displaystyle x}
   that were actually observed. Conversely, if 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbf {A} }
   is far from the correct parameter values then a low probability of the observed data would be expected.
Using MLE, we call the probability of the observed data for a given set of model parameter values (e.g., a pdf 
  
    
      
        
          p
          
            s
          
        
      
    
    {\displaystyle p_{s}}
   and a matrix 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbf {A} }
  ) the likelihood of the model parameter values given the observed data.
We define a likelihood function 
  
    
      
        
          L
          (
          W
          )
        
      
    
    {\displaystyle \mathbf {L(W)} }
   of 
  
    
      
        
          W
        
      
    
    {\displaystyle \mathbf {W} }
  :

  
    
      
        
          L
          (
          W
          )
        
        =
        
          p
          
            s
          
        
        (
        
          W
        
        x
        )
        
          |
        
        det
        
          W
        
        
          |
        
        .
      
    
    {\displaystyle \mathbf {L(W)} =p_{s}(\mathbf {W} x)|\det \mathbf {W} |.}
  
This equals to the probability density at 
  
    
      
        x
      
    
    {\displaystyle x}
  , since 
  
    
      
        s
        =
        
          W
        
        x
      
    
    {\displaystyle s=\mathbf {W} x}
  .
Thus, if we wish to find a 
  
    
      
        
          W
        
      
    
    {\displaystyle \mathbf {W} }
   that is most likely to have generated the observed mixtures 
  
    
      
        x
      
    
    {\displaystyle x}
   from the unknown source signals 
  
    
      
        s
      
    
    {\displaystyle s}
   with pdf 
  
    
      
        
          p
          
            s
          
        
      
    
    {\displaystyle p_{s}}
   then we need only find that 
  
    
      
        
          W
        
      
    
    {\displaystyle \mathbf {W} }
   which maximizes the likelihood 
  
    
      
        
          L
          (
          W
          )
        
      
    
    {\displaystyle \mathbf {L(W)} }
  . The unmixing matrix that maximizes equation is known as the MLE of the optimal unmixing matrix.
It is common practice to use the log likelihood, because this is easier to evaluate. As the logarithm is a monotonic function, the 
  
    
      
        
          W
        
      
    
    {\displaystyle \mathbf {W} }
   that maximizes the function 
  
    
      
        
          L
          (
          W
          )
        
      
    
    {\displaystyle \mathbf {L(W)} }
   also maximizes its logarithm 
  
    
      
        ln
        ⁡
        
          L
          (
          W
          )
        
      
    
    {\displaystyle \ln \mathbf {L(W)} }
  . This allows us to take the logarithm of equation above, which yields the log likelihood function

  
    
      
        ln
        ⁡
        
          L
          (
          W
          )
        
        =
        
          ∑
          
            i
          
        
        
          ∑
          
            t
          
        
        ln
        ⁡
        
          p
          
            s
          
        
        (
        
          w
          
            i
          
          
            T
          
        
        
          x
          
            t
          
        
        )
        +
        N
        ln
        ⁡
        
          |
        
        det
        
          W
        
        
          |
        
      
    
    {\displaystyle \ln \mathbf {L(W)} =\sum _{i}\sum _{t}\ln p_{s}(w_{i}^{T}x_{t})+N\ln |\det \mathbf {W} |}
  
If we substitute a commonly used high-Kurtosis model pdf for the source signals 
  
    
      
        
          p
          
            s
          
        
        =
        (
        1
        −
        tanh
        ⁡
        (
        s
        
          )
          
            2
          
        
        )
      
    
    {\displaystyle p_{s}=(1-\tanh(s)^{2})}
   then we have

  
    
      
        ln
        ⁡
        
          L
          (
          W
          )
        
        =
        
          
            1
            N
          
        
        
          ∑
          
            i
          
          
            M
          
        
        
          ∑
          
            t
          
          
            N
          
        
        ln
        ⁡
        (
        1
        −
        tanh
        ⁡
        (
        
          w
          
            i
          
          
            T
          
        
        
          x
          
            t
          
        
        
          )
          
            2
          
        
        )
        +
        ln
        ⁡
        
          |
        
        det
        
          W
        
        
          |
        
      
    
    {\displaystyle \ln \mathbf {L(W)} ={1 \over N}\sum _{i}^{M}\sum _{t}^{N}\ln(1-\tanh(w_{i}^{T}x_{t})^{2})+\ln |\det \mathbf {W} |}
  
This matrix 
  
    
      
        
          W
        
      
    
    {\displaystyle \mathbf {W} }
   that maximizes this function is the maximum likelihood estimation.

History and background
The early general framework for independent component analysis was introduced by Jeanny Hérault and Bernard Ans from 1984, further developed by Christian Jutten in 1985 and 1986, and refined by Pierre Comon in 1991, and popularized in his paper of 1994. In 1995, Tony Bell and Terry Sejnowski introduced a fast and efficient ICA algorithm based on infomax, a principle introduced by Ralph Linsker in 1987.
There are many algorithms available in the literature which do ICA. A largely used one, including in industrial applications, is the FastICA algorithm, developed by Hyvärinen and Oja, which uses the kurtosis as cost function. Other examples are rather related to blind source separation where a more general approach is used. For example, one can drop the independence assumption and separate mutually correlated signals, thus, statistically ""dependent"" signals. Sepp Hochreiter and Jürgen Schmidhuber showed how to obtain non-linear ICA or source separation as a by-product of regularization (1999). Their method does not require a priori knowledge about the number of independent sources.

Applications
ICA can be extended to analyze non-physical signals. For instance, ICA has been applied to discover discussion topics on a bag of news list archives.
Some ICA applications are listed below:

optical Imaging of neurons
neuronal spike sorting
face recognition
modelling receptive fields of primary visual neurons
predicting stock market prices
mobile phone communications 
colour based detection of the ripeness of tomatoes
removing artifacts, such as eye blinks, from EEG data.
analysis of changes in gene expression over time in single cell RNA-sequencing experiments.
studies of the resting state network of the brain.
astronomy and cosmology

See also
Notes
References
Comon, Pierre (1994): ""Independent Component Analysis: a new concept?"", Signal Processing, 36(3):287–314 (The original paper describing the concept of ICA)
Hyvärinen, A.; Karhunen, J.; Oja, E. (2001): Independent Component Analysis, New York: Wiley, ISBN 978-0-471-40540-5 ( Introductory chapter )
Hyvärinen, A.; Oja, E. (2000): ""Independent Component Analysis: Algorithms and Application"", Neural Networks, 13(4-5):411-430. (Technical but pedagogical introduction).
Comon, P.; Jutten C., (2010): Handbook of Blind Source Separation, Independent Component Analysis and Applications. Academic Press, Oxford UK. ISBN 978-0-12-374726-6
Lee, T.-W. (1998): Independent component analysis: Theory and applications, Boston, Mass: Kluwer Academic Publishers, ISBN 0-7923-8261-7
Acharyya, Ranjan (2008): A New Approach for Blind Source Separation of Convolutive Sources - Wavelet Based Separation Using Shrinkage Function ISBN 3-639-07797-0 ISBN 978-3639077971 (this book focuses on unsupervised learning with Blind Source Separation)

External links
What is independent component analysis? by Aapo Hyvärinen
Independent Component Analysis: A Tutorial by Aapo Hyvärinen
A Tutorial on Independent Component Analysis
FastICA as a package for Matlab, in R language, C++
ICALAB Toolboxes for Matlab, developed at RIKEN
High Performance Signal Analysis Toolkit provides C++ implementations of FastICA and Infomax
ICA toolbox Matlab tools for ICA with Bell-Sejnowski, Molgedey-Schuster and mean field ICA. Developed at DTU.
Demonstration of the cocktail party problem
EEGLAB Toolbox ICA of EEG for Matlab, developed at UCSD.
FMRLAB Toolbox ICA of fMRI for Matlab, developed at UCSD
MELODIC, part of the FMRIB Software Library.
Discussion of ICA used in a biomedical shape-representation context
FastICA, CuBICA, JADE and TDSEP algorithm for Python and more...
Group ICA Toolbox and Fusion ICA Toolbox
Tutorial: Using ICA for cleaning EEG signals",https://en.wikipedia.org/wiki/Independent_component_analysis,"['All articles needing additional references', 'All articles needing expert attention', 'All articles with unsourced statements', 'Articles needing additional references from October 2011', 'Articles needing expert attention from November 2008', 'Articles needing expert attention with no reason or talk parameter', 'Articles with multiple maintenance issues', 'Articles with unsourced statements from May 2013', 'Articles with unsourced statements from October 2012', 'Dimension reduction', 'Signal estimation', 'Statistics articles needing expert attention']",Data Science
105,Informatics,"Informatics is the study of computational systems, especially those for data storage and retrieval. (According to ACM Europe and Informatics Europe informatics is synonym for computer science and computing as a profession, in which the central notion is transformation of information.) In some countries, however, the term ""informatics"" is used in the context of library science, where it has a different meaning.

Different meanings
In some countries, depending on local interpretations, the term ""informatics"" is used synonymously to mean information systems, information science, information theory, information engineering, information technology, information processing, or other theoretical or practical fields. In Germany, the term informatics almost exactly corresponds to modern computer science. Accordingly, universities in continental Europe usually translate ""informatics"" as computer science, or sometimes information and computer science, although technical universities may translate it as computer science & engineering.In the United States, however, the term informatics is mostly used in context of data science, library science or its applications in healthcare (biomedical informatics), where it first appeared in the US.
The University of Washington uses this term to refer to social computing. In some countries, this term is associated with natural computation and neural computation.The Government of Canada uses the term to refer to operational units offering network and computer services to the various departments.

Etymology
In 1956, the German informatician Karl Steinbuch coined the word Informatik, publishing a paper called Informatik: Automatische Informationsverarbeitung (""Informatics: Automatic Information Processing""). The morphology—informat-ion + -ics—uses ""the accepted form for names of sciences, as conics, mathematics, linguistics, optics, or matters of practice, as economics, politics, tactics"", and so, linguistically, the meaning extends easily to encompass both the science of information and the practice of information processing. The German word Informatik is usually translated to English as computer science by universities or computer science & engineering by technical universities (German equivalents for institutes of technology). Depending on the context, informatics is also translated into computing, scientific computing or information and computer technology. The French term informatique was coined in 1962 by Philippe Dreyfus. In the same month was also proposed independently by Walter F. Bauer (1924–2015) and associates who co-founded software company Informatics Inc. The term for the new discipline quickly spread throughout Europe, but it did not catch on in the United States. Over the years, many different definitions of informatics have been developed, most of them claim that the essence of informatics is one of these concepts: information processing, algorithms, computation, information, algorithmic processes, computational processes or computational systems.

The earliest uses of the term informatics in the United States was during the 1950s with the beginning of computer use in healthcare.  Early practitioners interested in the field soon learned that there were no formal education programs, and none emerged until the late 1960s. Unfortunately, they introduced the term informatics only in the context of archival science, which is only a small part of informatics. Professional development, therefore, played a significant role in the development of health informatics. According to Imhoff et al., 2001, healthcare informatics is not only the application of computer technology to problems in healthcare, but covers all aspects of generation, handling, communication, storage, retrieval, management, analysis, discovery, and synthesis of data information and knowledge in the entire scope of healthcare. Furthermore, they stated that the primary goal of health informatics can be distinguished as follows: To provide solutions for problems related to data, information, and knowledge processing. To study general principles of processing data information and knowledge in medicine and healthcare. The term health informatics quickly spread throughout the United States in various forms such as nursing informatics, public health informatics or medical informatics. Analogous terms were later introduced for use of computers in various fields, such as business informatics, forest informatics, legal informatics etc. Unfortunately, these fields still mainly use term informatics in context of library science.

Informatics as library science
In the fields of geoinformatics or irrigation informatics, the term -informatics usually mean information science, in context related to library science. This was the first meaning of informatics introduced in Russia in 1966 by A.I. Mikhailov, R.S. Gilyarevskii, and A.I. Chernyi, which referred to a scientific discipline that studies the structure and properties of scientific information. In this context, the term was also used by the International Neuroinformatics Coordinating Facility. Some scientists use this term, however, to refer to the science of information processing, not data management.In the English-speaking world, the term informatics was first widely used in the compound medical informatics, taken to include ""the cognitive, information processing, and communication tasks of medical practice, education, and research, including information science and the technology to support these tasks"". Many such compounds are now in use; they can be viewed as different areas of ""applied informatics"".

Informatics as information processing science
In the early 1990s, K.K. Kolin proposed an interpretation of informatics as a fundamental science that studies information processes in nature, society, and technical systems.A broad interpretation of informatics, as ""the study of the structure, algorithms, behaviour, and interactions of natural and artificial computational systems,"" was introduced by the University of Edinburgh in 1994. This has led to the merger of the institutes of computer science, artificial intelligence and cognitive science into a single School of Informatics in 2002.More than a dozen nearby universities joined Scottish Informatics and Computer Science Alliance. Some non-European universities have also adopted this definition (eg. Kyoto University School of Informatics).
In 2003, Yingxu Wang popularized term cognitive informatics, described as follows:
Supplementary to matter and energy, information is the third essence for modeling the world. Cognitive informatics focuses on internal information processing mechanisms and the natural intelligence of the brain.
Informatics as a fundamental science of information in natural and artifficial systems was proposed again in Russia in 2006.In 2007, the influential book Decoding the Universe was published.
Former president of Association for Computing Machinery, Peter Denning wrote in 2007:
The old definition of computer science - the study of phenomena surrounding computers - is now obsolete. Computing is the study of natural and artificial information processes.
The 2008 Research Assessment Exercise, of the UK Funding Councils, includes a new, Computer Science and Informatics, unit of assessment (UoA), whose scope is described as follows:

The UoA includes the study of methods for acquiring, storing, processing, communicating and reasoning about information, and the role of interactivity in natural and artificial systems, through the implementation, organisation and use of computer hardware, software and other resources. The subjects are characterised by the rigorous application of analysis, experimentation and design.In 2008, the construction of the Informatics Forum was completed. In 2018, the MIT Schwarzman College of Computing was established. Its construction is planned to be completed in 2021.

Controversial fields
evolutionary informatics - a new field that comes from the concept of an intelligent design. According to Evolutionary Informatics Lab, evolutionary informatics studies how evolving systems incorporate, transform, and export information. In 2017, the influential book ""Introduction To Evolutionary Informatics"" was published.

Informatics as computer science
Related topics
Computer scientists study computational processes and systems. Computing Research Repository (CoRR) classification distinguishes the following main topics in computer science (alphabetic order):

Journals and conferences
Community
Related organisations
Academic schools and departments
See also
References


== Further reading ==",https://en.wikipedia.org/wiki/Informatics,"['Articles with short description', 'Artificial intelligence', 'CS1 German-language sources (de)', 'Cognitive science', 'Computational fields of study', 'Computer science education', 'Information science', 'Schools of informatics', 'Short description is different from Wikidata']",Data Science
106,Information privacy,"Information privacy is the relationship between the collection and dissemination of data, technology, the public expectation of privacy, and the legal and political issues surrounding them. It is also known as data privacy or data protection.
Data privacy is challenging since it attempts to use data while protecting an individual's privacy preferences and personally identifiable information. The fields of computer security, data security, and information security all design and use software, hardware, and human resources to address this issue.

Authorities
Laws
Authorities by country
Information types
Various types of personal information often come under privacy concerns.

Cable television
This describes the ability to control what information one reveals about oneself over cable television, and who can access that information. For example, third parties can track IP TV programs someone has watched at any given time. ""The addition of any information in a broadcasting stream is not required for an audience rating survey, additional devices are not requested to be installed in the houses of viewers or listeners, and without the necessity of their cooperations, audience ratings can be automatically performed in real-time.""

Educational
In the United Kingdom in 2012, the Education Secretary Michael Gove described the National Pupil Database as a ""rich dataset"" whose value could be ""maximised"" by making it more openly accessible, including to private companies. Kelly Fiveash of The Register said that this could mean ""a child's school life including exam results, attendance, teacher assessments and even characteristics"" could be available, with third-party organizations being responsible for anonymizing any publications themselves, rather than the data being anonymized by the government before being handed over. An example of a data request that Gove indicated had been rejected in the past, but might be possible under an improved version of privacy regulations, was for ""analysis on sexual exploitation"".

Financial
Information about a person's financial transactions, including the amount of assets, positions held in stocks or funds, outstanding debts, and purchases can be sensitive. If criminals gain access to information such as a person's accounts or credit card numbers, that person could become the victim of fraud or identity theft. Information about a person's purchases can reveal a great deal about that person's history, such as places he/she has visited, whom he/she has contacted with, products he/she has used, his/her activities and habits, or medications he/she has used. In some cases, corporations may use this information to target individuals with marketing customized towards those individual's personal preferences, which that person may or may not approve.

Internet
The ability to control the information one reveals about oneself over the internet, and who can access that information, has become a growing concern. These concerns include whether email can be stored or read by third parties without consent, or whether third parties can continue to track the websites that someone visited. Another concern is if websites one visited can collect, store, and possibly share personally identifiable information about users.
The advent of various search engines and the use of data mining created a capability for data about individuals to be collected and combined from a wide variety of sources very easily. The FTC has provided a set of guidelines that represent widely accepted concepts concerning fair information practices in an electronic marketplace called the Fair Information Practice Principles.
To avoid giving away too much personal information, emails should be encrypted. Browsing of web pages as well as other online activities should be done trace-less via ""anonymizers"", in case those are not trusted, by open-source distributed anonymizers, so called mix nets, such as I2P or Tor – The Onion Router. VPNs (Virtual Private Networks) are another ""anonymizer"" that can be used to give someone more protection while online. This includes obfuscating and encrypting web traffic so that other groups cannot see or mine it.Email isn't the only internet content with privacy concerns. In an age where increasing amounts of information is online, social networking sites pose additional privacy challenges. People may be tagged in photos or have valuable information exposed about themselves either by choice or unexpectedly by others, referred to as participatory surveillance. Data about location can also be accidentally published, for example, when someone posts a picture with a store as a background. Caution should be exercised when posting information online, social networks vary in what they allow users to make private and what remains publicly accessible. Without strong security settings in place and careful attention to what remains public, a person can be profiled by searching for and collecting disparate pieces of information, worst case leading to cases of cyberstalking or reputation damage.Cookies are used in websites that users may allow the website to retrieve some information from user's internet which it usually does not mention what the data being retrieved is. It is a common method used to monitor and track users' internet activity. In 2018, the General Data Protection Regulation (GDPR) passed regulation that forces websites to visibly disclose to consumers their information privacy practices, referred to as cookie notices. This was issued to give consumers the choice of what information about their behavior they consent to letting websites track, however its effectiveness is controversial. Some websites may engage in deceptive practices such as placing the cookie notices in places on the page that are not visible, or only giving consumers notice that their information is being tracked, but not allowing them to change their privacy settings.

Locational
As location tracking capabilities of mobile devices are advancing (location-based services), problems related to user privacy arise. Location data is among the most sensitive data currently being collected. A list of potentially sensitive professional and personal information that could be inferred about an individual knowing only his mobility trace was published recently by the Electronic Frontier Foundation. These include the movements of a competitor sales force, attendance of a particular church or an individual's presence in a motel, or at an abortion clinic. A recent MIT study by de Montjoye et al. showed that four spatio-temporal points, approximate places and times, are enough to uniquely identify 95% of 1.5 million people in a mobility database. The study further shows that these constraints hold even when the resolution of the dataset is low. Therefore, even coarse or blurred datasets provide little anonymity.

Medical
People may not wish for their medical records to be revealed to others due to the confidentiality and sensitivity of what the information could reveal about their health. For example, they might be concerned that it might affect their insurance coverage or employment. Or, it may be because they would not wish for others to know about any medical or psychological conditions or treatments that would bring embarrassment upon themselves. Revealing medical data could also reveal other details about one's personal life. There are three major categories of medical privacy: informational (the degree of control over personal information), physical (the degree of physical inaccessibility to others), and psychological (the extent to which the doctor respects patients’ cultural beliefs, inner thoughts, values, feelings, and religious practices and allows them to make personal decisions).
Physicians and psychiatrists in many cultures and countries have standards for doctor–patient relationships, which include maintaining confidentiality. In some cases, the physician–patient privilege is legally protected. These practices are in place to protect the dignity of patients, and to ensure that patients feel free to reveal complete and accurate information required for them to receive the correct treatment.
To view the United States' laws on governing privacy of private health information, see HIPAA and the HITECH Act.  The Australian law is the Privacy Act 1988 Australia as well as state-based health records legislation.

Political
Political privacy has been a concern since voting systems emerged in ancient times. The secret ballot is the simplest and most widespread measure to ensure that political views are not known to anyone other than the voters themselves—it is nearly universal in modern democracy, and considered to be a basic right of citizenship. In fact, even where other rights of privacy do not exist, this type of privacy very often does. Unfortunately, there are several forms of voting fraud or privacy violations possible with the use of digital voting machines.

Legality
The legal protection of the right to privacy in general – and of data privacy in particular – varies greatly around the world.Laws and regulations related to Privacy and Data Protection are constantly changing, it is seen as important to keep abreast of any changes in the law and to continually reassess compliance with data privacy and security regulations. Within academia, Institutional Review Boards function to assure that adequate measures are taken to ensure both the privacy and confidentiality of human subjects in research.Privacy concerns exist wherever personally identifiable information or other sensitive information is collected, stored, used, and finally destroyed or deleted – in digital form or otherwise. Improper or non-existent disclosure control can be the root cause for privacy issues. Informed consent mechanisms including dynamic consent are important in communicating to data subjects the different uses of their personally identifiable information.  Data privacy issues may arise in response to information from a wide range of sources, such as:
Healthcare records
Criminal justice investigations and proceedings
Financial institutions and transactions
Biological traits, such as genetic material
Residence and geographic records
Privacy breach
Location-based service and geolocation
Web surfing behavior or user preferences using persistent cookies
Academic research

Protection of privacy in information systems
As heterogeneous information systems with differing privacy rules are interconnected and information is shared, policy appliances will be required to reconcile, enforce, and monitor an increasing amount of privacy policy rules (and laws). There are two categories of technology to address privacy protection in commercial IT systems: communication and enforcement.

Policy communicationP3P – The Platform for Privacy Preferences. P3P is a standard for communicating privacy practices and comparing them to the preferences of individuals.Policy enforcementXACML – The Extensible Access Control Markup Language together with its Privacy Profile is a standard for expressing privacy policies in a machine-readable language which a software system can use to enforce the policy in enterprise IT systems.
EPAL – The Enterprise Privacy Authorization Language is very similar to XACML, but is not yet a standard.
WS-Privacy - ""Web Service Privacy"" will be a specification for communicating privacy policy in web services. For example, it may specify how privacy policy information can be embedded in the SOAP envelope of a web service message.Protecting privacy on the internetOn the internet many users give away a lot of information about themselves: unencrypted e-mails can be read by the administrators of an e-mail server, if the connection is not encrypted (no HTTPS), and also the internet service provider and other parties sniffing the network traffic of that connection are able to know the contents.
The same applies to any kind of traffic generated on the Internet, including web browsing, instant messaging, and others.
In order not to give away too much personal information, e-mails can be encrypted and browsing of webpages as well as other online activities can be done traceless via anonymizers, or by open source distributed anonymizers, so-called mix networks.
Well-known open-source mix nets include I2P – The Anonymous Network and Tor.

Improving privacy through individualizationComputer privacy can be improved through individualization. Currently security messages are designed for the ""average user"", i.e. the same message for everyone. Researchers have posited that individualized messages and security ""nudges"", crafted based on users' individual differences and personality traits, can be used for further improvements for each person's compliance with computer security and privacy.

United States Safe Harbor program and passenger name record issues
The United States Department of Commerce created the International Safe Harbor Privacy Principles certification program in response to the 1995 Directive on Data Protection (Directive 95/46/EC) of the European Commission. Both the United States and the European Union officially state that they are committed to upholding information privacy of individuals, but the former has caused friction between the two by failing to meet the standards of the EU's stricter laws on personal data. The negotiation of the Safe Harbor program was, in part, to address this long-running issue. Directive 95/46/EC declares in Chapter IV Article 25 that personal data may only be transferred from the countries in the European Economic Area to countries which provide adequate privacy protection. Historically, establishing adequacy required the creation of national laws broadly equivalent to those implemented by Directive 95/46/EU. Although there are exceptions to this blanket prohibition – for example where the disclosure to a country outside the EEA is made with the consent of the relevant individual (Article 26(1)(a)) – they are limited in practical scope. As a result, Article 25 created a legal risk to organisations which transfer personal data from Europe to the United States.
The program regulates the exchange of passenger name record information between the EU and the US. According to the EU directive, personal data may only be transferred to third countries if that country provides an adequate level of protection. Some exceptions to this rule are provided, for instance when the controller himself can guarantee that the recipient will comply with the data protection rules.
The European Commission has set up the ""Working party on the Protection of Individuals with regard to the Processing of Personal Data,"" commonly known as the ""Article 29 Working Party"". The Working Party gives advice about the level of protection in the European Union and third countries.The Working Party negotiated with U.S. representatives about the protection of personal data, the Safe Harbor Principles were the result. Notwithstanding that approval, the self-assessment approach of the Safe Harbor remains controversial with a number of European privacy regulators and commentators.The Safe Harbor program addresses this issue in the following way: rather than a blanket law imposed on all organisations in the United States, a voluntary program is enforced by the Federal Trade Commission. U.S. organisations which register with this program, having self-assessed their compliance with a number of standards, are ""deemed adequate"" for the purposes of Article 25. Personal information can be sent to such organisations from the EEA without the sender being in breach of Article 25 or its EU national equivalents. The Safe Harbor was approved as providing adequate protection for personal data, for the purposes of Article 25(6), by the European Commission on 26 July 2000.Under the Safe Harbor, adoptee organisations need to carefully consider their compliance with the onward transfer obligations, where personal data originating in the EU is transferred to the US Safe Harbor, and then onward to a third country. The alternative compliance approach of ""binding corporate rules"", recommended by many EU privacy regulators, resolves this issue. In addition, any dispute arising in relation to the transfer of HR data to the US Safe Harbor must be heard by a panel of EU privacy regulators.In July 2007, a new, controversial, Passenger Name Record agreement between the US and the EU was made. A short time afterwards, the Bush administration gave exemption for the Department of Homeland Security, for the Arrival and Departure Information System (ADIS) and for the Automated Target System from the 1974 Privacy Act.In February 2008, Jonathan Faull, the head of the EU's Commission of Home Affairs, complained about the US bilateral policy concerning PNR. The US had signed in February 2008 a memorandum of understanding (MOU) with the Czech Republic in exchange of a visa waiver scheme, without concerting before with Brussels. The tensions between Washington and Brussels are mainly caused by a lesser level of data protection in the US, especially since foreigners do not benefit from the US Privacy Act of 1974. Other countries approached for bilateral MOU included the United Kingdom, Estonia, Germany and Greece.

See also
Computer science specific
Organisations
Scholars working in the field

References
Further reading
Philip E. Agre; Marc Rotenberg (1998). Technology and privacy: the new landscape. MIT Press. ISBN 978-0-262-51101-8.

External links
InternationalFactsheet on ECtHR case law on data protection
International Conference of Data Protection and Privacy Commissioners
Biometrics Institute Privacy CharterEuropeEU data protection page
UNESCO Chair in Data Privacy
European Data Protection SupervisorLatin AmericaLatin American Data Protection Law ReviewNorth AmericaPrivacy and Access Council of Canada
Laboratory for International Data Privacy at Carnegie Mellon University.
Privacy Laws by StateJournalsIEEE Security & Privacy magazine
Transactions on Data Privacy",https://en.wikipedia.org/wiki/Information_privacy,"['Articles containing French-language text', 'CS1 maint: archived copy as title', 'CS1 maint: multiple names: authors list', 'CS1 maint: others', 'Data laws', 'Data protection', 'Information privacy', 'Privacy', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers']",Data Science
107,Information science,"Information science (also known as information studies) is an academic field which is primarily concerned with analysis, collection, classification, manipulation, storage, retrieval, movement, dissemination, and protection of information. Practitioners within and outside the field study the  application and the usage of knowledge in organizations along with the interaction between people, organizations, and any existing information systems with the aim of creating, replacing, improving, or understanding information systems. Historically, information science is associated with computer science, data science, psychology, technology, and intelligence agencies. However, information science also incorporates aspects of diverse fields such as archival science, cognitive science, commerce, law, linguistics, museology, management, mathematics, philosophy, public policy, and social sciences.

Foundations
Scope and approach
Information science focuses on understanding problems from the perspective of the stakeholders involved and then applying information and other technologies as needed. In other words, it tackles systemic problems first rather than individual pieces of technology within that system. In this respect, one can see information science as a response to technological determinism, the belief that technology ""develops by its own laws, that it realizes its own potential, limited only by the material resources available and the creativity of its developers. It must therefore be regarded as an autonomous system controlling and ultimately permeating all other subsystems of society.""Many universities have entire colleges, departments or schools devoted to the study of information science, while numerous information-science scholars work in disciplines such as communication, computer science, law, and sociology. Several institutions have formed an I-School Caucus (see List of I-Schools), but numerous others besides these also have comprehensive information foci.
Within information science, current issues as of 2013 include:

Human–computer interaction for science
Groupware
The Semantic Web
Value sensitive design
Iterative design processes
The ways people generate, use and find information

Definitions
The first known usage of the term ""information science"" was in 1955. An early definition of Information science (going back to 1968, the year when the American Documentation Institute renamed itself as the American Society for Information Science and Technology) states:

""Information science is that discipline that investigates the properties and behavior of information, the forces governing the flow of information, and the means of processing information for optimum accessibility and usability. It is concerned with that body of knowledge relating to the origination, collection, organization, storage, retrieval, interpretation, transmission, transformation, and utilization of information. This includes the investigation of information representations in both natural and artificial systems, the use of codes for efficient message transmission, and the study of information processing devices and techniques such as computers and their programming systems. It is an interdisciplinary science derived from and related to such fields as mathematics, logic, linguistics, psychology, computer technology, operations research, the graphic arts, communications, management, and other similar fields. It has both a pure science component, which inquires into the subject without regard to its application, and an applied science component, which develops services and products."" (Borko, 1968, p.3).

Related terms
Some authors use informatics as a synonym for information science. This is especially true when related to the concept developed by A. I. Mikhailov and other Soviet authors in the mid-1960s. The Mikhailov school saw informatics as a discipline related to the study of scientific information.
Informatics is difficult to precisely define because of the rapidly evolving and interdisciplinary nature of the field. Definitions reliant on the nature of the tools used for deriving meaningful information from data are emerging in Informatics academic programs.Regional differences and international terminology complicate the problem. Some people note that much of what is called ""Informatics"" today was once called ""Information Science"" – at least in fields such as Medical Informatics. For example, when library scientists began also to use the phrase ""Information Science"" to refer to their work, the term ""informatics"" emerged:

in the United States as a response by computer scientists to distinguish their work from that of library science
in Britain as a term for a science of information that studies natural, as well as artificial or engineered, information-processing systemsAnother term discussed as a synonym for ""information studies"" is ""information systems"". Brian Campbell Vickery's Information Systems (1973) placed information systems within IS. Ellis, Allen, & Wilson (1999), on the other hand, provided a bibliometric investigation describing the relation between two different fields: ""information science"" and ""information systems"".

Philosophy of information
Philosophy of information studies conceptual issues arising at the intersection of computer science, information technology, and philosophy. It includes the investigation of the conceptual nature and basic principles of information, including its dynamics, utilisation and sciences, as well as the elaboration and application of information-theoretic and computational methodologies to its philosophical problems.

Ontology
In science and information science, an ontology formally represents knowledge as a set of concepts within a domain, and the relationships between those concepts.  It can be used to reason about the entities within that domain and may be used to describe the domain.
More specifically, an ontology is a model for describing the world that consists of a set of types, properties, and relationship types. Exactly what is provided around these varies, but they are the essentials of an ontology. There is also generally an expectation that there be a close resemblance between the real world and the features of the model in an ontology.In theory, an ontology is a ""formal, explicit specification of a shared conceptualisation"". An ontology renders shared vocabulary and taxonomy which models a domain with the definition of objects and/or concepts and their properties and relations.Ontologies are the structural frameworks for organizing information and are used in artificial intelligence, the Semantic Web, systems engineering, software engineering, biomedical informatics, library science, enterprise bookmarking, and information architecture as a form of knowledge representation about the world or some part of it. The creation of domain ontologies is also fundamental to the definition and use of an enterprise architecture framework.

Careers
Information scientist
An information scientist is an individual, usually with a relevant subject degree or high level of subject knowledge, who provides focused information to scientific and technical research staff in industry or to subject faculty and students in academia. The industry *information specialist/scientist*  and the academic information subject specialist/librarian have, in general, similar subject background training, but the academic position holder will be required to hold a second advanced degree (MLS/MI/MA in IS, e.g.) in information and library studies in addition to a subject master’s.  The title also applies to an individual carrying out research in information science.

Systems analyst
A systems analyst works on creating, designing, and improving information systems for a specific need. Oftentimes a systems analyst works with a business to evaluate and implement organizational processes and techniques for accessing information in order to improve efficiency and productivity within the business.

Information professional
An information professional is an individual who preserves, organizes, and disseminates information. Information professionals are skilled in the organization and retrieval of recorded knowledge. Traditionally, their work has been with print materials, but these skills are being increasingly used with electronic, visual, audio, and digital materials. Information professionals work in a variety of public, private, non-profit, and academic institutions. Information professionals can also be found within organisational and industrial contexts. Performing roles that include system design and development and system analysis.

History
Early beginnings
Information science, in studying the collection, classification, manipulation, storage, retrieval and dissemination of information has origins in the common stock of human knowledge. Information analysis has been carried out by scholars at least as early as the time of the Abyssinian Empire with the emergence of cultural depositories, what is today known as libraries and archives. Institutionally, information science emerged in the 19th century along with many other social science disciplines. As a science, however, it finds its institutional roots in the history of science, beginning with publication of the first issues of Philosophical Transactions, generally considered the first scientific journal, in 1665 by the Royal Society (London).
The institutionalization of science occurred throughout the 18th century. In 1731, Benjamin Franklin established the Library Company of Philadelphia, the first library owned by a group of public citizens, which quickly expanded beyond the realm of books and became a center of scientific experiment, and which hosted public exhibitions of scientific experiments. Benjamin Franklin invested a town in Massachusetts with a collection of books that the town voted to make available to all free of charge, forming the first Public Library. Academie de Chirurgia (Paris) published Memoires pour les Chirurgiens, generally considered to be the first medical journal, in 1736. The American Philosophical Society, patterned on the Royal Society (London), was founded in Philadelphia in 1743. As numerous other scientific journals and societies were founded, Alois Senefelder developed the concept of lithography for use in mass printing work in Germany in 1796.

19th century
By the 19th century the first signs of information science emerged as separate and distinct from other sciences and social sciences but in conjunction with communication and computation. In 1801, Joseph Marie Jacquard invented a punched card system to control operations of the cloth weaving loom in France. It was the first use of ""memory storage of patterns"" system. As chemistry journals emerged throughout the 1820s and 1830s, Charles Babbage developed his ""difference engine,"" the first step towards the modern computer, in 1822 and his ""analytical engine” by 1834. By 1843 Richard Hoe developed the rotary press, and in 1844 Samuel Morse sent the first public telegraph message. By 1848 William F. Poole begins the Index to Periodical Literature, the first general periodical literature index in the US.
In 1854 George Boole published An Investigation into Laws of Thought..., which lays the foundations for Boolean algebra, which is later used in information retrieval. In 1860 a congress was held at Karlsruhe Technische Hochschule to discuss the feasibility of establishing a systematic and rational nomenclature for chemistry. The congress did not reach any conclusive results, but several key participants returned home with Stanislao Cannizzaro's outline (1858), which ultimately convinces them of the validity of his scheme for calculating atomic weights.By 1865, the Smithsonian Institution began a catalog of current scientific papers, which became the International Catalogue of Scientific Papers in 1902. The following year the Royal Society began publication of its Catalogue of Papers in London. In 1868, Christopher Sholes, Carlos Glidden, and S. W. Soule produced the first practical typewriter. By 1872 Lord Kelvin devised an analogue computer to predict the tides, and by 1875 Frank Stephen Baldwin was granted the first US patent for a practical calculating machine that performs four arithmetic functions. Alexander Graham Bell and Thomas Edison invented the telephone and phonograph in 1876 and 1877 respectively, and the American Library Association was founded in Philadelphia. In 1879 Index Medicus was first issued by the Library of the Surgeon General, U.S. Army, with John Shaw Billings as librarian, and later the library issues Index Catalogue, which achieved an international reputation as the most complete catalog of medical literature.

European documentation
The discipline of documentation science, which marks the earliest theoretical foundations of modern information science, emerged in the late part of the 19th century in Europe together with several more scientific indexes whose purpose was to organize scholarly literature. Many information science historians cite Paul Otlet and Henri La Fontaine as the fathers of information science with the founding of the International Institute of Bibliography (IIB) in 1895. A second generation of European Documentalists emerged after the Second World War, most notably Suzanne Briet. However, ""information science"" as a term is not popularly used in academia until sometime in the latter part of the 20th century.Documentalists emphasized the utilitarian integration of technology and technique toward specific social goals. According to Ronald Day, ""As an organized system of techniques and technologies, documentation was understood as a player in the historical development of global organization in modernity – indeed, a major player inasmuch as that organization was dependent on the organization and transmission of information.""
Otlet and Lafontaine (who won the Nobel Prize in 1913) not only envisioned later technical innovations but also projected a global vision for information and information technologies that speaks directly to postwar visions of a global ""information society"". Otlet and Lafontaine established numerous organizations dedicated to standardization, bibliography, international associations, and consequently, international cooperation. These organizations were fundamental for ensuring international production in commerce, information, communication and modern economic development, and they later found their global form in such institutions as the League of Nations and the United Nations. Otlet designed the Universal Decimal Classification, based on Melville Dewey’s decimal classification system.Although he lived decades before computers and networks emerged, what he discussed prefigured what ultimately became the World Wide Web. His vision of a great network of knowledge focused on documents and included the notions of hyperlinks, search engines, remote access, and social networks.
Otlet not only imagined that all the world's knowledge should be interlinked and made available remotely to anyone, but he also proceeded to build a structured document collection. This collection involved standardized paper sheets and cards filed in custom-designed cabinets according to a hierarchical index (which culled information worldwide from diverse sources) and a commercial information retrieval service (which answered written requests by copying relevant information from index cards). Users of this service were even warned if their query was likely to produce more than 50 results per search.
By 1937 documentation had formally been institutionalized, as evidenced by the founding of the American Documentation Institute (ADI), later called the American Society for Information Science and Technology.

Transition to modern information science
With the 1950s came increasing awareness of the potential of automatic devices for literature searching and information storage and retrieval. As these concepts grew in magnitude and potential, so did the variety of information science interests. By the 1960s and 70s, there was a move from batch processing to online modes, from mainframe to mini and microcomputers. Additionally, traditional boundaries among disciplines began to fade and many information science scholars joined with other programs. They further made themselves multidisciplinary by incorporating disciplines in the sciences, humanities and social sciences, as well as other professional programs, such as law and medicine in their curriculum. By the 1980s, large databases, such as Grateful Med at the National Library of Medicine, and user-oriented services such as Dialog and Compuserve, were for the first time accessible by individuals from their personal computers. The 1980s also saw the emergence of numerous special interest groups to respond to the changes. By the end of the decade, special interest groups were available involving non-print media, social sciences, energy and the environment, and community information systems. Today, information science largely examines technical bases, social consequences, and theoretical understanding of online databases, widespread use of databases in government, industry, and education, and the development of the Internet and World Wide Web.

Information dissemination in the 21st century
Changing definition
Dissemination has historically been interpreted as unilateral communication of information. With the advent of the internet, and the explosion in popularity of online communities, ""social media has changed the information landscape in many respects, and creates both new modes of communication and new types of information"", changing the interpretation of the definition of dissemination. The nature of social networks allows for faster diffusion of information than through organizational sources. The internet has changed the way we view, use, create, and store information, now it is time to re-evaluate the way we share and spread it.

Impact of social media on people and industry
Social media networks provide an open information environment for the mass of people who have limited time or access to traditional outlets of information diffusion, this is an ""increasingly mobile and social world [that] demands...new types of information skills"". Social media integration as an access point is a very useful and mutually beneficial tool for users and providers. All major news providers have visibility and an access point through networks such as Facebook and Twitter maximizing their breadth of audience. Through social media people are directed to, or provided with, information by people they know. The ability to ""share, like, and comment on...content"" increases the reach farther and wider than traditional methods. People like to interact with information, they enjoy including the people they know in their circle of knowledge. Sharing through social media has become so influential that publishers must ""play nice"" if they desire to succeed. Although, it is often mutually beneficial for publishers and Facebook to ""share, promote and uncover new content"" to improve both user base experiences. The impact of popular opinion can spread in unimaginable ways. Social media allows interaction through simple to learn and access tools; The Wall Street Journal offers an app through Facebook, and The Washington Post goes a step further and offers an independent social app that was downloaded by 19.5 million users in 6 months, proving how interested people are in the new way of being provided information.

Social media's power to facilitate topics
The connections and networks sustained through social media help information providers learn what is important to people. The connections people have throughout the world enable the exchange of information at an unprecedented rate. It is for this reason that these networks have been realized for the potential they provide. ""Most news media monitor Twitter for breaking news"", as well as news anchors frequently request the audience to tweet pictures of events. The users and viewers of the shared information have earned ""opinion-making and agenda-setting power"" This channel has been recognized for the usefulness of providing targeted information based on public demand.

Research vectors and applications
The following areas are some of those that information science investigates and develops.

Information access
Information access is an area of research at the intersection of Informatics, Information Science, Information Security, Language Technology, and Computer Science. The objectives of information access research are to automate the processing of large and unwieldy amounts of information and to simplify users' access to it. What about assigning privileges and restricting access to unauthorized users? The extent of access should be defined in the level of clearance granted for the information. Applicable technologies include information retrieval, text mining, text editing, machine translation, and text categorisation. In discussion, information access is often defined as concerning the insurance of free and closed or public access to information and is brought up in discussions on copyright, patent law, and public domain. Public libraries need resources to provide knowledge of information assurance.

Information architecture
Information architecture (IA) is the art and science of organizing and labelling websites, intranets, online communities and software to support usability. It is an emerging discipline and community of practice focused on bringing together principles of design and architecture to the digital landscape. Typically it involves a model or concept of information which is used and applied to activities that require explicit details of complex information systems. These activities include library systems and database development.

Information management
Information management (IM) is the collection and management of information from one or more sources and the distribution of that information to one or more audiences. This sometimes involves those who have a stake in, or a right to that information. Management means the organization of and control over the structure, processing and delivery of information. Throughout the 1970s this was largely limited to files, file maintenance, and the life cycle management of paper-based files, other media and records. With the proliferation of information technology starting in the 1970s, the job of information management took on a new light and also began to include the field of data maintenance.

Information retrieval
Information retrieval (IR) is the area of study concerned with searching for documents, for information within documents, and for metadata about documents, as well as that of searching structured storage, relational databases, and the World Wide Web. Automated information retrieval systems are used to reduce what has been called ""information overload"". Many universities and public libraries use IR systems to provide access to books, journals and other documents. Web search engines are the most visible IR applications.
An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.
An object is an entity that is represented by information in a database. User queries are matched against the database information. Depending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.
Most IR systems compute a numeric score on how well each object in the database match the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.

Information seeking
Information seeking is the process or activity of attempting to obtain information in both human and technological contexts. Information seeking is related to, but different from, information retrieval (IR).
Much library and information science (LIS) research has focused on the information-seeking practices of practitioners within various fields of professional work. Studies have been carried out into the information-seeking behaviors of librarians, academics, medical professionals, engineers and lawyers (among others). Much of this research has drawn on the work done by Leckie, Pettigrew (now Fisher) and Sylvain, who in 1996 conducted an extensive review of the LIS literature (as well as the literature of other academic fields) on professionals' information seeking. The authors proposed an analytic model of professionals' information seeking behaviour, intended to be generalizable across the professions, thus providing a platform for future research in the area. The model was intended to ""prompt new insights... and give rise to more refined and applicable theories of information seeking"" (1996, p. 188). The model has been adapted by Wilkinson (2001) who proposes a model of the information seeking of lawyers.

Information society
An information society is a society where the creation, distribution, diffusion, uses, integration and manipulation of information is a significant economic, political, and cultural activity. The aim of an information society is to gain competitive advantage internationally, through using IT in a creative and productive way.  The knowledge economy is its economic counterpart, whereby wealth is created through the economic exploitation of understanding. People who have the means to partake in this form of society are sometimes called digital citizens.
Basically, an information society is the means of getting information from one place to another (Wark, 1997, p. 22). As technology has become more advanced over time so too has the way we have adapted in sharing this information with each other.
Information society theory discusses the role of information and information technology in society, the question of which key concepts should be used for characterizing contemporary society, and how to define such concepts. It has become a specific branch of contemporary sociology.

Knowledge representation and reasoning
Knowledge representation (KR) is an area of artificial intelligence research aimed at representing knowledge in symbols to facilitate inferencing from those knowledge elements, creating new elements of knowledge. The KR can be made to be independent of the underlying knowledge model or knowledge base system (KBS) such as a semantic network.Knowledge Representation (KR) research involves analysis of how to reason accurately and effectively and how best to use a set of symbols to represent a set of facts within a knowledge domain. A symbol vocabulary and a system of logic are combined to enable inferences about elements in the KR to create new KR sentences. Logic is used to supply formal semantics of how reasoning functions should be applied to the symbols in the KR system. Logic is also used to define how operators can process and reshape the knowledge. Examples of operators and operations include, negation, conjunction, adverbs, adjectives, quantifiers and modal operators. The logic is interpretation theory. These elements—symbols, operators, and interpretation theory—are what give sequences of symbols meaning within a KR.

See also
Computer and information science
Journal of the Association for Information Science and Technology (JASIST)
List of computer science awards § Information science awards
Outline of information science
Outline of information technology

References
Further reading
Khosrow-Pour, Mehdi (2005-03-22). Encyclopedia of Information Science and Technology. Idea Group Reference. ISBN 978-1-59140-553-5.

External links
American Society for Information Science and Technology, a ""professional association that bridges the gap between information science practice and research. ASIS&T members represent the fields of information science, computer science, linguistics, management, librarianship, engineering, data science, information architecture, law, medicine, chemistry, education, and related technology"".
Knowledge Map of Information Science
Journal of Information Science
Digital Library of Information Science and Technology open access archive for the Information Sciences
Current Information Science Research at U.S. Geological Survey
Introduction to Information Science
The Nitecki Trilogy
Information science at the University of California at Berkeley in the 1960s: a memoir of student days
Chronology of Information Science and Technology
LIBRES – Library and Information Science Research Electronic Journal -
Curtin University of Technology, Perth, Western Australia
Shared decision-making",https://en.wikipedia.org/wiki/Information_science,"['All articles containing potentially dated statements', 'All articles to be split', 'All articles with specifically marked weasel-worded phrases', 'Articles containing potentially dated statements from 2013', 'Articles to be split from January 2020', 'Articles with short description', 'Articles with specifically marked weasel-worded phrases from July 2014', 'Information', 'Information science', 'Library science', 'Official website not in Wikidata', 'Short description is different from Wikidata', 'Webarchive template wayback links', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NARA identifiers', 'Wikipedia articles with SUDOC identifiers']",Data Science
108,Information explosion,"The information explosion is the rapid increase in the amount of published information or data and the effects of this abundance. As the amount of available data grows, the problem of managing the information becomes more difficult, which can lead to information overload. The Online Oxford English Dictionary indicates use of the phrase in a March 1964 New Statesman article. The New York Times first used the phrase in its editorial content in an article by Walter Sullivan on June 7, 1964, in which he described the phrase as ""much discussed"". (p11.)  The earliest use of the phrase seems to have been in an IBM advertising supplement to the New York Times published on April 30, 1961, and by Frank Fremont-Smith, Director of the American Institute of Biological Sciences Interdisciplinary Conference Program, in an April 1961 article in the AIBS Bulletin (p18.) Many sectors are seeing this rapid increase in the amount of information available such as healthcare, supermarkets, and even governments with birth certificate informations and immunization records. Another sector that is being affected by this phenomenon is journalism. Such a profession, which in the past was responsible for the dissemination of information, may be suppressed by the overabundance of information today.Techniques to gather knowledge from an overabundance of electronic information (e.g., data fusion may help in data mining) have existed since the 1970s. Another common technique to deal with such amount of information is qualitative research. Such approaches aim to organize the information, synthesizing, categorizing and systematizing in order to be more usable and easier to search.

Growth patterns
The world's technological capacity to store information grew from 2.6 (optimally compressed) exabytes in 1986 to 15.8 in 1993, over 54.5 in 2000, and to 295 (optimally compressed) exabytes in 2007. This is equivalent to less than one 730-MB CD-ROM per person in 1986 (539 MB per person), roughly 4 CD-ROM per person of 1993, 12 CD-ROM per person in the year 2000, and almost 61 CD-ROM per person in 2007. Piling up the imagined 404 billion CD-ROM from 2007 would create a stack from the Earth to the Moon and a quarter of this distance beyond (with 1.2 mm thickness per CD).
The world’s technological capacity to receive information through one-way broadcast networks was 432 exabytes of (optimally compressed) information in 1986, 715 (optimally compressed) exabytes in 1993, 1,200 (optimally compressed) exabytes in 2000, and 1,900 in 2007.
The world's effective capacity to exchange information through two-way telecommunication networks was 0.281 exabytes of (optimally compressed) information in 1986, 0.471 in 1993, 2.2 in 2000, and 65 (optimally compressed) exabytes in 2007.A new metric that is being used in an attempt to characterize the growth in person-specific information, is the disk storage per person (DSP), which is measured in megabytes/person (where megabytes is 106 bytes and is abbreviated MB). Global DSP (GDSP) is the total rigid disk drive space (in MB) of new units sold in a year divided by the world population in that year. The GDSP metric is a crude measure of how much disk storage could possibly be used to collect person-specific data on the world population. 
In 1983, one million fixed drives with an estimated total of 90 terabytes were sold worldwide; 30MB drives had the largest market segment. In 1996, 105 million drives, totaling 160,623 terabytes were sold with 1 and 2 gigabyte drives leading the industry. By the year 2000, with 20GB drive leading the industry, rigid drives sold for the year are projected to total 2,829,288 terabytes Rigid disk drive sales to top $34 billion in 1997.
According to Latanya Sweeney, there are three trends in data gathering today:
Type 1. Expansion of the number of fields being collected, known as the “collect more” trend.
Type 2. Replace an existing aggregate data collection with a person-specific one, known as the “collect specifically” trend.
Type 3. Gather information by starting a new person-specific data collection, known as the “collect it if you can” trend.

Related terms
Since ""information"" in electronic media is often used synonymously with ""data"", the term information explosion is closely related to the concept of data flood (also dubbed data deluge). Sometimes the term information flood is used as well. All of those basically boil down to the ever-increasing amount of electronic data exchanged per time unit. The awareness about non-manageable amounts of data grew along with the advent of ever more powerful data processing since the mid-1960s.

Challenges
Even though the abundance of information can be beneficial in several levels, some problems may be of concern such as privacy, legal and ethical guidelines, filtering and data accuracy. Filtering refers to finding useful information in the middle of so much data, which relates to the job of data scientists. A typical example of a necessity of data filtering (data mining) is in healthcare since in the next years is due to have EHRs (Electronic Health Records) of patients available. With so much information available, the doctors will need to be able to identify patterns and select important data for the diagnosis of the patient. On the other hand, according to some experts, having so much public data available makes it difficult to provide data that is actually anonymous.
Another point to take into account is the legal and ethical guidelines, which relates to who will be the owner of the data and how frequently he/she is obliged to the release this and for how long.
With so many sources of data, another problem will be accuracy of such. An untrusted source may be challenged by others, by ordering a new set of data, causing a repetition in the information.
According to Edward Huth, another concern is the accessibility and cost of such information. The accessibility rate could be improved by either reducing the costs or increasing the utility of the information. The reduction of costs according to the author, could be done by associations, which should assess which information was relevant and gather it in a more organized fashion.

Web servers
As of August 2005, there were over 70 million web servers. As of September 2007 there were over 135 million web servers.

Blogs
According to Technorati, the number of blogs doubles about every 6 months with a total of 35.3 million blogs as of April 2006. This is an example of the early stages of logistic growth, where growth is approximately exponential, since blogs are a recent innovation. As the number of blogs approaches the number of possible producers (humans), saturation occurs, growth declines, and the number of blogs eventually stabilizes.

See also
Big data
Curse of dimensionality
Data mining
Information environmentalism
Information society
Information Age
Information filtering system
Metcalfe's law
Neuroenhancement
Second half of the chessboard

References
External links
Conceptualizing Information Systems and Cognitive Sustainability in 21st Century 'Attention' Economies (Includes Syllabus)
How Much Information? 2003
Surviving the Information Explosion: How People Find Their Electronic Information [1]
Why the Information Explosion Can Be Bad for Data Mining, and How Data Fusion Provides a Way Out [2]
Information Explosion, Largest databases",https://en.wikipedia.org/wiki/Information_explosion,"['All articles containing potentially dated statements', 'Articles containing potentially dated statements from April 2006', 'Articles containing potentially dated statements from September 2007', 'Information Age', 'Information science', 'Library science']",Data Science
109,Information technology,"Information technology (IT) is the use of computers to store, retrieve, transmit, and manipulate data or information. IT is typically used within the context of business operations as opposed to personal or entertainment technologies. IT is considered to be a subset of information and communications technology (ICT). An information technology system (IT system) is generally an information system, a communications system, or, more specifically speaking, a computer system – including all hardware, software, and peripheral equipment – operated by a limited group of IT users.
Humans have been storing, retrieving, manipulating, and communicating information since the Sumerians in Mesopotamia developed writing in about 3000 BC. However, the term information technology in its modern sense first appeared in a 1958 article published in the Harvard Business Review; authors Harold J. Leavitt and Thomas L. Whisler commented that ""the new technology does not yet have a single established name. We shall call it information technology (IT)."" Their definition consists of three categories: techniques for processing, the application of statistical and mathematical methods to decision-making, and the simulation of higher-order thinking through computer programs.The term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several products or services within an economy are associated with information technology, including computer hardware, software, electronics, semiconductors, internet, telecom equipment, and e-commerce.Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of IT development: pre-mechanical (3000 BC – 1450 AD), mechanical (1450–1840), electromechanical (1840–1940), and electronic (1940–present). This article focuses on the most recent period (electronic).

History of computer technology
Devices have been used to aid computation for thousands of years, probably initially in the form of a tally stick. The Antikythera mechanism, dating from about the beginning of the first century BC, is generally considered to be the earliest known mechanical analog computer, and the earliest known geared mechanism. Comparable geared devices did not emerge in Europe until the 16th century, and it was not until 1645 that the first mechanical calculator capable of performing the four basic arithmetical operations was developed.Electronic computers, using either relays or valves, began to appear in the early 1940s. The electromechanical Zuse Z3, completed in 1941, was the world's first programmable computer, and by modern standards one of the first machines that could be considered a complete computing machine. Colossus, developed during the Second World War to decrypt German messages, was the first electronic digital computer. Although it was programmable, it was not general-purpose, being designed to perform only a single task. It also lacked the ability to store its program in memory; programming was carried out using plugs and switches to alter the internal wiring. The first recognizably modern electronic digital stored-program computer was the Manchester Baby, which ran its first program on 21 June 1948.The development of transistors in the late 1940s at Bell Laboratories allowed a new generation of computers to be designed with greatly reduced power consumption. The first commercially available stored-program computer, the Ferranti Mark I, contained 4050 valves and had a power consumption of 25 kilowatts. By comparison, the first transistorized computer developed at the University of Manchester and operational by November 1953, consumed only 150 watts in its final version.Several later breakthroughs in semiconductor technology include the integrated circuit (IC) invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor in 1959, the metal-oxide-semiconductor field-effect transistor (MOSFET) invented by Mohamed Atalla and Dawon Kahng at Bell Laboratories in 1959, and the microprocessor invented by Ted Hoff, Federico Faggin, Masatoshi Shima and Stanley Mazor at Intel in 1971. These important inventions led to the development of the personal computer (PC) in the 1970s, and the emergence of information and communications technology (ICT).

Electronic data processing
Data storage
Early electronic computers such as Colossus made use of punched tape, a long strip of paper on which data was represented by a series of holes, a technology now obsolete. Electronic data storage, which is used in modern computers, dates from World War II, when a form of delay line memory was developed to remove the clutter from radar signals, the first practical application of which was the mercury delay line. The first random-access digital storage device was the Williams tube, based on a standard cathode ray tube, but the information stored in it and delay line memory was volatile in that it had to be continuously refreshed, and thus was lost once power was removed. The earliest form of non-volatile computer storage was the magnetic drum, invented in 1932 and used in the Ferranti Mark 1, the world's first commercially available general-purpose electronic computer.IBM introduced the first hard disk drive in 1956, as a component of their 305 RAMAC computer system. Most digital data today is still stored magnetically on hard disks, or optically on media such as CD-ROMs. Until 2002 most information was stored on analog devices, but that year digital storage capacity exceeded analog for the first time. As of 2007 almost 94% of the data stored worldwide was held digitally: 52% on hard disks, 28% on optical devices and 11% on digital magnetic tape. It has been estimated that the worldwide capacity to store information on electronic devices grew from less than 3 exabytes in 1986 to 295 exabytes in 2007, doubling roughly every 3 years.

Databases
Database Management Systems (DMS) emerged in the 1960s to address the problem of storing and retrieving large amounts of data accurately and quickly. An early such systems was IBM's Information Management System (IMS), which is still widely deployed more than 50 years later. IMS stores data hierarchically, but in the 1970s Ted Codd proposed an alternative relational storage model based on set theory and predicate logic and the familiar concepts of tables, rows and columns. In 1981, the first commercially available relational database management system (RDBMS) was released by Oracle.All DMS consist of components, they allow the data they store to be accessed simultaneously by many users while maintaining its integrity. All databases are common in one point that the structure of the data they contain is defined and stored separately from the data itself, in a database schema.In recent years, the extensible markup language (XML) has become a popular format for data representation. Although XML data can be stored in normal file systems, it is commonly held in relational databases to take advantage of their ""robust implementation verified by years of both theoretical and practical effort"". As an evolution of the Standard Generalized Markup Language (SGML), XML's text-based structure offers the advantage of being both machine and human-readable.

Data retrieval
The relational database model introduced a programming-language independent Structured Query Language (SQL), based on relational algebra.
The terms ""data"" and ""information"" are not synonymous. Anything stored is data, but it only becomes information when it is organized and presented meaningfully. Most of the world's digital data is unstructured, and stored in a variety of different physical formats even within a single organization. Data warehouses began to be developed in the 1980s to integrate these disparate stores. They typically contain data extracted from various sources, including external sources such as the Internet, organized in such a way as to facilitate decision support systems (DSS).

Data transmission
Data transmission has three aspects: transmission, propagation, and reception. It can be broadly categorized as broadcasting, in which information is transmitted unidirectionally downstream, or telecommunications, with bidirectional upstream and downstream channels.XML has been increasingly employed as a means of data interchange since the early 2000s, particularly for machine-oriented interactions such as those involved in web-oriented protocols such as SOAP, describing ""data-in-transit rather than ... data-at-rest"".

Data manipulation
Hilbert and Lopez identify the exponential pace of technological change (a kind of Moore's law): machines' application-specific capacity to compute information per capita roughly doubled every 14 months between 1986 and 2007; the per capita capacity of the world's general-purpose computers doubled every 18 months during the same two decades; the global telecommunication capacity per capita doubled every 34 months; the world's storage capacity per capita required roughly 40 months to double (every 3 years); and per capita broadcast information has doubled every 12.3 years.Massive amounts of data are stored worldwide every day, but unless it can be analysed and presented effectively it essentially resides in what have been called data tombs: ""data archives that are seldom visited"". To address that issue, the field of data mining – ""the process of discovering interesting patterns and knowledge from large amounts of data"" – emerged in the late 1980s.
Hilbert and Lopez identify the exponential pace of technological change (a kind of Moore's law): machines' application-specific capacity to compute information per capita roughly doubled every 14 months between 1986 and 2007; the per capita capacity of the world's general-purpose computers doubled every 18 months during the same two decades; the global telecommunication capacity per capita doubled every 34 months; the world's storage capacity per capita required roughly 40 months to double (every 3 years); and per capita broadcast information has doubled every 12.3 years.

Perspectives
Academic perspective
In an academic context, the Association for Computing Machinery defines IT as ""undergraduate degree programs that prepare students to meet the computer technology needs of business, government, healthcare, schools, and other kinds of organizations .... IT specialists assume responsibility for selecting hardware and software products appropriate for an organization, integrating those products with organizational needs and infrastructure, and installing, customizing, and maintaining those applications for the organization’s computer users.""Undergraduate degrees in IT (B.S., A.S.) are similar to other computer science degrees.  In fact, they often times have the same foundational level courses.  Computer science (CS) programs tend to focus more on theory and design, whereas Information Technology programs are structured to equip the graduate with expertise in the practical application of technology solutions to support modern business and user needs.

Commercial and employment perspective
Companies in the information technology field are often discussed as a group as the ""tech sector"" or the ""tech industry"".  These titles can be misleading at times and should not be mistaken for “tech companies”; which are generally large scale, for-profit corporations that sell consumer technology and software.  It is also worth noting that from a business perspective, Information Technology departments are a “cost center” the majority of the time.  A cost center is a department or staff which incurs expenses, or “costs”, within a company rather than generating profits or revenue streams.  Modern businesses rely heavily on technology for their day-to-day operations, so the expenses delegated to cover technology that facilitates business in a more efficient manner is usually seen as “just the cost of doing business”.  IT departments are allocated funds by senior leadership and must attempt to achieve the desired deliverables while staying within that budget.  Government and the private sector might have different funding mechanisms, but the principles are more-or-less the same.  This is an often overlooked reason for the rapid interest in automation and Artificial Intelligence, but the constant pressure to do more with less is opening the door for automation to take control of at least some minor operations in large companies.
Many companies now have IT departments for managing the computers, networks, and other technical areas of their businesses. Companies have also sought to integrate IT with business outcomes and decision-making through a BizOps or business operations department.In a business context, the Information Technology Association of America has defined information technology as ""the study, design, development, application, implementation, support or management of computer-based information systems"". The responsibilities of those working in the field include network administration, software development and installation, and the planning and management of an organization's technology life cycle, by which hardware and software are maintained, upgraded and replaced.

Information services
Information services is a term somewhat loosely applied to a variety of IT-related services offered by commercial companies, as well as data brokers.

Ethical perspectives
The field of information ethics was established by mathematician Norbert Wiener in the 1940s. Some of the ethical issues associated with the use of information technology include:
Breaches of copyright by those downloading files stored without the permission of the copyright holders
Employers monitoring their employees' emails and other Internet usage
Unsolicited emails
Hackers accessing online databases
Web sites installing cookies or spyware to monitor a user's online activities, which may be used by data brokers

See also
References
Notes
Citations
Bibliography
Further reading
Allen, T.; Morton, M. S. Morton, eds. (1994), Information Technology and the Corporation of the 1990s, Oxford University Press
Gitta, Cosmas and South, David (2011). Southern Innovator Magazine Issue 1: Mobile Phones and Information Technology: United Nations Office for South-South Cooperation. ISSN 2222-9280
Gleick, James (2011).The Information: A History, a Theory, a Flood. New York: Pantheon Books.
Price, Wilson T. (1981), Introduction to Computer Data Processing, Holt-Saunders International Editions, ISBN 978-4-8337-0012-2
Shelly, Gary, Cashman, Thomas, Vermaat, Misty, and Walker, Tim. (1999). Discovering Computers 2000: Concepts for a Connected World. Cambridge, Massachusetts: Course Technology.
Webster, Frank, and Robins, Kevin. (1986). Information Technology – A Luddite Analysis. Norwood, NJ: Ablex.

External links
 Learning materials related to Information technology at Wikiversity
 Media related to Information technology at Wikimedia Commons
 Quotations related to Information technology at Wikiquote
""Operational technology (OT) - definitions and differences with IT"". i-SCOOP. Retrieved 20 March 2021.",https://en.wikipedia.org/wiki/Information_technology,"['Articles with short description', 'Commons category link from Wikidata', 'Computers', 'Information technology', 'Mass media technology', 'Pages containing links to subscription-only content', 'Short description is different from Wikidata', 'Use dmy dates from September 2016', 'Webarchive template wayback links', 'Wikipedia articles needing page number citations from September 2017', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NDL identifiers']",Data Science
110,International Conference on Machine Learning,"The International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning. Along with NeurIPS and ICLR, it is one of the three primary conferences of high impact in machine learning and artificial intelligence research. It is supported by the International Machine Learning Society (IMLS). Precise dates vary from year to year, but paper submissions are generally due at the end of January, and the conference is generally held during the following July. The first ICML was held 1980 in Pittsburgh.

Locations
ICML 2023 Seoul, South Korea
 ICML 2022 Baltimore, Maryland, United States
 ICML 2021 Vienna, Austria (virtual conference)
 ICML 2020 Vienna, Austria (virtual conference)
 ICML 2019 Los Angeles, United States
 ICML 2018 Stockholm, Sweden
 ICML 2017 Sydney, Australia
 ICML 2016 New York City, United States
 ICML 2015 Lille, France
 ICML 2014 Beijing, China
 ICML 2013 Atlanta, United States
 ICML 2012 Edinburgh, Great Britain
 ICML 2011 Bellevue, United States
 ICML 2010 Haifa, Israel
 ICML 2009 Montréal, Canada
 ICML 2008 Helsinki, Finland
 ICML 2007 Corvallis, Oregon, United States
 ICML 2006 Pittsburgh, Pennsylvania, United States
 ICML 2005 Bonn, Germany
 ICML 2004 Banff, Canada
 ICML 2003 Washington DC, United States
 ICML 2002 Sydney, Australia
 ICML 2001 Williamstown, Massachusetts, United States
 ICML 2000 Stanford, California, United States

See also
ICLR
Journal of Machine Learning Research
Machine Learning (journal)
NeurIPS

References
External links
Official site
www.conferencealert.com
www.conferencenext.com
www.conferencealerts.in
www.worldconferencealerts.com",https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning,"['All stub articles', 'Artificial intelligence conferences', 'Artificial intelligence stubs', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers', 'Wikipedia articles with multiple identifiers']",Data Science
111,Inter-disciplinary,"Interdisciplinarity or interdisciplinary studies involves the combination of two or more academic disciplines into one activity (e.g., a research project). It draws knowledge from several other fields like sociology, anthropology, psychology, economics etc.  It is about creating something by thinking across boundaries. It is related to an interdiscipline or an interdisciplinary field, which is an organizational unit that crosses traditional boundaries between academic disciplines or schools of thought, as new needs and professions emerge. Large engineering teams are usually interdisciplinary, as a power station or mobile phone or other project requires the melding of several specialties. However, the term ""interdisciplinary"" is sometimes confined to academic settings.
The term interdisciplinary is applied within education and training pedagogies to describe studies that use methods and insights of several established disciplines or traditional fields of study. Interdisciplinarity involves researchers, students, and teachers in the goals of connecting and integrating several academic schools of thought, professions, or technologies—along with their specific perspectives—in the pursuit of a common task. The epidemiology of HIV/AIDS or global warming requires understanding of diverse disciplines to solve complex problems. Interdisciplinary may be applied where the subject is felt to have been neglected or even misrepresented in the traditional disciplinary structure of research institutions, for example, women's studies or ethnic area studies. Interdisciplinarity can likewise be applied to complex subjects that can only be understood by combining the perspectives of two or more fields.
The adjective interdisciplinary is most often used in educational circles when researchers from two or more disciplines pool their approaches and modify them so that they are better suited to the problem at hand, including the case of the team-taught course where students are required to understand a given subject in terms of multiple traditional disciplines.  For example, the subject of land use may appear differently when examined by different disciplines, for instance, biology, chemistry, economics, geography, and politics.

Development
Although ""interdisciplinary"" and ""interdisciplinarity"" are frequently viewed as twentieth century terms, the concept has historical antecedents, most notably Greek philosophy. Julie Thompson Klein attests that ""the roots of the concepts lie in a number of ideas that resonate through modern discourse—the ideas of a unified science, general knowledge, synthesis and the integration of knowledge"", while Giles Gunn says that Greek historians and dramatists took elements from other realms of knowledge (such as medicine or philosophy) to further understand their own material. The building of Roman roads required men who understood surveying, material science, logistics and several other disciplines. Any broadminded humanist project involves interdisciplinarity, and history shows a crowd of cases, as seventeenth-century Leibniz's task to create a system of universal justice, which required linguistics, economics, management, ethics, law philosophy, politics, and even sinology.Interdisciplinary programs sometimes arise from a shared conviction that the traditional disciplines are unable or unwilling to address an important problem. For example, social science disciplines such as anthropology and sociology paid little attention to the social analysis of technology throughout most of the twentieth century. As a result, many social scientists with interests in technology have joined science, technology and society programs, which are typically staffed by scholars drawn from numerous disciplines. They may also arise from new research developments, such as nanotechnology, which cannot be addressed without combining the approaches of two or more disciplines. Examples include quantum information processing, an amalgamation of quantum physics and computer science, and bioinformatics, combining molecular biology with computer science. Sustainable development as a research area deals with problems requiring analysis and synthesis across economic, social and environmental spheres; often an integration of multiple social and natural science disciplines. Interdisciplinary research is also key to the study of health sciences, for example in studying optimal solutions to diseases. Some institutions of higher education offer accredited degree programs in Interdisciplinary Studies.
At another level, interdisciplinarity is seen as a remedy to the harmful effects of excessive specialization and isolation in information silos. On some views, however, interdisciplinarity is entirely indebted to those who specialize in one field of study—that is, without specialists, interdisciplinarians would have no information and no leading experts to consult. Others place the focus of interdisciplinarity on the need to transcend disciplines, viewing excessive specialization as problematic both epistemologically and politically. When interdisciplinary collaboration or research results in new solutions to problems, much information is given back to the various disciplines involved. Therefore, both disciplinarians and interdisciplinarians may be seen in complementary relation to one another.

Barriers
Because most participants in interdisciplinary ventures were trained in traditional disciplines, they must learn to appreciate differences of perspectives and methods. For example, a discipline that places more emphasis on quantitative rigor may produce practitioners who are more scientific in their training than others; in turn, colleagues in ""softer"" disciplines who may associate quantitative approaches with difficulty grasp the broader dimensions of a problem and lower rigor in theoretical and qualitative argumentation. An interdisciplinary program may not succeed if its members remain stuck in their disciplines (and in disciplinary attitudes). Those who lack experience in interdisciplinary collaborations may also not fully appreciate the intellectual contribution of colleagues from those discipline. From the disciplinary perspective, however, much interdisciplinary work may be seen as ""soft"", lacking in rigor, or ideologically motivated; these beliefs place barriers in the career paths of those who choose interdisciplinary work.  For example, interdisciplinary grant applications are often refereed by peer reviewers drawn from established disciplines; not surprisingly, interdisciplinary researchers may experience difficulty getting funding for their research. In addition, untenured researchers know that, when they seek promotion and tenure, it is likely that some of the evaluators will lack commitment to interdisciplinarity. They may fear that making a commitment to interdisciplinary research will increase the risk of being denied tenure.
Interdisciplinary programs may also fail if they are not given sufficient autonomy. For example, interdisciplinary faculty are usually recruited to a joint appointment, with responsibilities in both an interdisciplinary program (such as women's studies) and a traditional discipline (such as history). If the traditional discipline makes the tenure decisions, new interdisciplinary faculty will be hesitant to commit themselves fully to interdisciplinary work. Other barriers include the generally disciplinary orientation of most scholarly journals, leading to the perception, if not the fact, that interdisciplinary research is hard to publish. In addition, since traditional budgetary practices at most universities channel resources through the disciplines, it becomes difficult to account for a given scholar or teacher's salary and time. During periods of budgetary contraction, the natural tendency to serve the primary constituency (i.e., students majoring in the traditional discipline) makes resources scarce for teaching and research comparatively far from the center of the discipline as traditionally understood. For these same reasons, the introduction of new interdisciplinary programs is often resisted because it is perceived as a competition for diminishing funds.
Due to these and other barriers, interdisciplinary research areas are strongly motivated to become disciplines themselves. If they succeed, they can establish their own research funding programs and make their own tenure and promotion decisions. In so doing, they lower the risk of entry. Examples of former interdisciplinary research areas that have become disciplines, many of them named for their parent disciplines, include neuroscience, cybernetics, biochemistry and biomedical engineering. These new fields are occasionally referred to as ""interdisciplines"". On the other hand, even though interdisciplinary activities are now a focus of attention for institutions promoting learning and teaching, as well as organizational and social entities concerned with education, they are practically facing complex barriers, serious challenges and criticism. The most important obstacles and challenges faced by interdisciplinary activities in the past two decades can be divided into ""professional"", ""organizational"", and ""cultural"" obstacles.

Interdisciplinary studies and studies of interdisciplinarity
An initial distinction should be made between interdisciplinary studies, which can be found spread across the academy today, and the study of interdisciplinarity, which involves a much smaller group of researchers. The former is instantiated in thousands of research centers across the US and the world. The latter has one US organization, the Association for Interdisciplinary Studies (founded in 1979), two international organizations, the International Network of Inter- and Transdisciplinarity (founded in 2010) and the Philosophy of/as Interdisciplinarity Network (founded in 2009), and one research institute devoted to the theory and practice of interdisciplinarity, the Center for the Study of Interdisciplinarity at the University of North Texas (founded in 2008). As of 1 September 2014, the Center for the Study of Interdisciplinarity has ceased to exist. This is the result of administrative decisions at the University of North Texas.An interdisciplinary study is an academic program or process seeking to synthesize broad perspectives, knowledge, skills, interconnections, and epistemology in an educational setting. Interdisciplinary programs may be founded in order to facilitate the study of subjects which have some coherence, but which cannot be adequately understood from a single disciplinary perspective (for example, women's studies or medieval studies). More rarely, and at a more advanced level, interdisciplinarity may itself become the focus of study, in a critique of institutionalized disciplines' ways of segmenting knowledge.
In contrast, studies of interdisciplinarity raise to self-consciousness questions about how interdisciplinarity works, the nature and history of disciplinarity, and the future of knowledge in post-industrial society. Researchers at the Center for the Study of Interdisciplinarity have made the distinction between philosophy 'of' and 'as' interdisciplinarity, the former identifying a new, discrete area within philosophy that raises epistemological and metaphysical questions about the status of interdisciplinary thinking, with the latter pointing toward a philosophical practice that is sometimes called 'field philosophy'.Perhaps the most common complaint regarding interdisciplinary programs, by supporters and detractors alike, is the lack of synthesis—that is, students are provided with multiple disciplinary perspectives, but are not given effective guidance in resolving the conflicts and achieving a coherent view of the subject. Others have argued that the very idea of synthesis or integration of disciplines presupposes questionable politico-epistemic commitments. Critics of interdisciplinary programs feel that the ambition is simply unrealistic, given the knowledge and intellectual maturity of all but the exceptional undergraduate; some defenders concede the difficulty, but insist that cultivating interdisciplinarity as a habit of mind, even at that level, is both possible and essential to the education of informed and engaged citizens and leaders capable of analyzing, evaluating, and synthesizing information from multiple sources in order to render reasoned decisions.
While much has been written on the philosophy and promise of interdisciplinarity in academic programs and professional practice, social scientists are increasingly interrogating academic discourses on interdisciplinarity, as well as how interdisciplinarity actually works—and does not—in practice. Some have shown, for example, that some interdisciplinary enterprises that aim to serve society can produce deleterious outcomes for which no one can be held to account.

Politics of interdisciplinary studies
Since 1998, there has been an ascendancy in the value of interdisciplinary research and teaching and a growth in the number of bachelor's degrees awarded at U.S. universities classified as multi- or interdisciplinary studies. The number of interdisciplinary bachelor's degrees awarded annually rose from 7,000 in 1973 to 30,000 a year by 2005 according to data from the National Center of Educational Statistics (NECS). In addition, educational leaders from the Boyer Commission to Carnegie's President Vartan Gregorian to Alan I. Leshner, CEO of the American Association for the Advancement of Science have advocated for interdisciplinary rather than disciplinary approaches to problem-solving in the 21st century.  This has been echoed by federal funding agencies, particularly the National Institutes of Health under the direction of Elias Zerhouni, who has advocated that grant proposals be framed more as interdisciplinary collaborative projects than single-researcher, single-discipline ones.
At the same time, many thriving longstanding bachelor's in interdisciplinary studies programs in existence for 30 or more years, have been closed down, in spite of healthy enrollment. Examples include Arizona International (formerly part of the University of Arizona), the School of Interdisciplinary Studies at Miami University, and the Department of Interdisciplinary Studies at Wayne State University; others such as the Department of Interdisciplinary Studies at Appalachian State University, and George Mason University's New Century College, have been cut back. Stuart Henry has seen this trend as part of the hegemony of the disciplines in their attempt to recolonize the experimental knowledge production of otherwise marginalized fields of inquiry. This is due to threat perceptions seemingly based on the ascendancy of interdisciplinary studies against traditional academia.

Historical examples
There are many examples of when a particular idea, almost on the same period, arises in different disciplines. One case is the shift from the approach of focusing on ""specialized segments of attention"" (adopting one particular perspective), to the idea of ""instant sensory awareness of the whole"", an attention to the ""total field"", a ""sense of the whole pattern, of form and function as a unity"", an ""integral idea of structure and configuration"". This has happened in painting (with cubism), physics, poetry, communication and educational theory. According to Marshall McLuhan, this paradigm shift was due to the passage from an era shaped by mechanization, which brought sequentiality, to the era shaped by the instant speed of electricity, which brought simultaneity.

Efforts to simplify and defend the concept
An article in the Social Science Journal attempts to provide a simple, common-sense, definition of interdisciplinarity, bypassing the difficulties of defining that concept and obviating the need for such related concepts as transdisciplinarity, pluridisciplinarity, and multidisciplinary:

""To begin with, a discipline can be conveniently defined as any comparatively self-contained and isolated domain of human experience which possesses its own community of experts. Interdisciplinarity is best seen as bringing together distinctive components of two or more disciplines. In academic discourse, interdisciplinarity typically applies to four realms: knowledge, research, education, and theory. Interdisciplinary knowledge involves familiarity with components of two or more disciplines. Interdisciplinary research combines components of two or more disciplines in the search or creation of new knowledge, operations, or artistic expressions. Interdisciplinary education merges components of two or more disciplines in a single program of instruction. Interdisciplinary theory takes interdisciplinary knowledge, research, or education as its main objects of study.""
In turn, interdisciplinary richness of any two instances of knowledge, research, or education can be ranked by weighing four variables: number of disciplines involved, the ""distance"" between them, the novelty of any particular combination, and their extent of integration.Interdisciplinary knowledge and research are important because:

""Creativity often requires interdisciplinary knowledge.
Immigrants often make important contributions to their new field.
Disciplinarians often commit errors which can be best detected by people familiar with two or more disciplines.
Some worthwhile topics of research fall in the interstices among the traditional disciplines.
Many intellectual, social, and practical problems require interdisciplinary approaches.
Interdisciplinary knowledge and research serve to remind us of the unity-of-knowledge ideal.
Interdisciplinarians enjoy greater flexibility in their research.
More so than narrow disciplinarians, interdisciplinarians often treat themselves to the intellectual equivalent of traveling in new lands.
Interdisciplinarians may help breach communication gaps in the modern academy, thereby helping to mobilize its enormous intellectual resources in the cause of greater social rationality and justice.
By bridging fragmented disciplines, interdisciplinarians might play a role in the defense of academic freedom.""

Quotations
""The modern mind divides, specializes, thinks in categories: the Greek instinct was the opposite, to take the widest view, to see things as an organic whole [...]. The Olympic games were designed to test the arete of the whole man, not a merely specialized skill [...]. The great event was the pentathlon, if you won this, you were a man. Needless to say, the Marathon race was never heard of until modern times: the Greeks would have regarded it as a monstrosity.""
""Previously, men could be divided simply into the learned and the ignorant, those more or less the one, and those more or less the other. But your specialist cannot be brought in under either of these two categories. He is not learned, for he is formally ignorant of all that does not enter into his specialty; but neither is he ignorant, because he is 'a scientist,' and 'knows' very well his own tiny portion of the universe. We shall have to say that he is a learned ignoramus, which is a very serious matter, as it implies that he is a person who is ignorant, not in the fashion of the ignorant man, but with all the petulance of one who is learned in his own special line.""
""It is the custom among those who are called ""practical"" men to condemn any man capable of a wide survey as a visionary: no man is thought worthy of a voice in politics unless he ignores or does not know nine-tenths of the most important relevant facts.""

See also
References
Further reading
Alderman, Harold; Chiappori, Pierre Andre; Haddad, Lawrence; Hoddinott, John (1995). ""Unitary Versus Collective Models of the Household: Time to Shift the Burden of Proof?"". World Bank Research Observer. 10 (1): 1–19. doi:10.1093/wbro/10.1.1.
Augsburg, Tanya (2005). Becoming Interdisciplinary: An Introduction to Interdisciplinary Studies. Kendall/Hunt.
Association for Integrative Studies
Bagchi, Amiya Kumar (1982). The Political Economy of Underdevelopment. New York: Cambridge University Press.
Bernstein, Henry (1973). ""Introduction: Development and The Social Sciences"".  In Henry Bernstein (ed.). Underdevelopment and Development: The Third World Today. Harmondsworth: Penguin. pp. 13–30.
Center for the Study of Interdisciplinarity
Centre for Interdisciplinary Research in the Arts (University of Manchester)
Chambers, Robert (2001), ""Qualitative approaches: self-criticism and what can be gained from quantitative approaches"",  in Kanbur, Ravi (ed.), Qual–quant: qualitative and quantitative poverty appraisal - complementaries, tensions, and the way forward (PDF), Ithaca, New York: Cornell University, pp. 22–25.
Chubin, D. E. (1976). ""The conceptualization of scientific specialties"". The Sociological Quarterly. 17 (4): 448–476. doi:10.1111/j.1533-8525.1976.tb01715.x.
College for Interdisciplinary Studies, University of British Columbia, Vancouver, British Columbia, Canada
Callard, Felicity; Fitzgerald, Des (2015). Rethinking Interdisciplinarity across the Social Sciences and Neurosciences. Basingstoke: Palgrave Macmillan.
Davies. M.; Devlin, M. (2007). ""Interdisciplinary Higher Education: Implications for Teaching and Learning"" (PDF). Centre for the Study of Higher Education, The University of Melbourne. Archived from the original (PDF) on 2 December 2007. Retrieved 7 November 2007.
Frodeman, R.; Mitcham, C. (Fall 2007). ""New Directions in Interdisciplinarity: Broad, Deep, and Critical"". Bulletin of Science, Technology & Society. 27 (6): 506–514. doi:10.1177/0270467607308284. S2CID 145008466.
Franks, D.; Dale, P.; Hindmarsh, R.; Fellows, C.; Buckridge, M.; Cybinski, P. (2007). ""Interdisciplinary foundations: reflecting on interdisciplinarity and three decades of teaching and research at Griffith University, Australia"". Studies in Higher Education. 32 (2): 167–185. doi:10.1080/03075070701267228. S2CID 144173921.
Frodeman, R., Klein, J.T., and Mitcham, C. Oxford Handbook of Interdisciplinarity. Oxford University Press, 2010.
The Evergreen State College, Olympia, Washington
Gram Vikas (2007) Annual Report, p. 19.
Granovetter, Mark (1985). ""Economic Action and Social Structure: The Problem of Embeddedness"" (PDF). The American Journal of Sociology. 91 (3): 481–510. doi:10.1086/228311. S2CID 17242802. Archived from the original (PDF) on 2 August 2014. Retrieved 25 October 2017.
Hang Seng Centre for Cognitive Studies
Harriss, John (2002). ""The Case for Cross-Disciplinary Approaches in International Development"". World Development. 30 (3): 487–496. doi:10.1016/s0305-750x(01)00115-2.
Henry, Stuart (2005). ""Disciplinary hegemony meets interdisciplinary ascendancy: Can interdisciplinary/integrative studies survive, and if so how?"" (PDF). Issues in Integrative Studies. 23: 1–37.
Indiresan, P.V. (1990) Managing Development: Decentralisation, Geographical Socialism And Urban Replication. India: Sage
Interdisciplinary Arts Department, Columbia College Chicago
Interdisciplinarity and tenure
Interdisciplinary Studies Project, Harvard University School of Education, Project Zero
Jackson, Cecile (2002). ""Disciplining Gender?"". World Development. 30 (3): 497–509. doi:10.1016/s0305-750x(01)00113-9.
Jacobs, J.A.; Frickel, S. (2009). ""Interdisciplinarity: A Critical Assessment"" (PDF). Annual Review of Sociology. 35: 43–65. doi:10.1146/annurev-soc-070308-115954.
Johnston, R (2003). ""Integrating methodologists into teams of substantive experts"" (PDF). Studies in Intelligence. 47 (1). Archived from the original (PDF) on 10 August 2006. Retrieved 8 August 2006.
Kanbur, Ravi (March 2002). ""Economics, social science and development"" (PDF). World Development. 30 (3): 477–486. doi:10.1016/S0305-750X(01)00117-6. hdl:1813/57796.
Kanbur, Ravi (2003), ""Q-squared?: a commentry on qualitative and quantitative poverty appraisal"",  in Kanbur, Ravi (ed.), Q-squared, combining qualitative and quantitative methods in poverty appraisal, Delhi Bangalore: Permanent Black Distributed by Orient Longman, pp. 2–27, ISBN 9788178240534.
Klein, Julie Thompson (1996) Crossing Boundaries: Knowledge, Disciplinarities, and Interdisciplinarities (University Press of Virginia)
Klein, Julie Thompson (2006) ""Resources for interdisciplinary studies."" Change, (Mark/April). 52–58
Kleinberg, Ethan (2008). ""Interdisciplinary studies at the crossroads"". Liberal Education. 94 (1): 6–11.
Kockelmans, Joseph J. editor (1979) Interdisciplinarity and Higher Education, The Pennsylvania State University Press ISBN 9780271038261.
Lipton, Michael (1970). ""Interdisciplinary Studies in Less Developed Countries"". Journal of Development Studies. 7 (1): 5–18. doi:10.1080/00220387008421343.
Gerhard Medicus Interdisciplinarity in Human Sciences (Documents No. 6, 7 and 8 in English)
Moran, Joe. (2002). Interdisciplinarity.
NYU Gallatin School of Individualized Study, New York, NY
Poverty Action Lab (accessed on 4 November 2008)
Ravallion, Martin (2003), ""Can qualitative methods help quantitative poverty"",  in Kanbur, Ravi (ed.), Q-squared, combining qualitative and quantitative methods in poverty appraisal, Delhi Bangalore: Permanent Black Distributed by Orient Longman, pp. 58–67, ISBN 9788178240534
Rhoten, D. (2003). A multi-method analysis of the social and technical conditions for interdisciplinary collaboration.
School of Social Ecology at the University of California, Irvine
Schuurman, F.J. (2000). ""Paradigms Lost, paradigms regained? Development studies in the twenty-first century"". Third World Quarterly. 21 (1): 7–20. doi:10.1080/01436590013198. S2CID 145181997.
Sen, Amartya (1999). Development as freedom. New York: Oxford University Press. ISBN 9780198297581.
Siskin, L.S. & Little, J.W. (1995). The Subjects in Question. Teachers College Press. about the departmental organization of high schools and efforts to change that.
Stiglitz, Joseph (2002) Globalisation and its Discontents, United States of America, W.W. Norton and Company
Sumner, A and M. Tribe (2008) International Development Studies: Theories and Methods in Research and Practice, London: Sage
Thorbecke, Eric. (2006) ""The Evolution of the Development Doctrine, 1950–2005"". UNU-WIDER Research Paper No. 2006/155. United Nations University, World Institute for Development Economics Research
Trans- & inter-disciplinary science approaches- A guide to on-line resources on integration and trans- and inter-disciplinary approaches.
Truman State University's Interdisciplinary Studies Program
Waldman, Amy (2003). ""Distrust Opens the Door for Polio in India"". The New York Times. Retrieved 4 November 2008.
Peter Weingart and Nico Stehr, eds. 2000. Practicing Interdisciplinarity (University of Toronto Press)
Peter Weingart; Britta Padberg (30 April 2014). University Experiments in Interdisciplinarity: Obstacles and Opportunities. transcript Verlag. ISBN 978-3-8394-2616-6.
White, Howard (2002). ""Combining Quantitative and Qualitative Approaches in Poverty Analysis"". World Development. 30 (3): 511–522. doi:10.1016/s0305-750x(01)00114-0.

External links
Association for Interdisciplinary Studies
National Science Foundation Workshop Report: Interdisciplinary Collaboration in Innovative Science and Engineering Fields
Rethinking Interdisciplnarity online conference, organized by the Institut Nicod, CNRS, Paris [broken]
Center for the Study of Interdisciplinarity at the University of North Texas
Labyrinthe. Atelier interdisciplinaire, a journal (in French), with a special issue on La Fin des Disciplines?
Rupkatha Journal on Interdisciplinary Studies in Humanities: An Online Open Access E-Journal, publishing articles on a number of areas
Article about interdisciplinary modeling (in French with an English abstract)
Wolf, Dieter. Unity of Knowledge, an interdisciplinary project
Soka University of America has no disciplinary departments and emphasizes interdisciplinary concentrations in the Humanities, Social and Behavioral Sciences, International Studies, and Environmental Studies.
SystemsX.ch - The Swiss Initiative in Systems Biology
Tackling Your Inner 5-Year-Old: Saving the world requires an interdisciplinary perspective",https://en.wikipedia.org/wiki/Interdisciplinarity,"['Academia', 'Academic discipline interactions', 'All articles with dead external links', 'All articles with unsourced statements', 'Articles with dead external links from November 2017', 'Articles with permanently dead external links', 'Articles with short description', 'Articles with unsourced statements from April 2017', 'CS1 errors: missing periodical', 'CS1 maint: archived copy as title', 'CS1 maint: multiple names: authors list', 'Commons category link is on Wikidata', 'Epistemology', 'Knowledge', 'Occupations', 'Pedagogy', 'Philosophy of education', 'Short description matches Wikidata', 'Use dmy dates from January 2020', 'Wikipedia articles with BNE identifiers', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with multiple identifiers']",Data Science
112,Jim Gray (computer scientist),"James Nicholas Gray (1944 – declared dead in absentia 2012) was an American computer scientist who received the Turing Award in 1998 ""for seminal contributions to database and transaction processing research and technical leadership in system implementation"".

Early years and personal life
Gray was born in San Francisco, the second child of Ann Emma Sanbrailo, a teacher, and James Able Gray, who was in the U.S. Army; the family moved to Rome, Italy, where Gray spent most of the first three years of his life; he learned to speak Italian before English. The family then moved to Virginia, spending about four years there, until Gray's parents divorced, after which he returned to San Francisco with his mother. His father, an amateur inventor, patented a design for a ribbon cartridge for typewriters that earned him a substantial royalty stream.After being turned down for the Air Force Academy he entered the University of California, Berkeley as a freshman in 1961.  To help pay for college he worked as a co-op for General Dynamics, where he learned to use a Monroe calculator.  Discouraged by his chemistry grades, he left Berkeley for six months, returning after an experience in industry he later described as ""dreadful"". Gray earned his B.S. in Engineering Mathematics (Math and Statistics) in 1966.After marrying, Gray moved with his wife Loretta to New Jersey, his wife's home state; she got a job as a teacher and he got one at Bell Labs working on a digital simulation that was to be part of Multics. At Bell, he worked three days a week and spent two days as a Master's student at New York University's Courant Institute. After a year they traveled for several months before settling again in Berkeley, where Gray entered graduate school with Michael A. Harrison as his advisor. In 1969 he received his Ph.D. in programming languages, then did two years of postdoctoral work for IBM.While at Berkeley, Gray and Loretta had a daughter; they were later divorced. His second wife was Donna Carnes.

Research
Gray pursued his career primarily working as a researcher and software designer at a number of industrial companies, including IBM, Tandem Computers, and DEC. He joined Microsoft in 1995 and was a Technical Fellow for the company
until he was lost at sea in 2007.Gray contributed to several major database and transaction processing systems. IBM's System R was the precursor of the SQL relational databases that have become a standard throughout the world. For Microsoft, he worked on TerraServer-USA and Skyserver.
His best-known achievements include:

ACID, an acronym describing the requirements for reliable transaction processing and its software implementation
Granular database locking
Two-tier transaction commit semantics
The Five-minute rule for allocating storage
OLAP cube operator for data warehousingHe assisted in developing Virtual Earth. He was also one of the co-founders of the Conference on Innovative Data Systems Research.

Disappearance
Gray, an experienced sailor, owned a 40-foot sailboat. On January 28, 2007, he failed to return from a short solo trip to the Farallon Islands near San Francisco to scatter his mother's ashes. The weather was clear, and no distress call was received, nor was any signal detected from the boat's automatic Emergency Position-Indicating Radio Beacon.
A four-day Coast Guard search using planes, helicopters, and boats found nothing.
On February 1, 2007, the DigitalGlobe satellite did a scan of the area and the thousands of images were posted to Amazon Mechanical Turk. Students, colleagues, and friends of Gray, and computer scientists around the world formed a ""Jim Gray Group"" to study these images for clues.
On February 16 this search was suspended, and an underwater search using sophisticated equipment ended May 31.The University of California, Berkeley and Gray's family hosted a tribute on May 31, 2008.  Microsoft's WorldWide Telescope software is dedicated to Gray. In 2008, Microsoft opened a research center in Madison, Wisconsin, named after Jim Gray. On January 28, 2012, Gray was declared legally dead.

Jim Gray eScience Award
Each year, Microsoft Research presents the Jim Gray eScience Award to a researcher who has made an outstanding contribution to the field of data-intensive computing. Award recipients are selected for their ground-breaking, fundamental contributions to the field of eScience. Previous award winners include Alex Szalay (2007), Carole Goble (2008), Jeff Dozier (2009), Phil Bourne (2010), Mark Abbott (2011), Antony John Williams (2012), and Dr. David Lipman, M.D. (2013).

See also
List of people who disappeared mysteriously at sea

Notes
References
External links
Gray's Microsoft Research home page, last accessed 23 June 2013
James (""Jim"") Nicholas Gray, Turing Award citation
Video Behind the Code on Channel 9, interviewed by Barbara Fox, 2005
Video The Future of Software and Databases, expert panel discussion with Rick Cattell, Don Chamberlin, Daniela Florescu, Jim Gray and Jim Melton, Software Development 2002 conference
Oral History Interview with Jim Gray, Charles Babbage Institute, University of Minnesota.  Oral history interview by Philip L. Frana, 3 January 2002, San Francisco, California.
The Future of Databases, SQL Down Under.  Interview with Dr Greg Low, 2005.
Tribute by Mark Whitehorn for The Register April 30, 2007
EE380: The Search for Jim Gray, Panel Discussion at Stanford University (video archive) May 28, 2008(Proceedings) May 31, 2008

Tribute by James Hamilton
Why Do Computers Stop and What Can Be Done About It?, a technical report by Jim Gray, 1985",https://en.wikipedia.org/wiki/Jim_Gray_(computer_scientist),"['1944 births', '2000s missing person cases', '2007 deaths', 'American computer scientists', 'Articles with hCards', 'Courant Institute of Mathematical Sciences alumni', 'Database researchers', 'Digital Equipment Corporation people', 'Fellows of the Association for Computing Machinery', 'IBM Research computer scientists', 'Members of the United States National Academy of Engineering', 'Members of the United States National Academy of Sciences', 'Microsoft employees', 'Microsoft technical fellows', 'Missing person cases in California', 'People declared dead in absentia', 'People lost at sea', 'Scientists from California', 'Turing Award laureates', 'UC Berkeley College of Engineering alumni', 'Webarchive template wayback links', 'Wikipedia articles with ACM-DL identifiers', 'Wikipedia articles with DBLP identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MGP identifiers', 'Wikipedia articles with NDL identifiers', 'Wikipedia articles with NKC identifiers', 'Wikipedia articles with NLP identifiers', 'Wikipedia articles with PLWABN identifiers', 'Wikipedia articles with SNAC-ID identifiers', 'Wikipedia articles with SUDOC identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers', 'Wikipedia external links cleanup from June 2013', 'Wikipedia spam cleanup from June 2013']",Data Science
113,John Tukey,"John Wilder Tukey (; June 16, 1915 – July 26, 2000) was an American mathematician best known for development of the Fast Fourier Transform (FFT) algorithm and box plot. The Tukey range test, the Tukey lambda distribution, the Tukey test of additivity, and the Teichmüller–Tukey lemma all bear his name. He is also credited with coining the term 'bit' and the first published use of the word software.

Biography
Tukey was born in New Bedford, Massachusetts, in 1915 to a Latin teacher father and a private tutor mother. He was mainly taught by his mother and attended regular classes only for special subjects like French. Tukey obtained a B.A. in 1936 and M.Sc. in 1937, in chemistry, from Brown University, before moving to Princeton University, where he received a Ph.D. in mathematics after completing a doctoral dissertation titled ""On denumerability in topology.""During World War II, Tukey worked at the Fire Control Research Office and collaborated with Samuel Wilks and William Cochran. He is claimed to have helped design the U-2 spy plane. After the war, he returned to Princeton, dividing his time between the university and AT&T Bell Laboratories. In 1962, Tukey was elected to the American Philosophical Society. He became a full professor at 35 and founding chairman of the Princeton statistics department in 1965.Among many contributions to civil society, Tukey served on a committee of the American Statistical Association that produced a report challenging the conclusions of the Kinsey Report, Statistical Problems of the Kinsey Report on Sexual Behavior in the Human Male.
From 1960 to 1980, Tukey helped design the NBC television network polls used to predict and analyze elections. He was also a consultant to the Educational Testing Service, the Xerox Corporation and Merck & Company.
He was awarded the National Medal of Science by President Nixon in 1973. He was awarded the IEEE Medal of Honor in 1982 ""For his contributions to the spectral analysis of random processes and the fast Fourier transform (FFT) algorithm"".
Tukey retired in 1985. He died in New Brunswick, New Jersey, on July 26, 2000.

Scientific contributions
Early in his career Tukey worked on developing statistical methods for computers at Bell Labs where he invented the term ""bit"" in 1947.His statistical interests were many and varied. He is particularly remembered for his development with James Cooley of the Cooley–Tukey FFT algorithm. In 1970, he contributed significantly to what is today known as the jackknife estimation—also termed Quenouille–Tukey jackknife. He introduced the box plot in his 1977 book, ""Exploratory Data Analysis"".
Tukey's range test, the Tukey lambda distribution, Tukey's test of additivity, Tukey's lemma, and the Tukey window all bear his name. He is also the creator of several little-known methods such as the trimean and median-median line, an easier alternative to linear regression.
In 1974, he developed, with Jerome H. Friedman, the concept of the projection pursuit.

Statistical practice
He also contributed to statistical practice and articulated the important distinction between exploratory data analysis and confirmatory data analysis, believing that much statistical methodology placed too great an emphasis on the latter.
Though he believed in the utility of separating the two types of analysis, he pointed out that sometimes, especially in natural science, this was problematic and termed such situations uncomfortable science.
A. D. Gordon offered the following summary of Tukey's principles for statistical practice:
... the usefulness and limitation of mathematical statistics; the importance of having methods of statistical analysis that are robust to violations of the assumptions underlying their use; the need to amass experience of the behaviour of specific methods of analysis in order to provide guidance on their use; the importance of allowing the possibility of data's influencing the choice of method by which they are analysed; the need for statisticians to reject the role of ""guardian of proven truth"", and to resist attempts to provide once-for-all solutions and tidy over-unifications of the subject; the iterative nature of data analysis; implications of the increasing power, availability, and cheapness of computing facilities; the training of statisticians.

Statistical terms
Tukey coined many statistical terms that have become part of common usage, but the two most famous coinages attributed to him were related to computer science.
While working with John von Neumann on early computer designs, Tukey introduced the word ""bit"" as a contraction of ""binary digit"". The term ""bit"" was first used in an article by Claude Shannon in 1948.
In 2000, Fred Shapiro, a librarian at the Yale Law School, published a letter revealing that Tukey's 1958 paper ""The Teaching of Concrete Mathematics"" contained the earliest known usage of the term ""software"" found in a search of JSTOR's electronic archives, predating the OED's citation by two years. This led many to credit Tukey with coining the term, particularly in obituaries published that same year, although Tukey never claimed credit for any such coinage. In 1995, Paul Niquette claimed he had originally coined the term in October 1953, although he could not find any documents supporting his claim. The earliest known publication of the term ""software"" in an engineering context was in August 1953 by Richard R. Carhart, in a RAND Corporation research memorandum.

See also
List of pioneers in computer science

Publications
Andrews, David F.; Bickel, Peter J.; Hampel, Frank R.; Huber, Peter J.; Rogers, W. H.; Tukey, John Wilder (1972). Robust estimates of location: survey and advances. Princeton University Press. ISBN 978-0-691-08113-7. OCLC 369963.
Basford, Kaye E.; Tukey, John Wilder (1998). Graphical Analysis of Multiresponse Data. Chapman & Hall/CRC Press. ISBN 978-0-8493-0384-5. OCLC 154674707.
Blackman, R. B.; Tukey, John Wilder (1959). The measurement of power spectra from the point of view of communications engineering. Dover Publications. ISBN 978-0-486-60507-4.
Cochran, William Gemmell; Mosteller, Charles Frederick; Tukey, John Wilder (1954). Statistical problems of the Kinsey report on sexual behavior in the human male. Journal of the American Statistical Association.
Hoaglin, David C.; Mosteller, Charles Frederick; Tukey, John Wilder, eds. (1983). Understanding Robust and Exploratory Data Analysis. Wiley. ISBN 978-0-471-09777-8. OCLC 8495063.
Hoaglin, David C.; Mosteller, Charles Frederick; Tukey, John Wilder, eds. (1985). Exploring Data Tables, Trends and Shapes. Wiley. ISBN 978-0-471-09776-1. OCLC 11550398.
Hoaglin, David C.; Mosteller, Charles Frederick; Tukey, John Wilder, eds. (1991). Fundamentals of exploratory analysis of variance. Wiley. ISBN 978-0-471-52735-0. OCLC 23180322.
Morgenthaler, Stephan; Tukey, John Wilder, eds. (1991). Configural polysampling: a route to practical robustness. Wiley. ISBN 978-0-471-52372-7. OCLC 22381036.
Mosteller, Charles Frederick; Tukey, John Wilder (1977). Data analysis and regression: a second course in statistics. Addison-Wesley. ISBN 978-0-201-04854-4. OCLC 3235470.
Tukey, John Wilder (1940). Convergence and Uniformity in Topology. Princeton University Press. ISBN 978-0-691-09568-4. OCLC 227948615.
Tukey, John Wilder (1977). Exploratory Data Analysis. Addison-Wesley. ISBN 978-0-201-07616-5. OCLC 3058187.
Tukey, John Wilder; Ross, Ian C.; Bertrand, Verna (1973). Index to statistics and probability. R & D Press. ISBN 978-0-88274-001-0. OCLC 745715.The collected works of John W Tukey, edited by William S. ClevelandBrillinger, David R., ed. (1984). Volume I: Time series, 1949–1964. Wadsworth, Inc. ISBN 978-0-534-03303-3. OCLC 10998116.
Brillinger, David R., ed. (1985). Volume II: Time series, 1965–1984. Wadsworth, Inc. ISBN 978-0-534-03304-0. OCLC 159731367.
Jones, Lyle V., ed. (1985). Volume III: Philosophy and principles of data analysis, 1949–1964. Wadsworth & Brooks/Cole. ISBN 978-0-534-03305-7. OCLC 159731367.
Jones, Lyle V., ed. (1986). Volume IV: Philosophy and principles of data analysis, 1965–1986. Wadsworth & Brooks/Cole. ISBN 978-0-534-05101-3. OCLC 165832503.
Cleveland, William S., ed. (1988). Volume V: Graphics, 1965–1985. Wadsworth & Brooks/Cole. ISBN 978-0-534-05102-0. OCLC 230023465.
Mallows, Colin L., ed. (1990). Volume VI: More mathematical, 1938–1984. Wadsworth & Brooks/Cole. ISBN 978-0-534-05103-7. OCLC 232966724.
Cox, David R., ed. (1992). Volume VII: Factorial and ANOVA, 1949–1962. Wadsworth & Brooks/Cole. ISBN 978-0-534-05104-4. OCLC 165366083.
Braun, Henry I., ed. (1994). Volume VIII: Multiple comparisons, 1949–1983. Chapman & Hall/CRC Press. ISBN 978-0-412-05121-0. OCLC 165099761.About John TukeyO'Connor, John J.; Robertson, Edmund F., ""John Tukey"", MacTutor History of Mathematics archive, University of St Andrews.
Interview of John Tukey about his experience at Princeton

References
External links
Royal Society obit. by Peter McCullagh
John W. Tukey: His Life and Professional Contributions published in The Annals of Statistics
John Wilder Tukey (1915–2000) in Notices of the American Mathematical Society
Memories of John Tukey
Short biography by Mary Bittrich
""John Tukey, 85, Statistician; Coined the Word 'Software'"", The New York Times, 2000-07-28
""Remembering John W. Tukey"", special issue of Statistical Science
John Wilder Tukey at the Mathematics Genealogy Project",https://en.wikipedia.org/wiki/John_Tukey,"['1915 births', '2000 deaths', '20th-century American mathematicians', 'All articles with unsourced statements', 'American statisticians', 'Articles with hCards', 'Articles with short description', 'Articles with unsourced statements from May 2018', 'Brown University alumni', 'Burials at Princeton Cemetery', 'CS1 maint: untitled periodical', 'Exploratory data analysis', 'Fellows of the American Statistical Association', 'Foreign Members of the Royal Society', 'IEEE Medal of Honor recipients', 'Members of the United States National Academy of Sciences', 'National Medal of Science laureates', 'Pages using infobox scientist with unknown parameters', 'People from Massachusetts', 'Presidents of the Institute of Mathematical Statistics', 'Princeton University alumni', 'Princeton University faculty', 'Short description is different from Wikidata', 'Survey methodologists', 'Use dmy dates from August 2019', 'Wikipedia articles with BIBSYS identifiers', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MGP identifiers', 'Wikipedia articles with NKC identifiers', 'Wikipedia articles with NLI identifiers', 'Wikipedia articles with NTA identifiers', 'Wikipedia articles with SELIBR identifiers', 'Wikipedia articles with SNAC-ID identifiers', 'Wikipedia articles with SUDOC identifiers', 'Wikipedia articles with Trove identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers']",Data Science
114,Information visualization,"Information visualization or information visualisation is the study of visual representations of abstract data to reinforce human cognition. The abstract data include both numerical and non-numerical data, such as text and geographic information. It is related to data visualization, infographics, and scientific visualization. One definition is that it's information visualization when the spatial representation (e.g., the page layout of a graphic design) is chosen, whereas it's scientific visualization when the spatial representation is given.

Overview
The field of information visualization has emerged ""from research in human-computer interaction, computer science, graphics, visual design, psychology, and business methods. It is increasingly applied as a critical component in scientific research, digital libraries, data mining, financial data analysis, market studies, manufacturing production control, and drug discovery"".Information visualization presumes that ""visual representations and interaction techniques take advantage of the human eye’s broad bandwidth pathway into the mind to allow users to see, explore, and understand large amounts of information at once. Information visualization focused on the creation of approaches for conveying abstract information in intuitive ways.""Data analysis is an indispensable part of all applied research and problem solving in industry. The most fundamental data analysis approaches are visualization (histograms, scatter plots, surface plots, tree maps, parallel coordinate plots, etc.), statistics (hypothesis test, regression, PCA, etc.), data mining (association mining, etc.), and machine learning methods (clustering, classification, decision trees, etc.). Among these approaches,  information visualization, or visual data analysis, is the most reliant on the cognitive skills of human analysts, and allows the discovery of unstructured actionable insights that are limited only by human imagination and creativity. The analyst does not have to learn any sophisticated methods to be able to interpret the visualizations of the data. Information visualization is also a hypothesis generation scheme, which can be, and is typically followed by more analytical or formal analysis, such as statistical hypothesis testing.

History
The modern study of visualization started with computer graphics, which ""has from its beginning been used to study scientific problems. However, in its early days the lack of graphics power often limited its usefulness. The recent emphasis on visualization started in 1987 with the special issue of Computer Graphics on Visualization in Scientific Computing. Since then there have been several conferences and workshops, co-sponsored by the IEEE Computer Society and ACM SIGGRAPH"". They have been devoted to the general topics of data visualisation, information visualization and scientific visualisation, and more specific areas such as volume visualization.

In 1786, William Playfair published the first presentation graphics.

Techniques
Cartogram
Cladogram (phylogeny)
Concept Mapping
Dendrogram (classification)
Information visualization reference model
Graph drawing
Heatmap
HyperbolicTree
Multidimensional scaling
Parallel coordinates
Problem solving environment
Treemapping

Applications
Information visualization insights are being applied in areas such as:
Scientific research
Digital libraries
Data mining
Information graphics
Financial data analysis
Health care
Market studies
Manufacturing production control
Crime mapping
eGovernance and Policy Modeling

Organization
Notable academic and industry laboratories in the field are:

Adobe Research
IBM Research
Google Research
Microsoft Research
Panopticon Software
Scientific Computing and Imaging Institute
Tableau Software
University of Maryland Human-Computer Interaction Lab
VviConferences in this field, ranked by significance in data visualization research, are:

IEEE Visualization: An annual international conference on scientific visualization, information visualization, and visual analytics. Conference is held in October.
ACM SIGGRAPH: An annual international conference on computer graphics, convened by the ACM SIGGRAPH organization. Conference dates vary.
EuroVis: An annual Europe-wide conference on data visualization, organized by the Eurographics Working Group on Data Visualization and supported by the IEEE Visualization and Graphics Technical Committee (IEEE VGTC). Conference is usually held in June.
Conference on Human Factors in Computing Systems (CHI): An annual international conference on human-computer interaction, hosted by ACM SIGCHI. Conference is usually held in April or May.
Eurographics: An annual Europe-wide computer graphics conference, held by the European Association for Computer Graphics. Conference is usually held in April or May.
PacificVis: An annual visualization symposium held in the Asia-Pacific region, sponsored by the IEEE Visualization and Graphics Technical Committee (IEEE VGTC). Conference is usually held in March or April.For further examples, see: Category:Computer graphics organizations

See also
Computational visualistics
Data art
Data Presentation Architecture
Data visualization
Geovisualization
Infographics
Patent visualisation
Software visualization
Visual analytics
List of information graphics software
List of countries by economic complexity, example of Treemapping

References
Further reading
Ben Bederson and Ben Shneiderman (2003). The Craft of Information Visualization: Readings and Reflections. Morgan Kaufmann.
Stuart K. Card, Jock D. Mackinlay and Ben Shneiderman (1999). Readings in Information Visualization: Using Vision to Think, Morgan Kaufmann Publishers.
Jeffrey Heer, Stuart K. Card, James Landay (2005). ""Prefuse: a toolkit for interactive information visualization"". In: ACM Human Factors in Computing Systems CHI 2005.
Andreas Kerren, John T. Stasko, Jean-Daniel Fekete, and Chris North (2008). Information Visualization – Human-Centered Issues and Perspectives. Volume 4950 of LNCS State-of-the-Art Survey, Springer.
Riccardo Mazza (2009). Introduction to Information Visualization, Springer.
Spence, Robert Information Visualization: Design for Interaction (2nd Edition), Prentice Hall, 2007, ISBN 0-13-206550-9.
Colin Ware (2000). Information Visualization: Perception for design. San Francisco, CA: Morgan Kaufmann.
Kawa Nazemi (2014). Adaptive Semantics Visualization Eurographics Association.

External links
 Media related to Information visualization at Wikimedia Commons
Information Visualization at Curlie",https://en.wikipedia.org/wiki/Information_visualization,"['All articles to be merged', 'Articles to be merged from February 2021', 'Articles with Curlie links', 'Articles with short description', 'Commons category link from Wikidata', 'Computational science', 'Computer graphics', 'Infographics', 'Information visualization', 'Scientific modeling', 'Short description matches Wikidata', 'Webarchive template wayback links', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers']",Data Science
115,Jeff Hammerbacher,"Jeff Hammerbacher is a data scientist as well as formerly chief scientist and cofounder at Cloudera. In addition he was formerly on the faculty of the Icahn School of Medicine at Mount Sinai.

Career
Prior to co-founding Cloudera, Hammerbacher led the data team at Facebook. Hammerbacher was an Entrepreneur in Residence at Accel Partners immediately prior to joining Cloudera. Hammerbacher worked as a quantitative analyst on Wall Street.Hammerbacher has been featured for his work in Forbes, Fast Company, MIT Technology Review, Harvard Business Review, NY Times, Bloomberg BusinessWeek and others.He has been quoted saying, ""The best minds of my generation are thinking about how to make people click ads. That sucks.” about product and engineering talents being employed in masses by companies such as Google and Facebook to work on advertising-related algorithms.

Selected publications
Segaran, Toby; Hammerbacher, Jeff (2009). Beautiful Data: the stories behind elegant data solutions (First ed.). Sebastopol, California: O'Reilly. ISBN 9780596157111. OCLC 827947721.


== References ==",https://en.wikipedia.org/wiki/Jeff_Hammerbacher,"['All articles with a promotional tone', 'All articles with unsourced statements', 'All stub articles', 'American company founders', 'American scientist stubs', 'Articles with a promotional tone from November 2019', 'Articles with unsourced statements from September 2020', 'Articles with wikipuffery', 'Computer scientist stubs', 'Data scientists', 'Harvard University alumni', 'Icahn School of Medicine at Mount Sinai faculty', 'Living people', 'Place of birth missing (living people)', 'Wikipedia articles with DBLP identifiers', 'Wikipedia articles with ORCID identifiers', 'Year of birth missing (living people)']",Data Science
116,Jeffrey T. Leek,"Jeffrey Tullis Leek is an American biostatistician and data scientist working as a Professor at Johns Hopkins Bloomberg School of Public Health.  He is an author of the Simply Statistics blog, and runs several online courses through Coursera, as part of their Data Science Specialization. His most popular course is The Data Scientist's Toolbox., which he instructed along with Roger Peng and Brian Caffo. Leek is best known for his contributions to genomic data analysis and critical view of research and the accuracy of popular statistical methods.

Education
Leek graduated from Utah State University in 2003 with his Bachelors of Science. Then went on to study at the University of Washington achieving a Master's degree in 2005 and completed a PhD in Biostatistics in 2007 under the guidance of Prof. John D. Storey.

Research and career
Leek joined Johns Hopkins University as an assistant professor in Biostatistics in 2009, working at the Bloomberg School of Public Health. In 2014 he became an associate professor in Biostatistics and Oncology.Leek works in The Center for Computational Biology at Johns Hopkins University creating statistical packages for analysis of genomes.
He also co-edits a blog, Simply Statistics with Roger Peng and Rafa Irizarry, which contains a mix of articles on statistics and meta-research.Leek has conducted several talks at prestigious universities and locations such as a colloquium series at Harvard and a lecture at the New York Genome Center titled “Building a Comprehensive Resource for the Study of Human Gene Expression with Machine Learning and Data Science” as a part of their lecture series.
He is an expert in reproducibility, and his work and opinions have been published in notable scientific and medical journals such as Nature and the Proceedings of the National Academy of Sciences. Leek wrote a self-published book, The Elements of Data Analytic Style and is considered an expert on replication.

Recognition
Leek was elected as a Fellow of the American Statistical Association in 2020.

Selected publications
Leek's highly cited works include 

""Capturing Heterogeneity in Gene Expression Studies by Surrogate Variable Analysis""
""Tackling the Widespread and Critical Impact of Batch Effects in High-Throughput Data""


== References ==",https://en.wikipedia.org/wiki/Jeffrey_T._Leek,"['1979 births', 'American statisticians', 'Articles with hCards', 'Biostatisticians', 'Data scientists', 'Fellows of the American Statistical Association', 'Johns Hopkins Bloomberg School of Public Health faculty', 'Living people', 'University of Washington alumni', 'Utah State University alumni', 'Wikipedia articles with DBLP identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MGP identifiers', 'Wikipedia articles with ORCID identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers']",Data Science
117,Journal of Machine Learning Research,"The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling. The current editors-in-chief are Francis Bach (Inria), David Blei (Columbia University) and Bernhard Schölkopf (Max Planck Institute for Intelligent Systems).

History
The journal was established as an open-access alternative to the journal Machine Learning. In 2001, forty editorial board members of Machine Learning resigned, saying that in the era of the Internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. The open access model employed by the Journal of Machine Learning Research allows authors to publish articles for free and retain copyright, while archives are freely available online.Print editions of the journal were published by MIT Press until 2004 and by Microtome Publishing thereafter. From its inception, the journal received no revenue from the print edition and paid no subvention to MIT Press or Microtome Publishing.In response to the prohibitive costs of arranging workshop and conference proceedings publication with traditional academic publishing companies, the journal launched a proceedings publication arm in 2007 and now publishes proceedings for several leading machine learning conferences, including the International Conference on Machine Learning, COLT, AISTATS, and  workshops held at the Conference on Neural Information Processing Systems.

Further reading
""Top journals in computer science"". Times Higher Education. 14 May 2009. Retrieved 22 August 2009.

References
External links
Official website",https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research,"['All stub articles', 'Articles with outdated impact factors from 2018', 'Articles with short description', 'Computer science journal stubs', 'Computer science journals', 'Machine learning', 'Official website different in Wikidata and Wikipedia', 'Open access journals', 'Publications established in 2000', 'Short description is different from Wikidata']",Data Science
118,K-nearest neighbors algorithm,"In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric classification method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in data set. The output depends on whether k-NN is used for classification or regression:

In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.
A peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data.

Statistical setting
Suppose we have pairs 
  
    
      
        (
        
          X
          
            1
          
        
        ,
        
          Y
          
            1
          
        
        )
        ,
        (
        
          X
          
            2
          
        
        ,
        
          Y
          
            2
          
        
        )
        ,
        …
        ,
        (
        
          X
          
            n
          
        
        ,
        
          Y
          
            n
          
        
        )
      
    
    {\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\dots ,(X_{n},Y_{n})}
   taking values in 
  
    
      
        
          
            R
          
          
            d
          
        
        ×
        {
        1
        ,
        2
        }
      
    
    {\displaystyle \mathbb {R} ^{d}\times \{1,2\}}
  , where Y is the class label of X, so that 
  
    
      
        X
        
          |
        
        Y
        =
        r
        ∼
        
          P
          
            r
          
        
      
    
    {\displaystyle X|Y=r\sim P_{r}}
   for 
  
    
      
        r
        =
        1
        ,
        2
      
    
    {\displaystyle r=1,2}
   (and probability distributions 
  
    
      
        
          P
          
            r
          
        
      
    
    {\displaystyle P_{r}}
  ). Given some norm 
  
    
      
        ‖
        ⋅
        ‖
      
    
    {\displaystyle \|\cdot \|}
   on 
  
    
      
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle \mathbb {R} ^{d}}
   and a point 
  
    
      
        x
        ∈
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle x\in \mathbb {R} ^{d}}
  , let 
  
    
      
        (
        
          X
          
            (
            1
            )
          
        
        ,
        
          Y
          
            (
            1
            )
          
        
        )
        ,
        …
        ,
        (
        
          X
          
            (
            n
            )
          
        
        ,
        
          Y
          
            (
            n
            )
          
        
        )
      
    
    {\displaystyle (X_{(1)},Y_{(1)}),\dots ,(X_{(n)},Y_{(n)})}
   be a reordering of the training data such that 
  
    
      
        ‖
        
          X
          
            (
            1
            )
          
        
        −
        x
        ‖
        ≤
        ⋯
        ≤
        ‖
        
          X
          
            (
            n
            )
          
        
        −
        x
        ‖
      
    
    {\displaystyle \|X_{(1)}-x\|\leq \dots \leq \|X_{(n)}-x\|}
  .

Algorithm
The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.
In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.
A commonly used distance metric for continuous variables is Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the overlap metric (or Hamming distance). In the context of gene expression microarray data, for example, k-NN has been employed with correlation coefficients, such as Pearson and Spearman, as a metric. Often, the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis.
A drawback of the basic ""majority voting"" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the k nearest neighbors due to their large number. One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its k nearest neighbors. The class (or value, in regression problems) of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another way to overcome skew is by abstraction in data representation. For example, in a self-organizing map (SOM), each node is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. K-NN can then be applied to the SOM.

Parameter selection
The best choice of k depends upon the data; generally, larger values of k reduces effect of the noise on the classification, but make boundaries between classes less distinct. A good k can be selected by various heuristic techniques (see hyperparameter optimization). The special case where the class is predicted to be the class of the closest training sample (i.e. when k = 1) is called the nearest neighbor algorithm.
The accuracy of the k-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance. Much research effort has been put into selecting or scaling features to improve classification. A particularly popular approach is the use of evolutionary algorithms to optimize feature scaling. Another popular approach is to scale features by the mutual information of the training data with the training classes.In binary (two class) classification problems, it is helpful to choose k to be an odd number as this avoids tied votes. One popular way of choosing the empirically optimal k in this setting is via bootstrap method.

The 1-nearest neighbor classifier
The most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point x to the class of its closest neighbour in the feature space, that is 
  
    
      
        
          C
          
            n
          
          
            1
            n
            n
          
        
        (
        x
        )
        =
        
          Y
          
            (
            1
            )
          
        
      
    
    {\displaystyle C_{n}^{1nn}(x)=Y_{(1)}}
  .
As the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data).

The weighted nearest neighbour classifier
The k-nearest neighbour classifier can be viewed as assigning the k nearest neighbours a weight 
  
    
      
        1
        
          /
        
        k
      
    
    {\displaystyle 1/k}
   and all others 0 weight. This can be generalised to weighted nearest neighbour classifiers. That is, where the ith nearest neighbour is assigned a weight 
  
    
      
        
          w
          
            n
            i
          
        
      
    
    {\displaystyle w_{ni}}
  , with 
  
    
      
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          w
          
            n
            i
          
        
        =
        1
      
    
    {\displaystyle \sum _{i=1}^{n}w_{ni}=1}
  . An analogous result on the strong consistency of weighted nearest neighbour classifiers also holds.Let 
  
    
      
        
          C
          
            n
          
          
            w
            n
            n
          
        
      
    
    {\displaystyle C_{n}^{wnn}}
   denote the weighted nearest classifier with weights 
  
    
      
        {
        
          w
          
            n
            i
          
        
        
          }
          
            i
            =
            1
          
          
            n
          
        
      
    
    {\displaystyle \{w_{ni}\}_{i=1}^{n}}
  . Subject to regularity conditions on the class distributions the excess risk has the following asymptotic expansion

  
    
      
        
          
            
              R
            
          
          
            
              R
            
          
        
        (
        
          C
          
            n
          
          
            w
            n
            n
          
        
        )
        −
        
          
            
              R
            
          
          
            
              R
            
          
        
        (
        
          C
          
            B
            a
            y
            e
            s
          
        
        )
        =
        
          (
          
            
              B
              
                1
              
            
            
              s
              
                n
              
              
                2
              
            
            +
            
              B
              
                2
              
            
            
              t
              
                n
              
              
                2
              
            
          
          )
        
        {
        1
        +
        o
        (
        1
        )
        }
        ,
      
    
    {\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{wnn})-{\mathcal {R}}_{\mathcal {R}}(C^{Bayes})=\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\right)\{1+o(1)\},}
  for constants 
  
    
      
        
          B
          
            1
          
        
      
    
    {\displaystyle B_{1}}
   and 
  
    
      
        
          B
          
            2
          
        
      
    
    {\displaystyle B_{2}}
   where 
  
    
      
        
          s
          
            n
          
          
            2
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          w
          
            n
            i
          
          
            2
          
        
      
    
    {\displaystyle s_{n}^{2}=\sum _{i=1}^{n}w_{ni}^{2}}
   and 
  
    
      
        
          t
          
            n
          
        
        =
        
          n
          
            −
            2
            
              /
            
            d
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          w
          
            n
            i
          
        
        {
        
          i
          
            1
            +
            2
            
              /
            
            d
          
        
        −
        (
        i
        −
        1
        
          )
          
            1
            +
            2
            
              /
            
            d
          
        
        }
      
    
    {\displaystyle t_{n}=n^{-2/d}\sum _{i=1}^{n}w_{ni}\{i^{1+2/d}-(i-1)^{1+2/d}\}}
  .
The optimal weighting scheme 
  
    
      
        {
        
          w
          
            n
            i
          
          
            ∗
          
        
        
          }
          
            i
            =
            1
          
          
            n
          
        
      
    
    {\displaystyle \{w_{ni}^{*}\}_{i=1}^{n}}
  , that balances the two terms in the display above, is given as follows: set 
  
    
      
        
          k
          
            ∗
          
        
        =
        ⌊
        B
        
          n
          
            
              4
              
                d
                +
                4
              
            
          
        
        ⌋
      
    
    {\displaystyle k^{*}=\lfloor Bn^{\frac {4}{d+4}}\rfloor }
  , 

  
    
      
        
          w
          
            n
            i
          
          
            ∗
          
        
        =
        
          
            1
            
              k
              
                ∗
              
            
          
        
        
          [
          
            1
            +
            
              
                d
                2
              
            
            −
            
              
                d
                
                  2
                  
                    
                      
                        k
                        
                          ∗
                        
                      
                    
                    
                      2
                      
                        /
                      
                      d
                    
                  
                
              
            
            {
            
              i
              
                1
                +
                2
                
                  /
                
                d
              
            
            −
            (
            i
            −
            1
            
              )
              
                1
                +
                2
                
                  /
                
                d
              
            
            }
          
          ]
        
      
    
    {\displaystyle w_{ni}^{*}={\frac {1}{k^{*}}}\left[1+{\frac {d}{2}}-{\frac {d}{2{k^{*}}^{2/d}}}\{i^{1+2/d}-(i-1)^{1+2/d}\}\right]}
   for 
  
    
      
        i
        =
        1
        ,
        2
        ,
        …
        ,
        
          k
          
            ∗
          
        
      
    
    {\displaystyle i=1,2,\dots ,k^{*}}
   and

  
    
      
        
          w
          
            n
            i
          
          
            ∗
          
        
        =
        0
      
    
    {\displaystyle w_{ni}^{*}=0}
   for 
  
    
      
        i
        =
        
          k
          
            ∗
          
        
        +
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=k^{*}+1,\dots ,n}
  .With optimal weights the dominant term in the asymptotic expansion of the excess risk is 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            −
            
              
                4
                
                  d
                  +
                  4
                
              
            
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{-{\frac {4}{d+4}}})}
  . Similar results are true when using a bagged nearest neighbour classifier.

Properties
k-NN is a special case of a variable-bandwidth, kernel density ""balloon"" estimator with a uniform kernel.The naive version of the algorithm is easy to implement by computing the distances from the test example to all stored examples, but it is computationally intensive for large training sets. Using an approximate nearest neighbor search algorithm makes k-NN computationally tractable even for large data sets. Many nearest neighbor search algorithms have been proposed over the years; these generally seek to reduce the number of distance evaluations actually performed.
k-NN has some strong consistency results. As the amount of data approaches infinity, the two-class k-NN algorithm is guaranteed to yield an error rate no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data). Various improvements to the k-NN speed are possible by using proximity graphs.For multi-class k-NN classification, Cover and Hart (1967) prove an upper bound error rate of

  
    
      
        
          R
          
            ∗
          
        
         
        ≤
         
        
          R
          
            k
            
              N
              N
            
          
        
         
        ≤
         
        
          R
          
            ∗
          
        
        
          (
          
            2
            −
            
              
                
                  M
                  
                    R
                    
                      ∗
                    
                  
                
                
                  M
                  −
                  1
                
              
            
          
          )
        
      
    
    {\displaystyle R^{*}\ \leq \ R_{k\mathrm {NN} }\ \leq \ R^{*}\left(2-{\frac {MR^{*}}{M-1}}\right)}
  where 
  
    
      
        
          R
          
            ∗
          
        
      
    
    {\displaystyle R^{*}}
  is the Bayes error rate (which is the minimal error rate possible),  
  
    
      
        
          R
          
            k
            N
            N
          
        
      
    
    {\displaystyle R_{kNN}}
   is the k-NN error rate, and M is the number of classes in the problem.  For 
  
    
      
        M
        =
        2
      
    
    {\displaystyle M=2}
   and as the Bayesian error rate 
  
    
      
        
          R
          
            ∗
          
        
      
    
    {\displaystyle R^{*}}
   approaches zero, this limit reduces to ""not more than twice the Bayesian error rate"".

Error rates
There are many results on the error rate of the k nearest neighbour classifiers.  The k-nearest neighbour classifier is strongly (that is for any joint distribution on 
  
    
      
        (
        X
        ,
        Y
        )
      
    
    {\displaystyle (X,Y)}
  ) consistent provided 
  
    
      
        k
        :=
        
          k
          
            n
          
        
      
    
    {\displaystyle k:=k_{n}}
   diverges and 
  
    
      
        
          k
          
            n
          
        
        
          /
        
        n
      
    
    {\displaystyle k_{n}/n}
   converges to zero as 
  
    
      
        n
        →
        ∞
      
    
    {\displaystyle n\to \infty }
  .
Let 
  
    
      
        
          C
          
            n
          
          
            k
            n
            n
          
        
      
    
    {\displaystyle C_{n}^{knn}}
   denote the k nearest neighbour classifier based on a training set of size n. Under certain regularity conditions, the excess risk yields the following asymptotic expansion

  
    
      
        
          
            
              R
            
          
          
            
              R
            
          
        
        (
        
          C
          
            n
          
          
            k
            n
            n
          
        
        )
        −
        
          
            
              R
            
          
          
            
              R
            
          
        
        (
        
          C
          
            B
            a
            y
            e
            s
          
        
        )
        =
        
          {
          
            
              B
              
                1
              
            
            
              
                1
                k
              
            
            +
            
              B
              
                2
              
            
            
              
                (
                
                  
                    k
                    n
                  
                
                )
              
              
                4
                
                  /
                
                d
              
            
          
          }
        
        {
        1
        +
        o
        (
        1
        )
        }
        ,
      
    
    {\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{knn})-{\mathcal {R}}_{\mathcal {R}}(C^{Bayes})=\left\{B_{1}{\frac {1}{k}}+B_{2}\left({\frac {k}{n}}\right)^{4/d}\right\}\{1+o(1)\},}
  for some constants 
  
    
      
        
          B
          
            1
          
        
      
    
    {\displaystyle B_{1}}
   and 
  
    
      
        
          B
          
            2
          
        
      
    
    {\displaystyle B_{2}}
  .
The choice 
  
    
      
        
          k
          
            ∗
          
        
        =
        ⌊
        B
        
          n
          
            
              4
              
                d
                +
                4
              
            
          
        
        ⌋
      
    
    {\displaystyle k^{*}=\lfloor Bn^{\frac {4}{d+4}}\rfloor }
   offers a trade off between the two terms in the above display, for which the 
  
    
      
        
          k
          
            ∗
          
        
      
    
    {\displaystyle k^{*}}
  -nearest neighbour error converges to the Bayes error at the optimal (minimax) rate 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            −
            
              
                4
                
                  d
                  +
                  4
                
              
            
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{-{\frac {4}{d+4}}})}
  .

Metric learning
The K-nearest neighbor classification performance can often be significantly improved through (supervised) metric learning. Popular algorithms are neighbourhood components analysis and large margin nearest neighbor. Supervised metric learning algorithms use the label information to learn a new metric or pseudo-metric.

Feature extraction
When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters) then the input data will be transformed into a reduced representation set of features (also named features vector). Transforming the input data into the set of features is called feature extraction. If the features extracted are carefully chosen it is expected that the features set will extract the relevant information from the input data in order to perform the desired task using this reduced representation instead of the full size input. Feature extraction is performed on raw data prior to applying k-NN algorithm on the transformed data in feature space.
An example of a typical computer vision computation pipeline for face recognition using k-NN including feature extraction and dimension reduction pre-processing steps (usually implemented with OpenCV):

Haar face detection
Mean-shift tracking analysis
PCA or Fisher LDA projection into feature space, followed by k-NN classification

Dimension reduction
For high-dimensional data (e.g., with number of dimensions more than 10) dimension reduction is usually performed prior to applying the k-NN algorithm in order to avoid the effects of the curse of dimensionality.
The curse of dimensionality in the k-NN context basically means that Euclidean distance is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector (imagine multiple points lying more or less on a circle with the query point at the center; the distance from the query to all data points in the search space is almost the same).
Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA),  linear discriminant analysis (LDA), or canonical correlation analysis (CCA) techniques as a pre-processing step, followed by clustering by k-NN on feature vectors in reduced-dimension space. This process is also called low-dimensional embedding.For very-high-dimensional datasets (e.g. when performing a similarity search on live video streams, DNA data or high-dimensional time series) running a fast approximate k-NN search using locality sensitive hashing, ""random projections"", ""sketches""  or other high-dimensional similarity search techniques from the VLDB toolbox might be the only feasible option.

Decision boundary
Nearest neighbor rules in effect implicitly compute the decision boundary. It is also possible to compute the decision boundary explicitly, and to do so efficiently, so that the computational complexity is a function of the boundary complexity.

Data reduction
Data reduction is one of the most important problems for work with huge data sets. Usually, only some of the data points are needed for accurate classification. Those data are called the prototypes and can be found as follows:

Select the class-outliers, that is, training data that are classified incorrectly by k-NN (for a given k)
Separate the rest of the data into two sets: (i) the prototypes that are used for the classification decisions and (ii) the absorbed points that can be correctly classified by k-NN using prototypes. The absorbed points can then be removed from the training set.

Selection of class-outliers
A training example surrounded by examples of other classes is called a class outlier. Causes of class outliers include:

random error
insufficient training examples of this class (an isolated example appears instead of a cluster)
missing important features (the classes are separated in other dimensions which we do not know)
too many training examples of other classes (unbalanced classes) that create a ""hostile"" background for the given small classClass outliers with k-NN produce noise. They can be detected and separated for future analysis. Given two natural numbers, k>r>0, a training example is called a (k,r)NN class-outlier if its k nearest neighbors include more than r examples of other classes.

CNN for data reduction
Condensed nearest neighbor (CNN, the Hart algorithm) is an algorithm designed to reduce the data set for k-NN classification. It selects the set of prototypes U from the training data, such that 1NN with U can classify the examples almost as accurately as 1NN does with the whole data set.

Given a training set X, CNN works iteratively:

Scan all elements of X, looking for an element x whose nearest prototype from U has a different label than x.
Remove x from X and add it to U
Repeat the scan until no more prototypes are added to U.Use U instead of X for classification. The examples that are not prototypes are called ""absorbed"" points.
It is efficient to scan the training examples in order of decreasing border ratio. The border ratio of a training example x is defined as 

a(x) = ||x'-y||/||x-y||where ||x-y|| is the distance to the closest example y having a different color than x, and ||x'-y|| is the distance from y to its closest example x'  with the same label as x.
The border ratio is in the interval [0,1] because ||x'-y||never exceeds ||x-y||. This ordering gives preference to the borders of the classes for inclusion in the set of prototypes U. A point of a different label than x is called external to x. The calculation of the border ratio is illustrated by the figure on the right. The data points are labeled by colors: the initial point is x and its label is red. External points are blue and green. The closest to x external point is y. The closest to y red point is x' . The border ratio a(x) = ||x'-y|| / ||x-y||is the attribute of the initial point x.
Below is an illustration of CNN in a series of figures. There are three classes (red, green and blue). Fig. 1: initially there are 60 points in each class. Fig. 2 shows the 1NN classification map: each pixel is classified by 1NN using all the data. Fig. 3 shows the 5NN classification map. White areas correspond to the unclassified regions, where 5NN voting is tied (for example, if there are two green, two red and one blue points among 5 nearest neighbors). Fig. 4 shows the reduced data set. The crosses are the class-outliers selected by the (3,2)NN rule (all the three nearest neighbors of these instances belong to other classes); the squares are the prototypes, and the empty circles are the absorbed points. The left bottom corner shows the numbers of the class-outliers, prototypes and absorbed points for all three classes. The number of prototypes varies from 15% to 20% for different classes in this example. Fig. 5 shows that the 1NN classification map with the prototypes is very similar to that with the initial data set. The figures were produced using the Mirkes applet.
CNN model reduction for k-NN classifiers

k-NN regression
In k-NN regression, the k-NN algorithm is used for estimating continuous variables. One such algorithm uses a weighted average of the k nearest neighbors, weighted by the inverse of their distance. This algorithm works as follows:

Compute the Euclidean or Mahalanobis distance from the query example to the labeled examples.
Order the labeled examples by increasing distance.
Find a heuristically optimal number k of nearest neighbors, based on RMSE. This is done using cross validation.
Calculate an inverse distance weighted average with the k-nearest multivariate neighbors.

k-NN outlier
The distance to the kth nearest neighbor can also be seen as a local density estimate and thus is also a popular outlier score in anomaly detection. The larger the distance to the k-NN, the lower the local density, the more likely the query point is an outlier. Although quite simple, this outlier model, along with another classic data mining method, local outlier factor, works quite well also in comparison to more recent and more complex approaches, according to a large scale experimental analysis.

Validation of results
A confusion matrix or ""matching matrix"" is often used as a tool to validate the accuracy of k-NN classification. More robust statistical methods such as likelihood-ratio test can also be applied.

See also
Nearest centroid classifier
Closest pair of points problem

References
Further reading
Dasarathy, Belur V., ed. (1991). Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. ISBN 978-0-8186-8930-7.
Shakhnarovich, Gregory; Darrell, Trevor; Indyk, Piotr, eds. (2005). Nearest-Neighbor Methods in Learning and Vision. MIT Press. ISBN 978-0-262-19547-8.",https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm,"['All articles with unsourced statements', 'Articles with unsourced statements from December 2008', 'Articles with unsourced statements from March 2013', 'Articles with unsourced statements from September 2019', 'Classification algorithms', 'Machine learning algorithms', 'Nonparametric statistics', 'Search algorithms', 'Statistical classification', 'Wikipedia articles needing clarification from January 2019', 'Wikipedia articles needing clarification from July 2020']",Data Science
119,K-means clustering,"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.
The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.
The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.

Description
Given a set of observations (x1, x2, ..., xn), where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into k (≤ n) sets S = {S1, S2, ..., Sk} so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:
  
    
      
        
          
            
              a
              r
              g
              
              m
              i
              n
            
            
              S
            
          
        
        
          ∑
          
            i
            =
            1
          
          
            k
          
        
        
          ∑
          
            
              x
            
            ∈
            
              S
              
                i
              
            
          
        
        
          
            ‖
            
              
                x
              
              −
              
                
                  μ
                
                
                  i
                
              
            
            ‖
          
          
            2
          
        
        =
        
          
            
              a
              r
              g
              
              m
              i
              n
            
            
              S
            
          
        
        
          ∑
          
            i
            =
            1
          
          
            k
          
        
        
          |
        
        
          S
          
            i
          
        
        
          |
        
        Var
        ⁡
        
          S
          
            i
          
        
      
    
    {\displaystyle {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}={\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}|S_{i}|\operatorname {Var} S_{i}}
  where μi is the mean of points in Si. This is equivalent to minimizing the pairwise squared deviations of points in the same cluster:
  
    
      
        
          
            
              a
              r
              g
              
              m
              i
              n
            
            
              S
            
          
        
        
          ∑
          
            i
            =
            1
          
          
            k
          
        
        
        
          
            1
            
              2
              
                |
              
              
                S
                
                  i
                
              
              
                |
              
            
          
        
        
        
          ∑
          
            
              x
            
            ,
            
              y
            
            ∈
            
              S
              
                i
              
            
          
        
        
          
            ‖
            
              
                x
              
              −
              
                y
              
            
            ‖
          
          
            2
          
        
      
    
    {\displaystyle {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\,{\frac {1}{2|S_{i}|}}\,\sum _{\mathbf {x} ,\mathbf {y} \in S_{i}}\left\|\mathbf {x} -\mathbf {y} \right\|^{2}}
  The equivalence can be deduced from identity 
  
    
      
        
          ∑
          
            
              x
            
            ∈
            
              S
              
                i
              
            
          
        
        
          
            ‖
            
              
                x
              
              −
              
                
                  μ
                
                
                  i
                
              
            
            ‖
          
          
            2
          
        
        =
        
          ∑
          
            
              x
            
            ≠
            
              y
            
            ∈
            
              S
              
                i
              
            
          
        
        (
        
          x
        
        −
        
          
            μ
          
          
            i
          
        
        
          )
          
            T
          
        
        (
        
          
            μ
          
          
            i
          
        
        −
        
          y
        
        )
      
    
    {\displaystyle \sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}=\sum _{\mathbf {x} \neq \mathbf {y} \in S_{i}}(\mathbf {x} -{\boldsymbol {\mu }}_{i})^{T}({\boldsymbol {\mu }}_{i}-\mathbf {y} )}
  . Because the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in different clusters (between-cluster sum of squares, BCSS), which follows from the law of total variance.

History
The term ""k-means"" was first used by James MacQueen in 1967, though the idea goes back to Hugo Steinhaus in 1956. The standard algorithm was first proposed by Stuart Lloyd of Bell Labs in 1957 as a technique for pulse-code modulation, although it was not published as a journal article until 1982. In 1965, Edward W. Forgy published essentially the same method, which is why it is sometimes referred to as the Lloyd–Forgy algorithm.

Algorithms
Standard algorithm (naive k-means)
The most common algorithm uses an iterative refinement technique. Due to its ubiquity, it is often called ""the k-means algorithm""; it is also referred to as Lloyd's algorithm, particularly in the computer science community. It is sometimes also referred to as ""naïve k-means"", because there exist much faster alternatives.Given an initial set of k means m1(1),...,mk(1) (see below), the algorithm proceeds by alternating between two steps:
Assignment step: Assign each observation to the cluster with the nearest mean: that with the least squared Euclidean distance. (Mathematically, this means partitioning the observations according to the Voronoi diagram generated by the means.)

  
    
      
        
          S
          
            i
          
          
            (
            t
            )
          
        
        =
        
          {
          
            
              x
              
                p
              
            
            :
            
              
                ‖
                
                  
                    x
                    
                      p
                    
                  
                  −
                  
                    m
                    
                      i
                    
                    
                      (
                      t
                      )
                    
                  
                
                ‖
              
              
                2
              
            
            ≤
            
              
                ‖
                
                  
                    x
                    
                      p
                    
                  
                  −
                  
                    m
                    
                      j
                    
                    
                      (
                      t
                      )
                    
                  
                
                ‖
              
              
                2
              
            
             
            ∀
            j
            ,
            1
            ≤
            j
            ≤
            k
          
          }
        
        ,
      
    
    {\displaystyle S_{i}^{(t)}=\left\{x_{p}:\left\|x_{p}-m_{i}^{(t)}\right\|^{2}\leq \left\|x_{p}-m_{j}^{(t)}\right\|^{2}\ \forall j,1\leq j\leq k\right\},}
  
where each 
  
    
      
        
          x
          
            p
          
        
      
    
    {\displaystyle x_{p}}
   is assigned to exactly one 
  
    
      
        
          S
          
            (
            t
            )
          
        
      
    
    {\displaystyle S^{(t)}}
  , even if it could be assigned to two or more of them.
Update step: Recalculate means (centroids) for observations assigned to each cluster.

  
    
      
        
          m
          
            i
          
          
            (
            t
            +
            1
            )
          
        
        =
        
          
            1
            
              |
              
                S
                
                  i
                
                
                  (
                  t
                  )
                
              
              |
            
          
        
        
          ∑
          
            
              x
              
                j
              
            
            ∈
            
              S
              
                i
              
              
                (
                t
                )
              
            
          
        
        
          x
          
            j
          
        
      
    
    {\displaystyle m_{i}^{(t+1)}={\frac {1}{\left|S_{i}^{(t)}\right|}}\sum _{x_{j}\in S_{i}^{(t)}}x_{j}}
  The algorithm has converged when the assignments no longer change. The algorithm is not guaranteed to find the optimum.The algorithm is often presented as assigning objects to the nearest cluster by distance. Using a different distance function other than (squared) Euclidean distance may prevent the algorithm from converging. Various modifications of k-means such as spherical k-means and k-medoids have been proposed to allow using other distance measures.

Initialization methods
Commonly used initialization methods are Forgy and Random Partition. The Forgy method randomly chooses k observations from the dataset and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster's randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al., the Random Partition method is generally preferable for algorithms such as the k-harmonic means and fuzzy k-means. For expectation maximization and standard k-means algorithms, the Forgy method of initialization is preferable. A comprehensive study by Celebi et al., however, found that popular initialization methods such as Forgy, Random Partition, and Maximin often perform poorly, whereas Bradley and Fayyad's approach performs ""consistently"" in ""the best group"" and k-means++ performs ""generally well"".

Demonstration of the standard algorithm
		
		
		
		
The algorithm does not guarantee convergence to the global optimum. The result may depend on the initial clusters. As the algorithm is usually fast, it is common to run it multiple times with different starting conditions. However, worst-case performance can be slow: in particular certain point sets, even in two dimensions, converge in exponential time, that is 2Ω(n). These point sets do not seem to arise in practice: this is corroborated by the fact that the smoothed running time of k-means is polynomial.The ""assignment"" step is referred to as the ""expectation step"", while the ""update step"" is a maximization step, making this algorithm a variant of the generalized expectation-maximization algorithm.

Complexity
Finding the optimal solution to the k-means clustering problem for observations in d dimensions is:

NP-hard in general Euclidean space (of d dimensions) even for two clusters,
NP-hard for a general number of clusters k even in the plane,
if k and d (the dimension) are fixed, the problem can be exactly solved in time 
  
    
      
        O
        (
        
          n
          
            d
            k
            +
            1
          
        
        )
      
    
    {\displaystyle O(n^{dk+1})}
  , where n is the number of entities to be clustered.Thus, a variety of heuristic algorithms such as Lloyd's algorithm given above are generally used.
The running time of Lloyd's algorithm (and most variants) is 
  
    
      
        O
        (
        n
        k
        d
        i
        )
      
    
    {\displaystyle O(nkdi)}
  , where:

n is the number of d-dimensional vectors (to be clustered)
k the number of clusters
i the number of iterations needed until convergence.On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyd's algorithm is therefore often considered to be of ""linear"" complexity in practice, although it is in the worst case superpolynomial when performed until convergence.
In the worst-case, Lloyd's algorithm needs 
  
    
      
        i
        =
        
          2
          
            Ω
            (
            
              
                n
              
            
            )
          
        
      
    
    {\displaystyle i=2^{\Omega ({\sqrt {n}})}}
   iterations, so that the worst-case complexity of Lloyd's algorithm is superpolynomial.
Lloyd's k-means algorithm has polynomial smoothed running time. It is shown that for arbitrary set of n points in 
  
    
      
        [
        0
        ,
        1
        
          ]
          
            d
          
        
      
    
    {\displaystyle [0,1]^{d}}
  , if each point is independently perturbed by a normal distribution with mean 0 and variance 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\displaystyle \sigma ^{2}}
  , then the expected running time of k-means algorithm is bounded by 
  
    
      
        O
        (
        
          n
          
            34
          
        
        
          k
          
            34
          
        
        
          d
          
            8
          
        
        
          log
          
            4
          
        
        ⁡
        (
        n
        )
        
          /
        
        
          σ
          
            6
          
        
        )
      
    
    {\displaystyle O(n^{34}k^{34}d^{8}\log ^{4}(n)/\sigma ^{6})}
  , which is a polynomial in n, k, d and 
  
    
      
        1
        
          /
        
        σ
      
    
    {\displaystyle 1/\sigma }
  .
Better bounds are proven for simple cases. For example, it is shown that the running time of k-means algorithm is bounded by 
  
    
      
        O
        (
        d
        
          n
          
            4
          
        
        
          M
          
            2
          
        
        )
      
    
    {\displaystyle O(dn^{4}M^{2})}
   for n points in an integer lattice 
  
    
      
        {
        1
        ,
        …
        ,
        M
        
          }
          
            d
          
        
      
    
    {\displaystyle \{1,\dots ,M\}^{d}}
  .Lloyd's algorithm is the standard approach for this problem. However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naïve implementation very inefficient. Some implementations use caching and the triangle inequality in order to create bounds and accelerate Lloyd's algorithm.

Variations
Jenks natural breaks optimization: k-means applied to univariate data
k-medians clustering uses the median in each dimension instead of the mean, and this way minimizes 
  
    
      
        
          L
          
            1
          
        
      
    
    {\displaystyle L_{1}}
   norm (Taxicab geometry).
k-medoids (also: Partitioning Around Medoids, PAM) uses the medoid instead of the mean, and this way minimizes the sum of distances for arbitrary distance functions.
Fuzzy C-Means Clustering is a soft version of k-means, where each data point has a fuzzy degree of belonging to each cluster.
Gaussian mixture models trained with expectation-maximization algorithm (EM algorithm) maintains probabilistic assignments to clusters, instead of deterministic assignments, and multivariate Gaussian distributions instead of means.
k-means++ chooses initial centers in a way that gives a provable upper bound on the WCSS objective.
The filtering algorithm uses kd-trees to speed up each k-means step.
Some methods attempt to speed up each k-means step using the triangle inequality.
Escape local optima by swapping points between clusters.
The Spherical k-means clustering algorithm is suitable for textual data.
Hierarchical variants such as Bisecting k-means, X-means clustering and G-means clustering repeatedly split clusters to build a hierarchy, and can also try to automatically determine the optimal number of clusters in a dataset.
Internal cluster evaluation measures such as cluster silhouette can be helpful at determining the number of clusters.
Minkowski weighted k-means automatically calculates cluster specific feature weights, supporting the intuitive idea that a feature may have different degrees of relevance at different features. These weights can also be used to re-scale a given data set, increasing the likelihood of a cluster validity index to be optimized at the expected number of clusters.
Mini-batch k-means: k-means variation using ""mini batch"" samples for data sets that do not fit into memory.

Hartigan–Wong method
Hartigan and Wong's method provides a variation of k-means algorithm which progresses towards a local minimum of the minimum sum-of-squares problem with different solution updates. The method is a local search that iteratively attempts to relocate a sample into a different cluster as long as this process improves the objective function. When no sample can be relocated into a different cluster with an improvement of the objective, the method stops (in a local minimum). In a similar way as the classical k-means, the approach remains a heuristic since it does not necessarily guarantee that the final solution is globally optimum.
Let 
  
    
      
        φ
        (
        
          S
          
            j
          
        
        )
      
    
    {\displaystyle \varphi (S_{j})}
   be the individual cost of 
  
    
      
        
          S
          
            j
          
        
      
    
    {\displaystyle S_{j}}
   defined by 
  
    
      
        
          ∑
          
            x
            ∈
            
              S
              
                j
              
            
          
        
        (
        x
        −
        
          μ
          
            j
          
        
        
          )
          
            2
          
        
      
    
    {\displaystyle \sum _{x\in S_{j}}(x-\mu _{j})^{2}}
  , with 
  
    
      
        
          μ
          
            j
          
        
      
    
    {\displaystyle \mu _{j}}
   the center of the cluster.
Assignment step: Hartigan and Wong's method starts by partitioning the points into random clusters 
  
    
      
        {
        
          S
          
            j
          
        
        
          }
          
            j
            ∈
            {
            1
            ,
            ⋯
            k
            }
          
        
      
    
    {\displaystyle \{S_{j}\}_{j\in \{1,\cdots k\}}}
  .
Update step: Next it determines the 
  
    
      
        n
        ,
        m
        ∈
        {
        1
        ,
        …
        ,
        k
        }
      
    
    {\displaystyle n,m\in \{1,\ldots ,k\}}
   and 
  
    
      
        x
        ∈
        
          S
          
            n
          
        
      
    
    {\displaystyle x\in S_{n}}
   for which the following function reaches a maximum

  
    
      
        Δ
        (
        m
        ,
        n
        ,
        x
        )
        =
        φ
        (
        
          S
          
            n
          
        
        )
        +
        φ
        (
        
          S
          
            m
          
        
        )
        −
        φ
        (
        
          S
          
            n
          
        
        ∖
        {
        x
        }
        )
        −
        φ
        (
        
          S
          
            m
          
        
        ∪
        {
        x
        }
        )
        .
      
    
    {\displaystyle \Delta (m,n,x)=\varphi (S_{n})+\varphi (S_{m})-\varphi (S_{n}\smallsetminus \{x\})-\varphi (S_{m}\cup \{x\}).}
  For the 
  
    
      
        x
        ,
        n
        ,
        m
      
    
    {\displaystyle x,n,m}
   that reach this maximum, 
  
    
      
        x
      
    
    {\displaystyle x}
   moves from the cluster 
  
    
      
        
          S
          
            n
          
        
      
    
    {\displaystyle S_{n}}
   to the cluster 
  
    
      
        
          S
          
            m
          
        
      
    
    {\displaystyle S_{m}}
  .
Termination: The algorithm terminates once 
  
    
      
        Δ
        (
        m
        ,
        n
        ,
        x
        )
      
    
    {\displaystyle \Delta (m,n,x)}
   is less than zero for all 
  
    
      
        x
        ,
        n
        ,
        m
      
    
    {\displaystyle x,n,m}
  .
Different move acceptance strategies can be used. In a first-improvement strategy, any improving relocation can be applied, whereas in a best-improvement strategy, all possible relocations are iteratively tested and only the best is applied at each iteration. The former approach favors speed, whether the latter approach generally favors solution quality at the expense of additional computational time. The function 
  
    
      
        Δ
      
    
    {\displaystyle \Delta }
   used to calculate the result of a relocation can also be efficiently evaluated by using equality

  
    
      
        Δ
        (
        x
        ,
        n
        ,
        m
        )
        =
        
          
            
              ∣
              
                S
                
                  n
                
              
              ∣
            
            
              ∣
              
                S
                
                  n
                
              
              ∣
              −
              1
            
          
        
        ⋅
        ‖
        
          μ
          
            n
          
        
        −
        x
        
          ‖
          
            2
          
        
        −
        
          
            
              ∣
              
                S
                
                  m
                
              
              ∣
            
            
              ∣
              
                S
                
                  m
                
              
              ∣
              +
              1
            
          
        
        ⋅
        ‖
        
          μ
          
            m
          
        
        −
        x
        
          ‖
          
            2
          
        
        .
      
    
    {\displaystyle \Delta (x,n,m)={\frac {\mid S_{n}\mid }{\mid S_{n}\mid -1}}\cdot \lVert \mu _{n}-x\rVert ^{2}-{\frac {\mid S_{m}\mid }{\mid S_{m}\mid +1}}\cdot \lVert \mu _{m}-x\rVert ^{2}.}

Global optimization and metaheuristics
The classical k-means algorithm and its variations are known to only converge to local minima of the minimum-sum-of-squares clustering problem defined as  
  
    
      
        
          
            
              a
              r
              g
              
              m
              i
              n
            
            
              S
            
          
        
        
          ∑
          
            i
            =
            1
          
          
            k
          
        
        
          ∑
          
            
              x
            
            ∈
            
              S
              
                i
              
            
          
        
        
          
            ‖
            
              
                x
              
              −
              
                
                  μ
                
                
                  i
                
              
            
            ‖
          
          
            2
          
        
        .
      
    
    {\displaystyle {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}.}
  Many studies have attempted to improve the convergence behavior of the algorithm and maximize the chances of attaining the global optimum (or at least, local minima of better quality). Initialization and restart techniques discussed in the previous sections are one alternative to find better solutions. More recently, mathematical programming algorithms based on branch-and-bound and column generation have produced ‘’provenly optimal’’ solutions for datasets with up to 2,300 entities. As expected, due to the NP-hardness of the subjacent optimization problem, the computational time of optimal algorithms for K-means quickly increases beyond this size. Optimal solutions for small- and medium-scale still remain valuable as a benchmark tool, to evaluate the quality of other heuristics. To find high-quality local minima within a controlled computational time but without optimality guarantees, other works have explored metaheuristics and other global optimization techniques, e.g., based on incremental approaches and convex optimization, random swaps (i.e., iterated local search), variable neighborhood searchand genetic algorithms. It is indeed known that finding better local minima of the minimum sum-of-squares clustering problem can make the difference between failure and success to recover cluster structures in feature spaces of high dimension.

Discussion
Three key features of k-means that make it efficient are often regarded as its biggest drawbacks:

Euclidean distance is used as a metric and variance is used as a measure of cluster scatter.
The number of clusters k is an input parameter: an inappropriate choice of k may yield poor results. That is why, when performing k-means, it is important to run diagnostic checks for determining the number of clusters in the data set.
Convergence to a local minimum may produce counterintuitive (""wrong"") results (see example in Fig.).A key limitation of k-means is its cluster model. The concept is based on spherical clusters that are separable so that the mean converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying k-means with a value of 
  
    
      
        k
        =
        3
      
    
    {\displaystyle k=3}
   onto the well-known Iris flower data set, the result often fails to separate the three Iris species contained in the data set. With 
  
    
      
        k
        =
        2
      
    
    {\displaystyle k=2}
  , the two visible clusters (one containing two species) will be discovered, whereas with 
  
    
      
        k
        =
        3
      
    
    {\displaystyle k=3}
   one of the two clusters will be split into two even parts. In fact, 
  
    
      
        k
        =
        2
      
    
    {\displaystyle k=2}
   is more appropriate for this data set, despite the data set's containing 3 classes. As with any other clustering algorithm, the k-means result makes assumptions that the data satisfy certain criteria. It works well on some data sets, and fails on others.
The result of k-means can be seen as the Voronoi cells of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the ""mouse"" example. The Gaussian models used by the expectation-maximization algorithm (arguably a generalization of k-means) are more flexible by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than k-means as well as correlated clusters (not in this example). In counterpart, EM requires the optimization of a larger number of free parameters and poses some methodological issues due to vanishing clusters or badly-conditioned covariance matrices. K-means is closely related to nonparametric Bayesian modeling.

Applications
k-means clustering is rather easy to apply to even large data sets, particularly when using heuristics such as Lloyd's algorithm. It has been successfully used in market segmentation, computer vision, and astronomy among many other domains. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.

Vector quantization
k-means originates from signal processing, and still finds use in this domain. For example, in computer graphics, color quantization is the task of reducing the color palette of an image to a fixed number of colors k. The k-means algorithm can easily be used for this task and produces competitive results. A use case for this approach is image segmentation. Other uses of vector quantization include non-random sampling, as k-means can easily be used to choose k different but prototypical objects from a large data set for further analysis.

Cluster analysis
In cluster analysis, the k-means algorithm can be used to partition the input data set into k partitions (clusters).
However, the pure k-means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case). In particular, the parameter k is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation is that it cannot be used with arbitrary distance functions or on non-numerical data. For these use cases, many other algorithms are superior.

Feature learning
k-means clustering has been used as a feature learning (or dictionary learning) step, in either (semi-)supervised learning or unsupervised learning. The basic approach is first to train a k-means clustering representation, using the input training data (which need not be labelled). Then, to project any input datum into the new feature space, an ""encoding"" function, such as the thresholded matrix-product of the datum with the centroid locations, computes the distance from the datum to each centroid, or simply an indicator function for the nearest centroid, or some smooth transformation of the distance. Alternatively, transforming the sample-cluster distance through a Gaussian RBF, obtains the hidden layer of a radial basis function network.This use of k-means has been successfully combined with simple, linear classifiers for semi-supervised learning in NLP (specifically for named entity recognition) and in computer vision. On an object recognition task, it was found to exhibit comparable performance with more sophisticated feature learning approaches such as autoencoders and restricted Boltzmann machines. However, it generally requires more data, for equivalent performance, because each data point only contributes to one ""feature"".

Relation to other algorithms
Gaussian mixture model
The slow ""standard algorithm"" for k-means clustering, and its associated expectation-maximization algorithm, is a special case of a Gaussian mixture model, specifically, the limiting case when fixing all covariances to be diagonal, equal and have infinitesimal small variance. Instead of small variances, a hard cluster assignment can also be used to show another equivalence of k-means clustering to a special case of ""hard"" Gaussian mixture modelling. This does not mean that it is efficient to use Gaussian mixture modelling to compute k-means, but just that there is a theoretical relationship, and that Gaussian mixture modelling can be interpreted as a generalization of k-means; on the contrary, it has been suggested to use k-means clustering to find starting points for Gaussian mixture modelling on difficult data.

K-SVD
Another generalization of the k-means algorithm is the K-SVD algorithm, which estimates data points as a sparse linear combination of ""codebook vectors"". k-means corresponds to the special case of using a single codebook vector, with a weight of 1.

Principal component analysis
The relaxed solution of k-means clustering, specified by the cluster indicators, is given by principal component analysis (PCA).  The intuition is that k-means describe spherically shaped (ball-like) clusters. If the data has 2 clusters, the line connecting the two centroids is the best 1-dimensional projection direction, which is also the first PCA direction. Cutting the line at the center of mass separates the clusters (this is the continuous relaxation of the discrete cluster indicator). If the data have three clusters, the 2-dimensional plane spanned by three cluster centroids is the best 2-D projection. This plane is also defined by the first two PCA dimensions. Well-separated clusters are effectively modelled by ball-shaped clusters and thus discovered by k-means. Non-ball-shaped clusters are hard to separate when they are close. For example, two half-moon shaped clusters intertwined in space do not separate well when projected onto PCA subspace. k-means should not be expected to do well on this data. It is straightforward to produce counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.

Mean shift clustering
Basic mean shift clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. Then this set is iteratively replaced by the mean of those points in the set that are within a given distance of that point. By contrast, k-means restricts this updated set to k points usually much less than the number of points in the input data set, and replaces each point in this set by the mean of all points in the input set that are closer to that point than any other (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to k-means, called likelihood mean shift, replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set. One of the advantages of mean shift over k-means is that the number of clusters is not pre-specified, because mean shift is likely to find only a few clusters if only a small number exist. However, mean shift can be much slower than k-means, and still requires selection of a bandwidth parameter. Mean shift has soft variants.

Independent component analysis
Under sparsity assumptions and when input data is pre-processed with the whitening transformation, k-means produces the solution to the linear independent component analysis (ICA) task. This aids in explaining the successful application of k-means to feature learning.

Bilateral filtering
k-means implicitly assumes that the ordering of the input data set does not matter. The bilateral filter is similar to k-means and mean shift in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data. This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance.

Similar problems
The set of squared error minimizing cluster functions also includes the k-medoids algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e., it uses medoids in place of centroids.

Software implementations
Different implementations of the algorithm exhibit performance differences, with the fastest on a test data set finishing in 10 seconds, the slowest taking 25,988 seconds (~7 hours). The differences can be attributed to implementation quality, language and compiler differences, different termination criteria and precision levels, and the use of indexes for acceleration.

Free Software/Open Source
The following implementations are available under Free/Open Source Software licenses, with publicly available source code.

Accord.NET contains C# implementations for k-means, k-means++ and k-modes.
ALGLIB contains parallelized C++ and C# implementations for k-means and k-means++.
AOSP contains a Java implementation for k-means.
CrimeStat implements two spatial k-means algorithms, one of which allows the user to define the starting locations.
ELKI contains k-means (with Lloyd and MacQueen iteration, along with different initializations such as k-means++ initialization) and various more advanced clustering algorithms.
Smile contains k-means and various more other algorithms and results visualization (for java, kotlin and scala).
Julia contains a k-means implementation in the JuliaStats Clustering package.
KNIME contains nodes for k-means and k-medoids.
Mahout contains a MapReduce based k-means.
mlpack contains a C++ implementation of k-means.
Octave contains k-means.
OpenCV contains a k-means implementation.
Orange includes a component for k-means clustering with automatic selection of k and cluster silhouette scoring.
PSPP contains k-means, The QUICK CLUSTER command performs k-means clustering on the dataset.
R contains three k-means variations.
SciPy and scikit-learn contain multiple k-means implementations.
Spark MLlib implements a distributed k-means algorithm.
Torch contains an unsup package that provides k-means clustering.
Weka contains k-means and x-means.

Proprietary
The following implementations are available under proprietary license terms, and may not have publicly available source code.

See also
BFR algorithm
Centroidal Voronoi tessellation
Head/tail Breaks
k q-flats
K-means++
Linde–Buzo–Gray algorithm
Self-organizing map


== References ==",https://en.wikipedia.org/wiki/K-means_clustering,"['Articles with short description', 'CS1 French-language sources (fr)', 'CS1 errors: missing periodical', 'Cluster analysis algorithms', 'Short description is different from Wikidata']",Data Science
120,K-nearest neighbors classification,"In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric classification method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in data set. The output depends on whether k-NN is used for classification or regression:

In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.
A peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data.

Statistical setting
Suppose we have pairs 
  
    
      
        (
        
          X
          
            1
          
        
        ,
        
          Y
          
            1
          
        
        )
        ,
        (
        
          X
          
            2
          
        
        ,
        
          Y
          
            2
          
        
        )
        ,
        …
        ,
        (
        
          X
          
            n
          
        
        ,
        
          Y
          
            n
          
        
        )
      
    
    {\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\dots ,(X_{n},Y_{n})}
   taking values in 
  
    
      
        
          
            R
          
          
            d
          
        
        ×
        {
        1
        ,
        2
        }
      
    
    {\displaystyle \mathbb {R} ^{d}\times \{1,2\}}
  , where Y is the class label of X, so that 
  
    
      
        X
        
          |
        
        Y
        =
        r
        ∼
        
          P
          
            r
          
        
      
    
    {\displaystyle X|Y=r\sim P_{r}}
   for 
  
    
      
        r
        =
        1
        ,
        2
      
    
    {\displaystyle r=1,2}
   (and probability distributions 
  
    
      
        
          P
          
            r
          
        
      
    
    {\displaystyle P_{r}}
  ). Given some norm 
  
    
      
        ‖
        ⋅
        ‖
      
    
    {\displaystyle \|\cdot \|}
   on 
  
    
      
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle \mathbb {R} ^{d}}
   and a point 
  
    
      
        x
        ∈
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle x\in \mathbb {R} ^{d}}
  , let 
  
    
      
        (
        
          X
          
            (
            1
            )
          
        
        ,
        
          Y
          
            (
            1
            )
          
        
        )
        ,
        …
        ,
        (
        
          X
          
            (
            n
            )
          
        
        ,
        
          Y
          
            (
            n
            )
          
        
        )
      
    
    {\displaystyle (X_{(1)},Y_{(1)}),\dots ,(X_{(n)},Y_{(n)})}
   be a reordering of the training data such that 
  
    
      
        ‖
        
          X
          
            (
            1
            )
          
        
        −
        x
        ‖
        ≤
        ⋯
        ≤
        ‖
        
          X
          
            (
            n
            )
          
        
        −
        x
        ‖
      
    
    {\displaystyle \|X_{(1)}-x\|\leq \dots \leq \|X_{(n)}-x\|}
  .

Algorithm
The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.
In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.
A commonly used distance metric for continuous variables is Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the overlap metric (or Hamming distance). In the context of gene expression microarray data, for example, k-NN has been employed with correlation coefficients, such as Pearson and Spearman, as a metric. Often, the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis.
A drawback of the basic ""majority voting"" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the k nearest neighbors due to their large number. One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its k nearest neighbors. The class (or value, in regression problems) of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another way to overcome skew is by abstraction in data representation. For example, in a self-organizing map (SOM), each node is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. K-NN can then be applied to the SOM.

Parameter selection
The best choice of k depends upon the data; generally, larger values of k reduces effect of the noise on the classification, but make boundaries between classes less distinct. A good k can be selected by various heuristic techniques (see hyperparameter optimization). The special case where the class is predicted to be the class of the closest training sample (i.e. when k = 1) is called the nearest neighbor algorithm.
The accuracy of the k-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance. Much research effort has been put into selecting or scaling features to improve classification. A particularly popular approach is the use of evolutionary algorithms to optimize feature scaling. Another popular approach is to scale features by the mutual information of the training data with the training classes.In binary (two class) classification problems, it is helpful to choose k to be an odd number as this avoids tied votes. One popular way of choosing the empirically optimal k in this setting is via bootstrap method.

The 1-nearest neighbor classifier
The most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point x to the class of its closest neighbour in the feature space, that is 
  
    
      
        
          C
          
            n
          
          
            1
            n
            n
          
        
        (
        x
        )
        =
        
          Y
          
            (
            1
            )
          
        
      
    
    {\displaystyle C_{n}^{1nn}(x)=Y_{(1)}}
  .
As the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data).

The weighted nearest neighbour classifier
The k-nearest neighbour classifier can be viewed as assigning the k nearest neighbours a weight 
  
    
      
        1
        
          /
        
        k
      
    
    {\displaystyle 1/k}
   and all others 0 weight. This can be generalised to weighted nearest neighbour classifiers. That is, where the ith nearest neighbour is assigned a weight 
  
    
      
        
          w
          
            n
            i
          
        
      
    
    {\displaystyle w_{ni}}
  , with 
  
    
      
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          w
          
            n
            i
          
        
        =
        1
      
    
    {\displaystyle \sum _{i=1}^{n}w_{ni}=1}
  . An analogous result on the strong consistency of weighted nearest neighbour classifiers also holds.Let 
  
    
      
        
          C
          
            n
          
          
            w
            n
            n
          
        
      
    
    {\displaystyle C_{n}^{wnn}}
   denote the weighted nearest classifier with weights 
  
    
      
        {
        
          w
          
            n
            i
          
        
        
          }
          
            i
            =
            1
          
          
            n
          
        
      
    
    {\displaystyle \{w_{ni}\}_{i=1}^{n}}
  . Subject to regularity conditions on the class distributions the excess risk has the following asymptotic expansion

  
    
      
        
          
            
              R
            
          
          
            
              R
            
          
        
        (
        
          C
          
            n
          
          
            w
            n
            n
          
        
        )
        −
        
          
            
              R
            
          
          
            
              R
            
          
        
        (
        
          C
          
            B
            a
            y
            e
            s
          
        
        )
        =
        
          (
          
            
              B
              
                1
              
            
            
              s
              
                n
              
              
                2
              
            
            +
            
              B
              
                2
              
            
            
              t
              
                n
              
              
                2
              
            
          
          )
        
        {
        1
        +
        o
        (
        1
        )
        }
        ,
      
    
    {\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{wnn})-{\mathcal {R}}_{\mathcal {R}}(C^{Bayes})=\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\right)\{1+o(1)\},}
  for constants 
  
    
      
        
          B
          
            1
          
        
      
    
    {\displaystyle B_{1}}
   and 
  
    
      
        
          B
          
            2
          
        
      
    
    {\displaystyle B_{2}}
   where 
  
    
      
        
          s
          
            n
          
          
            2
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          w
          
            n
            i
          
          
            2
          
        
      
    
    {\displaystyle s_{n}^{2}=\sum _{i=1}^{n}w_{ni}^{2}}
   and 
  
    
      
        
          t
          
            n
          
        
        =
        
          n
          
            −
            2
            
              /
            
            d
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          w
          
            n
            i
          
        
        {
        
          i
          
            1
            +
            2
            
              /
            
            d
          
        
        −
        (
        i
        −
        1
        
          )
          
            1
            +
            2
            
              /
            
            d
          
        
        }
      
    
    {\displaystyle t_{n}=n^{-2/d}\sum _{i=1}^{n}w_{ni}\{i^{1+2/d}-(i-1)^{1+2/d}\}}
  .
The optimal weighting scheme 
  
    
      
        {
        
          w
          
            n
            i
          
          
            ∗
          
        
        
          }
          
            i
            =
            1
          
          
            n
          
        
      
    
    {\displaystyle \{w_{ni}^{*}\}_{i=1}^{n}}
  , that balances the two terms in the display above, is given as follows: set 
  
    
      
        
          k
          
            ∗
          
        
        =
        ⌊
        B
        
          n
          
            
              4
              
                d
                +
                4
              
            
          
        
        ⌋
      
    
    {\displaystyle k^{*}=\lfloor Bn^{\frac {4}{d+4}}\rfloor }
  , 

  
    
      
        
          w
          
            n
            i
          
          
            ∗
          
        
        =
        
          
            1
            
              k
              
                ∗
              
            
          
        
        
          [
          
            1
            +
            
              
                d
                2
              
            
            −
            
              
                d
                
                  2
                  
                    
                      
                        k
                        
                          ∗
                        
                      
                    
                    
                      2
                      
                        /
                      
                      d
                    
                  
                
              
            
            {
            
              i
              
                1
                +
                2
                
                  /
                
                d
              
            
            −
            (
            i
            −
            1
            
              )
              
                1
                +
                2
                
                  /
                
                d
              
            
            }
          
          ]
        
      
    
    {\displaystyle w_{ni}^{*}={\frac {1}{k^{*}}}\left[1+{\frac {d}{2}}-{\frac {d}{2{k^{*}}^{2/d}}}\{i^{1+2/d}-(i-1)^{1+2/d}\}\right]}
   for 
  
    
      
        i
        =
        1
        ,
        2
        ,
        …
        ,
        
          k
          
            ∗
          
        
      
    
    {\displaystyle i=1,2,\dots ,k^{*}}
   and

  
    
      
        
          w
          
            n
            i
          
          
            ∗
          
        
        =
        0
      
    
    {\displaystyle w_{ni}^{*}=0}
   for 
  
    
      
        i
        =
        
          k
          
            ∗
          
        
        +
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=k^{*}+1,\dots ,n}
  .With optimal weights the dominant term in the asymptotic expansion of the excess risk is 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            −
            
              
                4
                
                  d
                  +
                  4
                
              
            
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{-{\frac {4}{d+4}}})}
  . Similar results are true when using a bagged nearest neighbour classifier.

Properties
k-NN is a special case of a variable-bandwidth, kernel density ""balloon"" estimator with a uniform kernel.The naive version of the algorithm is easy to implement by computing the distances from the test example to all stored examples, but it is computationally intensive for large training sets. Using an approximate nearest neighbor search algorithm makes k-NN computationally tractable even for large data sets. Many nearest neighbor search algorithms have been proposed over the years; these generally seek to reduce the number of distance evaluations actually performed.
k-NN has some strong consistency results. As the amount of data approaches infinity, the two-class k-NN algorithm is guaranteed to yield an error rate no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data). Various improvements to the k-NN speed are possible by using proximity graphs.For multi-class k-NN classification, Cover and Hart (1967) prove an upper bound error rate of

  
    
      
        
          R
          
            ∗
          
        
         
        ≤
         
        
          R
          
            k
            
              N
              N
            
          
        
         
        ≤
         
        
          R
          
            ∗
          
        
        
          (
          
            2
            −
            
              
                
                  M
                  
                    R
                    
                      ∗
                    
                  
                
                
                  M
                  −
                  1
                
              
            
          
          )
        
      
    
    {\displaystyle R^{*}\ \leq \ R_{k\mathrm {NN} }\ \leq \ R^{*}\left(2-{\frac {MR^{*}}{M-1}}\right)}
  where 
  
    
      
        
          R
          
            ∗
          
        
      
    
    {\displaystyle R^{*}}
  is the Bayes error rate (which is the minimal error rate possible),  
  
    
      
        
          R
          
            k
            N
            N
          
        
      
    
    {\displaystyle R_{kNN}}
   is the k-NN error rate, and M is the number of classes in the problem.  For 
  
    
      
        M
        =
        2
      
    
    {\displaystyle M=2}
   and as the Bayesian error rate 
  
    
      
        
          R
          
            ∗
          
        
      
    
    {\displaystyle R^{*}}
   approaches zero, this limit reduces to ""not more than twice the Bayesian error rate"".

Error rates
There are many results on the error rate of the k nearest neighbour classifiers.  The k-nearest neighbour classifier is strongly (that is for any joint distribution on 
  
    
      
        (
        X
        ,
        Y
        )
      
    
    {\displaystyle (X,Y)}
  ) consistent provided 
  
    
      
        k
        :=
        
          k
          
            n
          
        
      
    
    {\displaystyle k:=k_{n}}
   diverges and 
  
    
      
        
          k
          
            n
          
        
        
          /
        
        n
      
    
    {\displaystyle k_{n}/n}
   converges to zero as 
  
    
      
        n
        →
        ∞
      
    
    {\displaystyle n\to \infty }
  .
Let 
  
    
      
        
          C
          
            n
          
          
            k
            n
            n
          
        
      
    
    {\displaystyle C_{n}^{knn}}
   denote the k nearest neighbour classifier based on a training set of size n. Under certain regularity conditions, the excess risk yields the following asymptotic expansion

  
    
      
        
          
            
              R
            
          
          
            
              R
            
          
        
        (
        
          C
          
            n
          
          
            k
            n
            n
          
        
        )
        −
        
          
            
              R
            
          
          
            
              R
            
          
        
        (
        
          C
          
            B
            a
            y
            e
            s
          
        
        )
        =
        
          {
          
            
              B
              
                1
              
            
            
              
                1
                k
              
            
            +
            
              B
              
                2
              
            
            
              
                (
                
                  
                    k
                    n
                  
                
                )
              
              
                4
                
                  /
                
                d
              
            
          
          }
        
        {
        1
        +
        o
        (
        1
        )
        }
        ,
      
    
    {\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{knn})-{\mathcal {R}}_{\mathcal {R}}(C^{Bayes})=\left\{B_{1}{\frac {1}{k}}+B_{2}\left({\frac {k}{n}}\right)^{4/d}\right\}\{1+o(1)\},}
  for some constants 
  
    
      
        
          B
          
            1
          
        
      
    
    {\displaystyle B_{1}}
   and 
  
    
      
        
          B
          
            2
          
        
      
    
    {\displaystyle B_{2}}
  .
The choice 
  
    
      
        
          k
          
            ∗
          
        
        =
        ⌊
        B
        
          n
          
            
              4
              
                d
                +
                4
              
            
          
        
        ⌋
      
    
    {\displaystyle k^{*}=\lfloor Bn^{\frac {4}{d+4}}\rfloor }
   offers a trade off between the two terms in the above display, for which the 
  
    
      
        
          k
          
            ∗
          
        
      
    
    {\displaystyle k^{*}}
  -nearest neighbour error converges to the Bayes error at the optimal (minimax) rate 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            −
            
              
                4
                
                  d
                  +
                  4
                
              
            
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{-{\frac {4}{d+4}}})}
  .

Metric learning
The K-nearest neighbor classification performance can often be significantly improved through (supervised) metric learning. Popular algorithms are neighbourhood components analysis and large margin nearest neighbor. Supervised metric learning algorithms use the label information to learn a new metric or pseudo-metric.

Feature extraction
When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters) then the input data will be transformed into a reduced representation set of features (also named features vector). Transforming the input data into the set of features is called feature extraction. If the features extracted are carefully chosen it is expected that the features set will extract the relevant information from the input data in order to perform the desired task using this reduced representation instead of the full size input. Feature extraction is performed on raw data prior to applying k-NN algorithm on the transformed data in feature space.
An example of a typical computer vision computation pipeline for face recognition using k-NN including feature extraction and dimension reduction pre-processing steps (usually implemented with OpenCV):

Haar face detection
Mean-shift tracking analysis
PCA or Fisher LDA projection into feature space, followed by k-NN classification

Dimension reduction
For high-dimensional data (e.g., with number of dimensions more than 10) dimension reduction is usually performed prior to applying the k-NN algorithm in order to avoid the effects of the curse of dimensionality.
The curse of dimensionality in the k-NN context basically means that Euclidean distance is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector (imagine multiple points lying more or less on a circle with the query point at the center; the distance from the query to all data points in the search space is almost the same).
Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA),  linear discriminant analysis (LDA), or canonical correlation analysis (CCA) techniques as a pre-processing step, followed by clustering by k-NN on feature vectors in reduced-dimension space. This process is also called low-dimensional embedding.For very-high-dimensional datasets (e.g. when performing a similarity search on live video streams, DNA data or high-dimensional time series) running a fast approximate k-NN search using locality sensitive hashing, ""random projections"", ""sketches""  or other high-dimensional similarity search techniques from the VLDB toolbox might be the only feasible option.

Decision boundary
Nearest neighbor rules in effect implicitly compute the decision boundary. It is also possible to compute the decision boundary explicitly, and to do so efficiently, so that the computational complexity is a function of the boundary complexity.

Data reduction
Data reduction is one of the most important problems for work with huge data sets. Usually, only some of the data points are needed for accurate classification. Those data are called the prototypes and can be found as follows:

Select the class-outliers, that is, training data that are classified incorrectly by k-NN (for a given k)
Separate the rest of the data into two sets: (i) the prototypes that are used for the classification decisions and (ii) the absorbed points that can be correctly classified by k-NN using prototypes. The absorbed points can then be removed from the training set.

Selection of class-outliers
A training example surrounded by examples of other classes is called a class outlier. Causes of class outliers include:

random error
insufficient training examples of this class (an isolated example appears instead of a cluster)
missing important features (the classes are separated in other dimensions which we do not know)
too many training examples of other classes (unbalanced classes) that create a ""hostile"" background for the given small classClass outliers with k-NN produce noise. They can be detected and separated for future analysis. Given two natural numbers, k>r>0, a training example is called a (k,r)NN class-outlier if its k nearest neighbors include more than r examples of other classes.

CNN for data reduction
Condensed nearest neighbor (CNN, the Hart algorithm) is an algorithm designed to reduce the data set for k-NN classification. It selects the set of prototypes U from the training data, such that 1NN with U can classify the examples almost as accurately as 1NN does with the whole data set.

Given a training set X, CNN works iteratively:

Scan all elements of X, looking for an element x whose nearest prototype from U has a different label than x.
Remove x from X and add it to U
Repeat the scan until no more prototypes are added to U.Use U instead of X for classification. The examples that are not prototypes are called ""absorbed"" points.
It is efficient to scan the training examples in order of decreasing border ratio. The border ratio of a training example x is defined as 

a(x) = ||x'-y||/||x-y||where ||x-y|| is the distance to the closest example y having a different color than x, and ||x'-y|| is the distance from y to its closest example x'  with the same label as x.
The border ratio is in the interval [0,1] because ||x'-y||never exceeds ||x-y||. This ordering gives preference to the borders of the classes for inclusion in the set of prototypes U. A point of a different label than x is called external to x. The calculation of the border ratio is illustrated by the figure on the right. The data points are labeled by colors: the initial point is x and its label is red. External points are blue and green. The closest to x external point is y. The closest to y red point is x' . The border ratio a(x) = ||x'-y|| / ||x-y||is the attribute of the initial point x.
Below is an illustration of CNN in a series of figures. There are three classes (red, green and blue). Fig. 1: initially there are 60 points in each class. Fig. 2 shows the 1NN classification map: each pixel is classified by 1NN using all the data. Fig. 3 shows the 5NN classification map. White areas correspond to the unclassified regions, where 5NN voting is tied (for example, if there are two green, two red and one blue points among 5 nearest neighbors). Fig. 4 shows the reduced data set. The crosses are the class-outliers selected by the (3,2)NN rule (all the three nearest neighbors of these instances belong to other classes); the squares are the prototypes, and the empty circles are the absorbed points. The left bottom corner shows the numbers of the class-outliers, prototypes and absorbed points for all three classes. The number of prototypes varies from 15% to 20% for different classes in this example. Fig. 5 shows that the 1NN classification map with the prototypes is very similar to that with the initial data set. The figures were produced using the Mirkes applet.
CNN model reduction for k-NN classifiers

k-NN regression
In k-NN regression, the k-NN algorithm is used for estimating continuous variables. One such algorithm uses a weighted average of the k nearest neighbors, weighted by the inverse of their distance. This algorithm works as follows:

Compute the Euclidean or Mahalanobis distance from the query example to the labeled examples.
Order the labeled examples by increasing distance.
Find a heuristically optimal number k of nearest neighbors, based on RMSE. This is done using cross validation.
Calculate an inverse distance weighted average with the k-nearest multivariate neighbors.

k-NN outlier
The distance to the kth nearest neighbor can also be seen as a local density estimate and thus is also a popular outlier score in anomaly detection. The larger the distance to the k-NN, the lower the local density, the more likely the query point is an outlier. Although quite simple, this outlier model, along with another classic data mining method, local outlier factor, works quite well also in comparison to more recent and more complex approaches, according to a large scale experimental analysis.

Validation of results
A confusion matrix or ""matching matrix"" is often used as a tool to validate the accuracy of k-NN classification. More robust statistical methods such as likelihood-ratio test can also be applied.

See also
Nearest centroid classifier
Closest pair of points problem

References
Further reading
Dasarathy, Belur V., ed. (1991). Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. ISBN 978-0-8186-8930-7.
Shakhnarovich, Gregory; Darrell, Trevor; Indyk, Piotr, eds. (2005). Nearest-Neighbor Methods in Learning and Vision. MIT Press. ISBN 978-0-262-19547-8.",https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm,"['All articles with unsourced statements', 'Articles with unsourced statements from December 2008', 'Articles with unsourced statements from March 2013', 'Articles with unsourced statements from September 2019', 'Classification algorithms', 'Machine learning algorithms', 'Nonparametric statistics', 'Search algorithms', 'Statistical classification', 'Wikipedia articles needing clarification from January 2019', 'Wikipedia articles needing clarification from July 2020']",Data Science
121,Learning to rank,"Learning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems. Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. ""relevant"" or ""not relevant"") for each item. The ranking model  purposes to rank, i.e. producing a permutation of items in new, unseen lists in a similar way to rankings in the training data.

Applications
In information retrieval
Ranking is a central part of many information retrieval problems, such as document retrieval, collaborative filtering, sentiment analysis, and online advertising.
A possible architecture of a machine-learned search engine is shown in the accompanying figure.
Training data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human assessors (or raters, as Google calls them),
who check results for some queries and determine relevance of each result. It is not feasible to check the relevance of all documents, and so typically a technique called pooling is used — only the top few documents, retrieved by some existing ranking models are checked.  Alternatively, training data may be derived automatically by analyzing clickthrough logs (i.e. search results which got clicks from users), query chains, or such search engines' features as Google's SearchWiki.
Training data is used by a learning algorithm to produce a ranking model which computes the relevance of documents for actual queries.
Typically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used. First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as the vector space model, boolean model, weighted AND, or BM25. This phase is called top-
  
    
      
        k
      
    
    {\displaystyle k}
   document retrieval and many heuristics were proposed in the literature to accelerate it, such as using a document's static quality score and tiered indexes. In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.

In other areas
Learning to rank algorithms have been applied in areas other than information retrieval:

In machine translation for ranking a set of hypothesized translations;
In computational biology for ranking candidate 3-D structures in protein structure prediction problem.
In recommender systems for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.
In software engineering, learning-to-rank methods have been used for fault localization.

Feature vectors
For the convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called feature vectors. Such an approach is sometimes called bag of features and is analogous to the bag of words model and vector space model used in information retrieval for representation of documents.
Components of such vectors are called features, factors or ranking signals. They may be divided into three groups (features from document retrieval are shown as examples):

Query-independent or static features — those features, which depend only on the document, but not on the query. For example, PageRank or document's length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document's static quality score (or static rank), which is often used to speed up search query evaluation.
Query-dependent or dynamic features — those features, which depend both on the contents of the document and the query, such as TF-IDF score or other non-machine-learned ranking functions.
Query-level features or query features, which depend only on the query. For example, the number of words in a query.Some examples of features, which were used in the well-known LETOR dataset:

TF, TF-IDF, BM25, and language modeling scores of document's zones (title, body, anchors text, URL) for a given query;
Lengths and IDF sums of document's zones;
Document's PageRank, HITS ranks and their variants.Selecting and designing good features is an important area in machine learning, which is called feature engineering.

Evaluation measures
There are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare the performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.
Examples of ranking quality measures:

Mean average precision (MAP);
DCG and NDCG;
Precision@n, NDCG@n, where ""@n"" denotes that the metrics are evaluated only on top n documents;
Mean reciprocal rank;
Kendall's tau;
Spearman's rho.DCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used. Other metrics such as MAP, MRR and precision, are defined only for binary judgments.
Recently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:

Expected reciprocal rank (ERR);
Yandex's pfound.Both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.

Approaches
Tie-Yan Liu of Microsoft Research Asia has analyzed existing algorithms for learning to rank problems in his paper ""Learning to Rank for Information Retrieval"". He categorized them into three groups by their input representation and loss function: the pointwise, pairwise, and listwise approach. In practice, listwise approaches often outperform pairwise approaches and pointwise approaches. This statement was further supported by a large scale experiment on the performance of different learning-to-rank methods on a large collection of benchmark data sets.

Pointwise approach
In this case, it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then the learning-to-rank problem can be approximated by a regression problem — given a single query-document pair, predict its score.
A number of existing supervised machine learning algorithms can be readily used for this purpose. Ordinal regression and classification algorithms can also be used in pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values.

Pairwise approach
In this case, the learning-to-rank problem is approximated by a classification problem — learning a binary classifier that can tell which document is better in a given pair of documents. The goal is to minimize the average number of inversions in ranking.

Listwise approach
These algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used.

List of methods
A partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:

Note: as most supervised learning algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.

History
Norbert Fuhr introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation; a specific variant of this approach (using polynomial regression) had been published by him three years earlier. Bill Cooper proposed logistic regression for the same purpose in 1992  and used it with his  Berkeley research group to train a successful ranking function for TREC.  Manning et al.  suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.
Several conferences, such as NIPS, SIGIR and ICML had workshops devoted to the learning-to-rank problem since mid-2000s (decade).

Practical usage by search engines
Commercial web search engines began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was AltaVista (later its technology was acquired by Overture, and then Yahoo), which launched a gradient boosting-trained ranking function in April 2003.Bing's search is said to be powered by RankNet algorithm, which was invented at Microsoft Research in 2005.
In November 2009 a Russian search engine Yandex announced that it had significantly increased its search quality due to deployment of a new proprietary MatrixNet algorithm, a variant of gradient boosting method which uses oblivious decision trees. Recently they have also sponsored a machine-learned ranking competition ""Internet Mathematics 2009"" based on their own search engine's production data. Yahoo has announced a similar competition in 2010.As of 2008, Google's Peter Norvig denied that their search engine exclusively relies on machine-learned ranking. Cuil's CEO, Tom Costello, suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models ""learn what people say they like, not what people actually like"".In January 2017 the technology was included in the open source search engine Apache Solr™, thus making machine learned search rank widely accessible also for enterprise search.

Vulnerabilities
Similar to recognition applications in computer vision, recent neural network based ranking algorithms are also found to be susceptible to covert  adversarial attacks, both on the candidates and the queries. With small perturbations imperceptible to human beings, ranking order could be arbitrarily altered. In addition, model-agnostic transferable adversarial examples are found to be possible, which enables black-box adversarial attacks on deep ranking systems without requiring access to their underlying implementations.Conversely, the robustness of such ranking systems can be improved via adversarial defenses such as the Madry defense.

See also
Content-based image retrieval
Multimedia information retrieval
Image retrieval
Triplet loss

References
External links
Competitions and public datasetsLETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval
Yandex's Internet Mathematics 2009
Yahoo! Learning to Rank Challenge
Microsoft Learning to Rank DatasetsOpen Source codeParallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011
C++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking
C++ and Python tools for using the SVM-Rank algorithm
Java implementation in the Apache Solr search engine",https://en.wikipedia.org/wiki/Learning_to_rank,"['All articles to be expanded', 'All articles with vague or ambiguous time', 'Articles to be expanded from December 2009', 'Articles using small message boxes', 'Articles with short description', 'CS1 errors: missing periodical', 'Information retrieval techniques', 'Machine learning', 'Ranking functions', 'Short description is different from Wikidata', 'Vague or ambiguous time from February 2014', 'Webarchive template wayback links']",Data Science
122,Linear discriminant analysis,"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.
LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label). Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method.
LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA, in contrast, does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made.
LDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.Discriminant analysis is used when groups are known a priori (unlike in cluster analysis). Each case must have a score on one or more quantitative predictor measures, and a score on a group measure. In simple terms, discriminant function analysis is classification - the act of distributing things into groups, classes or categories of the same type.

History
The original dichotomous discriminant analysis was developed by Sir Ronald Fisher in 1936. It is different from an ANOVA or MANOVA, which is used to predict one (ANOVA) or multiple (MANOVA) continuous dependent variables by one or more independent categorical variables. Discriminant function analysis is useful in determining whether a set of variables is effective in predicting category membership.

LDA for two classes
Consider a set of observations 
  
    
      
        
          
            
              x
              →
            
          
        
      
    
    {\displaystyle {\vec {x}}}
   (also called features, attributes, variables or measurements) for each sample of an object or event with known class 
  
    
      
        y
      
    
    {\displaystyle y}
  . This set of samples is called the training set. The classification problem is then to find a good predictor for the class 
  
    
      
        y
      
    
    {\displaystyle y}
   of any sample of the same distribution (not necessarily from the training set) given only an observation 
  
    
      
        
          
            
              x
              →
            
          
        
      
    
    {\displaystyle {\vec {x}}}
  .LDA approaches the problem by assuming that the conditional probability density functions 
  
    
      
        p
        (
        
          
            
              x
              →
            
          
        
        
          |
        
        y
        =
        0
        )
      
    
    {\displaystyle p({\vec {x}}|y=0)}
   and 
  
    
      
        p
        (
        
          
            
              x
              →
            
          
        
        
          |
        
        y
        =
        1
        )
      
    
    {\displaystyle p({\vec {x}}|y=1)}
   are both the normal distribution with mean and covariance parameters 
  
    
      
        
          (
          
            
              
                
                  
                    μ
                    →
                  
                
              
              
                0
              
            
            ,
            
              Σ
              
                0
              
            
          
          )
        
      
    
    {\displaystyle \left({\vec {\mu }}_{0},\Sigma _{0}\right)}
   and 
  
    
      
        
          (
          
            
              
                
                  
                    μ
                    →
                  
                
              
              
                1
              
            
            ,
            
              Σ
              
                1
              
            
          
          )
        
      
    
    {\displaystyle \left({\vec {\mu }}_{1},\Sigma _{1}\right)}
  , respectively. Under this assumption, the Bayes optimal solution is to predict points as being from the second class if the log of the likelihood ratios is bigger than some threshold T, so that:

  
    
      
        (
        
          
            
              x
              →
            
          
        
        −
        
          
            
              
                μ
                →
              
            
          
          
            0
          
        
        
          )
          
            T
          
        
        
          Σ
          
            0
          
          
            −
            1
          
        
        (
        
          
            
              x
              →
            
          
        
        −
        
          
            
              
                μ
                →
              
            
          
          
            0
          
        
        )
        +
        ln
        ⁡
        
          |
        
        
          Σ
          
            0
          
        
        
          |
        
        −
        (
        
          
            
              x
              →
            
          
        
        −
        
          
            
              
                μ
                →
              
            
          
          
            1
          
        
        
          )
          
            T
          
        
        
          Σ
          
            1
          
          
            −
            1
          
        
        (
        
          
            
              x
              →
            
          
        
        −
        
          
            
              
                μ
                →
              
            
          
          
            1
          
        
        )
        −
        ln
        ⁡
        
          |
        
        
          Σ
          
            1
          
        
        
          |
        
         
        >
         
        T
      
    
    {\displaystyle ({\vec {x}}-{\vec {\mu }}_{0})^{T}\Sigma _{0}^{-1}({\vec {x}}-{\vec {\mu }}_{0})+\ln |\Sigma _{0}|-({\vec {x}}-{\vec {\mu }}_{1})^{T}\Sigma _{1}^{-1}({\vec {x}}-{\vec {\mu }}_{1})-\ln |\Sigma _{1}|\ >\ T}
  Without any further assumptions, the resulting classifier is referred to as QDA (quadratic discriminant analysis).
LDA instead makes the additional simplifying homoscedasticity assumption (i.e. that the class covariances are identical, so 
  
    
      
        
          Σ
          
            0
          
        
        =
        
          Σ
          
            1
          
        
        =
        Σ
      
    
    {\displaystyle \Sigma _{0}=\Sigma _{1}=\Sigma }
  ) and that the covariances have full rank.
In this case, several terms cancel:

  
    
      
        
          
            
              
                x
                →
              
            
          
          
            T
          
        
        
          Σ
          
            0
          
          
            −
            1
          
        
        
          
            
              x
              →
            
          
        
        =
        
          
            
              
                x
                →
              
            
          
          
            T
          
        
        
          Σ
          
            1
          
          
            −
            1
          
        
        
          
            
              x
              →
            
          
        
      
    
    {\displaystyle {\vec {x}}^{T}\Sigma _{0}^{-1}{\vec {x}}={\vec {x}}^{T}\Sigma _{1}^{-1}{\vec {x}}}
  

  
    
      
        
          
            
              
                x
                →
              
            
          
          
            T
          
        
        
          
            
              Σ
              
                i
              
            
          
          
            −
            1
          
        
        
          
            
              
                μ
                →
              
            
          
          
            i
          
        
        =
        
          
            
              
                
                  
                    μ
                    →
                  
                
              
              
                i
              
            
          
          
            T
          
        
        
          
            
              Σ
              
                i
              
            
          
          
            −
            1
          
        
        
          
            
              x
              →
            
          
        
      
    
    {\displaystyle {\vec {x}}^{T}{\Sigma _{i}}^{-1}{\vec {\mu }}_{i}={{\vec {\mu }}_{i}}^{T}{\Sigma _{i}}^{-1}{\vec {x}}}
    because 
  
    
      
        
          Σ
          
            i
          
        
      
    
    {\displaystyle \Sigma _{i}}
   is Hermitianand the above decision criterion
becomes a threshold on the dot product

  
    
      
        
          
            
              w
              →
            
          
        
        ⋅
        
          
            
              x
              →
            
          
        
        >
        c
      
    
    {\displaystyle {\vec {w}}\cdot {\vec {x}}>c}
  for some threshold constant c, where

  
    
      
        
          
            
              w
              →
            
          
        
        =
        
          Σ
          
            −
            1
          
        
        (
        
          
            
              
                μ
                →
              
            
          
          
            1
          
        
        −
        
          
            
              
                μ
                →
              
            
          
          
            0
          
        
        )
      
    
    {\displaystyle {\vec {w}}=\Sigma ^{-1}({\vec {\mu }}_{1}-{\vec {\mu }}_{0})}
  

  
    
      
        c
        =
        
          
            
              w
              →
            
          
        
        ⋅
        
          
            1
            2
          
        
        (
        
          
            
              
                μ
                →
              
            
          
          
            1
          
        
        +
        
          
            
              
                μ
                →
              
            
          
          
            0
          
        
        )
      
    
    {\displaystyle c={\vec {w}}\cdot {\frac {1}{2}}({\vec {\mu }}_{1}+{\vec {\mu }}_{0})}
  This means that the criterion of an input 
  
    
      
        
          
            
              x
              →
            
          
        
      
    
    {\displaystyle {\vec {x}}}
   being in a class 
  
    
      
        y
      
    
    {\displaystyle y}
   is purely a function of this linear combination of the known observations.
It is often useful to see this conclusion in geometrical terms: the criterion of an input 
  
    
      
        
          
            
              x
              →
            
          
        
      
    
    {\displaystyle {\vec {x}}}
   being in a class 
  
    
      
        y
      
    
    {\displaystyle y}
   is purely a function of projection of multidimensional-space point 
  
    
      
        
          
            
              x
              →
            
          
        
      
    
    {\displaystyle {\vec {x}}}
   onto vector 
  
    
      
        
          
            
              w
              →
            
          
        
      
    
    {\displaystyle {\vec {w}}}
   (thus, we only consider its direction). In other words, the observation belongs to 
  
    
      
        y
      
    
    {\displaystyle y}
   if corresponding 
  
    
      
        
          
            
              x
              →
            
          
        
      
    
    {\displaystyle {\vec {x}}}
   is located on a certain side of a hyperplane perpendicular to 
  
    
      
        
          
            
              w
              →
            
          
        
      
    
    {\displaystyle {\vec {w}}}
  . The location of the plane is defined by the threshold c.

Assumptions
The assumptions of discriminant analysis are the same as those for MANOVA. The analysis is quite sensitive to outliers and the size of the smallest group must be larger than the number of predictor variables.
Multivariate normality: Independent variables are normal for each level of the grouping variable.
Homogeneity of variance/covariance (homoscedasticity): Variances among group variables are the same across levels of predictors. Can be tested with Box's M statistic. It has been suggested, however, that linear discriminant analysis be used when covariances are equal, and that quadratic discriminant analysis may be used when covariances are not equal.
Multicollinearity: Predictive power can decrease with an increased correlation between predictor variables.
Independence: Participants are assumed to be randomly sampled, and a participant's score on one variable is assumed to be independent of scores on that variable for all other participants.It has been suggested that discriminant analysis is relatively robust to slight violations of these assumptions, and it has also been shown that discriminant analysis may still be reliable when using dichotomous variables (where multivariate normality is often violated).

Discriminant functions
Discriminant analysis works by creating one or more linear combinations of predictors, creating a new latent variable for each function. These functions are called discriminant functions. The number of functions possible is either 
  
    
      
        
          N
          
            g
          
        
        −
        1
      
    
    {\displaystyle N_{g}-1}
   where 
  
    
      
        
          N
          
            g
          
        
      
    
    {\displaystyle N_{g}}
   = number of groups, or 
  
    
      
        p
      
    
    {\displaystyle p}
   (the number of predictors), whichever is smaller. The first function created maximizes the differences between groups on that function. The second function maximizes differences on that function, but also must not be correlated with the previous function. This continues with subsequent functions with the requirement that the new function not be correlated with any of the previous functions.
Given group 
  
    
      
        j
      
    
    {\displaystyle j}
  , with  
  
    
      
        
          
            R
          
          
            j
          
        
      
    
    {\displaystyle \mathbb {R} _{j}}
    sets of sample space, there is a discriminant rule such that if 
  
    
      
        x
        ∈
        
          
            R
          
          
            j
          
        
      
    
    {\displaystyle x\in \mathbb {R} _{j}}
  , then 
  
    
      
        x
        ∈
        j
      
    
    {\displaystyle x\in j}
  . Discriminant analysis then, finds “good” regions of  
  
    
      
        
          
            R
          
          
            j
          
        
      
    
    {\displaystyle \mathbb {R} _{j}}
   to minimize classification error, therefore leading to a high percent correct classified in the classification table.Each function is given a discriminant score to determine how well it predicts group placement. 

Structure Correlation Coefficients: The correlation between each predictor and the discriminant score of each function. This is a zero-order correlation (i.e., not corrected for the other predictors).
Standardized Coefficients: Each predictor's weight in the linear combination that is the discriminant function.  Like in a regression equation, these coefficients are partial (i.e., corrected for the other predictors). Indicates the unique contribution of each predictor in predicting group assignment.
Functions at Group Centroids: Mean discriminant scores for each grouping variable are given for each function. The farther apart the means are, the less error there will be in classification.

Discrimination rules
Maximum likelihood: Assigns x to the group that maximizes population (group) density.
Bayes Discriminant Rule: Assigns x to the group that maximizes 
  
    
      
        
          π
          
            i
          
        
        
          f
          
            i
          
        
        (
        x
        )
      
    
    {\displaystyle \pi _{i}f_{i}(x)}
  , where πi represents the prior probability of that classification, and 
  
    
      
        
          f
          
            i
          
        
        (
        x
        )
      
    
    {\displaystyle f_{i}(x)}
   represents the population density.
Fisher's linear discriminant rule: Maximizes the ratio between SSbetween and SSwithin, and finds a linear combination of the predictors to predict group.

Eigenvalues
An eigenvalue in discriminant analysis is the characteristic root of each function. It is an indication of how well that function differentiates the groups, where the larger the eigenvalue, the better the function differentiates. This however, should be interpreted with caution, as eigenvalues have no upper limit.
	The eigenvalue can be viewed as a ratio of SSbetween and SSwithin as in ANOVA when the dependent variable is the discriminant function, and the groups are the levels of the IV. This means that the largest eigenvalue is associated with the first function, the second largest with the second, etc..

Effect size
Some suggest the use of eigenvalues as effect size measures, however, this is generally not supported. Instead, the canonical correlation is the preferred measure of effect size. It is similar to the eigenvalue, but is the square root of the ratio of SSbetween and SStotal. It is the correlation between groups and the function. 
	Another popular measure of effect size is the percent of variance for each function.  This is calculated by: (λx/Σλi) X 100 where λx is the eigenvalue for the function and Σλi is the sum of all eigenvalues. This tells us how strong the prediction is for that particular function compared to the others. 
	Percent correctly classified can also be analyzed as an effect size. The kappa value can describe this while correcting for chance agreement.Kappa normalizes across all categorizes rather than biased by a significantly good or poorly performing classes.

Canonical discriminant analysis for k classes
Canonical discriminant analysis (CDA) finds axes (k − 1 canonical coordinates, k being the number of classes) that best separate the categories. These linear functions are uncorrelated and define, in effect, an optimal k − 1 space through the n-dimensional cloud of data that best separates (the projections in that space of) the k groups. See “Multiclass LDA” for details below.

Fisher's linear discriminant
The terms Fisher's linear discriminant and LDA are often used interchangeably, although Fisher's original article actually describes a slightly different discriminant, which does not make some of the assumptions of LDA such as normally distributed classes or equal class covariances.
Suppose two classes of observations have means 
  
    
      
        
          
            
              
                μ
                →
              
            
          
          
            0
          
        
        ,
        
          
            
              
                μ
                →
              
            
          
          
            1
          
        
      
    
    {\displaystyle {\vec {\mu }}_{0},{\vec {\mu }}_{1}}
   and covariances 
  
    
      
        
          Σ
          
            0
          
        
        ,
        
          Σ
          
            1
          
        
      
    
    {\displaystyle \Sigma _{0},\Sigma _{1}}
  . Then the linear combination of features 
  
    
      
        
          
            
              w
              →
            
          
        
        ⋅
        
          
            
              x
              →
            
          
        
      
    
    {\displaystyle {\vec {w}}\cdot {\vec {x}}}
   will have means 
  
    
      
        
          
            
              w
              →
            
          
        
        ⋅
        
          
            
              
                μ
                →
              
            
          
          
            i
          
        
      
    
    {\displaystyle {\vec {w}}\cdot {\vec {\mu }}_{i}}
   and variances 
  
    
      
        
          
            
              
                w
                →
              
            
          
          
            T
          
        
        
          Σ
          
            i
          
        
        
          
            
              w
              →
            
          
        
      
    
    {\displaystyle {\vec {w}}^{T}\Sigma _{i}{\vec {w}}}
   for 
  
    
      
        i
        =
        0
        ,
        1
      
    
    {\displaystyle i=0,1}
  . Fisher defined the separation between these two distributions to be the ratio of the variance between the classes to the variance within the classes:

  
    
      
        S
        =
        
          
            
              σ
              
                between
              
              
                2
              
            
            
              σ
              
                within
              
              
                2
              
            
          
        
        =
        
          
            
              (
              
                
                  
                    w
                    →
                  
                
              
              ⋅
              
                
                  
                    
                      μ
                      →
                    
                  
                
                
                  1
                
              
              −
              
                
                  
                    w
                    →
                  
                
              
              ⋅
              
                
                  
                    
                      μ
                      →
                    
                  
                
                
                  0
                
              
              
                )
                
                  2
                
              
            
            
              
                
                  
                    
                      w
                      →
                    
                  
                
                
                  T
                
              
              
                Σ
                
                  1
                
              
              
                
                  
                    w
                    →
                  
                
              
              +
              
                
                  
                    
                      w
                      →
                    
                  
                
                
                  T
                
              
              
                Σ
                
                  0
                
              
              
                
                  
                    w
                    →
                  
                
              
            
          
        
        =
        
          
            
              (
              
                
                  
                    w
                    →
                  
                
              
              ⋅
              (
              
                
                  
                    
                      μ
                      →
                    
                  
                
                
                  1
                
              
              −
              
                
                  
                    
                      μ
                      →
                    
                  
                
                
                  0
                
              
              )
              
                )
                
                  2
                
              
            
            
              
                
                  
                    
                      w
                      →
                    
                  
                
                
                  T
                
              
              (
              
                Σ
                
                  0
                
              
              +
              
                Σ
                
                  1
                
              
              )
              
                
                  
                    w
                    →
                  
                
              
            
          
        
      
    
    {\displaystyle S={\frac {\sigma _{\text{between}}^{2}}{\sigma _{\text{within}}^{2}}}={\frac {({\vec {w}}\cdot {\vec {\mu }}_{1}-{\vec {w}}\cdot {\vec {\mu }}_{0})^{2}}{{\vec {w}}^{T}\Sigma _{1}{\vec {w}}+{\vec {w}}^{T}\Sigma _{0}{\vec {w}}}}={\frac {({\vec {w}}\cdot ({\vec {\mu }}_{1}-{\vec {\mu }}_{0}))^{2}}{{\vec {w}}^{T}(\Sigma _{0}+\Sigma _{1}){\vec {w}}}}}
  This measure is, in some sense, a measure of the signal-to-noise ratio for the class labelling. It can be shown that the maximum separation occurs when

  
    
      
        
          
            
              w
              →
            
          
        
        ∝
        (
        
          Σ
          
            0
          
        
        +
        
          Σ
          
            1
          
        
        
          )
          
            −
            1
          
        
        (
        
          
            
              
                μ
                →
              
            
          
          
            1
          
        
        −
        
          
            
              
                μ
                →
              
            
          
          
            0
          
        
        )
      
    
    {\displaystyle {\vec {w}}\propto (\Sigma _{0}+\Sigma _{1})^{-1}({\vec {\mu }}_{1}-{\vec {\mu }}_{0})}
  When the assumptions of LDA are satisfied, the above equation is equivalent to LDA.
Be sure to note that the vector 
  
    
      
        
          
            
              w
              →
            
          
        
      
    
    {\displaystyle {\vec {w}}}
   is the normal to the discriminant hyperplane. As an example, in a two dimensional problem, the line that best divides the two groups is perpendicular to 
  
    
      
        
          
            
              w
              →
            
          
        
      
    
    {\displaystyle {\vec {w}}}
  .
Generally, the data points to be discriminated are projected onto 
  
    
      
        
          
            
              w
              →
            
          
        
      
    
    {\displaystyle {\vec {w}}}
  ; then the threshold that best separates the data is chosen from analysis of the one-dimensional distribution. There is no general rule for the threshold. However, if projections of points from both classes exhibit approximately the same distributions, a good choice would be the hyperplane between projections of the two means, 
  
    
      
        
          
            
              w
              →
            
          
        
        ⋅
        
          
            
              
                μ
                →
              
            
          
          
            0
          
        
      
    
    {\displaystyle {\vec {w}}\cdot {\vec {\mu }}_{0}}
   and 
  
    
      
        
          
            
              w
              →
            
          
        
        ⋅
        
          
            
              
                μ
                →
              
            
          
          
            1
          
        
      
    
    {\displaystyle {\vec {w}}\cdot {\vec {\mu }}_{1}}
  . In this case the parameter c in threshold condition 
  
    
      
        
          
            
              w
              →
            
          
        
        ⋅
        
          
            
              x
              →
            
          
        
        >
        c
      
    
    {\displaystyle {\vec {w}}\cdot {\vec {x}}>c}
   can be found explicitly:

  
    
      
        c
        =
        
          
            
              w
              →
            
          
        
        ⋅
        
          
            1
            2
          
        
        (
        
          
            
              
                μ
                →
              
            
          
          
            0
          
        
        +
        
          
            
              
                μ
                →
              
            
          
          
            1
          
        
        )
        =
        
          
            1
            2
          
        
        
          
            
              
                μ
                →
              
            
          
          
            1
          
          
            T
          
        
        
          Σ
          
            1
          
          
            −
            1
          
        
        
          
            
              
                μ
                →
              
            
          
          
            1
          
        
        −
        
          
            1
            2
          
        
        
          
            
              
                μ
                →
              
            
          
          
            0
          
          
            T
          
        
        
          Σ
          
            0
          
          
            −
            1
          
        
        
          
            
              
                μ
                →
              
            
          
          
            0
          
        
      
    
    {\displaystyle c={\vec {w}}\cdot {\frac {1}{2}}({\vec {\mu }}_{0}+{\vec {\mu }}_{1})={\frac {1}{2}}{\vec {\mu }}_{1}^{T}\Sigma _{1}^{-1}{\vec {\mu }}_{1}-{\frac {1}{2}}{\vec {\mu }}_{0}^{T}\Sigma _{0}^{-1}{\vec {\mu }}_{0}}
  .Otsu's method is related to Fisher's linear discriminant, and was created to binarize the histogram of pixels in a grayscale image by optimally picking the black/white threshold that minimizes intra-class variance and maximizes inter-class variance within/between grayscales assigned to black and white pixel classes.

Multiclass LDA
In the case where there are more than two classes, the analysis used in the derivation of the Fisher discriminant can be extended to find a subspace which appears to contain all of the class variability. This generalization is due to C. R. Rao. Suppose that each of C classes has a mean 
  
    
      
        
          μ
          
            i
          
        
      
    
    {\displaystyle \mu _{i}}
   and the same covariance 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
  . Then the scatter between class variability may be defined by the sample covariance of the class means

  
    
      
        
          Σ
          
            b
          
        
        =
        
          
            1
            C
          
        
        
          ∑
          
            i
            =
            1
          
          
            C
          
        
        (
        
          μ
          
            i
          
        
        −
        μ
        )
        (
        
          μ
          
            i
          
        
        −
        μ
        
          )
          
            T
          
        
      
    
    {\displaystyle \Sigma _{b}={\frac {1}{C}}\sum _{i=1}^{C}(\mu _{i}-\mu )(\mu _{i}-\mu )^{T}}
  where 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is the mean of the class means. The class separation in a direction 
  
    
      
        
          
            
              w
              →
            
          
        
      
    
    {\displaystyle {\vec {w}}}
   in this case will be given by

  
    
      
        S
        =
        
          
            
              
                
                  
                    
                      w
                      →
                    
                  
                
                
                  T
                
              
              
                Σ
                
                  b
                
              
              
                
                  
                    w
                    →
                  
                
              
            
            
              
                
                  
                    
                      w
                      →
                    
                  
                
                
                  T
                
              
              Σ
              
                
                  
                    w
                    →
                  
                
              
            
          
        
      
    
    {\displaystyle S={\frac {{\vec {w}}^{T}\Sigma _{b}{\vec {w}}}{{\vec {w}}^{T}\Sigma {\vec {w}}}}}
  This means that when 
  
    
      
        
          
            
              w
              →
            
          
        
      
    
    {\displaystyle {\vec {w}}}
   is an eigenvector of 
  
    
      
        
          Σ
          
            −
            1
          
        
        
          Σ
          
            b
          
        
      
    
    {\displaystyle \Sigma ^{-1}\Sigma _{b}}
   the separation will be equal to the corresponding eigenvalue.
If 
  
    
      
        
          Σ
          
            −
            1
          
        
        
          Σ
          
            b
          
        
      
    
    {\displaystyle \Sigma ^{-1}\Sigma _{b}}
   is diagonalizable, the variability between features will be contained in the subspace spanned by the eigenvectors corresponding to the C − 1 largest eigenvalues (since 
  
    
      
        
          Σ
          
            b
          
        
      
    
    {\displaystyle \Sigma _{b}}
   is of rank C − 1 at most). These eigenvectors are primarily used in feature reduction, as in PCA. The eigenvectors corresponding to the smaller eigenvalues will tend to be very sensitive to the exact choice of training data, and it is often necessary to use regularisation as described in the next section.
If classification is required, instead of dimension reduction, there are a number of alternative techniques available. For instance, the classes may be partitioned, and a standard Fisher discriminant or LDA used to classify each partition. A common example of this is ""one against the rest"" where the points from one class are put in one group, and everything else in the other, and then LDA applied. This will result in C classifiers, whose results are combined. Another common
method is pairwise classification, where a new classifier is created for each pair of classes (giving C(C − 1)/2 classifiers in total), with the individual classifiers combined to produce a final classification.

Incremental LDA
The typical implementation of the LDA technique requires that all the samples are available in advance. However,  there are situations where the entire data set is not available and the input data are observed as a stream. In this case, it is desirable for the LDA feature extraction to have the ability to update the computed LDA features by observing the new samples without running the algorithm on the whole data set. For example, in many real-time applications such as mobile robotics or on-line face recognition, it is important to update the extracted LDA features as soon as new observations are available. An LDA feature extraction technique that can update the LDA features by simply observing new samples is an incremental LDA algorithm, and this idea has been extensively studied over the last two decades. Chatterjee and Roychowdhury proposed an incremental self-organized LDA algorithm for updating the LDA features. In other work, Demir and Ozmehmet proposed online local learning algorithms for updating LDA features incrementally using error-correcting and the Hebbian learning rules. Later, Aliyari et al. derived fast incremental algorithms to update the LDA features by observing the new samples.

Practical use
In practice, the class means and covariances are not known. They can, however, be estimated from the training set. Either the maximum likelihood estimate or the maximum a posteriori estimate may be used in place of the exact value in the above equations. Although the estimates of the covariance may be considered optimal in some sense, this does not mean that the resulting discriminant obtained by substituting these values is optimal in any sense, even if the assumption of normally distributed classes is correct.
Another complication in applying LDA and Fisher's discriminant to real data occurs when the number of measurements of each sample (i.e., the dimensionality of each data vector) exceeds the number of samples in each class. In this case, the covariance estimates do not have full rank, and so cannot be inverted. There are a number of ways to deal with this. One is to use a pseudo inverse instead of the usual matrix inverse in the above formulae. However, better numeric stability may be achieved by first projecting the problem onto the subspace spanned by 
  
    
      
        
          Σ
          
            b
          
        
      
    
    {\displaystyle \Sigma _{b}}
  .
Another strategy to deal with small sample size is to use a shrinkage estimator of the covariance matrix, which
can be expressed mathematically as

  
    
      
        Σ
        =
        (
        1
        −
        λ
        )
        Σ
        +
        λ
        I
        
      
    
    {\displaystyle \Sigma =(1-\lambda )\Sigma +\lambda I\,}
  where 
  
    
      
        I
      
    
    {\displaystyle I}
   is the identity matrix, and 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   is the shrinkage intensity or regularisation parameter.
This leads to the framework of regularized discriminant analysis or shrinkage discriminant analysis.Also, in many practical cases linear discriminants are not suitable. LDA and Fisher's discriminant can be extended for use in non-linear classification via the kernel trick. Here, the original observations are effectively mapped into a higher dimensional non-linear space. Linear classification in this non-linear space is then equivalent to non-linear classification in the original space. The most commonly used example of this is the kernel Fisher discriminant.
LDA can be generalized to multiple discriminant analysis, where c becomes a categorical variable with N possible states, instead of only two. Analogously, if the class-conditional densities 
  
    
      
        p
        (
        
          
            
              x
              →
            
          
        
        ∣
        c
        =
        i
        )
      
    
    {\displaystyle p({\vec {x}}\mid c=i)}
   are normal with shared covariances, the sufficient statistic for 
  
    
      
        P
        (
        c
        ∣
        
          
            
              x
              →
            
          
        
        )
      
    
    {\displaystyle P(c\mid {\vec {x}})}
   are the values of N projections, which are the subspace spanned by the N means, affine projected by the inverse covariance matrix. These projections can be found by solving a generalized eigenvalue problem, where the numerator is the covariance matrix formed by treating the means as the samples, and the denominator is the shared covariance matrix. See “Multiclass LDA” above for details.

Applications
In addition to the examples given below, LDA is applied in positioning and product management.

Bankruptcy prediction
In bankruptcy prediction based on accounting ratios and other financial variables, linear discriminant analysis was the first statistical method applied to systematically explain which firms entered bankruptcy vs. survived. Despite limitations including known nonconformance of accounting ratios to the normal distribution assumptions of LDA, Edward Altman's 1968 model is still a leading model in practical applications.

Face recognition
In computerised face recognition, each face is represented by a large number of pixel values. Linear discriminant analysis is primarily used here to reduce the number of features to a more manageable number before classification. Each of the new dimensions is a linear combination of pixel values, which form a template. The linear combinations obtained using Fisher's linear discriminant are called Fisher faces, while those obtained using the related principal component analysis are called eigenfaces.

Marketing
In marketing, discriminant analysis was once often used to determine the factors which distinguish different types of customers and/or products on the basis of surveys or other forms of collected data. Logistic regression or other methods are now more commonly used. The use of discriminant analysis in marketing can be described by the following steps:

Formulate the problem and gather data—Identify the salient attributes consumers use to evaluate products in this category—Use quantitative marketing research techniques (such as surveys) to collect data from a sample of potential customers concerning their ratings of all the product attributes. The data collection stage is usually done by marketing research professionals. Survey questions ask the respondent to rate a product from one to five (or 1 to 7, or 1 to 10) on a range of attributes chosen by the researcher. Anywhere from five to twenty attributes are chosen. They could include things like: ease of use, weight, accuracy, durability, colourfulness, price, or size. The attributes chosen will vary depending on the product being studied. The same question is asked about all the products in the study. The data for multiple products is codified and input into a statistical program such as R, SPSS or SAS. (This step is the same as in Factor analysis).
Estimate the Discriminant Function Coefficients and determine the statistical significance and validity—Choose the appropriate discriminant analysis method. The direct method involves estimating the discriminant function so that all the predictors are assessed simultaneously. The stepwise method enters the predictors sequentially. The two-group method should be used when the dependent variable has two categories or states. The multiple discriminant method is used when the dependent variable has three or more categorical states. Use Wilks's Lambda to test for significance in SPSS or F stat in SAS. The most common method used to test validity is to split the sample into an estimation or analysis sample, and a validation or holdout sample. The estimation sample is used in constructing the discriminant function. The validation sample is used to construct a classification matrix which contains the number of correctly classified and incorrectly classified cases. The percentage of correctly classified cases is called the hit ratio.
Plot the results on a two dimensional map, define the dimensions, and interpret the results. The statistical program (or a related module) will map the results. The map will plot each product (usually in two-dimensional space). The distance of products to each other indicate either how different they are. The dimensions must be labelled by the researcher. This requires subjective judgement and is often very challenging. See perceptual mapping.

Biomedical studies
The main application of discriminant analysis in medicine is the assessment of severity state of a patient and prognosis of disease outcome. For example, during retrospective analysis, patients are divided into groups according to severity of disease – mild, moderate and severe form. Then results of clinical and laboratory analyses are studied in order to reveal variables which are statistically different in studied groups. Using these variables, discriminant functions are built which help to objectively classify disease in a future patient into mild, moderate or severe form.
In biology, similar principles are used in order to classify and define groups of different biological objects, for example, to define phage types of Salmonella enteritidis based on Fourier transform infrared spectra, to detect animal source of Escherichia coli studying its virulence factors etc.

Earth science
This method can be used to separate the alteration zones. For example, when different data from various zones are available, discriminant analysis can find the pattern within the data and classify it effectively.

Comparison to logistic regression
Discriminant function analysis is very similar to logistic regression, and both can be used to answer the same research questions. Logistic regression does not have as many assumptions and restrictions as discriminant analysis. However, when discriminant analysis’ assumptions are met, it is more powerful than logistic regression. Unlike logistic regression, discriminant analysis can be used with small sample sizes. It has been shown that when sample sizes are equal, and homogeneity of variance/covariance holds, discriminant analysis is more accurate. Despite all these advantages, logistic regression has none-the-less become the common choice, since the assumptions of discriminant analysis are rarely met.

Linear discriminant in high dimension
Geometric anomalities in high dimension lead to the well-known curse of dimensionality. Nevertheless, proper utilization of concentration of measure phenomena can make computation easier. An important case of these  blessing of dimensionality phenomena was highlighted by Donoho and Tanner: if a sample is essentially high-dimensional then each point can be separated from the rest of the sample by linear inequality, with high probability, even for exponentially large samples. These linear inequalities can be selected in the standard (Fisher's) form of the linear discriminant for a rich family of probability distribution. In particular, such theorems are proven for log-concave distributions including multidimensional normal distribution (the proof is based on the concentration inequalities for log-concave measures) and for product measures on a multidimensional cube (this is proven using Talagrand's concentration inequality for product probability spaces). Data separability by classical linear discriminants simplifies the problem of error correction for artificial intelligence systems in high dimension.

See also
Data mining
Decision tree learning
Factor analysis
Kernel Fisher discriminant analysis
Logit (for logistic regression)
Linear regression
Multiple discriminant analysis
Multidimensional scaling
Pattern recognition
Preference regression
Quadratic classifier
Statistical classification

References
Further reading
Duda, R. O.; Hart, P. E.; Stork, D. H. (2000). Pattern Classification (2nd ed.). Wiley Interscience. ISBN 978-0-471-05669-0. MR 1802993.
Hilbe, J. M. (2009). Logistic Regression Models. Chapman & Hall/CRC Press. ISBN 978-1-4200-7575-5.
Mika, S.;  et al. (1999). ""Fisher Discriminant Analysis with Kernels"". Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468). IEEE Conference on Neural Networks for Signal Processing IX. pp. 41–48. CiteSeerX 10.1.1.35.9904. doi:10.1109/NNSP.1999.788121. ISBN 978-0-7803-5673-3. S2CID 8473401.
McFarland, H. Richard; Donald, St. P. Richards (2001). ""Exact Misclassification Probabilities for Plug-In Normal Quadratic Discriminant Functions. I. The Equal-Means Case"". Journal of Multivariate Analysis. 77 (1): 21–53. doi:10.1006/jmva.2000.1924.
McFarland, H. Richard; Donald, St. P. Richards (2002). ""Exact Misclassification Probabilities for Plug-In Normal Quadratic Discriminant Functions. II. The Heterogeneous Case"". Journal of Multivariate Analysis. 82 (2): 299–330. doi:10.1006/jmva.2001.2034.
Haghighat, M.; Abdel-Mottaleb, M.; Alhalabi, W. (2016). ""Discriminant Correlation Analysis: Real-Time Feature Level Fusion for Multimodal Biometric Recognition"". IEEE Transactions on Information Forensics and Security. 11 (9): 1984–1996. doi:10.1109/TIFS.2016.2569061. S2CID 15624506.

External links
Discriminant Correlation Analysis (DCA) of the Haghighat article (see above)
ALGLIB contains open-source LDA implementation in C# / C++ / Pascal / VBA.
LDA in Python- LDA implementation in Python
LDA tutorial using MS Excel
Biomedical statistics. Discriminant analysis
StatQuest: Linear Discriminant Analysis (LDA) clearly explained on YouTube
Course notes, Discriminant function analysis by G. David Garson, NC State University
Discriminant analysis tutorial in Microsoft Excel by Kardi Teknomo
Course notes, Discriminant function analysis by David W. Stockburger, Missouri State University
Discriminant function analysis (DA) by John Poulsen and Aaron French, San Francisco State University",https://en.wikipedia.org/wiki/Linear_discriminant_analysis,"['Articles with short description', 'CS1 maint: archived copy as title', 'Classification algorithms', 'Market research', 'Market segmentation', 'Short description is different from Wikidata', 'Statistical classification', 'Wikipedia articles needing clarification from April 2012', 'Wikipedia articles needing clarification from April 2019', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with multiple identifiers']",Data Science
123,Knowledge,"Knowledge is a familiarity, awareness, or understanding of someone or something, such as facts (propositional knowledge), skills (procedural knowledge), or objects (acquaintance knowledge). By most accounts, knowledge can be acquired in many different ways and from many sources, including but not limited to perception, reason, memory, testimony, scientific inquiry, education, and practice. The philosophical study of knowledge is called epistemology.
The term ""knowledge"" can refer to a theoretical or practical understanding of a subject. It can be implicit (as with practical skill or expertise) or explicit (as with the theoretical understanding of a subject); formal or informal; systematic or particular. The philosopher Plato famously pointed out the need for a distinction between knowledge and true belief in the Theaetetus, leading many to attribute to him a definition of knowledge as ""justified true belief"". The difficulties with this definition raised by the Gettier problem have been the subject of extensive debate in epistemology for more than half a century.

Theories of knowledge
The eventual demarcation of philosophy from science was made possible by the notion that philosophy's core was ""theory of knowledge,"" a theory distinct from the sciences because it was their foundation... Without this idea of a ""theory of knowledge,"" it is hard to imagine what ""philosophy"" could have been in the age of modern science.
Knowledge is the primary subject of the field of epistemology, which studies what we know, how we come to know it, and what it means to know something.The definition of knowledge is a matter of ongoing debate among epistemologists. The classical definition, described but not ultimately endorsed by Plato, specifies that a statement must meet three criteria in order to be considered knowledge: it must be justified, true, and believed. Epistemologists today generally agree that these conditions are not sufficient, as various Gettier cases are thought to demonstrate. There are a number of alternative definitions which have been proposed, including Robert Nozick's proposal that all instances of knowledge must 'track the truth' and Simon Blackburn's proposal that those who have a justified true belief 'through a defect, flaw, or failure' fail to have knowledge. Richard Kirkham suggests that our definition of knowledge requires that the evidence for the belief necessitates its truth.In contrast to this approach, Ludwig Wittgenstein observed, following Moore's paradox, that one can say ""He believes it, but it isn't so,"" but not ""He knows it, but it isn't so."" He goes on to argue that these do not correspond to distinct mental states, but rather to distinct ways of talking about conviction. What is different here is not the mental state of the speaker, but the activity in which they are engaged. For example, on this account, to know that the kettle is boiling is not to be in a particular state of mind, but to perform a particular task with the statement that the kettle is boiling. Wittgenstein sought to bypass the difficulty of definition by looking to the way ""knowledge"" is used in natural languages. He saw knowledge as a case of a family resemblance. Following this idea, ""knowledge"" has been reconstructed as a cluster concept that points out relevant features but that is not adequately captured by any definition.

Self-knowledge
“Self-knowledge” usually refers to a person's knowledge of their own sensations, thoughts, beliefs, and other mental states. A number of questions regarding self-knowledge have been the subject of extensive debates in philosophy, including whether self-knowledge differs from other types of knowledge, whether we have privileged self-knowledge compared to knowledge of other minds, and the nature of our acquaintance with ourselves. David Hume famously expressed skepticism about whether we could ever have self-knowledge over and above our immediate awareness of a ""bundle of perceptions"", which was part of his broader skepticism about personal identity.

The value of knowledge
It is generally assumed that knowledge is more valuable than mere true belief. If so, what is the explanation? A formulation of the value problem in epistemology first occurs in Plato's Meno. Socrates points out to Meno that a man who knew the way to Larissa could lead others there correctly. But so, too, could a man who had true beliefs about how to get there, even if he had not gone there or had any knowledge of Larissa. Socrates says that it seems that both knowledge and true opinion can guide action. Meno then wonders why knowledge is valued more than true belief and why knowledge and true belief are different. Socrates responds that knowledge is more valuable than mere true belief because it is tethered or justified. Justification, or working out the reason for a true belief, locks down true belief.The problem is to identify what (if anything) makes knowledge more valuable than mere true belief, or that makes knowledge more valuable than a mere minimal conjunction of its components, such as justification, safety, sensitivity, statistical likelihood, and anti-Gettier conditions, on a particular analysis of knowledge that conceives of knowledge as divided into components (to which knowledge-first epistemological theories, which posit knowledge as fundamental, are notable exceptions). The value problem re-emerged in the philosophical literature on epistemology in the twenty-first century following the rise of virtue epistemology in the 1980s, partly because of the obvious link to the concept of value in ethics.In contemporary philosophy, epistemologists including Ernest Sosa, John Greco, Jonathan Kvanvig, Linda Zagzebski, and Duncan Pritchard have defended virtue epistemology as a solution to the value problem. They argue that epistemology should also evaluate the ""properties"" of people as epistemic agents (i.e. intellectual virtues), rather than merely the properties of propositions and propositional mental attitudes.

Scientific knowledge
The development of the scientific method has made a significant contribution to how knowledge of the physical world and its phenomena is acquired. To be termed scientific, a method of inquiry must be based on gathering observable and measurable evidence subject to specific principles of reasoning and experimentation. The scientific method consists of the collection of data through observation and experimentation, and the formulation and testing of hypotheses. Science, and the nature of scientific knowledge have also become the subject of philosophy. As science itself has developed, scientific knowledge now includes a broader usage in the soft sciences such as biology and the social sciences – discussed elsewhere as meta-epistemology, or genetic epistemology, and to some extent related to ""theory of cognitive development"". Note that ""epistemology"" is the study of knowledge and how it is acquired. Science is ""the process used everyday to logically complete thoughts through inference of facts determined by calculated experiments."" Sir Francis Bacon was critical in the historical development of the scientific method; his works established and popularized an inductive methodology for scientific inquiry. His famous aphorism, ""knowledge is power"", is found in the Meditations Sacrae (1597).Until recent times, at least in the Western tradition, it was simply taken for granted that knowledge was something possessed only by humans – and probably adult humans at that. Sometimes the notion might stretch to Society-as-such, as in (e. g.) ""the knowledge possessed by the Coptic culture"" (as opposed to its individual members), but that was not assured either. Nor was it usual to consider unconscious knowledge in any systematic way until this approach was popularized by Freud.Other biological domains where ""knowledge"" might be said to reside, include: (iii) the immune system, and (iv) in the DNA of the genetic code. See the list of four ""epistemological domains"": Popper, (1975); and Traill (2008: Table S, p. 31) – also references by both to Niels Jerne.
Such considerations seem to call for a separate definition of ""knowledge"" to cover the biological systems. For biologists, knowledge must be usefully available to the system, though that system need not be conscious. Thus the criteria seem to be:

The system should apparently be dynamic and self-organizing (unlike a mere book on its own).
The knowledge must constitute some sort of representation of ""the outside world"", or ways of dealing with it (directly or indirectly).
Some way must exist for the system to access this information quickly enough for it to be useful.Those who use the phrase ""scientific knowledge"" don't necessary claim to certainty, since scientists will never be absolutely certain when they are correct and when they are not. It is thus an irony of proper scientific method that one must doubt even when correct, in the hopes that this practice will lead to greater convergence on the truth in general.

Situated knowledge
Situated knowledge is knowledge specific to a particular situation. It was used by Donna Haraway as an extension of the feminist approaches of ""successor science"" suggested by Sandra Harding, one which ""offers a more adequate, richer, better account of a world, in order to live in it well and in critical, reflexive relation to our own as well as others' practices of domination and the unequal parts of privilege and oppression that makes up all positions."" This situation partially transforms science into a narrative, which Arturo Escobar explains as, ""neither fictions nor supposed facts."" This narrative of situation is historical textures woven of fact and fiction, and as Escobar explains further, ""even the most neutral scientific domains are narratives in this sense,"" insisting that rather than a purpose dismissing science as a trivial matter of contingency, ""it is to treat (this narrative) in the most serious way, without succumbing to its mystification as 'the truth' or to the ironic skepticism common to many critiques.""Haraway's argument stems from the limitations of the human perception, as well as the overemphasis of the sense of vision in science. According to Haraway, vision in science has been, ""used to signify a leap out of the marked body and into a conquering gaze from nowhere."" This is the ""gaze that mythically inscribes all the marked bodies, that makes the unmarked category claim the power to see and not be seen, to represent while escaping representation."" This causes a limitation of views in the position of science itself as a potential player in the creation of knowledge, resulting in a position of ""modest witness"". This is what Haraway terms a ""god trick"", or the aforementioned representation while escaping representation. In order to avoid this, ""Haraway perpetuates a tradition of thought which emphasizes the importance of the subject in terms of both ethical and political accountability"".Some methods of generating knowledge, such as trial and error, or learning from experience, tend to create highly situational knowledge.
Situational knowledge is often embedded in language, culture, or traditions. This integration of situational knowledge is an allusion to the community, and its attempts at collecting subjective perspectives into an embodiment ""of views from somewhere.""  Knowledge is also said to be related to the capacity of acknowledgement in human beings.Even though Haraway's arguments are largely based on feminist studies, this idea of different worlds, as well as the skeptic stance of situated knowledge is present in the main arguments of post-structuralism. Fundamentally, both argue the contingency of knowledge on the presence of history; power, and geography, as well as the rejection of universal rules or laws or elementary structures; and the idea of power as an inherited trait of objectification.

Partial knowledge
One discipline of epistemology focuses on partial knowledge. In most cases, it is not possible to understand an information domain exhaustively; our knowledge is always incomplete or partial. Most real problems have to be solved by taking advantage of a partial understanding of the problem context and problem data, unlike the typical math problems one might solve at school, where all data is given and one is given a complete understanding of formulas necessary to solve them (False consensus effect).
This idea is also present in the concept of bounded rationality which assumes that in real-life situations people often have a limited amount of information and make decisions accordingly.

Religious accounts of knowledge
In many expressions of Christianity, such as Catholicism and Anglicanism, knowledge is one of the seven gifts of the Holy Spirit.The Old Testament's tree of the knowledge of good and evil contained the knowledge that separated Man from God: ""And the LORD God said, Behold, the man is become as one of us, to know good and evil..."" (Genesis 3:22)
In Gnosticism, divine knowledge or gnosis is hoped to be attained.
विद्या दान (Vidya Daan) i.e. knowledge sharing is a major part of Daan, a tenet of all Dharmic Religions.Hindu Scriptures present two kinds of knowledge, Paroksh Gyan and Prataksh Gyan. Paroksh Gyan (also spelled Paroksha-Jnana) is secondhand knowledge: knowledge obtained from books, hearsay, etc. Pratyaksh Gyan (also spelled Pratyaksha-Jnana) is the knowledge borne of direct experience, i.e., knowledge that one discovers for oneself. Jnana yoga (""path of knowledge"") is one of three main types of yoga expounded by Krishna in the Bhagavad Gita. (It is compared and contrasted with Bhakti Yoga and Karma yoga.)
In Islam, knowledge (Arabic: علم, ʿilm) is given great significance. ""The Knowing"" (al-ʿAlīm) is one of the 99 names reflecting distinct attributes of God. The Qur'an asserts that knowledge comes from God (2:239) and various hadith encourage the acquisition of knowledge. Muhammad is reported to have said ""Seek knowledge from the cradle to the grave"" and ""Verily the men of knowledge are the inheritors of the prophets"". Islamic scholars, theologians and jurists are often given the title alim, meaning ""knowledgeble"".In Jewish tradition, knowledge (Hebrew: דעת da'ath) is considered one of the most valuable traits a person can acquire. Observant Jews recite three times a day in the Amidah ""Favor us with knowledge, understanding and discretion that come from you. Exalted are you, Existent-One, the gracious giver of knowledge."" The Tanakh states, ""A wise man gains power, and a man of knowledge maintains power"", and ""knowledge is chosen above gold"".

See also
Omniscience
Outline of knowledge – guide to the subject of knowledge presented as a tree structured list of its subtopics.
Outline of human intelligence - list of subtopics in tree structure
Analytic-synthetic distinction
Decolonization of knowledge
Desacralization of knowledge
Descriptive knowledge
Epistemic modal logic
Inductive inference
Inductive probability
Intelligence
Metaknowledge
Philosophical skepticism
Procedural knowledge
Society for the Diffusion of Useful Knowledge

References
External links

Knowledge at PhilPapers
""Knowledge"". Internet Encyclopedia of Philosophy.
Zalta, Edward N. (ed.). ""The Value of Knowledge"". Stanford Encyclopedia of Philosophy.
Zalta, Edward N. (ed.). ""The Analysis of Knowledge"". Stanford Encyclopedia of Philosophy.
Zalta, Edward N. (ed.). ""Knowledge by Acquaintance vs. Description"". Stanford Encyclopedia of Philosophy.
Knowledge at the Indiana Philosophy Ontology Project",https://en.wikipedia.org/wiki/Knowledge,"['All articles needing additional references', 'All articles with dead external links', 'Articles containing German-language text', 'Articles needing additional references from June 2020', 'Articles with Internet Encyclopedia of Philosophy links', 'Articles with dead external links from November 2016', 'Articles with long short description', 'Articles with short description', 'CS1 maint: archived copy as title', 'Concepts in epistemology', 'Harv and Sfn no-target errors', 'Intelligence', 'Knowledge', 'Main topic articles', 'Mind', 'Pages containing links to subscription-only content', 'Pages using Sister project links with default search', 'Short description is different from Wikidata', 'Use dmy dates from May 2020', 'Virtue', 'Wikipedia articles needing page number citations from March 2019', 'Wikipedia articles with GND identifiers', 'Wikipedia indefinitely move-protected pages', 'Wikipedia indefinitely semi-protected pages']",Data Science
124,Linear regression,"In statistics, linear regression is a linear approach to modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.
Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.
Linear regression has many practical uses. Most applications fall into one of the following two broad categories:

If the goal is prediction, forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.
If the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response.Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the ""lack of fit"" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms ""least squares"" and ""linear model"" are closely linked, they are not synonymous.

Introduction
Given a data set 
  
    
      
        {
        
          y
          
            i
          
        
        ,
        
        
          x
          
            i
            1
          
        
        ,
        …
        ,
        
          x
          
            i
            p
          
        
        
          }
          
            i
            =
            1
          
          
            n
          
        
      
    
    {\displaystyle \{y_{i},\,x_{i1},\ldots ,x_{ip}\}_{i=1}^{n}}
   of n statistical units, a linear regression model assumes that the relationship between the dependent variable y and the p-vector of regressors x is linear. This relationship is modeled through a disturbance term or error variable ε — an unobserved random variable that adds ""noise"" to the linear relationship between the dependent variable and regressors. Thus the model takes the form

  
    
      
        
          y
          
            i
          
        
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          x
          
            i
            1
          
        
        +
        ⋯
        +
        
          β
          
            p
          
        
        
          x
          
            i
            p
          
        
        +
        
          ε
          
            i
          
        
        =
        
          
            x
          
          
            i
          
          
            
              T
            
          
        
        
          β
        
        +
        
          ε
          
            i
          
        
        ,
        
        i
        =
        1
        ,
        …
        ,
        n
        ,
      
    
    {\displaystyle y_{i}=\beta _{0}+\beta _{1}x_{i1}+\cdots +\beta _{p}x_{ip}+\varepsilon _{i}=\mathbf {x} _{i}^{\mathsf {T}}{\boldsymbol {\beta }}+\varepsilon _{i},\qquad i=1,\ldots ,n,}
  where T denotes the transpose, so that xiTβ is the inner product between vectors xi and β.
Often these n equations are stacked together and written in matrix notation as

  
    
      
        
          y
        
        =
        X
        
          β
        
        +
        
          ε
        
        ,
        
      
    
    {\displaystyle \mathbf {y} =X{\boldsymbol {\beta }}+{\boldsymbol {\varepsilon }},\,}
  where

  
    
      
        
          y
        
        =
        
          
            (
            
              
                
                  
                    y
                    
                      1
                    
                  
                
              
              
                
                  
                    y
                    
                      2
                    
                  
                
              
              
                
                  ⋮
                
              
              
                
                  
                    y
                    
                      n
                    
                  
                
              
            
            )
          
        
        ,
        
      
    
    {\displaystyle \mathbf {y} ={\begin{pmatrix}y_{1}\\y_{2}\\\vdots \\y_{n}\end{pmatrix}},\quad }
  
  
    
      
        X
        =
        
          
            (
            
              
                
                  
                    
                      x
                    
                    
                      1
                    
                    
                      
                        T
                      
                    
                  
                
              
              
                
                  
                    
                      x
                    
                    
                      2
                    
                    
                      
                        T
                      
                    
                  
                
              
              
                
                  ⋮
                
              
              
                
                  
                    
                      x
                    
                    
                      n
                    
                    
                      
                        T
                      
                    
                  
                
              
            
            )
          
        
        =
        
          
            (
            
              
                
                  1
                
                
                  
                    x
                    
                      11
                    
                  
                
                
                  ⋯
                
                
                  
                    x
                    
                      1
                      p
                    
                  
                
              
              
                
                  1
                
                
                  
                    x
                    
                      21
                    
                  
                
                
                  ⋯
                
                
                  
                    x
                    
                      2
                      p
                    
                  
                
              
              
                
                  ⋮
                
                
                  ⋮
                
                
                  ⋱
                
                
                  ⋮
                
              
              
                
                  1
                
                
                  
                    x
                    
                      n
                      1
                    
                  
                
                
                  ⋯
                
                
                  
                    x
                    
                      n
                      p
                    
                  
                
              
            
            )
          
        
        ,
      
    
    {\displaystyle X={\begin{pmatrix}\mathbf {x} _{1}^{\mathsf {T}}\\\mathbf {x} _{2}^{\mathsf {T}}\\\vdots \\\mathbf {x} _{n}^{\mathsf {T}}\end{pmatrix}}={\begin{pmatrix}1&x_{11}&\cdots &x_{1p}\\1&x_{21}&\cdots &x_{2p}\\\vdots &\vdots &\ddots &\vdots \\1&x_{n1}&\cdots &x_{np}\end{pmatrix}},}
  
  
    
      
        
          β
        
        =
        
          
            (
            
              
                
                  
                    β
                    
                      0
                    
                  
                
              
              
                
                  
                    β
                    
                      1
                    
                  
                
              
              
                
                  
                    β
                    
                      2
                    
                  
                
              
              
                
                  ⋮
                
              
              
                
                  
                    β
                    
                      p
                    
                  
                
              
            
            )
          
        
        ,
        
        
          ε
        
        =
        
          
            (
            
              
                
                  
                    ε
                    
                      1
                    
                  
                
              
              
                
                  
                    ε
                    
                      2
                    
                  
                
              
              
                
                  ⋮
                
              
              
                
                  
                    ε
                    
                      n
                    
                  
                
              
            
            )
          
        
        .
      
    
    {\displaystyle {\boldsymbol {\beta }}={\begin{pmatrix}\beta _{0}\\\beta _{1}\\\beta _{2}\\\vdots \\\beta _{p}\end{pmatrix}},\quad {\boldsymbol {\varepsilon }}={\begin{pmatrix}\varepsilon _{1}\\\varepsilon _{2}\\\vdots \\\varepsilon _{n}\end{pmatrix}}.}
  Some remarks on notation and terminology:

  
    
      
        
          y
        
      
    
    {\displaystyle \mathbf {y} }
   is a vector of observed values 
  
    
      
        
          y
          
            i
          
        
         
        (
        i
        =
        1
        ,
        …
        ,
        n
        )
      
    
    {\displaystyle y_{i}\ (i=1,\ldots ,n)}
   of the variable called the regressand, endogenous variable, response variable, measured variable, criterion variable, or dependent variable. This variable is also sometimes known as the predicted variable, but this should not be confused with predicted values, which are denoted 
  
    
      
        
          
            
              y
              ^
            
          
        
      
    
    {\displaystyle {\hat {y}}}
  . The decision as to which variable in a data set is modeled as the dependent variable and which are modeled as the independent variables may be based on a presumption that the value of one of the variables is caused by, or directly influenced by the other variables. Alternatively, there may be an operational reason to model one of the variables in terms of the others, in which case there need be no presumption of causality.

  
    
      
        X
      
    
    {\displaystyle X}
   may be seen as a matrix of row-vectors 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   or of n-dimensional column-vectors 
  
    
      
        
          X
          
            j
          
        
      
    
    {\displaystyle X_{j}}
  , which are known as regressors, exogenous variables, explanatory variables, covariates, input variables, predictor variables, or independent variables (not to be confused with the concept of independent random variables). The matrix 
  
    
      
        X
      
    
    {\displaystyle X}
   is sometimes called the design matrix.
Usually a constant is included as one of the regressors. In particular, 
  
    
      
        
          
            x
          
          
            i
            0
          
        
        =
        1
      
    
    {\displaystyle \mathbf {x} _{i0}=1}
   for 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=1,\ldots ,n}
  . The corresponding element of β is called the intercept. Many statistical inference procedures for linear models require an intercept to be present, so it is often included even if theoretical considerations suggest that its value should be zero.
Sometimes one of the regressors can be a non-linear function of another regressor or of the data, as in polynomial regression and segmented regression. The model remains linear as long as it is linear in the parameter vector β.
The values xij may be viewed as either observed values of random variables Xj or as fixed values chosen prior to observing the dependent variable. Both interpretations may be appropriate in different cases, and they generally lead to the same estimation procedures; however different approaches to asymptotic analysis are used in these two situations.

  
    
      
        
          β
        
      
    
    {\displaystyle {\boldsymbol {\beta }}}
   is a 
  
    
      
        (
        p
        +
        1
        )
      
    
    {\displaystyle (p+1)}
  -dimensional parameter vector, where 
  
    
      
        
          β
          
            0
          
        
      
    
    {\displaystyle \beta _{0}}
   is the intercept term (if one is included in the model—otherwise 
  
    
      
        
          β
        
      
    
    {\displaystyle {\boldsymbol {\beta }}}
   is p-dimensional). Its elements are known as effects or regression coefficients (although the latter term is sometimes reserved for the estimated effects). Statistical estimation and inference in linear regression focuses on β. The elements of this parameter vector are interpreted as the partial derivatives of the dependent variable with respect to the various independent variables.

  
    
      
        
          ε
        
      
    
    {\displaystyle {\boldsymbol {\varepsilon }}}
   is a vector of values 
  
    
      
        
          ε
          
            i
          
        
      
    
    {\displaystyle \varepsilon _{i}}
  . This part of the model is called the error term, disturbance term, or sometimes noise (in contrast with the ""signal"" provided by the rest of the model). This variable captures all other factors which influence the dependent variable y other than the regressors x. The relationship between the error term and the regressors, for example their correlation, is a crucial consideration in formulating a linear regression model, as it will determine the appropriate estimation method.Fitting a linear model to a given data set usually requires estimating the regression coefficients  
  
    
      
        
          β
        
      
    
    {\displaystyle {\boldsymbol {\beta }}}
   such that the error term 
  
    
      
        
          ε
        
        =
        
          y
        
        −
        X
        
          β
        
      
    
    {\displaystyle {\boldsymbol {\varepsilon }}=\mathbf {y} -X{\boldsymbol {\beta }}}
   is minimized. For example, it is common to use the sum of squared errors 
  
    
      
        
          |
        
        
          |
        
        
          ε
        
        
          |
        
        
          |
        
      
    
    {\displaystyle ||{\boldsymbol {\varepsilon }}||}
   as the quantity of the fit.
Example. Consider a situation where a small ball is being tossed up in the air and then we measure its heights of ascent hi at various moments in time ti. Physics tells us that, ignoring the drag, the relationship can be modeled as

  
    
      
        
          h
          
            i
          
        
        =
        
          β
          
            1
          
        
        
          t
          
            i
          
        
        +
        
          β
          
            2
          
        
        
          t
          
            i
          
          
            2
          
        
        +
        
          ε
          
            i
          
        
        ,
      
    
    {\displaystyle h_{i}=\beta _{1}t_{i}+\beta _{2}t_{i}^{2}+\varepsilon _{i},}
  where β1 determines the initial velocity of the ball, β2 is proportional to the standard gravity, and εi is due to measurement errors. Linear regression can be used to estimate the values of β1 and β2 from the measured data. This model is non-linear in the time variable, but it is linear in the parameters β1 and β2; if we take regressors xi = (xi1, xi2)  = (ti, ti2), the model takes on the standard form

  
    
      
        
          h
          
            i
          
        
        =
        
          
            x
          
          
            i
          
          
            
              T
            
          
        
        
          β
        
        +
        
          ε
          
            i
          
        
        .
      
    
    {\displaystyle h_{i}=\mathbf {x} _{i}^{\mathsf {T}}{\boldsymbol {\beta }}+\varepsilon _{i}.}

Assumptions
Standard linear regression models with standard estimation techniques make a number of assumptions about the predictor variables, the response variables and their relationship.  Numerous extensions have been developed that allow each of these assumptions to be relaxed (i.e. reduced to a weaker form), and in some cases eliminated entirely. Generally these extensions make the estimation procedure more complex and time-consuming, and may also require more data in order to produce an equally precise model.

The following are the major assumptions made by standard linear regression models with standard estimation techniques (e.g. ordinary least squares):

Weak exogeneity.  This essentially means that the predictor variables x can be treated as fixed values, rather than random variables.  This means, for example, that the predictor variables are assumed to be error-free—that is, not contaminated with measurement errors. Although this assumption is not realistic in many settings, dropping it leads to significantly more difficult errors-in-variables models.
Linearity.  This means that the mean of the response variable is a linear combination of the parameters (regression coefficients) and the predictor variables.  Note that this assumption is much less restrictive than it may at first seem.  Because the predictor variables are treated as fixed values (see above), linearity is really only a restriction on the parameters.  The predictor variables themselves can be arbitrarily transformed, and in fact multiple copies of the same underlying predictor variable can be added, each one transformed differently.  This technique is used, for example, in polynomial regression, which uses linear regression to fit the response variable as an arbitrary polynomial function (up to a given rank) of a predictor variable. With this much flexibility, models such as polynomial regression often have ""too much power"", in that they tend to overfit the data.  As a result, some kind of regularization must typically be used to prevent unreasonable solutions coming out of the estimation process.  Common examples are ridge regression and lasso regression.  Bayesian linear regression can also be used, which by its nature is more or less immune to the problem of overfitting. (In fact, ridge regression and lasso regression can both be viewed as    special cases of Bayesian linear regression, with particular types of prior distributions placed on the regression coefficients.)
Constant variance (a.k.a. homoscedasticity).  This means that different values of the response variable have the same variance in their errors, regardless of the values of the predictor variables. In practice this assumption is invalid (i.e. the errors are heteroscedastic) if the response variable can vary over a wide scale. In order to check for heterogeneous error variance, or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a ""fanning effect"" between residual error and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predictive variables. Errors will not be evenly distributed across the regression line. Heteroscedasticity will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear regression line, and the mean squared error for the model will be wrong. Typically, for example, a response variable whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000 (a standard deviation of around $20,000), while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, which would imply their actual income would vary anywhere between -$10,000 and $30,000. (In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression estimation methods give less precise parameter estimates and misleading inferential quantities such as standard errors when substantial heteroscedasticity is present. However, various estimation techniques (e.g. weighted least squares and heteroscedasticity-consistent standard errors) can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g. fit the logarithm of the response variable using a linear regression model, which implies that the response variable has a log-normal distribution rather than a normal distribution).
Independence of errors.  This assumes that the errors of the response variables are uncorrelated with each other. (Actual statistical independence is a stronger condition than mere lack of correlation and is often not needed, although it can be exploited if it is known to hold.) Some methods (e.g. generalized least squares) are capable of handling correlated errors, although they typically require significantly more data unless some sort of regularization is used to bias the model towards assuming uncorrelated errors. Bayesian linear regression is a general way of handling this issue.
Lack of perfect multicollinearity in the predictors.  For standard least squares estimation methods, the design matrix X must have full column rank p; otherwise, we have a condition known as perfect multicollinearity in the predictor variables.  This can be triggered by having two or more perfectly correlated predictor variables (e.g. if the same predictor variable is mistakenly given twice, either without transforming one of the copies or by transforming one of the copies linearly). It can also happen if there is too little data available compared to the number of parameters to be estimated (e.g. fewer data points than regression coefficients). In the case of perfect multicollinearity, the parameter vector β will be non-identifiable—it has no unique solution.  At most we will be able to identify some of the parameters, i.e. narrow down its value to some linear subspace of Rp. See partial least squares regression.  Methods for fitting linear models with multicollinearity have been developed; some require additional assumptions such as ""effect sparsity""—that a large fraction of the effects are exactly zero. Note that the more computationally expensive iterated algorithms for parameter estimation, such as those used in generalized linear models, do not suffer from this problem.Beyond these assumptions, several other statistical properties of the data strongly influence the performance of different estimation methods:

The statistical relationship between the error terms and the regressors plays an important role in determining whether an estimation procedure has desirable sampling properties such as being unbiased and consistent.
The arrangement, or probability distribution of the predictor variables x has a major influence on the precision of estimates of β. Sampling and design of experiments are highly developed subfields of statistics that provide guidance for collecting data in such a way to achieve a precise estimate of β.

Interpretation
A fitted linear regression model can be used to identify the relationship between a single predictor variable xj and the response variable y when all the other predictor variables in the model are ""held fixed"". Specifically, the interpretation of βj is the expected change in y for a one-unit change in xj when the other covariates are held fixed—that is, the expected value of the partial derivative of y with respect to xj. This is sometimes called the unique effect of xj on y. In contrast, the marginal effect of xj on y can be assessed using a correlation coefficient or simple linear regression model relating only xj to y; this effect is the total derivative of y with respect to xj.
Care must be taken when interpreting regression results, as some of the regressors may not allow for marginal changes (such as dummy variables, or the intercept term), while others cannot be held fixed (recall the example from the introduction: it would be impossible to ""hold ti fixed"" and at the same time change the value of ti2).
It is possible that the unique effect can be nearly zero even when the marginal effect is large. This may imply that some other covariate captures all the information in xj, so that once that variable is in the model, there is no contribution of xj to the variation in y. Conversely, the unique effect of xj can be large while its marginal effect is nearly zero. This would happen if the other covariates explained a great deal of the variation of y, but they mainly explain variation in a way that is complementary to what is captured by xj. In this case, including the other variables in the model reduces the part of the variability of y that is unrelated to xj, thereby strengthening the apparent relationship with xj.
The meaning of the expression ""held fixed"" may depend on how the values of the predictor variables arise. If the experimenter directly sets the values of the predictor variables according to a study design, the comparisons of interest may literally correspond to comparisons among units whose predictor variables have been ""held fixed"" by the experimenter. Alternatively, the expression ""held fixed"" can refer to a selection that takes place in the context of data analysis. In this case, we ""hold a variable fixed"" by restricting our attention to the subsets of the data that happen to have a common value for the given predictor variable. This is the only interpretation of ""held fixed"" that can be used in an observational study.
The notion of a ""unique effect"" is appealing when studying a complex system where multiple interrelated components influence the response variable. In some cases, it can literally be interpreted as the causal effect of an intervention that is linked to the value of a predictor variable. However, it has been argued that in many cases multiple regression analysis fails to clarify the relationships between the predictor variables and the response variable when the predictors are correlated with each other and are not assigned following a study design. Commonality analysis may be helpful in disentangling the shared and unique impacts of correlated independent variables.

Extensions
Numerous extensions of linear regression have been developed, which allow some or all of the assumptions underlying the basic model to be relaxed.

Simple and multiple linear regression
The very simplest case of a single scalar predictor variable x and a single scalar response variable y is known as simple linear regression.  The extension to multiple and/or vector-valued predictor variables (denoted with a capital X) is known as multiple linear regression, also known as multivariable linear regression.
Multiple linear regression is a generalization of simple linear regression to the case of more than one independent variable, and a special case of general linear models, restricted to one dependent variable. The basic model for multiple linear regression is

  
    
      
        
          Y
          
            i
          
        
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          X
          
            i
            1
          
        
        +
        
          β
          
            2
          
        
        
          X
          
            i
            2
          
        
        +
        …
        +
        
          β
          
            p
          
        
        
          X
          
            i
            p
          
        
        +
        
          ϵ
          
            i
          
        
      
    
    {\displaystyle Y_{i}=\beta _{0}+\beta _{1}X_{i1}+\beta _{2}X_{i2}+\ldots +\beta _{p}X_{ip}+\epsilon _{i}}
  for each observation i = 1, ... , n.
In the formula above we consider n observations of one dependent variable and p independent variables. Thus, Yi is the ith observation of the dependent variable, Xij is ith observation of the jth independent variable, j = 1, 2, ..., p. The values βj represent parameters to be estimated, and εi is the ith independent identically distributed normal error.
In the more general multivariate linear regression, there is one equation of the above form for each of m > 1 dependent variables that share the same set of explanatory variables and hence are estimated simultaneously with each other:

  
    
      
        
          Y
          
            i
            j
          
        
        =
        
          β
          
            0
            j
          
        
        +
        
          β
          
            1
            j
          
        
        
          X
          
            i
            1
          
        
        +
        
          β
          
            2
            j
          
        
        
          X
          
            i
            2
          
        
        +
        …
        +
        
          β
          
            p
            j
          
        
        
          X
          
            i
            p
          
        
        +
        
          ϵ
          
            i
            j
          
        
      
    
    {\displaystyle Y_{ij}=\beta _{0j}+\beta _{1j}X_{i1}+\beta _{2j}X_{i2}+\ldots +\beta _{pj}X_{ip}+\epsilon _{ij}}
  for all observations indexed as i = 1, ... , n and for all dependent variables indexed as j = 1, ... , m.
Nearly all real-world regression models involve multiple predictors, and basic descriptions of linear regression are often phrased in terms of the multiple regression model.  Note, however, that in these cases the response variable y is still a scalar. Another term, multivariate linear regression, refers to cases where y is a vector, i.e., the same as general linear regression.

General linear models
The general linear model considers the situation when the response variable is not a scalar (for each observation) but a vector, yi. Conditional linearity of 
  
    
      
        E
        (
        
          y
        
        ∣
        
          
            x
          
          
            i
          
        
        )
        =
        
          
            x
          
          
            i
          
          
            
              T
            
          
        
        B
      
    
    {\displaystyle E(\mathbf {y} \mid \mathbf {x} _{i})=\mathbf {x} _{i}^{\mathsf {T}}B}
   is still assumed, with a matrix B replacing the vector β of the classical linear regression model. Multivariate analogues of ordinary least squares (OLS) and generalized least squares (GLS) have been developed. ""General linear models"" are also called ""multivariate linear models"". These are not the same as multivariable linear models (also called ""multiple linear models"").

Heteroscedastic models
Various models have been created that allow for heteroscedasticity, i.e. the errors for different response variables may have different variances.  For example, weighted least squares is a method for estimating linear regression models when the response variables may have different error variances, possibly with correlated errors. (See also Weighted linear least squares, and Generalized least squares.) Heteroscedasticity-consistent standard errors is an improved method for use with uncorrelated but potentially heteroscedastic errors.

Generalized linear models
Generalized linear models (GLMs) are a framework for modeling response variables that are bounded or discrete. This is used, for example:

when modeling positive quantities (e.g. prices or populations) that vary over a large scale—which are better described using a skewed distribution such as the log-normal distribution or Poisson distribution (although GLMs are not used for log-normal data, instead the response variable is simply transformed using the logarithm function);
when modeling categorical data, such as the choice of a given candidate in an election (which is better described using a Bernoulli distribution/binomial distribution for binary choices, or a categorical distribution/multinomial distribution for multi-way choices), where there are a fixed number of choices that cannot be meaningfully ordered;
when modeling ordinal data, e.g. ratings on a scale from 0 to 5, where the different outcomes can be ordered but where the quantity itself may not have any absolute meaning (e.g. a rating of 4 may not be ""twice as good"" in any objective sense as a rating of 2, but simply indicates that it is better than 2 or 3 but not as good as 5).Generalized linear models allow for an arbitrary link function, g, that relates the mean of the response variable(s) to the predictors: 
  
    
      
        E
        (
        Y
        )
        =
        
          g
          
            −
            1
          
        
        (
        X
        B
        )
      
    
    {\displaystyle E(Y)=g^{-1}(XB)}
  . The link function is often related to the distribution of the response, and in particular it typically has the effect of transforming between the 
  
    
      
        (
        −
        ∞
        ,
        ∞
        )
      
    
    {\displaystyle (-\infty ,\infty )}
   range of the linear predictor and the range of the response variable.
Some common examples of GLMs are:

Poisson regression for count data.
Logistic regression and probit regression for binary data.
Multinomial logistic regression and multinomial probit regression for categorical data.
Ordered logit and ordered probit regression for ordinal data.Single index models allow some degree of nonlinearity in the relationship between x and y, while preserving the central role of the linear predictor β′x as in the classical linear regression model. Under certain conditions, simply applying OLS to data from a single-index model will consistently estimate β up to a proportionality constant.

Hierarchical linear models
Hierarchical linear models (or multilevel regression) organizes the data into a hierarchy of regressions, for example where A is regressed on B, and B is regressed on C. It is often used where the variables of interest have a natural hierarchical structure such as in educational statistics, where students are nested in classrooms, classrooms are nested in schools, and schools are nested in some administrative grouping, such as a school district. The response variable might be a measure of student achievement such as a test score, and different covariates would be collected at the classroom, school, and school district levels.

Errors-in-variables
Errors-in-variables models (or ""measurement error models"") extend the traditional linear regression model to allow the predictor variables X to be observed with error. This error causes standard estimators of β to become biased. Generally, the form of bias is an attenuation, meaning that the effects are biased toward zero.

Others
In Dempster–Shafer theory, or a linear belief function in particular, a linear regression model may be represented as a partially swept matrix, which can be combined with similar matrices representing observations and other assumed normal distributions and state equations. The combination of swept or unswept matrices provides an alternative method for estimating linear regression models.

Estimation methods
A large number of procedures have been developed for parameter estimation and inference in linear regression. These methods differ in computational simplicity of algorithms, presence of a closed-form solution, robustness with respect to heavy-tailed distributions, and theoretical assumptions needed to validate desirable statistical properties such as consistency and asymptotic efficiency.
Some of the more common estimation techniques for linear regression are summarized below.

Least-squares estimation and related techniques
Assuming that the independent variable is 
  
    
      
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        =
        
          [
          
            
              x
              
                1
              
              
                i
              
            
            ,
            
              x
              
                2
              
              
                i
              
            
            ,
            …
            ,
            
              x
              
                m
              
              
                i
              
            
          
          ]
        
      
    
    {\displaystyle {\vec {x_{i}}}=\left[x_{1}^{i},x_{2}^{i},\ldots ,x_{m}^{i}\right]}
   and the model's parameters are 
  
    
      
        
          
            
              β
              →
            
          
        
        =
        
          [
          
            
              β
              
                0
              
            
            ,
            
              β
              
                1
              
            
            ,
            …
            ,
            
              β
              
                m
              
            
          
          ]
        
      
    
    {\displaystyle {\vec {\beta }}=\left[\beta _{0},\beta _{1},\ldots ,\beta _{m}\right]}
  , then the model's prediction would be 
  
    
      
        
          y
          
            i
          
        
        ≈
        
          β
          
            0
          
        
        +
        
          ∑
          
            j
            =
            1
          
          
            m
          
        
        
          β
          
            j
          
        
        ×
        
          x
          
            j
          
          
            i
          
        
      
    
    {\displaystyle y_{i}\approx \beta _{0}+\sum _{j=1}^{m}\beta _{j}\times x_{j}^{i}}
  . If 
  
    
      
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
      
    
    {\displaystyle {\vec {x_{i}}}}
   is extended to  
  
    
      
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        =
        
          [
          
            1
            ,
            
              x
              
                1
              
              
                i
              
            
            ,
            
              x
              
                2
              
              
                i
              
            
            ,
            …
            ,
            
              x
              
                m
              
              
                i
              
            
          
          ]
        
      
    
    {\displaystyle {\vec {x_{i}}}=\left[1,x_{1}^{i},x_{2}^{i},\ldots ,x_{m}^{i}\right]}
   then 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
   would become a dot product of the parameter and the independent variable, i.e. 
  
    
      
        
          y
          
            i
          
        
        ≈
        
          ∑
          
            j
            =
            0
          
          
            m
          
        
        
          β
          
            j
          
        
        ×
        
          x
          
            j
          
          
            i
          
        
        =
        
          
            
              β
              →
            
          
        
        
        
        .
        
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
      
    
    {\displaystyle y_{i}\approx \sum _{j=0}^{m}\beta _{j}\times x_{j}^{i}={\vec {\beta }}\,\,.\,{\vec {x_{i}}}}
  . In the least-squares setting, the optimum parameter is defined as such that minimizes the sum of mean squared loss: 

  
    
      
        
          
            
              
                
                  β
                  ^
                
              
              →
            
          
        
        =
        
          
            
              arg min
            
            
              
                β
                →
              
            
          
        
        
        L
        
          (
          
            D
            ,
            
              
                
                  β
                  →
                
              
            
          
          )
        
        =
        
          
            
              arg min
            
            
              
                β
                →
              
            
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          
            (
            
              
                
                  
                    β
                    →
                  
                
              
              
              .
              
              
                
                  
                    
                      x
                      
                        i
                      
                    
                    →
                  
                
              
              −
              
                y
                
                  i
                
              
            
            )
          
          
            2
          
        
      
    
    {\displaystyle {\vec {\hat {\beta }}}={\underset {\vec {\beta }}{\mbox{arg min}}}\,L\left(D,{\vec {\beta }}\right)={\underset {\vec {\beta }}{\mbox{arg min}}}\sum _{i=1}^{n}\left({\vec {\beta }}\,.\,{\vec {x_{i}}}-y_{i}\right)^{2}}
  Now putting the independent and dependent variables in matrices 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   respectively, the loss function can be rewritten as:

  
    
      
        
          
            
              
                L
                
                  (
                  
                    D
                    ,
                    
                      
                        
                          β
                          →
                        
                      
                    
                  
                  )
                
              
              
                
                =
                ‖
                X
                
                  
                    
                      β
                      →
                    
                  
                
                −
                Y
                
                  ‖
                  
                    2
                  
                
              
            
            
              
              
                
                =
                
                  
                    (
                    
                      X
                      
                        
                          
                            β
                            →
                          
                        
                      
                      −
                      Y
                    
                    )
                  
                  
                    
                      T
                    
                  
                
                
                  (
                  
                    X
                    
                      
                        
                          β
                          →
                        
                      
                    
                    −
                    Y
                  
                  )
                
              
            
            
              
              
                
                =
                
                  Y
                  
                    
                      T
                    
                  
                
                Y
                −
                
                  Y
                  
                    
                      T
                    
                  
                
                X
                
                  
                    
                      β
                      →
                    
                  
                
                −
                
                  
                    
                      
                        β
                        →
                      
                    
                  
                  
                    
                      T
                    
                  
                
                
                  X
                  
                    
                      T
                    
                  
                
                Y
                +
                
                  
                    
                      
                        β
                        →
                      
                    
                  
                  
                    
                      T
                    
                  
                
                
                  X
                  
                    
                      T
                    
                  
                
                X
                
                  
                    
                      β
                      →
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}L\left(D,{\vec {\beta }}\right)&=\|X{\vec {\beta }}-Y\|^{2}\\&=\left(X{\vec {\beta }}-Y\right)^{\textsf {T}}\left(X{\vec {\beta }}-Y\right)\\&=Y^{\textsf {T}}Y-Y^{\textsf {T}}X{\vec {\beta }}-{\vec {\beta }}^{\textsf {T}}X^{\textsf {T}}Y+{\vec {\beta }}^{\textsf {T}}X^{\textsf {T}}X{\vec {\beta }}\end{aligned}}}
  As the loss is convex the optimum solution lies at gradient zero. The gradient of the loss function is (using Denominator layout convention):

  
    
      
        
          
            
              
                
                  
                    
                      ∂
                      L
                      
                        (
                        
                          D
                          ,
                          
                            
                              
                                β
                                →
                              
                            
                          
                        
                        )
                      
                    
                    
                      ∂
                      
                        
                          
                            β
                            →
                          
                        
                      
                    
                  
                
              
              
                
                =
                
                  
                    
                      ∂
                      
                        (
                        
                          
                            Y
                            
                              
                                T
                              
                            
                          
                          Y
                          −
                          
                            Y
                            
                              
                                T
                              
                            
                          
                          X
                          
                            
                              
                                β
                                →
                              
                            
                          
                          −
                          
                            
                              
                                
                                  β
                                  →
                                
                              
                            
                            
                              
                                T
                              
                            
                          
                          
                            X
                            
                              
                                T
                              
                            
                          
                          Y
                          +
                          
                            
                              
                                
                                  β
                                  →
                                
                              
                            
                            
                              
                                T
                              
                            
                          
                          
                            X
                            
                              
                                T
                              
                            
                          
                          X
                          
                            
                              
                                β
                                →
                              
                            
                          
                        
                        )
                      
                    
                    
                      ∂
                      
                        
                          
                            β
                            →
                          
                        
                      
                    
                  
                
              
            
            
              
              
                
                =
                −
                2
                
                  X
                  
                    
                      T
                    
                  
                
                Y
                +
                2
                
                  X
                  
                    
                      T
                    
                  
                
                X
                
                  
                    
                      β
                      →
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\frac {\partial L\left(D,{\vec {\beta }}\right)}{\partial {\vec {\beta }}}}&={\frac {\partial \left(Y^{\textsf {T}}Y-Y^{\textsf {T}}X{\vec {\beta }}-{\vec {\beta }}^{\textsf {T}}X^{\textsf {T}}Y+{\vec {\beta }}^{\textsf {T}}X^{\textsf {T}}X{\vec {\beta }}\right)}{\partial {\vec {\beta }}}}\\&=-2X^{\textsf {T}}Y+2X^{\textsf {T}}X{\vec {\beta }}\end{aligned}}}
  Setting the gradient to zero produces the optimum parameter:

  
    
      
        
          
            
              
                −
                2
                
                  X
                  
                    
                      T
                    
                  
                
                Y
                +
                2
                
                  X
                  
                    
                      T
                    
                  
                
                X
                
                  
                    
                      β
                      →
                    
                  
                
              
              
                
                =
                0
              
            
            
              
                ⇒
                
                  X
                  
                    
                      T
                    
                  
                
                Y
              
              
                
                =
                
                  X
                  
                    
                      T
                    
                  
                
                X
                
                  
                    
                      β
                      →
                    
                  
                
              
            
            
              
                ⇒
                
                  
                    
                      
                        
                          β
                          ^
                        
                      
                      →
                    
                  
                
              
              
                
                =
                
                  
                    (
                    
                      
                        X
                        
                          
                            T
                          
                        
                      
                      X
                    
                    )
                  
                  
                    −
                    1
                  
                
                
                  X
                  
                    
                      T
                    
                  
                
                Y
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}-2X^{\textsf {T}}Y+2X^{\textsf {T}}X{\vec {\beta }}&=0\\\Rightarrow X^{\textsf {T}}Y&=X^{\textsf {T}}X{\vec {\beta }}\\\Rightarrow {\vec {\hat {\beta }}}&=\left(X^{\textsf {T}}X\right)^{-1}X^{\textsf {T}}Y\end{aligned}}}
  Note: To prove that the 
  
    
      
        
          
            
              β
              ^
            
          
        
      
    
    {\displaystyle {\hat {\beta }}}
   obtained is indeed the local minimum, one needs to differentiate once more to obtain the Hessian matrix and show that it is positive definite. This is provided by the Gauss–Markov theorem.
Linear least squares methods include mainly:

Ordinary least squares
Weighted least squares
Generalized least squares

Maximum-likelihood estimation and related techniques
Maximum likelihood estimation can be performed when the distribution of the error terms is known to belong to a certain parametric family ƒθ of probability distributions. When fθ is a normal distribution with zero mean and variance θ, the resulting estimate is identical to the OLS estimate. GLS estimates are maximum likelihood estimates when ε follows a multivariate normal distribution with a known covariance matrix.
Ridge regression and other forms of penalized estimation, such as Lasso regression, deliberately introduce bias into the estimation of β in order to reduce the variability of the estimate. The resulting estimates generally have lower mean squared error than the OLS estimates, particularly when multicollinearity is present or when overfitting is a problem. They are generally used when the goal is to predict the value of the response variable y for values of the predictors x that have not yet been observed. These methods are not as commonly used when the goal is inference, since it is difficult to account for the bias.
Least absolute deviation (LAD) regression is a robust estimation technique in that it is less sensitive to the presence of outliers than OLS (but is less efficient than OLS when no outliers are present). It is equivalent to maximum likelihood estimation under a Laplace distribution model for ε.
Adaptive estimation. If we assume that error terms are independent of the regressors, 
  
    
      
        
          ε
          
            i
          
        
        ⊥
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \varepsilon _{i}\perp \mathbf {x} _{i}}
  , then the optimal estimator is the 2-step MLE, where the first step is used to non-parametrically estimate the distribution of the error term.

Other estimation techniques
Bayesian linear regression applies the framework of Bayesian statistics to linear regression. (See also Bayesian multivariate linear regression.) In particular, the regression coefficients β are assumed to be random variables with a specified prior distribution.  The prior distribution can bias the solutions for the regression coefficients, in a way similar to (but more general than) ridge regression or lasso regression.  In addition, the Bayesian estimation process produces not a single point estimate for the ""best"" values of the regression coefficients but an entire posterior distribution, completely describing the uncertainty surrounding the quantity.  This can be used to estimate the ""best"" coefficients using the mean, mode, median, any quantile (see quantile regression), or any other function of the posterior distribution.
Quantile regression focuses on the conditional quantiles of y given X rather than the conditional mean of y given X. Linear quantile regression models a particular conditional quantile, for example the conditional median, as a linear function βTx of the predictors.
Mixed models are widely used to analyze linear regression relationships involving dependent data when the dependencies have a known structure. Common applications of mixed models include analysis of data involving repeated measurements, such as longitudinal data, or data obtained from cluster sampling. They are generally fit as parametric models, using maximum likelihood or Bayesian estimation. In the case where the errors are modeled as normal random variables, there is a close connection between mixed models and generalized least squares. Fixed effects estimation is an alternative approach to analyzing this type of data.
Principal component regression (PCR) is used when the number of predictor variables is large, or when strong correlations exist among the predictor variables. This two-stage procedure first reduces the predictor variables using principal component analysis then uses the reduced variables in an OLS regression fit. While it often works well in practice, there is no general theoretical reason that the most informative linear function of the predictor variables should lie among the dominant principal components of the multivariate distribution of the predictor variables. The partial least squares regression is the extension of the PCR method which does not suffer from the mentioned deficiency.
Least-angle regression is an estimation procedure for linear regression models that was developed to handle high-dimensional covariate vectors, potentially with more covariates than observations.
The Theil–Sen estimator is a simple robust estimation technique that chooses the slope of the fit line to be the median of the slopes of the lines through pairs of sample points. It has similar statistical efficiency properties to simple linear regression but is much less sensitive to outliers.
Other robust estimation techniques, including the α-trimmed mean approach, and L-, M-, S-, and R-estimators have been introduced.

Applications
Linear regression is widely used in biological, behavioral and social sciences to describe possible relationships between variables. It ranks as one of the most important tools used in these disciplines.

Trend line
A trend line represents a trend, the long-term movement in time series data after other components have been accounted for. It tells whether a particular data set (say GDP, oil prices or stock prices) have increased or decreased over the period of time. A trend line could simply be drawn by eye through a set of data points, but more properly their position and slope is calculated using statistical techniques like linear regression. Trend lines typically are straight lines, although some variations use higher degree polynomials depending on the degree of curvature desired in the line.
Trend lines are sometimes used in business analytics to show changes in data over time. This has the advantage of being simple. Trend lines are often used to argue that a particular action or event (such as training, or an advertising campaign) caused observed changes at a point in time. This is a simple technique, and does not require a control group, experimental design, or a sophisticated analysis technique. However, it suffers from a lack of scientific validity in cases where other potential changes can affect the data.

Epidemiology
Early evidence relating tobacco smoking to mortality and morbidity came from observational studies employing regression analysis. In order to reduce spurious correlations when analyzing observational data, researchers usually include several variables in their regression models in addition to the variable of primary interest. For example, in a regression model in which cigarette smoking is the independent variable of primary interest and the dependent variable is lifespan measured in years, researchers might include education and income as additional independent variables, to ensure that any observed effect of smoking on lifespan is not due to those other socio-economic factors. However, it is never possible to include all possible confounding variables in an empirical analysis. For example, a hypothetical gene might increase mortality and also cause people to smoke more. For this reason, randomized controlled trials are often able to generate more compelling evidence of causal relationships than can be obtained using regression analyses of observational data. When controlled experiments are not feasible, variants of regression analysis such as instrumental variables regression may be used to attempt to estimate causal relationships from observational data.

Finance
The capital asset pricing model uses linear regression as well as the concept of beta for analyzing and quantifying the systematic risk of an investment. This comes directly from the beta coefficient of the linear regression model that relates the return on the investment to the return on all risky assets.

Economics
Linear regression is the predominant empirical tool in economics.  For example, it is used to predict consumption spending, fixed investment spending, inventory investment, purchases of a country's exports, spending on imports, the demand to hold liquid assets, labor demand, and labor supply.

Environmental science
Linear regression finds application in a wide range of environmental science applications. In Canada, the Environmental Effects Monitoring Program uses statistical analyses on fish and benthic surveys to measure the effects of pulp mill or metal mine effluent on the aquatic ecosystem.

Machine learning
Linear regression plays an important role in the subfield of artificial intelligence known as machine learning. The linear regression algorithm is one of the fundamental supervised machine-learning algorithms due to its relative simplicity and well-known properties.

History
Least squares linear regression, as a means of finding a good rough linear fit to a set of points was performed by Legendre (1805) and Gauss (1809) for the prediction of planetary movement. Quetelet was responsible for making the procedure well-known and for using it extensively in the social sciences.

See also
References
Citations
Sources
Further reading
Pedhazur, Elazar J (1982). Multiple regression in behavioral research: Explanation and prediction (2nd ed.). New York: Holt, Rinehart and Winston. ISBN 978-0-03-041760-3.
Mathieu Rouaud, 2013: Probability, Statistics and Estimation Chapter 2: Linear Regression, Linear Regression with Error Bars and Nonlinear Regression.
National Physical Laboratory (1961). ""Chapter 1: Linear Equations and Matrices: Direct Methods"". Modern Computing Methods. Notes on Applied Science. 16 (2nd ed.). Her Majesty's Stationery Office.

External links
Least-Squares Regression, PhET Interactive simulations, University of Colorado at Boulder
DIY Linear Fit",https://en.wikipedia.org/wiki/Linear_regression,"['All articles to be expanded', 'All articles with unsourced statements', 'Articles to be expanded from January 2010', 'Articles using small message boxes', 'Articles with short description', 'Articles with unsourced statements from June 2018', 'Commons category link is on Wikidata', 'Estimation theory', 'Parametric statistics', 'Short description is different from Wikidata', 'Single-equation methods (econometrics)', 'Webarchive template wayback links', 'Wikipedia articles needing clarification from March 2012', 'Wikipedia articles needing clarification from May 2018']",Data Science
125,List of datasets for machine-learning research,"These datasets are used for machine-learning research and have been cited in peer-reviewed academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.

Image data
Datasets consisting primarily of images or videos for tasks such as object detection, facial recognition, and multi-label classification.

Facial recognition
In computer vision, face images have been used extensively to develop facial recognition systems, face detection, and many other projects that use images of faces.

Action recognition
Object detection and recognition
Handwriting and character recognition
Aerial images
Other images
Text data
Datasets consisting primarily of text for tasks such as natural language processing, sentiment analysis, translation, and cluster analysis.

Reviews
News articles
Messages
Twitter and tweets
Dialogues
Other text
Sound data
Datasets of sounds and sound features.

Speech
Music
Other sounds
Signal data
Datasets containing electric signal information requiring some sort of Signal processing for further analysis.

Electrical
Motion-tracking
Other signals
Physical data
Datasets from physical systems.

High-energy physics
Systems
Astronomy
Earth science
Other physical
Biological data
Datasets from biological systems.

Human
Animal
Plant
Microbe
Drug Discovery
Anomaly data
Question Answering data
This section includes datasets that deals with structured data.

Multivariate data
Datasets consisting of rows of observations and columns of attributes characterizing those observations. Typically used for regression analysis or classification but other types of algorithms can also be used. This section includes datasets that do not fit in the above categories.

Financial
Weather
Census
Transit
Internet
Games
Other multivariate
Curated repositories of datasets
As datasets come in myriad formats and can sometimes be difficult to use, there has been considerable work put into curating and standardizing the format of datasets to make them easier to use for machine learning research.

OpenML: Web platform with Python, R, Java, and other APIs for downloading hundreds of machine learning datasets, evaluating algorithms on datasets, and benchmarking algorithm performance against dozens of other algorithms.
PMLB: A large, curated repository of benchmark datasets for evaluating supervised machine learning algorithms. Provides classification and regression datasets in a standardized format that are accessible through a Python API.
Metatext NLP: https://metatext.io/datasets web repository maintained by community, containing nearly 1000 benchmark datasets, and counting. Provides many tasks from classification to QA, and various languages from English, Portuguese to Arabic.
Appen: Off The Shelf and Open Source Datasets hosted and maintained by the company. These biological, image, physical, question answering, signal, sound, text, and video resources number over 250 and can be applied to over 25 different use cases.

See also
Comparison of deep learning software
List of manual image annotation tools
List of biological databases


== References ==",https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research,"['Articles with short description', 'Artificial intelligence', 'CS1 errors: URL', 'CS1 errors: external links', 'CS1 errors: missing periodical', 'CS1 maint: multiple names: authors list', 'Datasets in machine learning', 'Machine learning', 'Short description matches Wikidata', 'Use dmy dates from September 2017']",Data Science
126,Local outlier factor,"In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.LOF shares some concepts with DBSCAN and OPTICS such as the concepts of ""core distance"" and ""reachability distance"", which are used for local density estimation.

Basic idea
The local outlier factor is based on a concept of a local density, where locality is given by k nearest neighbors, whose distance is used to estimate the density. By comparing the local density of an object to the local densities of its neighbors, one can identify regions of similar density, and points that have a substantially lower density than their neighbors. These are considered to be outliers.
The local density is estimated by the typical distance at which a point can be ""reached"" from its neighbors. The definition of ""reachability distance"" used in LOF is an additional measure to produce more stable results within clusters. The ""reachability distance"" used by LOF has some subtle details that are often found incorrect in secondary sources, e.g., in the textbook of Ethem Alpaydin.

Formal
Let k-distance(A) be the distance of the object A to the k-th nearest neighbor. Note that the set of the k nearest neighbors includes all objects at this distance, which can in the case of a ""tie"" be more than k objects. We denote the set of k nearest neighbors as Nk(A).

This distance is used to define what is called reachability distance:
reachability-distancek(A,B)=max{k-distance(B), d(A,B)} 
In words, the reachability distance of an object A from B is the true distance of the two objects, but at least the k-distance of B. Objects that belong to the k nearest neighbors of B (the ""core"" of B, see DBSCAN cluster analysis) are considered to be equally distant. The reason for this distance is to get more stable results. Note that this is not a distance in the mathematical definition, since it is not symmetric. (While it is a common mistake to always use the k-distance(A), this yields a slightly different method, referred to as Simplified-LOF)
The local reachability density of an object A is defined by
lrdk(A):=1/(∑B∈ Nk(A)reachability-distancek(A, B)/|Nk(A)|)
which is the inverse of the average reachability distance of the object A from its neighbors. Note that it is not the average reachability of the neighbors from A (which by definition would be the k-distance(A)), but the distance at which A can be ""reached"" from its neighbors. With duplicate points, this value can become infinite.
The local reachability densities are then compared with those of the neighbors using

LOFk(A):=∑B∈ Nk(A)lrdk(B)/lrdk(A)/|Nk(A)|
= ∑B∈ Nk(A)lrdk(B)/|Nk(A)| · lrdk(A)

which is the average local reachability density of the neighbors divided by the object's own local reachability density. A value of approximately 1 indicates that the object is comparable to its neighbors (and thus not an outlier). A value below 1 indicates a denser region (which would be an inlier), while values significantly larger than 1 indicate outliers.
LOF(k) ~ 1 means Similar density as neighbors, 
LOF(k) < 1 means Higher density than neighbors (Inlier),
LOF(k) > 1 means Lower density than neighbors (Outlier)

Advantages
Due to the local approach, LOF is able to identify outliers in a data set that would not be outliers in another area of the data set. For example, a point at a ""small"" distance to a very dense cluster is an outlier, while a point within a sparse cluster might exhibit similar distances to its neighbors.
While the geometric intuition of LOF is only applicable to low-dimensional vector spaces, the algorithm can be applied in any context a dissimilarity function can be defined. It has experimentally been shown to work very well in numerous setups, often outperforming the competitors, for example in network intrusion detection and on processed classification benchmark data.The LOF family of methods can be easily generalized and then applied to various other problems, such as detecting outliers in geographic data, video streams or authorship networks.

Disadvantages and Extensions
The resulting values are quotient-values and hard to interpret. A value of 1 or even less indicates a clear inlier, but there is no clear rule for when a point is an outlier. In one data set, a value of 1.1 may already be an outlier, in another dataset and parameterization (with strong local fluctuations) a value of 2 could still be an inlier. These differences can also occur within a dataset due to the locality of the method. There exist extensions of LOF that try to improve over LOF in these aspects:

Feature Bagging for Outlier Detection runs LOF on multiple projections and combines the results for improved detection qualities in high dimensions. This is the first ensemble learning approach to outlier detection, for other variants see ref.
Local Outlier Probability (LoOP) is a method derived from LOF but using inexpensive local statistics to become less sensitive to the choice of the parameter k. In addition, the resulting values are scaled to a value range of [0:1].
Interpreting and Unifying Outlier Scores proposes a normalization of the LOF outlier scores to the interval [0:1] using statistical scaling to increase usability and can be seen an improved version of the LoOP ideas.
On Evaluation of Outlier Rankings and Outlier Scores proposes methods for measuring similarity and diversity of methods for building advanced outlier detection ensembles using LOF variants and other algorithms and improving on the Feature Bagging approach discussed above.
Local outlier detection reconsidered: a generalized view on locality with applications to spatial, video, and network outlier detection discusses the general pattern in various local outlier detection methods (including, e.g., LOF, a simplified version of LOF and LoOP) and abstracts from this into a general framework. This framework is then applied, e.g., to detecting outliers in geographic data, video streams and authorship networks.


== References ==",https://en.wikipedia.org/wiki/Local_outlier_factor,"['All articles with unsourced statements', 'Articles with unsourced statements from August 2020', 'CS1 maint: uses authors parameter', 'Data mining', 'Machine learning algorithms', 'Statistical outliers']",Data Science
127,Long short-term memory,"Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or IDSs (intrusion detection systems).
A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.
LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the vanishing gradient problem that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, hidden Markov models and other sequence learning methods in numerous applications.

History
1995-1997: LSTM was proposed by Sepp Hochreiter and Jürgen Schmidhuber. By introducing Constant Error Carousel (CEC) units, LSTM deals with the vanishing gradient problem. The initial version of LSTM block included cells, input and output gates.1999: Felix Gers and his advisor Jürgen Schmidhuber and Fred Cummins introduced the forget gate (also called “keep gate”) into LSTM architecture, 
enabling the LSTM to reset its own state.2000: Gers & Schmidhuber & Cummins added peephole connections (connections from the cell to the gates) into the architecture. Additionally, the output activation function was omitted.2009: An LSTM based model won the ICDAR connected handwriting recognition competition. Three such models were submitted by a team led by Alex Graves. One was the most accurate model in the competition and another was the fastest.2013: LSTM networks were a major component of a network that achieved a record 17.7% phoneme error rate on the classic TIMIT natural speech dataset.2014: Kyunghyun Cho et al. put forward a simplified variant called Gated recurrent unit (GRU).2015: Google started using an LSTM for speech recognition on Google Voice. According to the official blog post, the new model cut transcription errors by 49%. 2016: Google started using an LSTM to suggest messages in the Allo conversation app. In the same year, Google released the Google Neural Machine Translation system for Google Translate which used LSTMs to reduce translation errors by 60%.Apple announced in its Worldwide Developers Conference that it would start using the LSTM for quicktype in the iPhone and for Siri.Amazon released Polly, which generates the voices behind Alexa, using a bidirectional LSTM for the text-to-speech technology.2017:  Facebook performed some 4.5 billion automatic translations every day using long short-term memory networks.Researchers from Michigan State University, IBM Research, and Cornell University published a study in the Knowledge Discovery and Data Mining (KDD) conference. Their study describes a novel neural network that performs better on certain data sets than the widely used long short-term memory neural network.
Microsoft reported reaching 94.9% recognition accuracy on the Switchboard corpus, incorporating a vocabulary of 165,000 words. The approach used ""dialog session-based long-short-term memory"".2019: Researchers from the University of Waterloo proposed a related RNN architecture which represents continuous windows of time. It was derived using the Legendre polynomials and outperforms the LSTM on some memory-related benchmarks.An LSTM model climbed to third place on the in Large Text Compression Benchmark.

Idea
In theory, classic (or ""vanilla"") RNNs can keep track of arbitrary long-term dependencies in the input sequences. The problem with vanilla RNNs is computational (or practical) in nature: when training a vanilla RNN using back-propagation, the gradients which are back-propagated can ""vanish"" (that is, they can tend to zero) or ""explode"" (that is, they can tend to infinity), because of the computations involved in the process, which use finite-precision numbers. RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow unchanged. However, LSTM networks can still suffer from the exploding gradient problem.

Variants
In the equations below, the lowercase variables represent vectors. Matrices 
  
    
      
        
          W
          
            q
          
        
      
    
    {\displaystyle W_{q}}
   and 
  
    
      
        
          U
          
            q
          
        
      
    
    {\displaystyle U_{q}}
   contain, respectively, the weights of the input and recurrent connections, where the subscript 
  
    
      
        
          
          
            q
          
        
      
    
    {\displaystyle _{q}}
   can either be the input gate 
  
    
      
        i
      
    
    {\displaystyle i}
  , output gate 
  
    
      
        o
      
    
    {\displaystyle o}
  , the forget gate 
  
    
      
        f
      
    
    {\displaystyle f}
   or the memory cell 
  
    
      
        c
      
    
    {\displaystyle c}
  , depending on the activation being calculated. In this section, we are thus using a ""vector notation"". So, for example, 
  
    
      
        
          c
          
            t
          
        
        ∈
        
          
            R
          
          
            h
          
        
      
    
    {\displaystyle c_{t}\in \mathbb {R} ^{h}}
   is not just one cell of one LSTM unit, but contains 
  
    
      
        h
      
    
    {\displaystyle h}
   LSTM unit's cells.

LSTM with a forget gate
The compact forms of the equations for the forward pass of an LSTM unit with a forget gate are:

  
    
      
        
          
            
              
                
                  f
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  W
                  
                    f
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    f
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    f
                  
                
                )
              
            
            
              
                
                  i
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  W
                  
                    i
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    i
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    i
                  
                
                )
              
            
            
              
                
                  o
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  W
                  
                    o
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    o
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    o
                  
                
                )
              
            
            
              
                
                  
                    
                      
                        c
                        ~
                      
                    
                  
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    c
                  
                
                (
                
                  W
                  
                    c
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    c
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    c
                  
                
                )
              
            
            
              
                
                  c
                  
                    t
                  
                
              
              
                
                =
                
                  f
                  
                    t
                  
                
                ∘
                
                  c
                  
                    t
                    −
                    1
                  
                
                +
                
                  i
                  
                    t
                  
                
                ∘
                
                  
                    
                      
                        c
                        ~
                      
                    
                  
                  
                    t
                  
                
              
            
            
              
                
                  h
                  
                    t
                  
                
              
              
                
                =
                
                  o
                  
                    t
                  
                
                ∘
                
                  σ
                  
                    h
                  
                
                (
                
                  c
                  
                    t
                  
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}x_{t}+U_{i}h_{t-1}+b_{i})\\o_{t}&=\sigma _{g}(W_{o}x_{t}+U_{o}h_{t-1}+b_{o})\\{\tilde {c}}_{t}&=\sigma _{c}(W_{c}x_{t}+U_{c}h_{t-1}+b_{c})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ {\tilde {c}}_{t}\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}
  where the initial values are 
  
    
      
        
          c
          
            0
          
        
        =
        0
      
    
    {\displaystyle c_{0}=0}
   and 
  
    
      
        
          h
          
            0
          
        
        =
        0
      
    
    {\displaystyle h_{0}=0}
   and the operator 
  
    
      
        ∘
      
    
    {\displaystyle \circ }
   denotes the Hadamard product (element-wise product). The subscript 
  
    
      
        t
      
    
    {\displaystyle t}
   indexes the time step.

Variables
x
          
            t
          
        
        ∈
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle x_{t}\in \mathbb {R} ^{d}}
  : input vector to the LSTM unit

  
    
      
        
          f
          
            t
          
        
        ∈
        
          
            R
          
          
            h
          
        
      
    
    {\displaystyle f_{t}\in \mathbb {R} ^{h}}
  : forget gate's activation vector

  
    
      
        
          i
          
            t
          
        
        ∈
        
          
            R
          
          
            h
          
        
      
    
    {\displaystyle i_{t}\in \mathbb {R} ^{h}}
  : input/update gate's activation vector

  
    
      
        
          o
          
            t
          
        
        ∈
        
          
            R
          
          
            h
          
        
      
    
    {\displaystyle o_{t}\in \mathbb {R} ^{h}}
  : output gate's activation vector

  
    
      
        
          h
          
            t
          
        
        ∈
        
          
            R
          
          
            h
          
        
      
    
    {\displaystyle h_{t}\in \mathbb {R} ^{h}}
  : hidden state vector also known as output vector of the LSTM unit

  
    
      
        
          
            
              
                c
                ~
              
            
          
          
            t
          
        
        ∈
        
          
            R
          
          
            h
          
        
      
    
    {\displaystyle {\tilde {c}}_{t}\in \mathbb {R} ^{h}}
  : cell input activation vector

  
    
      
        
          c
          
            t
          
        
        ∈
        
          
            R
          
          
            h
          
        
      
    
    {\displaystyle c_{t}\in \mathbb {R} ^{h}}
  : cell state vector

  
    
      
        W
        ∈
        
          
            R
          
          
            h
            ×
            d
          
        
      
    
    {\displaystyle W\in \mathbb {R} ^{h\times d}}
  , 
  
    
      
        U
        ∈
        
          
            R
          
          
            h
            ×
            h
          
        
      
    
    {\displaystyle U\in \mathbb {R} ^{h\times h}}
   and 
  
    
      
        b
        ∈
        
          
            R
          
          
            h
          
        
      
    
    {\displaystyle b\in \mathbb {R} ^{h}}
  : weight matrices and bias vector parameters which need to be learned during trainingwhere the superscripts 
  
    
      
        d
      
    
    {\displaystyle d}
   and 
  
    
      
        h
      
    
    {\displaystyle h}
   refer to the number of input features and number of hidden units, respectively.

Activation functions
σ
          
            g
          
        
      
    
    {\displaystyle \sigma _{g}}
  : sigmoid function.

  
    
      
        
          σ
          
            c
          
        
      
    
    {\displaystyle \sigma _{c}}
  : hyperbolic tangent function.

  
    
      
        
          σ
          
            h
          
        
      
    
    {\displaystyle \sigma _{h}}
  : hyperbolic tangent function or, as the peephole LSTM paper suggests, 
  
    
      
        
          σ
          
            h
          
        
        (
        x
        )
        =
        x
      
    
    {\displaystyle \sigma _{h}(x)=x}
  .

Peephole LSTM
The figure on the right is a graphical representation of an LSTM unit with peephole connections (i.e. a peephole LSTM). Peephole connections allow the gates to access the constant error carousel (CEC), whose activation is the cell state. 
  
    
      
        
          h
          
            t
            −
            1
          
        
      
    
    {\displaystyle h_{t-1}}
   is not used, 
  
    
      
        
          c
          
            t
            −
            1
          
        
      
    
    {\displaystyle c_{t-1}}
   is used instead in most places.

  
    
      
        
          
            
              
                
                  f
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  W
                  
                    f
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    f
                  
                
                
                  c
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    f
                  
                
                )
              
            
            
              
                
                  i
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  W
                  
                    i
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    i
                  
                
                
                  c
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    i
                  
                
                )
              
            
            
              
                
                  o
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  W
                  
                    o
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    o
                  
                
                
                  c
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    o
                  
                
                )
              
            
            
              
                
                  c
                  
                    t
                  
                
              
              
                
                =
                
                  f
                  
                    t
                  
                
                ∘
                
                  c
                  
                    t
                    −
                    1
                  
                
                +
                
                  i
                  
                    t
                  
                
                ∘
                
                  σ
                  
                    c
                  
                
                (
                
                  W
                  
                    c
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  b
                  
                    c
                  
                
                )
              
            
            
              
                
                  h
                  
                    t
                  
                
              
              
                
                =
                
                  o
                  
                    t
                  
                
                ∘
                
                  σ
                  
                    h
                  
                
                (
                
                  c
                  
                    t
                  
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}c_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}x_{t}+U_{i}c_{t-1}+b_{i})\\o_{t}&=\sigma _{g}(W_{o}x_{t}+U_{o}c_{t-1}+b_{o})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}x_{t}+b_{c})\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}

Peephole convolutional LSTM
Peephole convolutional LSTM. The 
  
    
      
        ∗
      
    
    {\displaystyle *}
   denotes the convolution operator.

  
    
      
        
          
            
              
                
                  f
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  W
                  
                    f
                  
                
                ∗
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    f
                  
                
                ∗
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  V
                  
                    f
                  
                
                ∘
                
                  c
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    f
                  
                
                )
              
            
            
              
                
                  i
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  W
                  
                    i
                  
                
                ∗
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    i
                  
                
                ∗
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  V
                  
                    i
                  
                
                ∘
                
                  c
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    i
                  
                
                )
              
            
            
              
                
                  c
                  
                    t
                  
                
              
              
                
                =
                
                  f
                  
                    t
                  
                
                ∘
                
                  c
                  
                    t
                    −
                    1
                  
                
                +
                
                  i
                  
                    t
                  
                
                ∘
                
                  σ
                  
                    c
                  
                
                (
                
                  W
                  
                    c
                  
                
                ∗
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    c
                  
                
                ∗
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    c
                  
                
                )
              
            
            
              
                
                  o
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    g
                  
                
                (
                
                  W
                  
                    o
                  
                
                ∗
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    o
                  
                
                ∗
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  V
                  
                    o
                  
                
                ∘
                
                  c
                  
                    t
                  
                
                +
                
                  b
                  
                    o
                  
                
                )
              
            
            
              
                
                  h
                  
                    t
                  
                
              
              
                
                =
                
                  o
                  
                    t
                  
                
                ∘
                
                  σ
                  
                    h
                  
                
                (
                
                  c
                  
                    t
                  
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}*x_{t}+U_{f}*h_{t-1}+V_{f}\circ c_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}*x_{t}+U_{i}*h_{t-1}+V_{i}\circ c_{t-1}+b_{i})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}*x_{t}+U_{c}*h_{t-1}+b_{c})\\o_{t}&=\sigma _{g}(W_{o}*x_{t}+U_{o}*h_{t-1}+V_{o}\circ c_{t}+b_{o})\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}}

Training
A RNN using LSTM units can be trained in a supervised fashion, on a set of training sequences, using an optimization algorithm, like gradient descent, combined with backpropagation through time to compute the gradients needed during the optimization process, in order to change each weight of the LSTM network in proportion to the derivative of the error (at the output layer of the LSTM network) with respect to corresponding weight.
A problem with using gradient descent for standard RNNs is that error gradients vanish exponentially quickly with the size of the time lag between important events. This is due to 
  
    
      
        
          lim
          
            n
            →
            ∞
          
        
        
          W
          
            n
          
        
        =
        0
      
    
    {\displaystyle \lim _{n\to \infty }W^{n}=0}
   if the spectral radius of 
  
    
      
        W
      
    
    {\displaystyle W}
   is smaller than 1.However, with LSTM units, when error values are back-propagated from the output layer, the error remains in the LSTM unit's cell. This ""error carousel"" continuously feeds error back to each of the LSTM unit's gates, until they learn to cut off the value.

CTC score function
Many applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.

Alternatives
Sometimes, it can be advantageous to train (parts of) an LSTM by neuroevolution or by policy gradient methods, especially when there is no ""teacher"" (that is, training labels).

Success
There have been several successful stories of training, in a non-supervised fashion, RNNs with LSTM units.
In 2018, Bill Gates called it a “huge milestone in advancing artificial intelligence” when bots developed by OpenAI were able to beat humans in the game of Dota 2. OpenAI Five consists of five independent but coordinated neural networks. Each network is trained by a policy gradient method without supervising teacher and contains a single-layer, 1024-unit Long-Short-Term-Memory that sees the current game state and emits actions through several possible action heads.In 2018, OpenAI also trained a similar LSTM by policy gradients to control a human-like robot hand that manipulates physical objects with unprecedented dexterity.In 2019, DeepMind's program AlphaStar used a deep LSTM core to excel at the complex video game Starcraft II. This was viewed as significant progress towards Artificial General Intelligence.

Applications
Applications of LSTM include:

Robot control
Time series prediction
Speech recognition
Rhythm learning
Music composition
Grammar learning
Handwriting recognition
Human action recognition
Sign language translation
Protein homology detection
Predicting subcellular localization of proteins
Time series anomaly detection
Several prediction tasks in the area of business process management
Prediction in medical care pathways
Semantic parsing
Object co-segmentation
Airport passenger management
Short-term traffic forecast
Drug design
Market Prediction

See also
Recurrent neural network
Deep learning
Gated recurrent unit
Differentiable neural computer
Long-term potentiation
Prefrontal cortex basal ganglia working memory
Time series
Seq2seq
Highway network

References
External links
Recurrent Neural Networks with over 30 LSTM papers by Jürgen Schmidhuber's group at IDSIA
Gers, Felix (2001). ""Long Short-Term Memory in Recurrent Neural Networks"" (PDF). PhD thesis.
Gers, Felix A.; Schraudolph, Nicol N.; Schmidhuber, Jürgen (Aug 2002). ""Learning precise timing with LSTM recurrent networks"" (PDF). Journal of Machine Learning Research. 3: 115–143.
Abidogun, Olusola Adeniyi (2005). Data Mining, Fraud Detection and Mobile Telecommunications: Call Pattern Analysis with Unsupervised Neural Networks. Master's Thesis (Thesis). University of the Western Cape. hdl:11394/249. Archived (PDF) from the original on May 22, 2012.
original with two chapters devoted to explaining recurrent neural networks, especially LSTM.
Monner, Derek D.; Reggia, James A. (2010). ""A generalized LSTM-like training algorithm for second-order recurrent neural networks"" (PDF). Neural Networks. 25 (1): 70–83. doi:10.1016/j.neunet.2011.07.003. PMC 3217173. PMID 21803542. High-performing extension of LSTM that has been simplified to a single node type and can train arbitrary architectures
Dolphin, R. ""LSTM Networks - A Detailed Explanation"". Article.
Herta, Christian. ""How to implement LSTM in Python with Theano"". Tutorial.",https://en.wikipedia.org/wiki/Long_short-term_memory,"['All articles with unsourced statements', 'Articles with short description', 'Articles with unsourced statements from October 2017', 'Artificial neural networks', 'CS1 maint: multiple names: authors list', 'Short description is different from Wikidata']",Data Science
128,Machine Learning (journal),"Machine Learning  is a peer-reviewed scientific journal, published since 1986.
It should be distinguished from the journal Machine intelligence which was established in the mid-1960s.In 2001, forty editors and members of the editorial board of Machine Learning resigned in order to support the Journal of Machine Learning Research (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet.Following the mass resignation, Kluwer changed their publishing policy to allow authors to self-archive their papers online after peer-review.

Selected articles
J.R. Quinlan (1986). ""Induction of Decision Trees"". Machine Learning. 1: 81–106. doi:10.1007/BF00116251.
Nick Littlestone (1988). ""Learning Quickly When Irrelevant Attributes Abound: A New Linear-threshold Algorithm"" (PDF). Machine Learning. 2 (4): 285–318. doi:10.1007/BF00116827.
Robert E. Schapire (1990). ""The Strength of Weak Learnability"" (PDF). Machine Learning. 5 (2): 197–227. doi:10.1007/BF00116037. Archived from the original (PDF) on 2012-10-10. Retrieved 2020-02-22.
J. Quinlan (1990). ""Learning Logical Descriptions from Relations"" (PDF). Machine Learning. 5 (3): 239–266. doi:10.1007/BF00117105.
John R. Anderson and Michael Matessa (1992). ""Explorations of an Incremental, Bayesian Algorithm for Categorization"". Machine Learning. 9 (4): 275–308. doi:10.1007/BF00994109.
David Klahr (1994). ""Children, Adults, and Machines as Discovery Systems"". Machine Learning. 14 (3): 313–320. doi:10.1007/BF00993981.
Thomas Dean and Dana Angluin and Kenneth Basye and Sean Engelson and Leslie Kaelbling and Evangelos Kokkevis and Oded Maron (1995). ""Inferring Finite Automata with Stochastic Output Functions and an Application to Map Learning"". Machine Learning. 18: 81–108. doi:10.1007/BF00993822.
Luc De Raedt and Luc Dehaspe (1997). ""Clausal Discovery"". Machine Learning. 26 (2/3): 99–146. doi:10.1023/A:1007361123060.
C. de la Higuera (1997). ""Characteristic Sets for Grammatical Inference"". Machine Learning. 27: 1–14.
Robert E. Schapire and Yoram Singer (1999). ""Improved Boosting Algorithms Using Confidence-rated Predictions"". Machine Learning. 37 (3): 297–336. doi:10.1023/A:1007614523901.
Robert E. Schapire and Yoram Singer (2000). ""BoosTexter: A Boosting-based System for Text Categorization"". Machine Learning. 39 (2/3): 135–168. doi:10.1023/A:1007649029923.
P. Rossmanith and T. Zeugmann (2001). ""Stochastic Finite Learning of the Pattern Languages"". Machine Learning. 44 (1–2): 67–91. doi:10.1023/A:1010875913047.
Parekh, Rajesh; Honavar, Vasant (2001). ""Learning DFA from Simple Examples"". Machine Learning. 44 (1/2): 9–35. doi:10.1023/A:1010822518073.
Ayhan Demiriz and Kristin P. Bennett and John Shawe-Taylor (2002). ""Linear Programming Boosting via Column Generation"". Machine Learning. 46: 225–254. doi:10.1023/A:1012470815092.
Simon Colton and Stephen Muggleton (2006). ""Mathematical Applications of Inductive Logic Programming"" (PDF). Machine Learning. 64 (1–3): 25–64. doi:10.1007/s10994-006-8259-x.
Will Bridewell and Pat Langley and Ljupco Todorovski and Saso Dzeroski (2008). ""Inductive Process Modeling"". Machine Learning.
Stephen Muggleton and Alireza Tamaddoni-Nezhad (2008). ""QG/GA: a stochastic search for Progol"". Machine Learning. 70 (2–3): 121–133. doi:10.1007/s10994-007-5029-3.


== References ==",https://en.wikipedia.org/wiki/Machine_Learning_(journal),"['All stub articles', 'Articles with outdated impact factors from 2018', 'Articles with short description', 'Computer science journal stubs', 'Computer science journals', 'Delayed open access journals', 'Machine learning', 'Publications established in 1986', 'Short description is different from Wikidata', 'Springer Science+Business Media academic journals']",Data Science
129,Logistic regression,"In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick.  This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc.  Each object being detected in the image would be assigned a probability between 0 and 1, with a sum of one.
Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail which is represented by an indicator variable, where the two values are labeled ""0"" and ""1"". In the logistic model, the log-odds (the logarithm of the odds) for the value labeled ""1"" is a linear combination of one or more independent variables (""predictors""); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled ""1"" can vary between 0 (certainly the value ""0"") and 1 (certainly the value ""1""), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio.
In a binary logistic regression model, the dependent variable has two levels (categorical). Outputs with more than two values are modeled by multinomial logistic regression and, if the multiple categories are ordered, by ordinal logistic regression (for example the proportional odds ordinal logistic model). The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier. The coefficients are generally not computed by a closed-form expression, unlike linear least squares; see § Model fitting. The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson, beginning in Berkson (1944), where he coined ""logit""; see § History.

Applications
Logistic regression is used in various fields, including machine learning, most medical fields, and social sciences. For example, the Trauma and Injury Severity Score (TRISS), which is widely used to predict mortality in injured patients, was originally developed by Boyd et al. using logistic regression.  Many other medical scales used to assess severity of a patient have been developed using logistic regression. Logistic regression may be used to predict the risk of developing a given disease (e.g. diabetes; coronary heart disease), based on observed characteristics of the patient (age, sex, body mass index, results of various blood tests, etc.).  Another example might be to predict whether a Nepalese voter will vote Nepali Congress or Communist Party of Nepal or Any Other Party, based on age, income, sex, race, state of residence, votes in previous elections, etc. The technique can also be used in engineering, especially for predicting the probability of failure of a given process, system or product. It is also used in marketing applications such as prediction of a customer's propensity to purchase a product or halt a subscription, etc. In economics it can be used to predict the likelihood of a person's choosing to be in the labor force, and a business application would be to predict the likelihood of a homeowner defaulting on a mortgage. Conditional random fields, an extension of logistic regression to sequential data, are used in natural language processing.

Examples
Logistic model
Let us try to understand logistic regression by considering a logistic model with given parameters, then seeing how the coefficients can be estimated from data. Consider a model with two predictors, 
  
    
      
        
          x
          
            1
          
        
      
    
    {\displaystyle x_{1}}
   and 
  
    
      
        
          x
          
            2
          
        
      
    
    {\displaystyle x_{2}}
  , and one binary (Bernoulli) response variable 
  
    
      
        Y
      
    
    {\displaystyle Y}
  , which we denote 
  
    
      
        p
        =
        P
        (
        Y
        =
        1
        )
      
    
    {\displaystyle p=P(Y=1)}
  . We assume a linear relationship between the predictor variables and the log-odds (also called logit) of the event that 
  
    
      
        Y
        =
        1
      
    
    {\displaystyle Y=1}
  . This linear relationship can be written in the following mathematical form (where ℓ is the log-odds, 
  
    
      
        b
      
    
    {\displaystyle b}
   is the base of the logarithm, and 
  
    
      
        
          β
          
            i
          
        
      
    
    {\displaystyle \beta _{i}}
   are parameters of the model):

  
    
      
        ℓ
        =
        
          log
          
            b
          
        
        ⁡
        
          
            p
            
              1
              −
              p
            
          
        
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          x
          
            1
          
        
        +
        
          β
          
            2
          
        
        
          x
          
            2
          
        
      
    
    {\displaystyle \ell =\log _{b}{\frac {p}{1-p}}=\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}}
  We can recover the odds by exponentiating the log-odds:

  
    
      
        
          
            p
            
              1
              −
              p
            
          
        
        =
        
          b
          
            
              β
              
                0
              
            
            +
            
              β
              
                1
              
            
            
              x
              
                1
              
            
            +
            
              β
              
                2
              
            
            
              x
              
                2
              
            
          
        
      
    
    {\displaystyle {\frac {p}{1-p}}=b^{\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}}}
  .By simple algebraic manipulation (and dividing numerator and denominator by 
  
    
      
        
          b
          
            
              β
              
                0
              
            
            +
            
              β
              
                1
              
            
            
              x
              
                1
              
            
            +
            
              β
              
                2
              
            
            
              x
              
                2
              
            
          
        
      
    
    {\displaystyle b^{\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}}}
  ), the probability that 
  
    
      
        Y
        =
        1
      
    
    {\displaystyle Y=1}
   is

  
    
      
        p
        =
        
          
            
              b
              
                
                  β
                  
                    0
                  
                
                +
                
                  β
                  
                    1
                  
                
                
                  x
                  
                    1
                  
                
                +
                
                  β
                  
                    2
                  
                
                
                  x
                  
                    2
                  
                
              
            
            
              
                b
                
                  
                    β
                    
                      0
                    
                  
                  +
                  
                    β
                    
                      1
                    
                  
                  
                    x
                    
                      1
                    
                  
                  +
                  
                    β
                    
                      2
                    
                  
                  
                    x
                    
                      2
                    
                  
                
              
              +
              1
            
          
        
        =
        
          
            1
            
              1
              +
              
                b
                
                  −
                  (
                  
                    β
                    
                      0
                    
                  
                  +
                  
                    β
                    
                      1
                    
                  
                  
                    x
                    
                      1
                    
                  
                  +
                  
                    β
                    
                      2
                    
                  
                  
                    x
                    
                      2
                    
                  
                  )
                
              
            
          
        
        =
        
          S
          
            b
          
        
        (
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          x
          
            1
          
        
        +
        
          β
          
            2
          
        
        
          x
          
            2
          
        
        )
      
    
    {\displaystyle p={\frac {b^{\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}}}{b^{\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}}+1}}={\frac {1}{1+b^{-(\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2})}}}=S_{b}(\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2})}
  .Where 
  
    
      
        
          S
          
            b
          
        
      
    
    {\displaystyle S_{b}}
   is the sigmoid function with base 
  
    
      
        b
      
    
    {\displaystyle b}
  .
The above formula shows that once 
  
    
      
        
          β
          
            i
          
        
      
    
    {\displaystyle \beta _{i}}
   are fixed, we can easily compute either the log-odds that 
  
    
      
        Y
        =
        1
      
    
    {\displaystyle Y=1}
   for a given observation, or the probability that 
  
    
      
        Y
        =
        1
      
    
    {\displaystyle Y=1}
   for a given observation. The main use-case of a logistic model is to be given an observation 
  
    
      
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        )
      
    
    {\displaystyle (x_{1},x_{2})}
  , and estimate the probability 
  
    
      
        p
      
    
    {\displaystyle p}
   that 
  
    
      
        Y
        =
        1
      
    
    {\displaystyle Y=1}
  . In most applications, the base 
  
    
      
        b
      
    
    {\displaystyle b}
   of the logarithm is usually taken to be e. However in some cases it can be easier to communicate results by working in base 2, or base 10.
We consider an example with 
  
    
      
        b
        =
        10
      
    
    {\displaystyle b=10}
  , and coefficients 
  
    
      
        
          β
          
            0
          
        
        =
        −
        3
      
    
    {\displaystyle \beta _{0}=-3}
  , 
  
    
      
        
          β
          
            1
          
        
        =
        1
      
    
    {\displaystyle \beta _{1}=1}
  , and 
  
    
      
        
          β
          
            2
          
        
        =
        2
      
    
    {\displaystyle \beta _{2}=2}
  . To be concrete, the model is

  
    
      
        
          log
          
            10
          
        
        ⁡
        
          
            p
            
              1
              −
              p
            
          
        
        =
        ℓ
        =
        −
        3
        +
        
          x
          
            1
          
        
        +
        2
        
          x
          
            2
          
        
      
    
    {\displaystyle \log _{10}{\frac {p}{1-p}}=\ell =-3+x_{1}+2x_{2}}
  where 
  
    
      
        p
      
    
    {\displaystyle p}
   is the probability of the event that 
  
    
      
        Y
        =
        1
      
    
    {\displaystyle Y=1}
  .
This can be interpreted as follows:

  
    
      
        
          β
          
            0
          
        
        =
        −
        3
      
    
    {\displaystyle \beta _{0}=-3}
   is the y-intercept. It is the log-odds of the event that 
  
    
      
        Y
        =
        1
      
    
    {\displaystyle Y=1}
  , when the predictors 
  
    
      
        
          x
          
            1
          
        
        =
        
          x
          
            2
          
        
        =
        0
      
    
    {\displaystyle x_{1}=x_{2}=0}
  . By exponentiating, we can see that when 
  
    
      
        
          x
          
            1
          
        
        =
        
          x
          
            2
          
        
        =
        0
      
    
    {\displaystyle x_{1}=x_{2}=0}
   the odds of the event that 
  
    
      
        Y
        =
        1
      
    
    {\displaystyle Y=1}
   are 1-to-1000, or 
  
    
      
        
          10
          
            −
            3
          
        
      
    
    {\displaystyle 10^{-3}}
  . Similarly, the probability of the event that 
  
    
      
        Y
        =
        1
      
    
    {\displaystyle Y=1}
   when 
  
    
      
        
          x
          
            1
          
        
        =
        
          x
          
            2
          
        
        =
        0
      
    
    {\displaystyle x_{1}=x_{2}=0}
   can be computed as 
  
    
      
        1
        
          /
        
        (
        1000
        +
        1
        )
        =
        1
        
          /
        
        1001
      
    
    {\displaystyle 1/(1000+1)=1/1001}
  .

  
    
      
        
          β
          
            1
          
        
        =
        1
      
    
    {\displaystyle \beta _{1}=1}
   means that increasing 
  
    
      
        
          x
          
            1
          
        
      
    
    {\displaystyle x_{1}}
   by 1 increases the log-odds by 
  
    
      
        1
      
    
    {\displaystyle 1}
  . So if 
  
    
      
        
          x
          
            1
          
        
      
    
    {\displaystyle x_{1}}
   increases by 1, the odds that 
  
    
      
        Y
        =
        1
      
    
    {\displaystyle Y=1}
   increase by a factor of 
  
    
      
        
          10
          
            1
          
        
      
    
    {\displaystyle 10^{1}}
  . Note that the probability of 
  
    
      
        Y
        =
        1
      
    
    {\displaystyle Y=1}
   has also increased, but it has not increased by as much as the odds have increased.

  
    
      
        
          β
          
            2
          
        
        =
        2
      
    
    {\displaystyle \beta _{2}=2}
   means that increasing 
  
    
      
        
          x
          
            2
          
        
      
    
    {\displaystyle x_{2}}
   by 1 increases the log-odds by 
  
    
      
        2
      
    
    {\displaystyle 2}
  . So if 
  
    
      
        
          x
          
            2
          
        
      
    
    {\displaystyle x_{2}}
   increases by 1, the odds that 
  
    
      
        Y
        =
        1
      
    
    {\displaystyle Y=1}
   increase by a factor of 
  
    
      
        
          10
          
            2
          
        
        .
      
    
    {\displaystyle 10^{2}.}
   Note how the effect of 
  
    
      
        
          x
          
            2
          
        
      
    
    {\displaystyle x_{2}}
   on the log-odds is twice as great as the effect of 
  
    
      
        
          x
          
            1
          
        
      
    
    {\displaystyle x_{1}}
  , but the effect on the odds is 10 times greater. But the effect on the probability of 
  
    
      
        Y
        =
        1
      
    
    {\displaystyle Y=1}
   is not as much as 10 times greater, it's only the effect on the odds that is 10 times greater.In order to estimate the parameters 
  
    
      
        
          β
          
            i
          
        
      
    
    {\displaystyle \beta _{i}}
   from data, one must do logistic regression.

Probability of passing an exam versus hours of study
To answer the following question:

A group of 20 students spends between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability of the student passing the exam?

The reason for using logistic regression for this problem is that the values of the dependent variable, pass and fail, while represented by ""1"" and ""0"", are not cardinal numbers. If the problem was changed so that pass/fail was replaced with the grade 0–100 (cardinal numbers), then simple regression analysis could be used.
The table shows the number of hours each student spent studying, and whether they passed (1) or failed (0).

The graph shows the probability of passing the exam versus the number of hours studying, with the logistic regression curve fitted to the data.

The logistic regression analysis gives the following output.

The output indicates that hours studying is significantly associated with the probability of passing the exam (
  
    
      
        p
        =
        0.0167
      
    
    {\displaystyle p=0.0167}
  , Wald test). The output also provides the coefficients for 
  
    
      
        
          Intercept
        
        =
        −
        4.0777
      
    
    {\displaystyle {\text{Intercept}}=-4.0777}
   and 
  
    
      
        
          Hours
        
        =
        1.5046
      
    
    {\displaystyle {\text{Hours}}=1.5046}
  . These coefficients are entered in the logistic regression equation to estimate the odds (probability) of passing the exam:

  
    
      
        
          
            
              
                
                  Log-odds of passing exam
                
              
              
                
                =
                1.5046
                ⋅
                
                  Hours
                
                −
                4.0777
                =
                1.5046
                ⋅
                (
                
                  Hours
                
                −
                2.71
                )
              
            
            
              
                
                  Odds of passing exam
                
              
              
                
                =
                exp
                ⁡
                
                  (
                  
                    1.5046
                    ⋅
                    
                      Hours
                    
                    −
                    4.0777
                  
                  )
                
                =
                exp
                ⁡
                
                  (
                  
                    1.5046
                    ⋅
                    (
                    
                      Hours
                    
                    −
                    2.71
                    )
                  
                  )
                
              
            
            
              
                
                  Probability of passing exam
                
              
              
                
                =
                
                  
                    1
                    
                      1
                      +
                      exp
                      ⁡
                      
                        (
                        
                          −
                          
                            (
                            
                              1.5046
                              ⋅
                              
                                Hours
                              
                              −
                              4.0777
                            
                            )
                          
                        
                        )
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{Log-odds of passing exam}}&=1.5046\cdot {\text{Hours}}-4.0777=1.5046\cdot ({\text{Hours}}-2.71)\\{\text{Odds of passing exam}}&=\exp \left(1.5046\cdot {\text{Hours}}-4.0777\right)=\exp \left(1.5046\cdot ({\text{Hours}}-2.71)\right)\\{\text{Probability of passing exam}}&={\frac {1}{1+\exp \left(-\left(1.5046\cdot {\text{Hours}}-4.0777\right)\right)}}\end{aligned}}}
  One additional hour of study is estimated to increase log-odds of passing by 1.5046, so multiplying odds of passing by 
  
    
      
        exp
        ⁡
        (
        1.5046
        )
        ≈
        4.5.
      
    
    {\displaystyle \exp(1.5046)\approx 4.5.}
   The form with the x-intercept (2.71) shows that this estimates even odds (log-odds 0, odds 1, probability 1/2) for a student who studies 2.71 hours.
For example, for a student who studies 2 hours, entering the value 
  
    
      
        
          Hours
        
        =
        2
      
    
    {\displaystyle {\text{Hours}}=2}
   in the equation gives the estimated probability of passing the exam of 0.26:

  
    
      
        
          Probability of passing exam
        
        =
        
          
            1
            
              1
              +
              exp
              ⁡
              
                (
                
                  −
                  
                    (
                    
                      1.5046
                      ⋅
                      2
                      −
                      4.0777
                    
                    )
                  
                
                )
              
            
          
        
        =
        0.26
      
    
    {\displaystyle {\text{Probability of passing exam}}={\frac {1}{1+\exp \left(-\left(1.5046\cdot 2-4.0777\right)\right)}}=0.26}
  Similarly, for a student who studies 4 hours, the estimated probability of passing the exam is 0.87:

  
    
      
        
          Probability of passing exam
        
        =
        
          
            1
            
              1
              +
              exp
              ⁡
              
                (
                
                  −
                  
                    (
                    
                      1.5046
                      ⋅
                      4
                      −
                      4.0777
                    
                    )
                  
                
                )
              
            
          
        
        =
        0.87
      
    
    {\displaystyle {\text{Probability of passing exam}}={\frac {1}{1+\exp \left(-\left(1.5046\cdot 4-4.0777\right)\right)}}=0.87}
  This table shows the probability of passing the exam for several values of hours studying.

The output from the logistic regression analysis gives a p-value of 
  
    
      
        p
        =
        0.0167
      
    
    {\displaystyle p=0.0167}
  , which is based on the Wald z-score. Rather than the Wald method, the recommended method to calculate the p-value for logistic regression is the likelihood-ratio test (LRT), which for this data gives 
  
    
      
        p
        =
        0.0006
      
    
    {\displaystyle p=0.0006}
  .

Discussion
Logistic regression can be binomial, ordinal or multinomial. Binomial or binary logistic regression deals with situations in which the observed outcome for a dependent variable can have only two possible types, ""0"" and ""1"" (which may represent, for example, ""dead"" vs. ""alive"" or ""win"" vs. ""loss""). Multinomial logistic regression deals with situations where the outcome can have three or more possible types (e.g., ""disease A"" vs. ""disease B"" vs. ""disease C"") that are not ordered. Ordinal logistic regression deals with dependent variables that are ordered.
In binary logistic regression, the outcome is usually coded as ""0"" or ""1"", as this leads to the most straightforward interpretation. If a particular observed outcome for the dependent variable is the noteworthy possible outcome (referred to as a ""success"" or an ""instance"" or a ""case"") it is usually coded as ""1"" and the contrary outcome (referred to as a ""failure"" or a ""noninstance"" or a ""noncase"") as ""0"". Binary logistic regression is used to predict the odds of being a case based on the values of the independent variables (predictors). The odds are defined as the probability that a particular outcome is a case divided by the probability that it is a noninstance.
Like other forms of regression analysis, logistic regression makes use of one or more predictor variables that may be either continuous or categorical. Unlike ordinary linear regression, however, logistic regression is used for predicting dependent variables that take membership in one of a limited number of categories (treating the dependent variable in the binomial case as the outcome of a Bernoulli trial) rather than a continuous outcome. Given this difference, the assumptions of linear regression are violated. In particular, the residuals cannot be normally distributed. In addition, linear regression may make nonsensical predictions for a binary dependent variable. What is needed is a way to convert a binary variable into a continuous one that can take on any real value (negative or positive). To do that, binomial logistic regression first calculates the odds of the event happening for different levels of each independent variable, and then takes its logarithm to create a continuous criterion as a transformed version of the dependent variable. The logarithm of the odds is the logit of the probability, the logit is defined as follows:

  
    
      
        logit
        ⁡
        p
        =
        ln
        ⁡
        
          
            p
            
              1
              −
              p
            
          
        
        
        
          for 
        
        0
        <
        p
        <
        1
        
        .
      
    
    {\displaystyle \operatorname {logit} p=\ln {\frac {p}{1-p}}\quad {\text{for }}0<p<1\,.}
  Although the dependent variable in logistic regression is Bernoulli, the logit is on an unrestricted scale. The logit function is the link function in this kind of generalized linear model, i.e.

  
    
      
        logit
        ⁡
        
          
            E
          
        
        ⁡
        (
        Y
        )
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        x
      
    
    {\displaystyle \operatorname {logit} \operatorname {\mathcal {E}} (Y)=\beta _{0}+\beta _{1}x}
  Y is the Bernoulli-distributed response variable and x is the predictor variable; the β values are the linear parameters.
The logit of the probability of success is then fitted to the predictors. The predicted value of the logit is converted back into predicted odds, via the inverse of the natural logarithm – the exponential function. Thus, although the observed dependent variable in binary logistic regression is a 0-or-1 variable, the logistic regression estimates the odds, as a continuous variable, that the dependent variable is a ‘success’. In some applications, the odds are all that is needed. In others, a specific yes-or-no prediction is needed for whether the dependent variable is or is not a ‘success’; this categorical prediction can be based on the computed odds of success, with predicted odds above some chosen cutoff value being translated into a prediction of success.
The assumption of linear predictor effects can easily be relaxed using techniques such as spline functions.

Logistic regression vs. other approaches
Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative distribution function of logistic distribution. Thus, it treats the same set of problems as probit regression using similar techniques, with the latter using a cumulative normal distribution curve instead.  Equivalently, in the latent variable interpretations of these two methods, the first assumes a standard logistic distribution of errors and the second a standard normal distribution of errors.Logistic regression can be seen as a special case of the generalized linear model and thus analogous to linear regression. The model of logistic regression, however, is based on quite different assumptions (about the relationship between the dependent and independent variables) from those of linear regression. In particular, the key differences between these two models can be seen in the following two features of logistic regression. First, the conditional distribution 
  
    
      
        y
        ∣
        x
      
    
    {\displaystyle y\mid x}
   is a Bernoulli distribution rather than a Gaussian distribution, because the dependent variable is binary. Second, the predicted values are probabilities and are therefore restricted to (0,1) through the logistic distribution function because logistic regression predicts the probability of particular outcomes rather than the outcomes themselves.
Logistic regression is an alternative to Fisher's 1936 method, linear discriminant analysis.  If the assumptions of linear discriminant analysis hold, the conditioning can be reversed to produce logistic regression.  The converse is not true, however, because logistic regression does not require the multivariate normal assumption of discriminant analysis.

Latent variable interpretation
The logistic regression can be understood simply as finding the 
  
    
      
        β
      
    
    {\displaystyle \beta }
   parameters that best fit:

  
    
      
        y
        =
        
          
            {
            
              
                
                  1
                
                
                  
                    β
                    
                      0
                    
                  
                  +
                  
                    β
                    
                      1
                    
                  
                  x
                  +
                  ε
                  >
                  0
                
              
              
                
                  0
                
                
                  
                    else
                  
                
              
            
            
          
        
      
    
    {\displaystyle y={\begin{cases}1&\beta _{0}+\beta _{1}x+\varepsilon >0\\0&{\text{else}}\end{cases}}}
  where 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   is an error distributed by the standard logistic distribution.  (If the standard normal distribution is used instead, it is a probit model.)
The associated latent variable is 
  
    
      
        
          y
          ′
        
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        x
        +
        ε
      
    
    {\displaystyle y'=\beta _{0}+\beta _{1}x+\varepsilon }
  . The error term 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   is not observed, and so the 
  
    
      
        
          y
          ′
        
      
    
    {\displaystyle y'}
   is also an unobservable, hence termed ""latent"" (the observed data are values of 
  
    
      
        y
      
    
    {\displaystyle y}
   and 
  
    
      
        x
      
    
    {\displaystyle x}
  ). Unlike ordinary regression, however, the 
  
    
      
        β
      
    
    {\displaystyle \beta }
   parameters cannot be expressed by any direct formula of the 
  
    
      
        y
      
    
    {\displaystyle y}
   and 
  
    
      
        x
      
    
    {\displaystyle x}
   values in the observed data.  Instead they are to be found by an iterative search process, usually implemented by a software program, that finds the maximum of a complicated ""likelihood expression"" that is a function of all of the observed 
  
    
      
        y
      
    
    {\displaystyle y}
   and 
  
    
      
        x
      
    
    {\displaystyle x}
   values.  The estimation approach is explained below.

Logistic function, odds, odds ratio, and logit
Definition of the logistic function
An explanation of logistic regression can begin with an explanation of the standard logistic function. The logistic function is a sigmoid function, which takes any real input 
  
    
      
        t
      
    
    {\displaystyle t}
  , and outputs a value between zero and one. For the logit, this is interpreted as taking input log-odds and having output probability. The standard logistic function 
  
    
      
        σ
        :
        
          R
        
        →
        (
        0
        ,
        1
        )
      
    
    {\displaystyle \sigma :\mathbb {R} \rightarrow (0,1)}
   is defined as follows:

  
    
      
        σ
        (
        t
        )
        =
        
          
            
              e
              
                t
              
            
            
              
                e
                
                  t
                
              
              +
              1
            
          
        
        =
        
          
            1
            
              1
              +
              
                e
                
                  −
                  t
                
              
            
          
        
      
    
    {\displaystyle \sigma (t)={\frac {e^{t}}{e^{t}+1}}={\frac {1}{1+e^{-t}}}}
  A graph of the logistic function on the t-interval (−6,6) is shown in Figure 1.
Let us assume that 
  
    
      
        t
      
    
    {\displaystyle t}
   is a linear function of a single explanatory variable 
  
    
      
        x
      
    
    {\displaystyle x}
   (the case where 
  
    
      
        t
      
    
    {\displaystyle t}
   is a linear combination of multiple explanatory variables is treated similarly). We can then express 
  
    
      
        t
      
    
    {\displaystyle t}
   as follows:

  
    
      
        t
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        x
      
    
    {\displaystyle t=\beta _{0}+\beta _{1}x}
  And the general logistic function 
  
    
      
        p
        :
        
          R
        
        →
        (
        0
        ,
        1
        )
      
    
    {\displaystyle p:\mathbb {R} \rightarrow (0,1)}
   can now be written as:

  
    
      
        p
        (
        x
        )
        =
        σ
        (
        t
        )
        =
        
          
            1
            
              1
              +
              
                e
                
                  −
                  (
                  
                    β
                    
                      0
                    
                  
                  +
                  
                    β
                    
                      1
                    
                  
                  x
                  )
                
              
            
          
        
      
    
    {\displaystyle p(x)=\sigma (t)={\frac {1}{1+e^{-(\beta _{0}+\beta _{1}x)}}}}
  In the logistic model, 
  
    
      
        p
        (
        x
        )
      
    
    {\displaystyle p(x)}
   is interpreted as the probability of the dependent variable 
  
    
      
        Y
      
    
    {\displaystyle Y}
   equaling a success/case rather than a failure/non-case. It's clear that the response variables 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
   are not identically distributed: 
  
    
      
        P
        (
        
          Y
          
            i
          
        
        =
        1
        ∣
        X
        )
      
    
    {\displaystyle P(Y_{i}=1\mid X)}
   differs from one data point 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
   to another, though they are independent given design matrix 
  
    
      
        X
      
    
    {\displaystyle X}
   and shared parameters 
  
    
      
        β
      
    
    {\displaystyle \beta }
  .

Definition of the inverse of the logistic function
We can now define the logit (log odds) function as the inverse 
  
    
      
        g
        =
        
          σ
          
            −
            1
          
        
      
    
    {\displaystyle g=\sigma ^{-1}}
   of the standard logistic function. It is easy to see that it satisfies:

  
    
      
        g
        (
        p
        (
        x
        )
        )
        =
        
          σ
          
            −
            1
          
        
        (
        p
        (
        x
        )
        )
        =
        logit
        ⁡
        p
        (
        x
        )
        =
        ln
        ⁡
        
          (
          
            
              
                p
                (
                x
                )
              
              
                1
                −
                p
                (
                x
                )
              
            
          
          )
        
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        x
        ,
      
    
    {\displaystyle g(p(x))=\sigma ^{-1}(p(x))=\operatorname {logit} p(x)=\ln \left({\frac {p(x)}{1-p(x)}}\right)=\beta _{0}+\beta _{1}x,}
  and equivalently, after exponentiating both sides we have the odds:

  
    
      
        
          
            
              p
              (
              x
              )
            
            
              1
              −
              p
              (
              x
              )
            
          
        
        =
        
          e
          
            
              β
              
                0
              
            
            +
            
              β
              
                1
              
            
            x
          
        
        .
      
    
    {\displaystyle {\frac {p(x)}{1-p(x)}}=e^{\beta _{0}+\beta _{1}x}.}

Interpretation of these terms
In the above equations, the terms are as follows:

  
    
      
        g
      
    
    {\displaystyle g}
   is the logit function. The equation for 
  
    
      
        g
        (
        p
        (
        x
        )
        )
      
    
    {\displaystyle g(p(x))}
   illustrates that the logit (i.e., log-odds or natural logarithm of the odds) is equivalent to the linear regression expression.

  
    
      
        ln
      
    
    {\displaystyle \ln }
   denotes the natural logarithm.

  
    
      
        p
        (
        x
        )
      
    
    {\displaystyle p(x)}
   is the probability that the dependent variable equals a case, given some linear combination of the predictors. The formula for 
  
    
      
        p
        (
        x
        )
      
    
    {\displaystyle p(x)}
   illustrates that the probability of the dependent variable equaling a case is equal to the value of the logistic function of the linear regression expression. This is important in that it shows that the value of the linear regression expression can vary from negative to positive infinity and yet, after transformation, the resulting expression for the probability 
  
    
      
        p
        (
        x
        )
      
    
    {\displaystyle p(x)}
   ranges between 0 and 1.

  
    
      
        
          β
          
            0
          
        
      
    
    {\displaystyle \beta _{0}}
   is the intercept from the linear regression equation (the value of the criterion when the predictor is equal to zero).

  
    
      
        
          β
          
            1
          
        
        x
      
    
    {\displaystyle \beta _{1}x}
   is the regression coefficient multiplied by some value of the predictor.
base 
  
    
      
        e
      
    
    {\displaystyle e}
   denotes the exponential function.

Definition of the odds
The odds of the dependent variable equaling a case (given some linear combination 
  
    
      
        x
      
    
    {\displaystyle x}
   of the predictors) is equivalent to the exponential function of the linear regression expression. This illustrates how the logit serves as a link function between the probability and the linear regression expression. Given that the logit ranges between negative and positive infinity, it provides an adequate criterion upon which to conduct linear regression and the logit is easily converted back into the odds.So we define odds of the dependent variable equaling a case (given some linear combination 
  
    
      
        x
      
    
    {\displaystyle x}
   of the predictors) as follows:

  
    
      
        
          odds
        
        =
        
          e
          
            
              β
              
                0
              
            
            +
            
              β
              
                1
              
            
            x
          
        
        .
      
    
    {\displaystyle {\text{odds}}=e^{\beta _{0}+\beta _{1}x}.}

The odds ratio
For a continuous independent variable the odds ratio can be defined as:

  
    
      
        
          O
          R
        
        =
        
          
            
              odds
              ⁡
              (
              x
              +
              1
              )
            
            
              odds
              ⁡
              (
              x
              )
            
          
        
        =
        
          
            
              (
              
                
                  
                    F
                    (
                    x
                    +
                    1
                    )
                  
                  
                    1
                    −
                    F
                    (
                    x
                    +
                    1
                    )
                  
                
              
              )
            
            
              (
              
                
                  
                    F
                    (
                    x
                    )
                  
                  
                    1
                    −
                    F
                    (
                    x
                    )
                  
                
              
              )
            
          
        
        =
        
          
            
              e
              
                
                  β
                  
                    0
                  
                
                +
                
                  β
                  
                    1
                  
                
                (
                x
                +
                1
                )
              
            
            
              e
              
                
                  β
                  
                    0
                  
                
                +
                
                  β
                  
                    1
                  
                
                x
              
            
          
        
        =
        
          e
          
            
              β
              
                1
              
            
          
        
      
    
    {\displaystyle \mathrm {OR} ={\frac {\operatorname {odds} (x+1)}{\operatorname {odds} (x)}}={\frac {\left({\frac {F(x+1)}{1-F(x+1)}}\right)}{\left({\frac {F(x)}{1-F(x)}}\right)}}={\frac {e^{\beta _{0}+\beta _{1}(x+1)}}{e^{\beta _{0}+\beta _{1}x}}}=e^{\beta _{1}}}
  This exponential relationship provides an interpretation for 
  
    
      
        
          β
          
            1
          
        
      
    
    {\displaystyle \beta _{1}}
  : The odds multiply by 
  
    
      
        
          e
          
            
              β
              
                1
              
            
          
        
      
    
    {\displaystyle e^{\beta _{1}}}
   for every 1-unit increase in x.For a binary independent variable the odds ratio is defined as 
  
    
      
        
          
            
              a
              d
            
            
              b
              c
            
          
        
      
    
    {\displaystyle {\frac {ad}{bc}}}
   where a, b, c and d are cells in a 2×2 contingency table.

Multiple explanatory variables
If there are multiple explanatory variables, the above expression 
  
    
      
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        x
      
    
    {\displaystyle \beta _{0}+\beta _{1}x}
   can be revised to 
  
    
      
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          x
          
            1
          
        
        +
        
          β
          
            2
          
        
        
          x
          
            2
          
        
        +
        ⋯
        +
        
          β
          
            m
          
        
        
          x
          
            m
          
        
        =
        
          β
          
            0
          
        
        +
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          β
          
            i
          
        
        
          x
          
            i
          
        
      
    
    {\displaystyle \beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}+\cdots +\beta _{m}x_{m}=\beta _{0}+\sum _{i=1}^{m}\beta _{i}x_{i}}
  . Then when this is used in the equation relating the log odds of a success to the values of the predictors, the linear regression will be a multiple regression with m explanators; the parameters 
  
    
      
        
          β
          
            j
          
        
      
    
    {\displaystyle \beta _{j}}
   for all j = 0, 1, 2, ..., m are all estimated.
Again, the more traditional equations are:

  
    
      
        log
        ⁡
        
          
            p
            
              1
              −
              p
            
          
        
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          x
          
            1
          
        
        +
        
          β
          
            2
          
        
        
          x
          
            2
          
        
        +
        ⋯
        +
        
          β
          
            m
          
        
        
          x
          
            m
          
        
      
    
    {\displaystyle \log {\frac {p}{1-p}}=\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}+\cdots +\beta _{m}x_{m}}
  and

  
    
      
        p
        =
        
          
            1
            
              1
              +
              
                b
                
                  −
                  (
                  
                    β
                    
                      0
                    
                  
                  +
                  
                    β
                    
                      1
                    
                  
                  
                    x
                    
                      1
                    
                  
                  +
                  
                    β
                    
                      2
                    
                  
                  
                    x
                    
                      2
                    
                  
                  +
                  ⋯
                  +
                  
                    β
                    
                      m
                    
                  
                  
                    x
                    
                      m
                    
                  
                  )
                
              
            
          
        
      
    
    {\displaystyle p={\frac {1}{1+b^{-(\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}+\cdots +\beta _{m}x_{m})}}}}
  where usually 
  
    
      
        b
        =
        e
      
    
    {\displaystyle b=e}
  .

Model fitting
Logistic regression is an important machine learning algorithm. The goal is to model the probability of a random variable 
  
    
      
        Y
      
    
    {\displaystyle Y}
   being 0 or 1 given experimental data.Consider a generalized linear model function parameterized by 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  ,

  
    
      
        
          h
          
            θ
          
        
        (
        X
        )
        =
        
          
            1
            
              1
              +
              
                e
                
                  −
                  
                    θ
                    
                      T
                    
                  
                  X
                
              
            
          
        
        =
        Pr
        (
        Y
        =
        1
        ∣
        X
        ;
        θ
        )
      
    
    {\displaystyle h_{\theta }(X)={\frac {1}{1+e^{-\theta ^{T}X}}}=\Pr(Y=1\mid X;\theta )}
  Therefore,

  
    
      
        Pr
        (
        Y
        =
        0
        ∣
        X
        ;
        θ
        )
        =
        1
        −
        
          h
          
            θ
          
        
        (
        X
        )
      
    
    {\displaystyle \Pr(Y=0\mid X;\theta )=1-h_{\theta }(X)}
  and since 
  
    
      
        Y
        ∈
        {
        0
        ,
        1
        }
      
    
    {\displaystyle Y\in \{0,1\}}
  , we see that 
  
    
      
        Pr
        (
        y
        ∣
        X
        ;
        θ
        )
      
    
    {\displaystyle \Pr(y\mid X;\theta )}
   is given by 
  
    
      
        Pr
        (
        y
        ∣
        X
        ;
        θ
        )
        =
        
          h
          
            θ
          
        
        (
        X
        
          )
          
            y
          
        
        (
        1
        −
        
          h
          
            θ
          
        
        (
        X
        )
        
          )
          
            (
            1
            −
            y
            )
          
        
        .
      
    
    {\displaystyle \Pr(y\mid X;\theta )=h_{\theta }(X)^{y}(1-h_{\theta }(X))^{(1-y)}.}
   We now calculate the likelihood function assuming that all the observations in the sample are independently Bernoulli distributed,

  
    
      
        
          
            
              
                L
                (
                θ
                ∣
                y
                ;
                x
                )
              
              
                
                =
                Pr
                (
                Y
                ∣
                X
                ;
                θ
                )
              
            
            
              
              
                
                =
                
                  ∏
                  
                    i
                  
                
                Pr
                (
                
                  y
                  
                    i
                  
                
                ∣
                
                  x
                  
                    i
                  
                
                ;
                θ
                )
              
            
            
              
              
                
                =
                
                  ∏
                  
                    i
                  
                
                
                  h
                  
                    θ
                  
                
                (
                
                  x
                  
                    i
                  
                
                
                  )
                  
                    
                      y
                      
                        i
                      
                    
                  
                
                (
                1
                −
                
                  h
                  
                    θ
                  
                
                (
                
                  x
                  
                    i
                  
                
                )
                
                  )
                  
                    (
                    1
                    −
                    
                      y
                      
                        i
                      
                    
                    )
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}L(\theta \mid y;x)&=\Pr(Y\mid X;\theta )\\&=\prod _{i}\Pr(y_{i}\mid x_{i};\theta )\\&=\prod _{i}h_{\theta }(x_{i})^{y_{i}}(1-h_{\theta }(x_{i}))^{(1-y_{i})}\end{aligned}}}
  Typically, the log likelihood is maximized,

  
    
      
        
          N
          
            −
            1
          
        
        log
        ⁡
        L
        (
        θ
        ∣
        y
        ;
        x
        )
        =
        
          N
          
            −
            1
          
        
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        log
        ⁡
        Pr
        (
        
          y
          
            i
          
        
        ∣
        
          x
          
            i
          
        
        ;
        θ
        )
      
    
    {\displaystyle N^{-1}\log L(\theta \mid y;x)=N^{-1}\sum _{i=1}^{N}\log \Pr(y_{i}\mid x_{i};\theta )}
  which is maximized using optimization techniques such as gradient descent.
Assuming the 
  
    
      
        (
        x
        ,
        y
        )
      
    
    {\displaystyle (x,y)}
   pairs are drawn uniformly from the underlying distribution, then in the limit of large N,

  
    
      
        
          
            
              
              
                
                
                  lim
                  
                    N
                    →
                    +
                    ∞
                  
                
                
                  N
                  
                    −
                    1
                  
                
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    N
                  
                
                log
                ⁡
                Pr
                (
                
                  y
                  
                    i
                  
                
                ∣
                
                  x
                  
                    i
                  
                
                ;
                θ
                )
                =
                
                  ∑
                  
                    x
                    ∈
                    
                      
                        X
                      
                    
                  
                
                
                  ∑
                  
                    y
                    ∈
                    
                      
                        Y
                      
                    
                  
                
                Pr
                (
                X
                =
                x
                ,
                Y
                =
                y
                )
                log
                ⁡
                Pr
                (
                Y
                =
                y
                ∣
                X
                =
                x
                ;
                θ
                )
              
            
            
              
                =
                

                
              
              
                
                
                  ∑
                  
                    x
                    ∈
                    
                      
                        X
                      
                    
                  
                
                
                  ∑
                  
                    y
                    ∈
                    
                      
                        Y
                      
                    
                  
                
                Pr
                (
                X
                =
                x
                ,
                Y
                =
                y
                )
                
                  (
                  
                    −
                    log
                    ⁡
                    
                      
                        
                          Pr
                          (
                          Y
                          =
                          y
                          ∣
                          X
                          =
                          x
                          )
                        
                        
                          Pr
                          (
                          Y
                          =
                          y
                          ∣
                          X
                          =
                          x
                          ;
                          θ
                          )
                        
                      
                    
                    +
                    log
                    ⁡
                    Pr
                    (
                    Y
                    =
                    y
                    ∣
                    X
                    =
                    x
                    )
                  
                  )
                
              
            
            
              
                =
                

                
              
              
                
                −
                
                  D
                  
                    KL
                  
                
                (
                Y
                ∥
                
                  Y
                  
                    θ
                  
                
                )
                −
                H
                (
                Y
                ∣
                X
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&\lim \limits _{N\rightarrow +\infty }N^{-1}\sum _{i=1}^{N}\log \Pr(y_{i}\mid x_{i};\theta )=\sum _{x\in {\mathcal {X}}}\sum _{y\in {\mathcal {Y}}}\Pr(X=x,Y=y)\log \Pr(Y=y\mid X=x;\theta )\\[6pt]={}&\sum _{x\in {\mathcal {X}}}\sum _{y\in {\mathcal {Y}}}\Pr(X=x,Y=y)\left(-\log {\frac {\Pr(Y=y\mid X=x)}{\Pr(Y=y\mid X=x;\theta )}}+\log \Pr(Y=y\mid X=x)\right)\\[6pt]={}&-D_{\text{KL}}(Y\parallel Y_{\theta })-H(Y\mid X)\end{aligned}}}
  where 
  
    
      
        H
        (
        Y
        ∣
        X
        )
      
    
    {\displaystyle H(Y\mid X)}
   is the conditional entropy and 
  
    
      
        
          D
          
            KL
          
        
      
    
    {\displaystyle D_{\text{KL}}}
   is the Kullback–Leibler divergence. This leads to the intuition that by maximizing the log-likelihood of a model, you are minimizing the KL divergence of your model from the maximal entropy distribution. Intuitively searching for the model that makes the fewest assumptions in its parameters.

""Rule of ten""
A widely used rule of thumb, the ""one in ten rule"", states that logistic regression models give stable values for the explanatory variables if based on a minimum of about 10 events per explanatory variable (EPV); where event denotes the cases belonging to the less frequent category in the dependent variable. Thus a study designed to use 
  
    
      
        k
      
    
    {\displaystyle k}
   explanatory variables for an event (e.g. myocardial infarction) expected to occur in a proportion 
  
    
      
        p
      
    
    {\displaystyle p}
   of participants in the study will require a total of 
  
    
      
        10
        k
        
          /
        
        p
      
    
    {\displaystyle 10k/p}
   participants. However, there is considerable debate about the reliability of this rule, which is based on simulation studies and lacks a secure theoretical underpinning. According to some authors the rule is overly conservative, some circumstances; with the authors stating ""If we (somewhat subjectively) regard confidence interval coverage less than 93 percent, type I error greater than 7 percent, or relative bias greater than 15 percent as problematic, our results indicate that problems are fairly frequent with 2–4 EPV, uncommon with 5–9 EPV, and still observed with 10–16 EPV. The worst instances of each problem were not severe with 5–9 EPV and usually comparable to those with 10–16 EPV"".Others have found results that are not consistent with the above, using different criteria.  A useful criterion is whether the fitted model will be expected to achieve the same predictive discrimination in a new sample as it appeared to achieve in the model development sample.  For that criterion, 20 events per candidate variable may be required.  Also, one can argue that 96 observations are needed only to estimate the model's intercept precisely enough that the margin of error in predicted probabilities is ±0.1 with an 0.95 confidence level.

Maximum likelihood estimation (MLE)
The regression coefficients are usually estimated using maximum likelihood estimation. Unlike linear regression with normally distributed residuals, it is not possible to find a closed-form expression for the coefficient values that maximize the likelihood function, so that an iterative process must be used instead; for example Newton's method. This process begins with a tentative solution, revises it slightly to see if it can be improved, and repeats this revision until no more improvement is made, at which point the process is said to have converged.In some instances, the model may not reach convergence. Non-convergence of a model indicates that the coefficients are not meaningful because the iterative process was unable to find appropriate solutions. A failure to converge may occur for a number of reasons: having a large ratio of predictors to cases, multicollinearity, sparseness, or complete separation.

Having a large ratio of variables to cases results in an overly conservative Wald statistic (discussed below) and can lead to non-convergence. Regularized logistic regression is specifically intended to be used in this situation.
Multicollinearity refers to unacceptably high correlations between predictors. As multicollinearity increases, coefficients remain unbiased but standard errors increase and the likelihood of model convergence decreases. To detect multicollinearity amongst the predictors, one can conduct a linear regression analysis with the predictors of interest for the sole purpose of examining the tolerance statistic   used to assess whether multicollinearity is unacceptably high.
Sparseness in the data refers to having a large proportion of empty cells (cells with zero counts). Zero cell counts are particularly problematic with categorical predictors. With continuous predictors, the model can infer values for the zero cell counts, but this is not the case with categorical predictors. The model will not converge with zero cell counts for categorical predictors because the natural logarithm of zero is an undefined value so that the final solution to the model cannot be reached. To remedy this problem, researchers may collapse categories in a theoretically meaningful way or add a constant to all cells.
Another numerical problem that may lead to a lack of convergence is complete separation, which refers to the instance in which the predictors perfectly predict the criterion – all cases are accurately classified. In such instances, one should reexamine the data, as there is likely some kind of error.
One can also take semi-parametric or non-parametric approaches, e.g., via local-likelihood or nonparametric quasi-likelihood methods, which avoid assumptions of a parametric form for the index function and is robust to the choice of the link function (e.g., probit or logit).

Cross-entropy Loss function
In machine learning applications where logistic regression is used for binary classification, the MLE minimises the Cross entropy loss function.

Iteratively reweighted least squares (IRLS)
Binary logistic regression (
  
    
      
        y
        =
        0
      
    
    {\displaystyle y=0}
   or 
  
    
      
        y
        =
        1
      
    
    {\displaystyle y=1}
  ) can, for example, be calculated using iteratively reweighted least squares (IRLS), which is equivalent to maximizing the log-likelihood of a Bernoulli distributed  process using Newton's method. If the problem is written in vector matrix form, with parameters 
  
    
      
        
          
            w
          
          
            T
          
        
        =
        [
        
          β
          
            0
          
        
        ,
        
          β
          
            1
          
        
        ,
        
          β
          
            2
          
        
        ,
        …
        ]
      
    
    {\displaystyle \mathbf {w} ^{T}=[\beta _{0},\beta _{1},\beta _{2},\ldots ]}
  , explanatory variables 
  
    
      
        
          x
        
        (
        i
        )
        =
        [
        1
        ,
        
          x
          
            1
          
        
        (
        i
        )
        ,
        
          x
          
            2
          
        
        (
        i
        )
        ,
        …
        
          ]
          
            T
          
        
      
    
    {\displaystyle \mathbf {x} (i)=[1,x_{1}(i),x_{2}(i),\ldots ]^{T}}
   and expected value of the Bernoulli distribution 
  
    
      
        μ
        (
        i
        )
        =
        
          
            1
            
              1
              +
              
                e
                
                  −
                  
                    
                      w
                    
                    
                      T
                    
                  
                  
                    x
                  
                  (
                  i
                  )
                
              
            
          
        
      
    
    {\displaystyle \mu (i)={\frac {1}{1+e^{-\mathbf {w} ^{T}\mathbf {x} (i)}}}}
  , the parameters 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   can be found using the following iterative algorithm:

  
    
      
        
          
            w
          
          
            k
            +
            1
          
        
        =
        
          
            (
            
              
                
                  X
                
                
                  T
                
              
              
                
                  S
                
                
                  k
                
              
              
                X
              
            
            )
          
          
            −
            1
          
        
        
          
            X
          
          
            T
          
        
        
          (
          
            
              
                S
              
              
                k
              
            
            
              X
            
            
              
                w
              
              
                k
              
            
            +
            
              y
            
            −
            
              
                μ
              
              
                k
              
            
          
          )
        
      
    
    {\displaystyle \mathbf {w} _{k+1}=\left(\mathbf {X} ^{T}\mathbf {S} _{k}\mathbf {X} \right)^{-1}\mathbf {X} ^{T}\left(\mathbf {S} _{k}\mathbf {X} \mathbf {w} _{k}+\mathbf {y} -\mathbf {\boldsymbol {\mu }} _{k}\right)}
  where 
  
    
      
        
          S
        
        =
        diag
        ⁡
        (
        μ
        (
        i
        )
        (
        1
        −
        μ
        (
        i
        )
        )
        )
      
    
    {\displaystyle \mathbf {S} =\operatorname {diag} (\mu (i)(1-\mu (i)))}
   is a diagonal weighting matrix, 
  
    
      
        
          μ
        
        =
        [
        μ
        (
        1
        )
        ,
        μ
        (
        2
        )
        ,
        …
        ]
      
    
    {\displaystyle {\boldsymbol {\mu }}=[\mu (1),\mu (2),\ldots ]}
   the vector of expected values,

  
    
      
        
          X
        
        =
        
          
            [
            
              
                
                  1
                
                
                  
                    x
                    
                      1
                    
                  
                  (
                  1
                  )
                
                
                  
                    x
                    
                      2
                    
                  
                  (
                  1
                  )
                
                
                  …
                
              
              
                
                  1
                
                
                  
                    x
                    
                      1
                    
                  
                  (
                  2
                  )
                
                
                  
                    x
                    
                      2
                    
                  
                  (
                  2
                  )
                
                
                  …
                
              
              
                
                  ⋮
                
                
                  ⋮
                
                
                  ⋮
                
              
            
            ]
          
        
      
    
    {\displaystyle \mathbf {X} ={\begin{bmatrix}1&x_{1}(1)&x_{2}(1)&\ldots \\1&x_{1}(2)&x_{2}(2)&\ldots \\\vdots &\vdots &\vdots \end{bmatrix}}}
  The regressor matrix and 
  
    
      
        
          y
        
        (
        i
        )
        =
        [
        y
        (
        1
        )
        ,
        y
        (
        2
        )
        ,
        …
        
          ]
          
            T
          
        
      
    
    {\displaystyle \mathbf {y} (i)=[y(1),y(2),\ldots ]^{T}}
   the vector of response variables. More details can be found in the literature.

Evaluating goodness of fit
Goodness of fit in linear regression models is generally measured using R2. Since this has no direct analog in logistic regression, various methods including the following can be used instead.

Deviance and likelihood ratio tests
In linear regression analysis, one is concerned with partitioning variance via the sum of squares calculations – variance in the criterion is essentially divided into variance accounted for by the predictors and residual variance. In logistic regression analysis, deviance is used in lieu of a sum of squares calculations. Deviance is analogous to the sum of squares calculations in linear regression  and is a measure of the lack of fit to the data in a logistic regression model. When a ""saturated"" model is available (a model with a theoretically perfect fit), deviance is calculated by comparing a given model with the saturated model.  This computation gives the likelihood-ratio test:

  
    
      
        D
        =
        −
        2
        ln
        ⁡
        
          
            likelihood of the fitted model
            likelihood of the saturated model
          
        
        .
      
    
    {\displaystyle D=-2\ln {\frac {\text{likelihood of the fitted model}}{\text{likelihood of the saturated model}}}.}
  In the above equation, D represents the deviance and ln represents the natural logarithm. The log of this likelihood ratio (the ratio of the fitted model to the saturated model) will produce a negative value, hence the need for a negative sign. D can be shown to follow an approximate chi-squared distribution.  Smaller values indicate better fit as the fitted model deviates less from the saturated model. When assessed upon a chi-square distribution, nonsignificant chi-square values indicate very little unexplained variance and thus, good model fit. Conversely, a significant chi-square value indicates that a significant amount of the variance is unexplained.
When the saturated model is not available (a common case), deviance is calculated simply as −2·(log likelihood of the fitted model), and the reference to the saturated model's log likelihood can be removed from all that follows without harm.
Two measures of deviance are particularly important in logistic regression: null deviance and model deviance. The null deviance represents the difference between a model with only the intercept (which means ""no predictors"") and the saturated model. The model deviance represents the difference between a model with at least one predictor and the saturated model. In this respect, the null model provides a baseline upon which to compare predictor models. Given that deviance is a measure of the difference between a given model and the saturated model, smaller values indicate better fit. Thus, to assess the contribution of a predictor or set of predictors, one can subtract the model deviance from the null deviance and assess the difference on a 
  
    
      
        
          χ
          
            s
            −
            p
          
          
            2
          
        
        ,
      
    
    {\displaystyle \chi _{s-p}^{2},}
    chi-square distribution with degrees of freedom equal to the difference in the number of parameters estimated.
Let

  
    
      
        
          
            
              
                
                  D
                  
                    null
                  
                
              
              
                
                =
                −
                2
                ln
                ⁡
                
                  
                    likelihood of null model
                    likelihood of the saturated model
                  
                
              
            
            
              
                
                  D
                  
                    fitted
                  
                
              
              
                
                =
                −
                2
                ln
                ⁡
                
                  
                    likelihood of fitted model
                    likelihood of the saturated model
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}D_{\text{null}}&=-2\ln {\frac {\text{likelihood of null model}}{\text{likelihood of the saturated model}}}\\[6pt]D_{\text{fitted}}&=-2\ln {\frac {\text{likelihood of fitted model}}{\text{likelihood of the saturated model}}}.\end{aligned}}}
  Then the difference of both is:

  
    
      
        
          
            
              
                
                  D
                  
                    null
                  
                
                −
                
                  D
                  
                    fitted
                  
                
              
              
                
                =
                −
                2
                
                  (
                  
                    ln
                    ⁡
                    
                      
                        likelihood of null model
                        likelihood of the saturated model
                      
                    
                    −
                    ln
                    ⁡
                    
                      
                        likelihood of fitted model
                        likelihood of the saturated model
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                −
                2
                ln
                ⁡
                
                  
                    
                      (
                      
                        
                          
                            likelihood of null model
                            likelihood of the saturated model
                          
                        
                      
                      )
                    
                    
                      (
                      
                        
                          
                            likelihood of fitted model
                            likelihood of the saturated model
                          
                        
                      
                      )
                    
                  
                
              
            
            
              
              
                
                =
                −
                2
                ln
                ⁡
                
                  
                    likelihood of the null model
                    likelihood of fitted model
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}D_{\text{null}}-D_{\text{fitted}}&=-2\left(\ln {\frac {\text{likelihood of null model}}{\text{likelihood of the saturated model}}}-\ln {\frac {\text{likelihood of fitted model}}{\text{likelihood of the saturated model}}}\right)\\[6pt]&=-2\ln {\frac {\left({\dfrac {\text{likelihood of null model}}{\text{likelihood of the saturated model}}}\right)}{\left({\dfrac {\text{likelihood of fitted model}}{\text{likelihood of the saturated model}}}\right)}}\\[6pt]&=-2\ln {\frac {\text{likelihood of the null model}}{\text{likelihood of fitted model}}}.\end{aligned}}}
  If the model deviance is significantly smaller than the null deviance then one can conclude that the predictor or set of predictors significantly improved model fit. This is analogous to the F-test used in linear regression analysis to assess the significance of prediction.

Pseudo-R-squared
In linear regression the squared multiple correlation, R² is used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors. In logistic regression analysis, there is no agreed upon analogous measure, but there are several competing measures each with limitations.Four of the most commonly used indices and one less commonly used one are examined on this page:

Likelihood ratio R²L
Cox and Snell R²CS
Nagelkerke R²N
McFadden R²McF
Tjur R²TR²L is given by Cohen:

  
    
      
        
          R
          
            L
          
          
            2
          
        
        =
        
          
            
              
                D
                
                  null
                
              
              −
              
                D
                
                  fitted
                
              
            
            
              D
              
                null
              
            
          
        
        .
      
    
    {\displaystyle R_{\text{L}}^{2}={\frac {D_{\text{null}}-D_{\text{fitted}}}{D_{\text{null}}}}.}
  This is the most analogous index to the squared multiple correlations in linear regression. It represents the proportional reduction in the deviance wherein the deviance is treated as a measure of variation analogous but not identical to the variance in linear regression analysis. One limitation of the likelihood ratio R² is that it is not monotonically related to the odds ratio, meaning that it does not necessarily increase as the odds ratio increases and does not necessarily decrease as the odds ratio decreases.
R²CS is an alternative index of goodness of fit related to the R² value from linear regression. It is given by:

  
    
      
        
          
            
              
                
                  R
                  
                    CS
                  
                  
                    2
                  
                
              
              
                
                =
                1
                −
                
                  
                    (
                    
                      
                        
                          L
                          
                            0
                          
                        
                        
                          L
                          
                            M
                          
                        
                      
                    
                    )
                  
                  
                    2
                    
                      /
                    
                    n
                  
                
              
            
            
              
              
                
                =
                1
                −
                
                  e
                  
                    2
                    (
                    ln
                    ⁡
                    (
                    
                      L
                      
                        0
                      
                    
                    )
                    −
                    ln
                    ⁡
                    (
                    
                      L
                      
                        M
                      
                    
                    )
                    )
                    
                      /
                    
                    n
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}R_{\text{CS}}^{2}&=1-\left({\frac {L_{0}}{L_{M}}}\right)^{2/n}\\[5pt]&=1-e^{2(\ln(L_{0})-\ln(L_{M}))/n}\end{aligned}}}
  where LM and L0 are the likelihoods for the model being fitted and the null model, respectively. The Cox and Snell index is problematic as its maximum value is 
  
    
      
        1
        −
        
          L
          
            0
          
          
            2
            
              /
            
            n
          
        
      
    
    {\displaystyle 1-L_{0}^{2/n}}
  . The highest this upper bound can be is 0.75, but it can easily be as low as 0.48 when the marginal proportion of cases is small.R²N provides a correction to the Cox and Snell R² so that the maximum value is equal to 1. Nevertheless, the Cox and Snell and likelihood ratio R²s show greater agreement with each other than either does with the Nagelkerke R². Of course, this might not be the case for values exceeding 0.75 as the Cox and Snell index is capped at this value. The likelihood ratio R² is often preferred to the alternatives as it is most analogous to R² in linear regression, is independent of the base rate (both Cox and Snell and Nagelkerke R²s increase as the proportion of cases increase from 0 to 0.5) and varies between 0 and 1.
R²McF is defined as

  
    
      
        
          R
          
            McF
          
          
            2
          
        
        =
        1
        −
        
          
            
              ln
              ⁡
              (
              
                L
                
                  M
                
              
              )
            
            
              ln
              ⁡
              (
              
                L
                
                  0
                
              
              )
            
          
        
        ,
      
    
    {\displaystyle R_{\text{McF}}^{2}=1-{\frac {\ln(L_{M})}{\ln(L_{0})}},}
  and is preferred over R²CS by Allison. The two expressions R²McF and R²CS are then related respectively by,

  
    
      
        
          
            
              
                
                  R
                  
                    CS
                  
                  
                    2
                  
                
                =
                1
                −
                
                  
                    (
                    
                      
                        
                          1
                          
                            L
                            
                              0
                            
                          
                        
                      
                    
                    )
                  
                  
                    
                      
                        2
                        (
                        
                          R
                          
                            McF
                          
                          
                            2
                          
                        
                        )
                      
                      n
                    
                  
                
              
            
            
              
                
                  R
                  
                    McF
                  
                  
                    2
                  
                
                =
                −
                
                  
                    
                      n
                      2
                    
                  
                
                ⋅
                
                  
                    
                      
                        ln
                        ⁡
                        (
                        1
                        −
                        
                          R
                          
                            CS
                          
                          
                            2
                          
                        
                        )
                      
                      
                        ln
                        ⁡
                        
                          L
                          
                            0
                          
                        
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{matrix}R_{\text{CS}}^{2}=1-\left({\dfrac {1}{L_{0}}}\right)^{\frac {2(R_{\text{McF}}^{2})}{n}}\\[1.5em]R_{\text{McF}}^{2}=-{\dfrac {n}{2}}\cdot {\dfrac {\ln(1-R_{\text{CS}}^{2})}{\ln L_{0}}}\end{matrix}}}
  However, Allison now prefers R²T which is a relatively new measure developed by Tjur. It can be calculated in two steps:
For each level of the dependent variable, find the mean of the predicted probabilities of an event.
Take the absolute value of the difference between these meansA word of caution is in order when interpreting pseudo-R² statistics. The reason these indices of fit are referred to as pseudo R² is that they do not represent the proportionate reduction in error as the R² in linear regression does. Linear regression assumes homoscedasticity, that the error variance is the same for all values of the criterion. Logistic regression will always be heteroscedastic – the error variances differ for each value of the predicted score. For each value of the predicted score there would be a different value of the proportionate reduction in error. Therefore, it is inappropriate to think of R² as a proportionate reduction in error in a universal sense in logistic regression.

Hosmer–Lemeshow test
The Hosmer–Lemeshow test uses a test statistic that asymptotically follows a 
  
    
      
        
          χ
          
            2
          
        
      
    
    {\displaystyle \chi ^{2}}
   distribution to assess whether or not the observed event rates match expected event rates in subgroups of the model population.  This test is considered to be obsolete by some statisticians because of its dependence on arbitrary binning of predicted probabilities and relative low power.

Coefficients
After fitting the model, it is likely that researchers will want to examine the contribution of individual predictors. To do so, they will want to examine the regression coefficients. In linear regression, the regression coefficients represent the change in the criterion for each unit change in the predictor. In logistic regression, however, the regression coefficients represent the change in the logit for each unit change in the predictor. Given that the logit is not intuitive, researchers are likely to focus on a predictor's effect on the exponential function of the regression coefficient – the odds ratio (see definition). In linear regression, the significance of a regression coefficient is assessed by computing a t test. In logistic regression, there are several different tests designed to assess the significance of an individual predictor, most notably the likelihood ratio test and the Wald statistic.

Likelihood ratio test
The likelihood-ratio test discussed above to assess model fit is also the recommended procedure to assess the contribution of individual ""predictors"" to a given model. In the case of a single predictor model, one simply compares the deviance of the predictor model with that of the null model on a chi-square distribution with a single degree of freedom. If the predictor model has significantly smaller deviance (c.f chi-square using the difference in degrees of freedom of the two models), then one can conclude that there is a significant association between the ""predictor"" and the outcome. Although some common statistical packages (e.g. SPSS) do provide likelihood ratio test statistics, without this computationally intensive test it would be more difficult to assess the contribution of individual predictors in the multiple logistic regression case. To assess the contribution of individual predictors one can enter the predictors hierarchically, comparing each new model with the previous to determine the contribution of each predictor. There is some debate among statisticians about the appropriateness of so-called ""stepwise"" procedures. The fear is that they may not preserve nominal statistical properties and may become misleading.

Wald statistic
Alternatively, when assessing the contribution of individual predictors in a given model, one may examine the significance of the Wald statistic. The Wald statistic, analogous to the t-test in linear regression, is used to assess the significance of coefficients. The Wald statistic is the ratio of the square of the regression coefficient to the square of the standard error of the coefficient and is asymptotically distributed as a chi-square distribution.

  
    
      
        
          W
          
            j
          
        
        =
        
          
            
              β
              
                j
              
              
                2
              
            
            
              S
              
                E
                
                  
                    β
                    
                      j
                    
                  
                
                
                  2
                
              
            
          
        
      
    
    {\displaystyle W_{j}={\frac {\beta _{j}^{2}}{SE_{\beta _{j}}^{2}}}}
  Although several statistical packages (e.g., SPSS, SAS) report the Wald statistic to assess the contribution of individual predictors, the Wald statistic has limitations. When the regression coefficient is large, the standard error of the regression coefficient also tends to be larger increasing the probability of Type-II error. The Wald statistic also tends to be biased when data are sparse.

Case-control sampling
Suppose cases are rare. Then we might wish to sample them more frequently than their prevalence in the population. For example, suppose there is a disease that affects 1 person in 10,000 and to collect our data we need to do a complete physical. It may be too expensive to do thousands of physicals of healthy people in order to obtain data for only a few diseased individuals. Thus, we may evaluate more diseased individuals, perhaps all of the rare outcomes. This is also retrospective sampling, or equivalently it is called unbalanced data. As a rule of thumb, sampling controls at a rate of five times the number of cases will produce sufficient control data.Logistic regression is unique in that it may be estimated on unbalanced data, rather than randomly sampled data, and still yield correct coefficient estimates of the effects of each independent variable on the outcome.  That is to say, if we form a logistic model from such data, if the model is correct in the general population, the 
  
    
      
        
          β
          
            j
          
        
      
    
    {\displaystyle \beta _{j}}
   parameters are all correct except for 
  
    
      
        
          β
          
            0
          
        
      
    
    {\displaystyle \beta _{0}}
  . We can correct 
  
    
      
        
          β
          
            0
          
        
      
    
    {\displaystyle \beta _{0}}
   if we know the true prevalence as follows:

  
    
      
        
          
            
              
                β
                ^
              
            
          
          
            0
          
          
            ∗
          
        
        =
        
          
            
              
                β
                ^
              
            
          
          
            0
          
        
        +
        log
        ⁡
        
          
            π
            
              1
              −
              π
            
          
        
        −
        log
        ⁡
        
          
            
              
                
                  π
                  ~
                
              
            
            
              1
              −
              
                
                  
                    π
                    ~
                  
                
              
            
          
        
      
    
    {\displaystyle {\widehat {\beta }}_{0}^{*}={\widehat {\beta }}_{0}+\log {\frac {\pi }{1-\pi }}-\log {{\tilde {\pi }} \over {1-{\tilde {\pi }}}}}
  where 
  
    
      
        π
      
    
    {\displaystyle \pi }
   is the true prevalence and 
  
    
      
        
          
            
              π
              ~
            
          
        
      
    
    {\displaystyle {\tilde {\pi }}}
   is the prevalence in the sample.

Formal mathematical specification
There are various equivalent specifications of logistic regression, which fit into different types of more general models.  These different specifications allow for different sorts of useful generalizations.

Setup
The basic setup of logistic regression is as follows. We are given a dataset containing N points. Each point i consists of a set of m input variables x1,i ... xm,i (also called independent variables, predictor variables, features, or attributes), and a binary outcome variable Yi (also known as a dependent variable, response variable, output variable, or class), i.e. it can assume only the two possible values 0 (often meaning ""no"" or ""failure"") or 1 (often meaning ""yes"" or ""success""). The goal of logistic regression is to use the dataset to create a predictive model of the outcome variable.
Some examples:

The observed outcomes are the presence or absence of a given disease (e.g. diabetes) in a set of patients, and the explanatory variables might be characteristics of the patients thought to be pertinent (sex, race, age, blood pressure, body-mass index, etc.).
The observed outcomes are the votes (e.g. Democratic or Republican) of a set of people in an election, and the explanatory variables are the demographic characteristics of each person (e.g. sex, race, age, income, etc.).  In such a case, one of the two outcomes is arbitrarily coded as 1, and the other as 0.As in linear regression, the outcome variables Yi are assumed to depend on the explanatory variables x1,i ... xm,i.

Explanatory variablesAs shown above in the above examples, the explanatory variables may be of any type: real-valued, binary, categorical, etc.  The main distinction is between continuous variables (such as income, age and blood pressure) and discrete variables (such as sex or race). Discrete variables referring to more than two possible choices are typically coded using dummy variables (or indicator variables), that is, separate explanatory variables taking the value 0 or 1 are created for each possible value of the discrete variable, with a 1 meaning ""variable does have the given value"" and a 0 meaning ""variable does not have that value"".
For example, a four-way discrete variable of blood type with the possible values ""A, B, AB, O"" can be converted to four separate two-way dummy variables, ""is-A, is-B, is-AB, is-O"", where only one of them has the value 1 and all the rest have the value 0. This allows for separate regression coefficients to be matched for each possible value of the discrete variable. (In a case like this, only three of the four dummy variables are independent of each other, in the sense that once the values of three of the variables are known, the fourth is automatically determined.  Thus, it is necessary to encode only three of the four possibilities as dummy variables.  This also means that when all four possibilities are encoded, the overall model is not identifiable in the absence of additional constraints such as a regularization constraint.  Theoretically, this could cause problems, but in reality almost all logistic regression models are fitted with regularization constraints.)

Outcome variablesFormally, the outcomes Yi are described as being Bernoulli-distributed data, where each outcome is determined by an unobserved probability pi that is specific to the outcome at hand, but related to the explanatory variables.  This can be expressed in any of the following equivalent forms:

  
    
      
        
          
            
              
                
                  Y
                  
                    i
                  
                
                ∣
                
                  x
                  
                    1
                    ,
                    i
                  
                
                ,
                …
                ,
                
                  x
                  
                    m
                    ,
                    i
                  
                
                 
              
              
                
                ∼
                Bernoulli
                ⁡
                (
                
                  p
                  
                    i
                  
                
                )
              
            
            
              
                
                  
                    E
                  
                
                ⁡
                [
                
                  Y
                  
                    i
                  
                
                ∣
                
                  x
                  
                    1
                    ,
                    i
                  
                
                ,
                …
                ,
                
                  x
                  
                    m
                    ,
                    i
                  
                
                ]
              
              
                
                =
                
                  p
                  
                    i
                  
                
              
            
            
              
                Pr
                (
                
                  Y
                  
                    i
                  
                
                =
                y
                ∣
                
                  x
                  
                    1
                    ,
                    i
                  
                
                ,
                …
                ,
                
                  x
                  
                    m
                    ,
                    i
                  
                
                )
              
              
                
                =
                
                  
                    {
                    
                      
                        
                          
                            p
                            
                              i
                            
                          
                        
                        
                          
                            if 
                          
                          y
                          =
                          1
                        
                      
                      
                        
                          1
                          −
                          
                            p
                            
                              i
                            
                          
                        
                        
                          
                            if 
                          
                          y
                          =
                          0
                        
                      
                    
                    
                  
                
              
            
            
              
                Pr
                (
                
                  Y
                  
                    i
                  
                
                =
                y
                ∣
                
                  x
                  
                    1
                    ,
                    i
                  
                
                ,
                …
                ,
                
                  x
                  
                    m
                    ,
                    i
                  
                
                )
              
              
                
                =
                
                  p
                  
                    i
                  
                  
                    y
                  
                
                (
                1
                −
                
                  p
                  
                    i
                  
                
                
                  )
                  
                    (
                    1
                    −
                    y
                    )
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}Y_{i}\mid x_{1,i},\ldots ,x_{m,i}\ &\sim \operatorname {Bernoulli} (p_{i})\\\operatorname {\mathcal {E}} [Y_{i}\mid x_{1,i},\ldots ,x_{m,i}]&=p_{i}\\\Pr(Y_{i}=y\mid x_{1,i},\ldots ,x_{m,i})&={\begin{cases}p_{i}&{\text{if }}y=1\\1-p_{i}&{\text{if }}y=0\end{cases}}\\\Pr(Y_{i}=y\mid x_{1,i},\ldots ,x_{m,i})&=p_{i}^{y}(1-p_{i})^{(1-y)}\end{aligned}}}
  The meanings of these four lines are:

The first line expresses the probability distribution of each Yi: Conditioned on the explanatory variables, it follows a Bernoulli distribution with parameters pi, the probability of the outcome of 1 for trial i. As noted above, each separate trial has its own probability of success, just as each trial has its own explanatory variables.  The probability of success pi is not observed, only the outcome of an individual Bernoulli trial using that probability.
The second line expresses the fact that the expected value of each Yi is equal to the probability of success pi, which is a general property of the Bernoulli distribution.  In other words, if we run a large number of Bernoulli trials using the same probability of success pi, then take the average of all the 1 and 0 outcomes, then the result would be close to pi.  This is because doing an average this way simply computes the proportion of successes seen, which we expect to converge to the underlying probability of success.
The third line writes out the probability mass function of the Bernoulli distribution, specifying the probability of seeing each of the two possible outcomes.
The fourth line is another way of writing the probability mass function, which avoids having to write separate cases and is more convenient for certain types of calculations.  This relies on the fact that Yi can take only the value 0 or 1.  In each case, one of the exponents will be 1, ""choosing"" the value under it, while the other is 0, ""canceling out"" the value under it.  Hence, the outcome is either pi or 1 − pi, as in the previous line.Linear predictor functionThe basic idea of logistic regression is to use the mechanism already developed for linear regression by modeling the probability pi using a linear predictor function, i.e. a linear combination of the explanatory variables and a set of regression coefficients that are specific to the model at hand but the same for all trials.  The linear predictor function 
  
    
      
        f
        (
        i
        )
      
    
    {\displaystyle f(i)}
   for a particular data point i is written as:

  
    
      
        f
        (
        i
        )
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          x
          
            1
            ,
            i
          
        
        +
        ⋯
        +
        
          β
          
            m
          
        
        
          x
          
            m
            ,
            i
          
        
        ,
      
    
    {\displaystyle f(i)=\beta _{0}+\beta _{1}x_{1,i}+\cdots +\beta _{m}x_{m,i},}
  where 
  
    
      
        
          β
          
            0
          
        
        ,
        …
        ,
        
          β
          
            m
          
        
      
    
    {\displaystyle \beta _{0},\ldots ,\beta _{m}}
   are regression coefficients indicating the relative effect of a particular explanatory variable on the outcome.
The model is usually put into a more compact form as follows:

The regression coefficients β0, β1, ..., βm are grouped into a single vector β of size m + 1.
For each data point i, an additional explanatory pseudo-variable x0,i is added, with a fixed value of 1, corresponding to the intercept coefficient β0.
The resulting explanatory variables x0,i, x1,i, ..., xm,i are then grouped into a single vector Xi of size m + 1.This makes it possible to write the linear predictor function as follows:

  
    
      
        f
        (
        i
        )
        =
        
          β
        
        ⋅
        
          
            X
          
          
            i
          
        
        ,
      
    
    {\displaystyle f(i)={\boldsymbol {\beta }}\cdot \mathbf {X} _{i},}
  using the notation for a dot product between two vectors.

As a generalized linear model
The particular model used by logistic regression, which distinguishes it from standard linear regression and from other types of regression analysis used for binary-valued outcomes, is the way the probability of a particular outcome is linked to the linear predictor function:

  
    
      
        logit
        ⁡
        (
        
          
            E
          
        
        ⁡
        [
        
          Y
          
            i
          
        
        ∣
        
          x
          
            1
            ,
            i
          
        
        ,
        …
        ,
        
          x
          
            m
            ,
            i
          
        
        ]
        )
        =
        logit
        ⁡
        (
        
          p
          
            i
          
        
        )
        =
        ln
        ⁡
        
          (
          
            
              
                p
                
                  i
                
              
              
                1
                −
                
                  p
                  
                    i
                  
                
              
            
          
          )
        
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          x
          
            1
            ,
            i
          
        
        +
        ⋯
        +
        
          β
          
            m
          
        
        
          x
          
            m
            ,
            i
          
        
      
    
    {\displaystyle \operatorname {logit} (\operatorname {\mathcal {E}} [Y_{i}\mid x_{1,i},\ldots ,x_{m,i}])=\operatorname {logit} (p_{i})=\ln \left({\frac {p_{i}}{1-p_{i}}}\right)=\beta _{0}+\beta _{1}x_{1,i}+\cdots +\beta _{m}x_{m,i}}
  Written using the more compact notation described above, this is:

  
    
      
        logit
        ⁡
        (
        
          
            E
          
        
        ⁡
        [
        
          Y
          
            i
          
        
        ∣
        
          
            X
          
          
            i
          
        
        ]
        )
        =
        logit
        ⁡
        (
        
          p
          
            i
          
        
        )
        =
        ln
        ⁡
        
          (
          
            
              
                p
                
                  i
                
              
              
                1
                −
                
                  p
                  
                    i
                  
                
              
            
          
          )
        
        =
        
          β
        
        ⋅
        
          
            X
          
          
            i
          
        
      
    
    {\displaystyle \operatorname {logit} (\operatorname {\mathcal {E}} [Y_{i}\mid \mathbf {X} _{i}])=\operatorname {logit} (p_{i})=\ln \left({\frac {p_{i}}{1-p_{i}}}\right)={\boldsymbol {\beta }}\cdot \mathbf {X} _{i}}
  This formulation expresses logistic regression as a type of generalized linear model, which predicts variables with various types of probability distributions by fitting a linear predictor function of the above form to some sort of arbitrary transformation of the expected value of the variable.
The intuition for transforming using the logit function (the natural log of the odds) was explained above.  It also has the practical effect of converting the probability (which is bounded to be between 0 and 1) to a variable that ranges over 
  
    
      
        (
        −
        ∞
        ,
        +
        ∞
        )
      
    
    {\displaystyle (-\infty ,+\infty )}
   — thereby matching the potential range of the linear prediction function on the right side of the equation.
Note that both the probabilities pi and the regression coefficients are unobserved, and the means of determining them is not part of the model itself.  They are typically determined by some sort of optimization procedure, e.g. maximum likelihood estimation, that finds values that best fit the observed data (i.e. that give the most accurate predictions for the data already observed), usually subject to regularization conditions that seek to exclude unlikely values, e.g. extremely large values for any of the regression coefficients.  The use of a regularization condition is equivalent to doing maximum a posteriori (MAP) estimation, an extension of maximum likelihood.  (Regularization is most commonly done using a squared regularizing function, which is equivalent to placing a zero-mean Gaussian prior distribution on the coefficients, but other regularizers are also possible.)  Whether or not regularization is used, it is usually not possible to find a closed-form solution; instead, an iterative numerical method must be used, such as iteratively reweighted least squares (IRLS) or, more commonly these days, a quasi-Newton method such as the L-BFGS method.The interpretation of the βj parameter estimates is as the additive effect on the log of the odds for a unit change in the j the explanatory variable.  In the case of a dichotomous explanatory variable, for instance, gender 
  
    
      
        
          e
          
            β
          
        
      
    
    {\displaystyle e^{\beta }}
   is the estimate of the odds of having the outcome for, say, males compared with females.
An equivalent formula uses the inverse of the logit function, which is the logistic function, i.e.:

  
    
      
        
          
            E
          
        
        ⁡
        [
        
          Y
          
            i
          
        
        ∣
        
          
            X
          
          
            i
          
        
        ]
        =
        
          p
          
            i
          
        
        =
        
          logit
          
            −
            1
          
        
        ⁡
        (
        
          β
        
        ⋅
        
          
            X
          
          
            i
          
        
        )
        =
        
          
            1
            
              1
              +
              
                e
                
                  −
                  
                    β
                  
                  ⋅
                  
                    
                      X
                    
                    
                      i
                    
                  
                
              
            
          
        
      
    
    {\displaystyle \operatorname {\mathcal {E}} [Y_{i}\mid \mathbf {X} _{i}]=p_{i}=\operatorname {logit} ^{-1}({\boldsymbol {\beta }}\cdot \mathbf {X} _{i})={\frac {1}{1+e^{-{\boldsymbol {\beta }}\cdot \mathbf {X} _{i}}}}}
  The formula can also be written as a probability distribution (specifically, using a probability mass function):

  
    
      
        Pr
        (
        
          Y
          
            i
          
        
        =
        y
        ∣
        
          
            X
          
          
            i
          
        
        )
        =
        
          
            
              p
              
                i
              
            
          
          
            y
          
        
        (
        1
        −
        
          p
          
            i
          
        
        
          )
          
            1
            −
            y
          
        
        =
        
          
            (
            
              
                
                  e
                  
                    
                      β
                    
                    ⋅
                    
                      
                        X
                      
                      
                        i
                      
                    
                  
                
                
                  1
                  +
                  
                    e
                    
                      
                        β
                      
                      ⋅
                      
                        
                          X
                        
                        
                          i
                        
                      
                    
                  
                
              
            
            )
          
          
            y
          
        
        
          
            (
            
              1
              −
              
                
                  
                    e
                    
                      
                        β
                      
                      ⋅
                      
                        
                          X
                        
                        
                          i
                        
                      
                    
                  
                  
                    1
                    +
                    
                      e
                      
                        
                          β
                        
                        ⋅
                        
                          
                            X
                          
                          
                            i
                          
                        
                      
                    
                  
                
              
            
            )
          
          
            1
            −
            y
          
        
        =
        
          
            
              e
              
                
                  β
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                ⋅
                y
              
            
            
              1
              +
              
                e
                
                  
                    β
                  
                  ⋅
                  
                    
                      X
                    
                    
                      i
                    
                  
                
              
            
          
        
      
    
    {\displaystyle \Pr(Y_{i}=y\mid \mathbf {X} _{i})={p_{i}}^{y}(1-p_{i})^{1-y}=\left({\frac {e^{{\boldsymbol {\beta }}\cdot \mathbf {X} _{i}}}{1+e^{{\boldsymbol {\beta }}\cdot \mathbf {X} _{i}}}}\right)^{y}\left(1-{\frac {e^{{\boldsymbol {\beta }}\cdot \mathbf {X} _{i}}}{1+e^{{\boldsymbol {\beta }}\cdot \mathbf {X} _{i}}}}\right)^{1-y}={\frac {e^{{\boldsymbol {\beta }}\cdot \mathbf {X} _{i}\cdot y}}{1+e^{{\boldsymbol {\beta }}\cdot \mathbf {X} _{i}}}}}

As a latent-variable model
The above model has an equivalent formulation as a latent-variable model.  This formulation is common in the theory of discrete choice models and makes it easier to extend to certain more complicated models with multiple, correlated choices, as well as to compare logistic regression to the closely related probit model.
Imagine that, for each trial i, there is a continuous latent variable Yi* (i.e. an unobserved random variable) that is distributed as follows:

  
    
      
        
          Y
          
            i
          
          
            ∗
          
        
        =
        
          β
        
        ⋅
        
          
            X
          
          
            i
          
        
        +
        ε
        
      
    
    {\displaystyle Y_{i}^{\ast }={\boldsymbol {\beta }}\cdot \mathbf {X} _{i}+\varepsilon \,}
  where

  
    
      
        ε
        ∼
        Logistic
        ⁡
        (
        0
        ,
        1
        )
        
      
    
    {\displaystyle \varepsilon \sim \operatorname {Logistic} (0,1)\,}
  i.e. the latent variable can be written directly in terms of the linear predictor function and an additive random error variable that is distributed according to a standard logistic distribution.
Then Yi can be viewed as an indicator for whether this latent variable is positive:

  
    
      
        
          Y
          
            i
          
        
        =
        
          
            {
            
              
                
                  1
                
                
                  
                    if 
                  
                  
                    Y
                    
                      i
                    
                    
                      ∗
                    
                  
                  >
                  0
                   
                  
                     i.e. 
                  
                  −
                  ε
                  <
                  
                    β
                  
                  ⋅
                  
                    
                      X
                    
                    
                      i
                    
                  
                  ,
                
              
              
                
                  0
                
                
                  
                    otherwise.
                  
                
              
            
            
          
        
      
    
    {\displaystyle Y_{i}={\begin{cases}1&{\text{if }}Y_{i}^{\ast }>0\ {\text{ i.e. }}-\varepsilon <{\boldsymbol {\beta }}\cdot \mathbf {X} _{i},\\0&{\text{otherwise.}}\end{cases}}}
  The choice of modeling the error variable specifically with a standard logistic distribution, rather than a general logistic distribution with the location and scale set to arbitrary values, seems restrictive, but in fact, it is not.  It must be kept in mind that we can choose the regression coefficients ourselves, and very often can use them to offset changes in the parameters of the error variable's distribution.  For example, a logistic error-variable distribution with a non-zero location parameter μ (which sets the mean) is equivalent to a distribution with a zero location parameter, where μ has been added to the intercept coefficient.  Both situations produce the same value for Yi* regardless of settings of explanatory variables.  Similarly, an arbitrary scale parameter s is equivalent to setting the scale parameter to 1 and then dividing all regression coefficients by s.  In the latter case, the resulting value of Yi* will be smaller by a factor of s than in the former case, for all sets of explanatory variables — but critically, it will always remain on the same side of 0, and hence lead to the same Yi choice.
(Note that this predicts that the irrelevancy of the scale parameter may not carry over into more complex models where more than two choices are available.)
It turns out that this formulation is exactly equivalent to the preceding one, phrased in terms of the generalized linear model and without any latent variables.  This can be shown as follows, using the fact that the cumulative distribution function (CDF) of the standard logistic distribution is the logistic function, which is the inverse of the logit function, i.e.

  
    
      
        Pr
        (
        ε
        <
        x
        )
        =
        
          logit
          
            −
            1
          
        
        ⁡
        (
        x
        )
      
    
    {\displaystyle \Pr(\varepsilon <x)=\operatorname {logit} ^{-1}(x)}
  Then:

  
    
      
        
          
            
              
                Pr
                (
                
                  Y
                  
                    i
                  
                
                =
                1
                ∣
                
                  
                    X
                  
                  
                    i
                  
                
                )
              
              
                
                =
                Pr
                (
                
                  Y
                  
                    i
                  
                  
                    ∗
                  
                
                >
                0
                ∣
                
                  
                    X
                  
                  
                    i
                  
                
                )
              
            
            
              
              
                
                =
                Pr
                (
                
                  β
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                +
                ε
                >
                0
                )
              
            
            
              
              
                
                =
                Pr
                (
                ε
                >
                −
                
                  β
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                )
              
            
            
              
              
                
                =
                Pr
                (
                ε
                <
                
                  β
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                )
              
              
              
                
                  (because the logistic distribution is symmetric)
                
              
            
            
              
              
                
                =
                
                  logit
                  
                    −
                    1
                  
                
                ⁡
                (
                
                  β
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                )
              
              
            
            
              
              
                
                =
                
                  p
                  
                    i
                  
                
              
              
              
                
                  (see above)
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\Pr(Y_{i}=1\mid \mathbf {X} _{i})&=\Pr(Y_{i}^{\ast }>0\mid \mathbf {X} _{i})\\[5pt]&=\Pr({\boldsymbol {\beta }}\cdot \mathbf {X} _{i}+\varepsilon >0)\\[5pt]&=\Pr(\varepsilon >-{\boldsymbol {\beta }}\cdot \mathbf {X} _{i})\\[5pt]&=\Pr(\varepsilon <{\boldsymbol {\beta }}\cdot \mathbf {X} _{i})&&{\text{(because the logistic distribution is symmetric)}}\\[5pt]&=\operatorname {logit} ^{-1}({\boldsymbol {\beta }}\cdot \mathbf {X} _{i})&\\[5pt]&=p_{i}&&{\text{(see above)}}\end{aligned}}}
  This formulation—which is standard in discrete choice models—makes clear the relationship between logistic regression (the ""logit model"") and the probit model, which uses an error variable distributed according to a standard normal distribution instead of a standard logistic distribution.  Both the logistic and normal distributions are symmetric with a basic unimodal, ""bell curve"" shape.  The only difference is that the logistic distribution has somewhat heavier tails, which means that it is less sensitive to outlying data (and hence somewhat more robust to model mis-specifications or erroneous data).

Two-way latent-variable model
Yet another formulation uses two separate latent variables:

  
    
      
        
          
            
              
                
                  Y
                  
                    i
                  
                  
                    0
                    ∗
                  
                
              
              
                
                =
                
                  
                    β
                  
                  
                    0
                  
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                +
                
                  ε
                  
                    0
                  
                
                
              
            
            
              
                
                  Y
                  
                    i
                  
                  
                    1
                    ∗
                  
                
              
              
                
                =
                
                  
                    β
                  
                  
                    1
                  
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                +
                
                  ε
                  
                    1
                  
                
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}Y_{i}^{0\ast }&={\boldsymbol {\beta }}_{0}\cdot \mathbf {X} _{i}+\varepsilon _{0}\,\\Y_{i}^{1\ast }&={\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}+\varepsilon _{1}\,\end{aligned}}}
  where

  
    
      
        
          
            
              
                
                  ε
                  
                    0
                  
                
              
              
                
                ∼
                
                  EV
                  
                    1
                  
                
                ⁡
                (
                0
                ,
                1
                )
              
            
            
              
                
                  ε
                  
                    1
                  
                
              
              
                
                ∼
                
                  EV
                  
                    1
                  
                
                ⁡
                (
                0
                ,
                1
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\varepsilon _{0}&\sim \operatorname {EV} _{1}(0,1)\\\varepsilon _{1}&\sim \operatorname {EV} _{1}(0,1)\end{aligned}}}
  where EV1(0,1) is a standard type-1 extreme value distribution: i.e.

  
    
      
        Pr
        (
        
          ε
          
            0
          
        
        =
        x
        )
        =
        Pr
        (
        
          ε
          
            1
          
        
        =
        x
        )
        =
        
          e
          
            −
            x
          
        
        
          e
          
            −
            
              e
              
                −
                x
              
            
          
        
      
    
    {\displaystyle \Pr(\varepsilon _{0}=x)=\Pr(\varepsilon _{1}=x)=e^{-x}e^{-e^{-x}}}
  Then

  
    
      
        
          Y
          
            i
          
        
        =
        
          
            {
            
              
                
                  1
                
                
                  
                    if 
                  
                  
                    Y
                    
                      i
                    
                    
                      1
                      ∗
                    
                  
                  >
                  
                    Y
                    
                      i
                    
                    
                      0
                      ∗
                    
                  
                  ,
                
              
              
                
                  0
                
                
                  
                    otherwise.
                  
                
              
            
            
          
        
      
    
    {\displaystyle Y_{i}={\begin{cases}1&{\text{if }}Y_{i}^{1\ast }>Y_{i}^{0\ast },\\0&{\text{otherwise.}}\end{cases}}}
  This model has a separate latent variable and a separate set of regression coefficients for each possible outcome of the dependent variable.  The reason for this separation is that it makes it easy to extend logistic regression to multi-outcome categorical variables, as in the multinomial logit model. In such a model, it is natural to model each possible outcome using a different set of regression coefficients.  It is also possible to motivate each of the separate latent variables as the theoretical utility associated with making the associated choice, and thus motivate logistic regression in terms of utility theory. (In terms of utility theory, a rational actor always chooses the choice with the greatest associated utility.) This is the approach taken by economists when formulating discrete choice models, because it both provides a theoretically strong foundation and facilitates intuitions about the model, which in turn makes it easy to consider various sorts of extensions. (See the example below.)
The choice of the type-1 extreme value distribution seems fairly arbitrary, but it makes the mathematics work out, and it may be possible to justify its use through rational choice theory.
It turns out that this model is equivalent to the previous model, although this seems non-obvious, since there are now two sets of regression coefficients and error variables, and the error variables have a different distribution.  In fact, this model reduces directly to the previous one with the following substitutions:

  
    
      
        
          β
        
        =
        
          
            β
          
          
            1
          
        
        −
        
          
            β
          
          
            0
          
        
      
    
    {\displaystyle {\boldsymbol {\beta }}={\boldsymbol {\beta }}_{1}-{\boldsymbol {\beta }}_{0}}
  

  
    
      
        ε
        =
        
          ε
          
            1
          
        
        −
        
          ε
          
            0
          
        
      
    
    {\displaystyle \varepsilon =\varepsilon _{1}-\varepsilon _{0}}
  An intuition for this comes from the fact that, since we choose based on the maximum of two values, only their difference matters, not the exact values — and this effectively removes one degree of freedom. Another critical fact is that the difference of two type-1 extreme-value-distributed variables is a logistic distribution, i.e. 
  
    
      
        ε
        =
        
          ε
          
            1
          
        
        −
        
          ε
          
            0
          
        
        ∼
        Logistic
        ⁡
        (
        0
        ,
        1
        )
        .
      
    
    {\displaystyle \varepsilon =\varepsilon _{1}-\varepsilon _{0}\sim \operatorname {Logistic} (0,1).}
   We can demonstrate the equivalent as follows:

  
    
      
        
          
            
              
                Pr
                (
                
                  Y
                  
                    i
                  
                
                =
                1
                ∣
                
                  
                    X
                  
                  
                    i
                  
                
                )
                =
                

                
              
              
                
                Pr
                
                  (
                  
                    
                      Y
                      
                        i
                      
                      
                        1
                        ∗
                      
                    
                    >
                    
                      Y
                      
                        i
                      
                      
                        0
                        ∗
                      
                    
                    ∣
                    
                      
                        X
                      
                      
                        i
                      
                    
                  
                  )
                
              
              
            
            
              
                =
                

                
              
              
                
                Pr
                
                  (
                  
                    
                      Y
                      
                        i
                      
                      
                        1
                        ∗
                      
                    
                    −
                    
                      Y
                      
                        i
                      
                      
                        0
                        ∗
                      
                    
                    >
                    0
                    ∣
                    
                      
                        X
                      
                      
                        i
                      
                    
                  
                  )
                
              
              
            
            
              
                =
                

                
              
              
                
                Pr
                
                  (
                  
                    
                      
                        β
                      
                      
                        1
                      
                    
                    ⋅
                    
                      
                        X
                      
                      
                        i
                      
                    
                    +
                    
                      ε
                      
                        1
                      
                    
                    −
                    
                      (
                      
                        
                          
                            β
                          
                          
                            0
                          
                        
                        ⋅
                        
                          
                            X
                          
                          
                            i
                          
                        
                        +
                        
                          ε
                          
                            0
                          
                        
                      
                      )
                    
                    >
                    0
                  
                  )
                
              
              
            
            
              
                =
                

                
              
              
                
                Pr
                
                  (
                  
                    (
                    
                      
                        β
                      
                      
                        1
                      
                    
                    ⋅
                    
                      
                        X
                      
                      
                        i
                      
                    
                    −
                    
                      
                        β
                      
                      
                        0
                      
                    
                    ⋅
                    
                      
                        X
                      
                      
                        i
                      
                    
                    )
                    +
                    (
                    
                      ε
                      
                        1
                      
                    
                    −
                    
                      ε
                      
                        0
                      
                    
                    )
                    >
                    0
                  
                  )
                
              
              
            
            
              
                =
                

                
              
              
                
                Pr
                (
                (
                
                  
                    β
                  
                  
                    1
                  
                
                −
                
                  
                    β
                  
                  
                    0
                  
                
                )
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                +
                (
                
                  ε
                  
                    1
                  
                
                −
                
                  ε
                  
                    0
                  
                
                )
                >
                0
                )
              
              
            
            
              
                =
                

                
              
              
                
                Pr
                (
                (
                
                  
                    β
                  
                  
                    1
                  
                
                −
                
                  
                    β
                  
                  
                    0
                  
                
                )
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                +
                ε
                >
                0
                )
              
              
              
                
                  (substitute 
                
                ε
                
                   as above)
                
              
            
            
              
                =
                

                
              
              
                
                Pr
                (
                
                  β
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                +
                ε
                >
                0
                )
              
              
              
                
                  (substitute 
                
                
                  β
                
                
                   as above)
                
              
            
            
              
                =
                

                
              
              
                
                Pr
                (
                ε
                >
                −
                
                  β
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                )
              
              
              
                
                  (now, same as above model)
                
              
            
            
              
                =
                

                
              
              
                
                Pr
                (
                ε
                <
                
                  β
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                )
              
              
            
            
              
                =
                

                
              
              
                
                  logit
                  
                    −
                    1
                  
                
                ⁡
                (
                
                  β
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                )
              
            
            
              
                =
                

                
              
              
                
                  p
                  
                    i
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\Pr(Y_{i}=1\mid \mathbf {X} _{i})={}&\Pr \left(Y_{i}^{1\ast }>Y_{i}^{0\ast }\mid \mathbf {X} _{i}\right)&\\[5pt]={}&\Pr \left(Y_{i}^{1\ast }-Y_{i}^{0\ast }>0\mid \mathbf {X} _{i}\right)&\\[5pt]={}&\Pr \left({\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}+\varepsilon _{1}-\left({\boldsymbol {\beta }}_{0}\cdot \mathbf {X} _{i}+\varepsilon _{0}\right)>0\right)&\\[5pt]={}&\Pr \left(({\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}-{\boldsymbol {\beta }}_{0}\cdot \mathbf {X} _{i})+(\varepsilon _{1}-\varepsilon _{0})>0\right)&\\[5pt]={}&\Pr(({\boldsymbol {\beta }}_{1}-{\boldsymbol {\beta }}_{0})\cdot \mathbf {X} _{i}+(\varepsilon _{1}-\varepsilon _{0})>0)&\\[5pt]={}&\Pr(({\boldsymbol {\beta }}_{1}-{\boldsymbol {\beta }}_{0})\cdot \mathbf {X} _{i}+\varepsilon >0)&&{\text{(substitute }}\varepsilon {\text{ as above)}}\\[5pt]={}&\Pr({\boldsymbol {\beta }}\cdot \mathbf {X} _{i}+\varepsilon >0)&&{\text{(substitute }}{\boldsymbol {\beta }}{\text{ as above)}}\\[5pt]={}&\Pr(\varepsilon >-{\boldsymbol {\beta }}\cdot \mathbf {X} _{i})&&{\text{(now, same as above model)}}\\[5pt]={}&\Pr(\varepsilon <{\boldsymbol {\beta }}\cdot \mathbf {X} _{i})&\\[5pt]={}&\operatorname {logit} ^{-1}({\boldsymbol {\beta }}\cdot \mathbf {X} _{i})\\[5pt]={}&p_{i}\end{aligned}}}

Example
As an example, consider a province-level election where the choice is between a right-of-center party, a left-of-center party, and a secessionist party (e.g. the Parti Québécois, which wants Quebec to secede from Canada).  We would then use three latent variables, one for each choice.  Then, in accordance with utility theory, we can then interpret the latent variables as expressing the utility that results from making each of the choices.  We can also interpret the regression coefficients as indicating the strength that the associated factor (i.e. explanatory variable) has in contributing to the utility — or more correctly, the amount by which a unit change in an explanatory variable changes the utility of a given choice.  A voter might expect that the right-of-center party would lower taxes, especially on rich people.  This would give low-income people no benefit, i.e. no change in utility (since they usually don't pay taxes); would cause moderate benefit (i.e. somewhat more money, or moderate utility increase) for middle-incoming people; would cause significant benefits for high-income people.  On the other hand, the left-of-center party might be expected to raise taxes and offset it with increased welfare and other assistance for the lower and middle classes.  This would cause significant positive benefit to low-income people, perhaps a weak benefit to middle-income people, and significant negative benefit to high-income people.  Finally, the secessionist party would take no direct actions on the economy, but simply secede. A low-income or middle-income voter might expect basically no clear utility gain or loss from this, but a high-income voter might expect negative utility since he/she is likely to own companies, which will have a harder time doing business in such an environment and probably lose money.
These intuitions can be expressed as follows:

This clearly shows that

Separate sets of regression coefficients need to exist for each choice.  When phrased in terms of utility, this can be seen very easily. Different choices have different effects on net utility; furthermore, the effects vary in complex ways that depend on the characteristics of each individual, so there need to be separate sets of coefficients for each characteristic, not simply a single extra per-choice characteristic.
Even though income is a continuous variable, its effect on utility is too complex for it to be treated as a single variable.  Either it needs to be directly split up into ranges, or higher powers of income need to be added so that polynomial regression on income is effectively done.

As a ""log-linear"" model
Yet another formulation combines the two-way latent variable formulation above with the original formulation higher up without latent variables, and in the process provides a link to one of the standard formulations of the multinomial logit.
Here, instead of writing the logit of the probabilities pi as a linear predictor, we separate the linear predictor into two, one for each of the two outcomes:

  
    
      
        
          
            
              
                ln
                ⁡
                Pr
                (
                
                  Y
                  
                    i
                  
                
                =
                0
                )
              
              
                
                =
                
                  
                    β
                  
                  
                    0
                  
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                −
                ln
                ⁡
                Z
              
            
            
              
                ln
                ⁡
                Pr
                (
                
                  Y
                  
                    i
                  
                
                =
                1
                )
              
              
                
                =
                
                  
                    β
                  
                  
                    1
                  
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
                −
                ln
                ⁡
                Z
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\ln \Pr(Y_{i}=0)&={\boldsymbol {\beta }}_{0}\cdot \mathbf {X} _{i}-\ln Z\\\ln \Pr(Y_{i}=1)&={\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}-\ln Z\end{aligned}}}
  Note that two separate sets of regression coefficients have been introduced, just as in the two-way latent variable model, and the two equations appear a form that writes the logarithm of the associated probability as a linear predictor, with an extra term 
  
    
      
        −
        ln
        ⁡
        Z
      
    
    {\displaystyle -\ln Z}
   at the end.  This term, as it turns out, serves as the normalizing factor ensuring that the result is a distribution.  This can be seen by exponentiating both sides:

  
    
      
        
          
            
              
                Pr
                (
                
                  Y
                  
                    i
                  
                
                =
                0
                )
              
              
                
                =
                
                  
                    1
                    Z
                  
                
                
                  e
                  
                    
                      
                        β
                      
                      
                        0
                      
                    
                    ⋅
                    
                      
                        X
                      
                      
                        i
                      
                    
                  
                
              
            
            
              
                Pr
                (
                
                  Y
                  
                    i
                  
                
                =
                1
                )
              
              
                
                =
                
                  
                    1
                    Z
                  
                
                
                  e
                  
                    
                      
                        β
                      
                      
                        1
                      
                    
                    ⋅
                    
                      
                        X
                      
                      
                        i
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\Pr(Y_{i}=0)&={\frac {1}{Z}}e^{{\boldsymbol {\beta }}_{0}\cdot \mathbf {X} _{i}}\\[5pt]\Pr(Y_{i}=1)&={\frac {1}{Z}}e^{{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}}\end{aligned}}}
  In this form it is clear that the purpose of Z is to ensure that the resulting distribution over Yi is in fact a probability distribution, i.e. it sums to 1.  This means that Z is simply the sum of all un-normalized probabilities, and by dividing each probability by Z, the probabilities become ""normalized"".  That is:

  
    
      
        Z
        =
        
          e
          
            
              
                β
              
              
                0
              
            
            ⋅
            
              
                X
              
              
                i
              
            
          
        
        +
        
          e
          
            
              
                β
              
              
                1
              
            
            ⋅
            
              
                X
              
              
                i
              
            
          
        
      
    
    {\displaystyle Z=e^{{\boldsymbol {\beta }}_{0}\cdot \mathbf {X} _{i}}+e^{{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}}}
  and the resulting equations are

  
    
      
        
          
            
              
                Pr
                (
                
                  Y
                  
                    i
                  
                
                =
                0
                )
              
              
                
                =
                
                  
                    
                      e
                      
                        
                          
                            β
                          
                          
                            0
                          
                        
                        ⋅
                        
                          
                            X
                          
                          
                            i
                          
                        
                      
                    
                    
                      
                        e
                        
                          
                            
                              β
                            
                            
                              0
                            
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                      +
                      
                        e
                        
                          
                            
                              β
                            
                            
                              1
                            
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                    
                  
                
              
            
            
              
                Pr
                (
                
                  Y
                  
                    i
                  
                
                =
                1
                )
              
              
                
                =
                
                  
                    
                      e
                      
                        
                          
                            β
                          
                          
                            1
                          
                        
                        ⋅
                        
                          
                            X
                          
                          
                            i
                          
                        
                      
                    
                    
                      
                        e
                        
                          
                            
                              β
                            
                            
                              0
                            
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                      +
                      
                        e
                        
                          
                            
                              β
                            
                            
                              1
                            
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                    
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\Pr(Y_{i}=0)&={\frac {e^{{\boldsymbol {\beta }}_{0}\cdot \mathbf {X} _{i}}}{e^{{\boldsymbol {\beta }}_{0}\cdot \mathbf {X} _{i}}+e^{{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}}}}\\[5pt]\Pr(Y_{i}=1)&={\frac {e^{{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}}}{e^{{\boldsymbol {\beta }}_{0}\cdot \mathbf {X} _{i}}+e^{{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}}}}.\end{aligned}}}
  Or generally:

  
    
      
        Pr
        (
        
          Y
          
            i
          
        
        =
        c
        )
        =
        
          
            
              e
              
                
                  
                    β
                  
                  
                    c
                  
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
              
            
            
              
                ∑
                
                  h
                
              
              
                e
                
                  
                    
                      β
                    
                    
                      h
                    
                  
                  ⋅
                  
                    
                      X
                    
                    
                      i
                    
                  
                
              
            
          
        
      
    
    {\displaystyle \Pr(Y_{i}=c)={\frac {e^{{\boldsymbol {\beta }}_{c}\cdot \mathbf {X} _{i}}}{\sum _{h}e^{{\boldsymbol {\beta }}_{h}\cdot \mathbf {X} _{i}}}}}
  This shows clearly how to generalize this formulation to more than two outcomes, as in multinomial logit.
Note that this general formulation is exactly the softmax function as in

  
    
      
        Pr
        (
        
          Y
          
            i
          
        
        =
        c
        )
        =
        softmax
        ⁡
        (
        c
        ,
        
          
            β
          
          
            0
          
        
        ⋅
        
          
            X
          
          
            i
          
        
        ,
        
          
            β
          
          
            1
          
        
        ⋅
        
          
            X
          
          
            i
          
        
        ,
        …
        )
        .
      
    
    {\displaystyle \Pr(Y_{i}=c)=\operatorname {softmax} (c,{\boldsymbol {\beta }}_{0}\cdot \mathbf {X} _{i},{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i},\dots ).}
  In order to prove that this is equivalent to the previous model, note that the above model is overspecified, in that 
  
    
      
        Pr
        (
        
          Y
          
            i
          
        
        =
        0
        )
      
    
    {\displaystyle \Pr(Y_{i}=0)}
   and 
  
    
      
        Pr
        (
        
          Y
          
            i
          
        
        =
        1
        )
      
    
    {\displaystyle \Pr(Y_{i}=1)}
   cannot be independently specified: rather 
  
    
      
        Pr
        (
        
          Y
          
            i
          
        
        =
        0
        )
        +
        Pr
        (
        
          Y
          
            i
          
        
        =
        1
        )
        =
        1
      
    
    {\displaystyle \Pr(Y_{i}=0)+\Pr(Y_{i}=1)=1}
   so knowing one automatically determines the other.  As a result, the model is nonidentifiable, in that multiple combinations of β0 and β1 will produce the same probabilities for all possible explanatory variables.  In fact, it can be seen that adding any constant vector to both of them will produce the same probabilities:

  
    
      
        
          
            
              
                Pr
                (
                
                  Y
                  
                    i
                  
                
                =
                1
                )
              
              
                
                =
                
                  
                    
                      e
                      
                        (
                        
                          
                            β
                          
                          
                            1
                          
                        
                        +
                        
                          C
                        
                        )
                        ⋅
                        
                          
                            X
                          
                          
                            i
                          
                        
                      
                    
                    
                      
                        e
                        
                          (
                          
                            
                              β
                            
                            
                              0
                            
                          
                          +
                          
                            C
                          
                          )
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                      +
                      
                        e
                        
                          (
                          
                            
                              β
                            
                            
                              1
                            
                          
                          +
                          
                            C
                          
                          )
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                    
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      
                        e
                        
                          
                            
                              β
                            
                            
                              1
                            
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                      
                        e
                        
                          
                            C
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                    
                    
                      
                        e
                        
                          
                            
                              β
                            
                            
                              0
                            
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                      
                        e
                        
                          
                            C
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                      +
                      
                        e
                        
                          
                            
                              β
                            
                            
                              1
                            
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                      
                        e
                        
                          
                            C
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                    
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      
                        e
                        
                          
                            C
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                      
                        e
                        
                          
                            
                              β
                            
                            
                              1
                            
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                    
                    
                      
                        e
                        
                          
                            C
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                      (
                      
                        e
                        
                          
                            
                              β
                            
                            
                              0
                            
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                      +
                      
                        e
                        
                          
                            
                              β
                            
                            
                              1
                            
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                      )
                    
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      e
                      
                        
                          
                            β
                          
                          
                            1
                          
                        
                        ⋅
                        
                          
                            X
                          
                          
                            i
                          
                        
                      
                    
                    
                      
                        e
                        
                          
                            
                              β
                            
                            
                              0
                            
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                      +
                      
                        e
                        
                          
                            
                              β
                            
                            
                              1
                            
                          
                          ⋅
                          
                            
                              X
                            
                            
                              i
                            
                          
                        
                      
                    
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\Pr(Y_{i}=1)&={\frac {e^{({\boldsymbol {\beta }}_{1}+\mathbf {C} )\cdot \mathbf {X} _{i}}}{e^{({\boldsymbol {\beta }}_{0}+\mathbf {C} )\cdot \mathbf {X} _{i}}+e^{({\boldsymbol {\beta }}_{1}+\mathbf {C} )\cdot \mathbf {X} _{i}}}}\\[5pt]&={\frac {e^{{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}}e^{\mathbf {C} \cdot \mathbf {X} _{i}}}{e^{{\boldsymbol {\beta }}_{0}\cdot \mathbf {X} _{i}}e^{\mathbf {C} \cdot \mathbf {X} _{i}}+e^{{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}}e^{\mathbf {C} \cdot \mathbf {X} _{i}}}}\\[5pt]&={\frac {e^{\mathbf {C} \cdot \mathbf {X} _{i}}e^{{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}}}{e^{\mathbf {C} \cdot \mathbf {X} _{i}}(e^{{\boldsymbol {\beta }}_{0}\cdot \mathbf {X} _{i}}+e^{{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}})}}\\[5pt]&={\frac {e^{{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}}}{e^{{\boldsymbol {\beta }}_{0}\cdot \mathbf {X} _{i}}+e^{{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}}}}.\end{aligned}}}
  As a result, we can simplify matters, and restore identifiability, by picking an arbitrary value for one of the two vectors.  We choose to set 
  
    
      
        
          
            β
          
          
            0
          
        
        =
        
          0
        
        .
      
    
    {\displaystyle {\boldsymbol {\beta }}_{0}=\mathbf {0} .}
    Then,

  
    
      
        
          e
          
            
              
                β
              
              
                0
              
            
            ⋅
            
              
                X
              
              
                i
              
            
          
        
        =
        
          e
          
            
              0
            
            ⋅
            
              
                X
              
              
                i
              
            
          
        
        =
        1
      
    
    {\displaystyle e^{{\boldsymbol {\beta }}_{0}\cdot \mathbf {X} _{i}}=e^{\mathbf {0} \cdot \mathbf {X} _{i}}=1}
  and so

  
    
      
        Pr
        (
        
          Y
          
            i
          
        
        =
        1
        )
        =
        
          
            
              e
              
                
                  
                    β
                  
                  
                    1
                  
                
                ⋅
                
                  
                    X
                  
                  
                    i
                  
                
              
            
            
              1
              +
              
                e
                
                  
                    
                      β
                    
                    
                      1
                    
                  
                  ⋅
                  
                    
                      X
                    
                    
                      i
                    
                  
                
              
            
          
        
        =
        
          
            1
            
              1
              +
              
                e
                
                  −
                  
                    
                      β
                    
                    
                      1
                    
                  
                  ⋅
                  
                    
                      X
                    
                    
                      i
                    
                  
                
              
            
          
        
        =
        
          p
          
            i
          
        
      
    
    {\displaystyle \Pr(Y_{i}=1)={\frac {e^{{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}}}{1+e^{{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}}}}={\frac {1}{1+e^{-{\boldsymbol {\beta }}_{1}\cdot \mathbf {X} _{i}}}}=p_{i}}
  which shows that this formulation is indeed equivalent to the previous formulation. (As in the two-way latent variable formulation, any settings where 
  
    
      
        
          β
        
        =
        
          
            β
          
          
            1
          
        
        −
        
          
            β
          
          
            0
          
        
      
    
    {\displaystyle {\boldsymbol {\beta }}={\boldsymbol {\beta }}_{1}-{\boldsymbol {\beta }}_{0}}
   will produce equivalent results.)
Note that most treatments of the multinomial logit model start out either by extending the ""log-linear"" formulation presented here or the two-way latent variable formulation presented above, since both clearly show the way that the model could be extended to multi-way outcomes.  In general, the presentation with latent variables is more common in econometrics and political science, where discrete choice models and utility theory reign, while the ""log-linear"" formulation here is more common in computer science, e.g. machine learning and natural language processing.

As a single-layer perceptron
The model has an equivalent formulation

  
    
      
        
          p
          
            i
          
        
        =
        
          
            1
            
              1
              +
              
                e
                
                  −
                  (
                  
                    β
                    
                      0
                    
                  
                  +
                  
                    β
                    
                      1
                    
                  
                  
                    x
                    
                      1
                      ,
                      i
                    
                  
                  +
                  ⋯
                  +
                  
                    β
                    
                      k
                    
                  
                  
                    x
                    
                      k
                      ,
                      i
                    
                  
                  )
                
              
            
          
        
        .
        
      
    
    {\displaystyle p_{i}={\frac {1}{1+e^{-(\beta _{0}+\beta _{1}x_{1,i}+\cdots +\beta _{k}x_{k,i})}}}.\,}
  This functional form is commonly called a single-layer perceptron or single-layer artificial neural network. A single-layer neural network computes a continuous output instead of a step function. The derivative of pi with respect to  X = (x1, ..., xk) is computed from the general form:

  
    
      
        y
        =
        
          
            1
            
              1
              +
              
                e
                
                  −
                  f
                  (
                  X
                  )
                
              
            
          
        
      
    
    {\displaystyle y={\frac {1}{1+e^{-f(X)}}}}
  where f(X) is an analytic function in X. With this choice, the single-layer neural network is identical to the logistic regression model. This function has a continuous derivative, which allows it to be used in backpropagation. This function is also preferred because its derivative is easily calculated:

  
    
      
        
          
            
              
                d
              
              y
            
            
              
                d
              
              X
            
          
        
        =
        y
        (
        1
        −
        y
        )
        
          
            
              
                d
              
              f
            
            
              
                d
              
              X
            
          
        
        .
        
      
    
    {\displaystyle {\frac {\mathrm {d} y}{\mathrm {d} X}}=y(1-y){\frac {\mathrm {d} f}{\mathrm {d} X}}.\,}

In terms of binomial data
A closely related model assumes that each i is associated not with a single Bernoulli trial but with ni independent identically distributed trials, where the observation Yi is the number of successes observed (the sum of the individual Bernoulli-distributed random variables), and hence follows a binomial distribution:

  
    
      
        
          Y
          
            i
          
        
        
        ∼
        Bin
        ⁡
        (
        
          n
          
            i
          
        
        ,
        
          p
          
            i
          
        
        )
        ,
        
           for 
        
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle Y_{i}\,\sim \operatorname {Bin} (n_{i},p_{i}),{\text{ for }}i=1,\dots ,n}
  An example of this distribution is the fraction of seeds (pi) that germinate after ni are planted.
In terms of expected values, this model is expressed as follows:

  
    
      
        
          p
          
            i
          
        
        =
        
          
            E
          
        
        ⁡
        
          [
          
            
              
              
                
                  
                    
                      Y
                      
                        i
                      
                    
                    
                      n
                      
                        i
                      
                    
                  
                
                
              
              |
            
            
            
              
                X
              
              
                i
              
            
          
          ]
        
        
        ,
      
    
    {\displaystyle p_{i}=\operatorname {\mathcal {E}} \left[\left.{\frac {Y_{i}}{n_{i}}}\,\right|\,\mathbf {X} _{i}\right]\,,}
  so that

  
    
      
        logit
        ⁡
        
          (
          
            
              
                E
              
            
            ⁡
            
              [
              
                
                  
                  
                    
                      
                        
                          Y
                          
                            i
                          
                        
                        
                          n
                          
                            i
                          
                        
                      
                    
                    
                  
                  |
                
                
                
                  
                    X
                  
                  
                    i
                  
                
              
              ]
            
          
          )
        
        =
        logit
        ⁡
        (
        
          p
          
            i
          
        
        )
        =
        ln
        ⁡
        
          (
          
            
              
                p
                
                  i
                
              
              
                1
                −
                
                  p
                  
                    i
                  
                
              
            
          
          )
        
        =
        
          β
        
        ⋅
        
          
            X
          
          
            i
          
        
        
        ,
      
    
    {\displaystyle \operatorname {logit} \left(\operatorname {\mathcal {E}} \left[\left.{\frac {Y_{i}}{n_{i}}}\,\right|\,\mathbf {X} _{i}\right]\right)=\operatorname {logit} (p_{i})=\ln \left({\frac {p_{i}}{1-p_{i}}}\right)={\boldsymbol {\beta }}\cdot \mathbf {X} _{i}\,,}
  Or equivalently:

  
    
      
        Pr
        (
        
          Y
          
            i
          
        
        =
        y
        ∣
        
          
            X
          
          
            i
          
        
        )
        =
        
          
            
              (
            
            
              
                n
                
                  i
                
              
              y
            
            
              )
            
          
        
        
          p
          
            i
          
          
            y
          
        
        (
        1
        −
        
          p
          
            i
          
        
        
          )
          
            
              n
              
                i
              
            
            −
            y
          
        
        =
        
          
            
              (
            
            
              
                n
                
                  i
                
              
              y
            
            
              )
            
          
        
        
          
            (
            
              
                1
                
                  1
                  +
                  
                    e
                    
                      −
                      
                        β
                      
                      ⋅
                      
                        
                          X
                        
                        
                          i
                        
                      
                    
                  
                
              
            
            )
          
          
            y
          
        
        
          
            (
            
              1
              −
              
                
                  1
                  
                    1
                    +
                    
                      e
                      
                        −
                        
                          β
                        
                        ⋅
                        
                          
                            X
                          
                          
                            i
                          
                        
                      
                    
                  
                
              
            
            )
          
          
            
              n
              
                i
              
            
            −
            y
          
        
        
        .
      
    
    {\displaystyle \Pr(Y_{i}=y\mid \mathbf {X} _{i})={n_{i} \choose y}p_{i}^{y}(1-p_{i})^{n_{i}-y}={n_{i} \choose y}\left({\frac {1}{1+e^{-{\boldsymbol {\beta }}\cdot \mathbf {X} _{i}}}}\right)^{y}\left(1-{\frac {1}{1+e^{-{\boldsymbol {\beta }}\cdot \mathbf {X} _{i}}}}\right)^{n_{i}-y}\,.}
  This model can be fit using the same sorts of methods as the above more basic model.

Bayesian
In a Bayesian statistics context, prior distributions are normally placed on the regression coefficients, usually in the form of Gaussian distributions.  There is no conjugate prior of the likelihood function in logistic regression.  When Bayesian inference was performed analytically, this made the posterior distribution difficult to calculate except in very low dimensions.  Now, though, automatic software such as OpenBUGS, JAGS, PyMC3, Stan or Turing.jl allows these posteriors to be computed using simulation, so lack of conjugacy is not a concern.  However, when the sample size or the number of parameters is large, full Bayesian simulation can be slow, and people often use approximate methods such as variational Bayesian methods and expectation propagation.

History
A detailed history of the logistic regression is given in Cramer (2002). The logistic function was developed as a model of population growth and named ""logistic"" by Pierre François Verhulst in the 1830s and 1840s, under the guidance of Adolphe Quetelet; see Logistic function § History for details. In his earliest paper (1838), Verhulst did not specify how he fit the curves to the data. In his more detailed paper (1845), Verhulst determined the three parameters of the model by making the curve pass through three observed points, which yielded poor predictions.The logistic function was independently developed in chemistry as a model of autocatalysis (Wilhelm Ostwald, 1883). An autocatalytic reaction is one in which one of the products is itself a catalyst for the same reaction, while the supply of one of the reactants is fixed. This naturally gives rise to the logistic equation for the same reason as population growth: the reaction is self-reinforcing but constrained.
The logistic function was independently rediscovered as a model of population growth in 1920 by Raymond Pearl and Lowell Reed, published as Pearl & Reed (1920), which led to its use in modern statistics. They were initially unaware of Verhulst's work and presumably learned about it from L. Gustave du Pasquier, but they gave him little credit and did not adopt his terminology. Verhulst's priority was acknowledged and the term ""logistic"" revived by Udny Yule in 1925 and has been followed since. Pearl and Reed first applied the model to the population of the United States, and also initially fitted the curve by making it pass through three points; as with Verhulst, this again yielded poor results.In the 1930s, the probit model was developed and systematized by Chester Ittner Bliss, who coined the term ""probit"" in Bliss (1934), and by John Gaddum in Gaddum (1933), and the model fit by maximum likelihood estimation by Ronald A. Fisher in Fisher (1935), as an addendum to Bliss's work. The probit model was principally used in bioassay, and had been preceded by earlier work dating to 1860; see Probit model § History. The probit model influenced the subsequent development of the logit model and these models competed with each other.The logistic model was likely first used as an alternative to the probit model in bioassay by Edwin Bidwell Wilson and his student Jane Worcester in Wilson & Worcester (1943). However, the development of the logistic model as a general alternative to the probit model was principally due to the work of Joseph Berkson over many decades, beginning in Berkson (1944), where he coined ""logit"", by analogy with ""probit"", and continuing through Berkson (1951) and following years. The logit model was initially dismissed as inferior to the probit model, but ""gradually achieved an equal footing with the logit"", particularly between 1960 and 1970. By 1970, the logit model achieved parity with the probit model in use in statistics journals and thereafter surpassed it. This relative popularity was due to the adoption of the logit outside of bioassay, rather than displacing the probit within bioassay, and its informal use in practice; the logit's popularity is credited to the logit model's computational simplicity, mathematical properties, and generality, allowing its use in varied fields.Various refinements occurred during that time, notably by David Cox, as in Cox (1958).The multinomial logit model was introduced independently in Cox (1966) and Thiel (1969), which greatly increased the scope of application and the popularity of the logit model. In 1973 Daniel McFadden linked the multinomial logit to the theory of discrete choice, specifically Luce's choice axiom, showing that the multinomial logit followed from the assumption of independence of irrelevant alternatives and interpreting odds of alternatives as relative preferences; this gave a theoretical foundation for the logistic regression.

Extensions
There are large numbers of extensions:

Multinomial logistic regression (or multinomial logit) handles the case of a multi-way categorical dependent variable (with unordered values, also called ""classification"").  Note that the general case of having dependent variables with more than two values is termed polytomous regression.
Ordered logistic regression (or ordered logit) handles ordinal dependent variables (ordered values).
Mixed logit is an extension of multinomial logit that allows for correlations among the choices of the dependent variable.
An extension of the logistic model to sets of interdependent variables is the conditional random field.
Conditional logistic regression handles matched or stratified data when the strata are small. It is mostly used in the analysis of observational studies.

Software
Most statistical software can do binary logistic regression.

SPSS
[1] for basic logistic regression.
Stata
SAS
PROC LOGISTIC for basic logistic regression.
PROC CATMOD when all the variables are categorical.
PROC GLIMMIX for multilevel model logistic regression.
R
glm in the stats package (using family = binomial)
lrm in the rms package
GLMNET package for an efficient implementation regularized logistic regression
lmer for mixed effects logistic regression
Rfast package command gm_logistic for fast and heavy calculations involving large scale data.
arm package for bayesian logistic regression
Python
Logit in the Statsmodels module.
LogisticRegression in the Scikit-learn module.
LogisticRegressor in the TensorFlow module.
Full example of logistic regression in the Theano tutorial [2]
Bayesian Logistic Regression with ARD prior code, tutorial
Variational Bayes Logistic Regression with ARD prior code , tutorial
Bayesian Logistic Regression  code, tutorial
NCSS
Logistic Regression in NCSS
Matlab
mnrfit in the Statistics and Machine Learning Toolbox (with ""incorrect"" coded as 2 instead of 0)
fminunc/fmincon, fitglm, mnrfit, fitclinear, mle can all do logistic regression.
Java (JVM)
LibLinear
Apache Flink
Apache Spark
SparkML supports Logistic Regression
FPGA
Logistic Regresesion IP core in HLS for FPGA.Notably, Microsoft Excel's statistics extension package does not include it.

See also
Logistic function
Discrete choice
Jarrow–Turnbull model
Limited dependent variable
Multinomial logit model
Ordered logit
Hosmer–Lemeshow test
Brier score
mlpack - contains a C++ implementation of logistic regression
Local case-control sampling
Logistic model tree

References
Further reading
External links
 Media related to Logistic regression at Wikimedia Commons
Econometrics Lecture (topic: Logit model) on YouTube by Mark Thoma
Logistic Regression tutorial
mlelr: software in C for teaching purposes",https://en.wikipedia.org/wiki/Logistic_regression,"['All articles that are excessively detailed', 'All articles to be expanded', 'All articles with incomplete citations', 'All articles with specifically marked weasel-worded phrases', 'All articles with style issues', 'All articles with unsourced statements', 'Articles to be expanded from October 2016', 'Articles using small message boxes', 'Articles with incomplete citations from July 2020', 'Articles with short description', 'Articles with specifically marked weasel-worded phrases from October 2019', 'Articles with unsourced statements from January 2017', 'Articles with unsourced statements from October 2019', 'Commons category link from Wikidata', 'Harv and Sfn no-target errors', 'Logistic regression', 'Prediction', 'Regression models', 'Short description is different from Wikidata', 'Wikipedia articles needing clarification from May 2017', 'Wikipedia articles needing page number citations from May 2012', 'Wikipedia articles needing page number citations from October 2019', 'Wikipedia articles that are excessively detailed from March 2019', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with style issues from March 2019']",Data Science
130,Machine learning,"Machine learning (ML) is the study of computer algorithms that improve automatically through experience and by the use of data. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as ""training data"", in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.
A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers; but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. In its application across business problems, machine learning is also referred to as predictive analytics.

Overview
Machine learning involves computers discovering how they can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; on the computer's part, no learning is needed. For more advanced tasks, it can be challenging for a human to manually create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step.The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. This can then be used as training data for the computer to improve the algorithm(s) it uses to determine correct answers. For example, to train a system for the task of digital character recognition, the MNIST dataset of handwritten digits has often been used.

Machine learning approaches
Machine learning approaches are traditionally divided into three broad categories, depending on the nature of the ""signal"" or ""feedback"" available to the learning system:

Supervised learning: The computer is presented with example inputs and their desired outputs, given by a ""teacher"", and the goal is to learn a general rule that maps inputs to outputs.
Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).
Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximize.Other approaches have been developed which don't fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example topic modeling, dimensionality reduction or meta learning.As of 2020, deep learning has become the dominant approach for much ongoing work in the field of machine learning.

History and relationships to other fields
The term machine learning was coined in 1959 by Arthur Samuel, an American IBMer and pioneer in the field of computer gaming and artificial intelligence. A representative book of the machine learning research during the 1960s was the Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: ""A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E."" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper ""Computing Machinery and Intelligence"", in which the question ""Can machines think?"" is replaced with the question ""Can machines do what we (as thinking entities) can do?"".Modern day machine learning has two objectives, one is to classify data based on models which have been developed, the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. Where as, a machine learning algorithm for stock trading may inform the trader of future potential predictions.

Artificial intelligence
As a scientific endeavor, machine learning grew out of the quest for artificial intelligence. In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what was then termed ""neural networks""; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favor. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as ""connectionism"", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.Machine learning (ML), reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.As of 2020, many sources continue to assert that machine learning remains a subfield of AI. The main disagreement is whether all of ML is part of AI, as this would mean that anyone using ML could claim they are using AI. Others have the view that not all of ML is part of AI where only an 'intelligent' subset of ML is part of AI.The question to what is the difference between ML and AI is answered by Judea Pearl in The Book of Why. Accordingly ML learns and predicts based on passive observations, whereas AI implies an agent  interacting with the environment to learn and take actions that maximize its chance of successfully achieving its goals.

Data mining
Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as ""unsupervised learning"" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.

Optimization
Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples).

Generalization
The difference between optimization and machine learning arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples. Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms.

Statistics
Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field.Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model, wherein ""algorithmic model"" means more or less the machine learning algorithms like Random forest.
Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.

Theory
A core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.
The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.
For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.

Approaches
Types of learning algorithms
The types of machine learning algorithms differ in their approach, the type of data they input and output, and the type of task or problem that they are intended to solve.

Supervised learning
Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal.  In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function will allow the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.Types of supervised learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email.
Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.

Unsupervised learning
Unsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. The algorithms, therefore, learn from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. A central application of unsupervised learning is in the field of density estimation in statistics, such as finding the probability density function. Though unsupervised learning encompasses other domains involving summarizing and explaining data features.
Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.

Semi-supervised learning
Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels,  yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.
In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.

Reinforcement learning
Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In machine learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP, and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.

Self learning
Self-learning as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a learning with no external rewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.
The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: 

 In situation s perform an action a;
 Receive consequence situation s’;
 Compute emotion of being in consequence situation v(s’);
 Update crossbar memory  w’(a,s) = w(a,s) + v(s’).

It is a system with only one input, situation s, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.

Feature learning
Several learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal components analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.
Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.

Sparse dictionary learning
Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions, and is assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the K-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.

Anomaly detection
In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier (the key difference to many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.

Robot learning
In developmental robotics, robot learning algorithms generate their own sequences of learning experiences, also known as a curriculum, to cumulatively acquire new skills through self-guided exploration and social interaction with humans. These robots use guidance mechanisms such as active learning, maturation, motor synergies and imitation.

Association rules
Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of ""interestingness"".Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves ""rules"" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.
Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule 
  
    
      
        {
        
          o
          n
          i
          o
          n
          s
          ,
          p
          o
          t
          a
          t
          o
          e
          s
        
        }
        ⇒
        {
        
          b
          u
          r
          g
          e
          r
        
        }
      
    
    {\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}
   found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.Inductive logic programming (ILP) is an approach to rule-learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.
Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.

Models
Performing machine learning involves creating a model, which is trained on some training data and then can process additional data to make predictions. Various types of models have been used and researched for machine learning systems.

Artificial neural networks
Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems ""learn"" to perform tasks by considering examples, generally without being programmed with any task-specific rules.
An ANN is a model based on a collection of connected units or nodes called ""artificial neurons"", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a ""signal"", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called ""edges"". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.
The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.

Decision trees
Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision making.

Support-vector machines
Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.  An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.

Regression analysis
Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization (mathematics) methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.

Bayesian networks
A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.

Genetic algorithms
A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.

Training models
Usually, machine learning models require a lot of data in order for them to perform well. Usually, when training a machine learning model, one needs to collect a large, representative sample of data from a training set. Data from the training set can be as varied as a corpus of text, a collection of images, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased data can result in skewed or undesired predictions. Algorithmic bias is a potential result from data not fully prepared for training.

Federated learning
Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.

Applications
There are many applications for machine learning, including:

In 2006, the media-services provider Netflix held the first ""Netflix Prize"" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%.  A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (""everything is a recommendation"") and they changed their recommendation engine accordingly. In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors' jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists. In 2019 Springer Nature published the first research book created using machine learning.

Limitations
Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.Machine learning has been used as a strategy to update the evidence related to systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.

Bias
Machine learning approaches in particular can suffer from different data biases. A machine learning system trained on current customers only may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on man-made data, machine learning is likely to pick up the same constitutional and unconscious biases already present in society. Language models learned from data have been shown to contain human-like biases. Machine learning systems used for criminal risk assessment have been found to be biased against black people. In 2015, Google photos would often tag black people as gorillas, and in 2018 this still was not well resolved, but Google reportedly was still using the workaround to remove all gorillas from the training data, and thus was not able to recognize real gorillas at all. Similar issues with recognizing non-white people have been found in many other systems. In 2016, Microsoft tested a chatbot that learned from Twitter, and it quickly picked up racist and sexist language. Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that ""There’s nothing artificial about AI...It’s inspired by people, it’s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.”

Model assessments
Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The total operating characteristic (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used receiver operating characteristic (ROC) and ROC's associated area under the curve (AUC).

Ethics
Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices. For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants. Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.
The evolvement of AI systems raises a lot questions in the realm of ethics and morality. AI can be well equipped in making decisions in certain fields such technical and scientific which rely heavily on data and historical information. These decisions rely on objectivity and logical reasoning. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.Other forms of ethical challenges, not related to personal biases, are more seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is huge potential for machine learning in health care to provide professionals a great tool to diagnose, medicate, and even plan recovery paths for patients, but this will not happen until the personal biases mentioned previously, and these ""greed"" biases are addressed.

Hardware
Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of non-linear hidden units. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.

Software
Software suites containing a variety of machine learning algorithms include the following:

Free and open-source software
Proprietary software with free and open-source editions
KNIME
RapidMiner

Proprietary software
Journals
Journal of Machine Learning Research
Machine Learning
Nature Machine Intelligence
Neural Computation

Conferences
Association for Computational Linguistics (ACL)
European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)
International Conference on Machine Learning (ICML)
International Conference on Learning Representations (ICLR)
International Conference on Intelligent Robots and Systems (IROS)
Conference on Knowledge Discovery and Data Mining (KDD)
Conference on Neural Information Processing Systems (NeurIPS)

See also
Automated machine learning – Process of automating the application of machine learning
Big data – Information assets characterized by high volume, velocity, and variety
Differentiable programming – Programming paradigm
List of important publications in machine learning – Wikimedia list article
List of datasets for machine-learning research – Wikimedia list article

References
Further reading
External links
International Machine Learning Society
mloss is an academic database of open-source machine learning software.
Machine Learning Crash Course by Google. This is a free course on machine learning through the use of TensorFlow.",https://en.wikipedia.org/wiki/Machine_learning,"['Articles with short description', 'CS1 errors: missing periodical', 'CS1 maint: uses authors parameter', 'Commons category link from Wikidata', 'Cybernetics', 'Harv and Sfn no-target errors', 'Learning', 'Machine learning', 'Short description is different from Wikidata', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NDL identifiers']",Data Science
131,Mean-shift,"Mean shift is a non-parametric feature-space analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm. Application domains include cluster analysis in computer vision and image processing.

History
The mean shift procedure was originally presented in 1975 by Fukunaga and Hostetler.

Overview
Mean shift is a procedure for locating the maxima—the modes—of a density function given discrete data sampled from that function. This is an iterative method, and we start with an initial estimate 
  
    
      
        x
      
    
    {\displaystyle x}
  .  Let a kernel function 
  
    
      
        K
        (
        
          x
          
            i
          
        
        −
        x
        )
      
    
    {\displaystyle K(x_{i}-x)}
   be given.  This function determines the weight of nearby points for re-estimation of the mean.  Typically a Gaussian kernel on the distance to the current estimate is used, 
  
    
      
        K
        (
        
          x
          
            i
          
        
        −
        x
        )
        =
        
          e
          
            −
            c
            
              |
            
            
              |
            
            
              x
              
                i
              
            
            −
            x
            
              |
            
            
              
                |
              
              
                2
              
            
          
        
      
    
    {\displaystyle K(x_{i}-x)=e^{-c||x_{i}-x||^{2}}}
  .  The weighted mean of the density in the window determined by 
  
    
      
        K
      
    
    {\displaystyle K}
   is

  
    
      
        m
        (
        x
        )
        =
        
          
            
              
                ∑
                
                  
                    x
                    
                      i
                    
                  
                  ∈
                  N
                  (
                  x
                  )
                
              
              K
              (
              
                x
                
                  i
                
              
              −
              x
              )
              
                x
                
                  i
                
              
            
            
              
                ∑
                
                  
                    x
                    
                      i
                    
                  
                  ∈
                  N
                  (
                  x
                  )
                
              
              K
              (
              
                x
                
                  i
                
              
              −
              x
              )
            
          
        
      
    
    {\displaystyle m(x)={\frac {\sum _{x_{i}\in N(x)}K(x_{i}-x)x_{i}}{\sum _{x_{i}\in N(x)}K(x_{i}-x)}}}
  where 
  
    
      
        N
        (
        x
        )
      
    
    {\displaystyle N(x)}
   is the neighborhood of 
  
    
      
        x
      
    
    {\displaystyle x}
  , a set of points for which 
  
    
      
        K
        (
        
          x
          
            i
          
        
        )
        ≠
        0
      
    
    {\displaystyle K(x_{i})\neq 0}
  .
The difference 
  
    
      
        m
        (
        x
        )
        −
        x
      
    
    {\displaystyle m(x)-x}
   is called mean shift in Fukunaga and Hostetler. 
The mean-shift algorithm now sets 
  
    
      
        x
        ←
        m
        (
        x
        )
      
    
    {\displaystyle x\leftarrow m(x)}
  , and repeats the estimation until 
  
    
      
        m
        (
        x
        )
      
    
    {\displaystyle m(x)}
   converges.
Although the mean shift algorithm has been widely used in many applications, a rigid proof for the convergence of the algorithm using a general kernel in a high dimensional space is still not known. Aliyari Ghassabeh showed the convergence of the mean shift algorithm in one-dimension with a differentiable, convex, and strictly decreasing profile function. However, the one-dimensional case has limited real world applications. Also, the convergence of the algorithm in higher dimensions with a finite number of the (or isolated) stationary points has been proved. However, sufficient conditions for a general kernel function to have finite (or isolated) stationary points have not been provided.
Gaussian Mean-Shift is an Expectation–maximization algorithm.

Details
Let data be a finite set 
  
    
      
        S
      
    
    {\displaystyle S}
   embedded in the 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional Euclidean space, 
  
    
      
        X
      
    
    {\displaystyle X}
  . Let 
  
    
      
        K
      
    
    {\displaystyle K}
   be a flat kernel that is the characteristic function of the 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
  -ball in 
  
    
      
        X
      
    
    {\displaystyle X}
  ,

In each iteration of the algorithm, 
  
    
      
        s
        ←
        m
        (
        s
        )
      
    
    {\displaystyle s\leftarrow m(s)}
   is performed for all 
  
    
      
        s
        ∈
        S
      
    
    {\displaystyle s\in S}
   simultaneously. The first question, then, is how to estimate the density function given a sparse set of samples. One of the simplest approaches is to just smooth the data, e.g., by convolving it with a fixed kernel of width 
  
    
      
        h
      
    
    {\displaystyle h}
  ,

where 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   are the input samples and 
  
    
      
        k
        (
        r
        )
      
    
    {\displaystyle k(r)}
   is the kernel function (or Parzen window). 
  
    
      
        h
      
    
    {\displaystyle h}
   is the only parameter in the algorithm and is called the bandwidth. This approach is known as kernel density estimation or the Parzen window technique. Once we have computed 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   from equation above, we can find its local maxima using gradient ascent or some other optimization technique. The problem with this ""brute force"" approach is that, for higher dimensions, it becomes computationally prohibitive to evaluate 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   over the complete search space. Instead, mean shift uses a variant of what is known in the optimization literature as multiple restart gradient descent. Starting at some guess for a local maximum, 
  
    
      
        
          y
          
            k
          
        
      
    
    {\displaystyle y_{k}}
  , which can be a random input data point 
  
    
      
        
          x
          
            1
          
        
      
    
    {\displaystyle x_{1}}
  , mean shift computes the gradient of the density estimate 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   at 
  
    
      
        
          y
          
            k
          
        
      
    
    {\displaystyle y_{k}}
   and takes an uphill step in that direction.

Types of kernels
Kernel definition: Let 
  
    
      
        X
      
    
    {\displaystyle X}
   be the 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional Euclidean space, 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
  . The norm of 
  
    
      
        x
      
    
    {\displaystyle x}
   is a non-negative number, 
  
    
      
        ‖
        x
        
          ‖
          
            2
          
        
        =
        
          x
          
            ⊤
          
        
        x
        ≥
        0
      
    
    {\displaystyle \|x\|^{2}=x^{\top }x\geq 0}
  . A function 
  
    
      
        K
        :
        X
        →
        
          R
        
      
    
    {\displaystyle K:X\rightarrow \mathbb {R} }
   is said to be a kernel if there exists a profile, 
  
    
      
        k
        :
        [
        0
        ,
        ∞
        [
        →
        
          R
        
      
    
    {\displaystyle k:[0,\infty [\rightarrow \mathbb {R} }
   , such that

  
    
      
        K
        (
        x
        )
        =
        k
        (
        ‖
        x
        
          ‖
          
            2
          
        
        )
      
    
    {\displaystyle K(x)=k(\|x\|^{2})}
  
and 

k is non-negative.
k is non-increasing: 
  
    
      
        k
        (
        a
        )
        ≥
        k
        (
        b
        )
      
    
    {\displaystyle k(a)\geq k(b)}
   if 
  
    
      
        a
        <
        b
      
    
    {\displaystyle a<b}
  .
k is piecewise continuous and 
  
    
      
        
          ∫
          
            0
          
          
            ∞
          
        
        k
        (
        r
        )
        
        d
        r
        <
        ∞
         
      
    
    {\displaystyle \int _{0}^{\infty }k(r)\,dr<\infty \ }
  The two most frequently used kernel profiles for mean shift are:

Flat kernel
Gaussian kernel
where the standard deviation parameter 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
   works as the bandwidth parameter, 
  
    
      
        h
      
    
    {\displaystyle h}
  .

Applications
Clustering
Consider a set of points in two-dimensional space. Assume a circular window centered at C and having radius r as the kernel. Mean shift is a hill climbing algorithm which involves shifting this kernel iteratively to a higher density region until convergence. Every shift is defined by a mean shift vector. The mean shift vector always points toward the direction of the maximum increase in the density. At every iteration the kernel is shifted to the centroid or the mean of the points within it. The method of calculating this mean depends on the choice of the kernel. In this case if a Gaussian kernel is chosen instead of a flat kernel, then every point will first be assigned a weight which will decay exponentially as the distance from the kernel's center increases. At convergence, there will be no direction at which a shift can accommodate more points inside the kernel.

Tracking
The mean shift algorithm can be used for visual tracking.  The simplest such algorithm would create a confidence map in the new image based on the color histogram of the object in the previous image, and use mean shift to find the peak of a confidence map near the object's old position. The confidence map is a probability density function on the new image, assigning each pixel of the new image a probability, which is the probability of the pixel color occurring in the object in the previous image. A few algorithms, such as kernel-based object tracking, 
ensemble tracking, 
CAMshift  
expand on this idea.

Smoothing
Let 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   and 
  
    
      
        
          z
          
            i
          
        
        ,
        i
        =
        1
        ,
        .
        .
        .
        ,
        n
        ,
      
    
    {\displaystyle z_{i},i=1,...,n,}
   be the 
  
    
      
        d
      
    
    {\displaystyle d}
  -dimensional input and filtered image pixels in the joint spatial-range domain. For each pixel,

Initialize 
  
    
      
        j
        =
        1
      
    
    {\displaystyle j=1}
   and 
  
    
      
        
          y
          
            i
            ,
            1
          
        
        =
        
          x
          
            i
          
        
      
    
    {\displaystyle y_{i,1}=x_{i}}
  
Compute 
  
    
      
        
          y
          
            i
            ,
            j
            +
            1
          
        
      
    
    {\displaystyle y_{i,j+1}}
   according to 
  
    
      
        m
        (
        ⋅
        )
      
    
    {\displaystyle m(\cdot )}
   until convergence, 
  
    
      
        y
        =
        
          y
          
            i
            ,
            c
          
        
      
    
    {\displaystyle y=y_{i,c}}
  .
Assign 
  
    
      
        
          z
          
            i
          
        
        =
        (
        
          x
          
            i
          
          
            s
          
        
        ,
        
          y
          
            i
            ,
            c
          
          
            r
          
        
        )
      
    
    {\displaystyle z_{i}=(x_{i}^{s},y_{i,c}^{r})}
  . The superscripts s and r denote the spatial and range components of a vector, respectively. The assignment specifies that the filtered data at the spatial location axis will have the range component of the point of convergence 
  
    
      
        
          y
          
            i
            ,
            c
          
          
            r
          
        
      
    
    {\displaystyle y_{i,c}^{r}}
  .

Strengths
Mean shift is an application-independent tool suitable for real data analysis.
Does not assume any predefined shape on data clusters.
It is capable of handling arbitrary feature spaces.
The procedure relies on choice of a single parameter: bandwidth.
The bandwidth/window size 'h' has a physical meaning, unlike k-means.

Weaknesses
The selection of a window size is not trivial.
Inappropriate window size can cause modes to be merged, or generate additional “shallow” modes.
Often requires using adaptive window size.

Availability
Variants of the algorithm can be found in machine learning and image processing packages:

ELKI. Java data mining tool with many clustering algorithms.
ImageJ. Image filtering using the mean shift filter.
mlpack. Efficient dual-tree algorithm-based implementation.
OpenCV contains mean-shift implementation via cvMeanShift Method
Orfeo toolbox. A C++ implementation.
Scikit-learn Numpy/Python implementation uses ball tree for efficient neighboring points lookup

See also
DBSCAN
OPTICS algorithm
Kernel density estimation (KDE)
Kernel (statistics)


== References ==",https://en.wikipedia.org/wiki/Mean_shift,"['Cluster analysis algorithms', 'Computer vision', 'Webarchive template wayback links']",Data Science
132,Memtransistor,"The memtransistor is an experimental multi-terminal electronic component that might be used in the construction of artificial neural networks. It is a combination of the memristor and transistor. Applications of memristor technology is limited by it being a two terminal device. The multiple connections of the memtransistor enable it to more accurately model a neuron with its multiple synaptic connections. A neural network produced from these would provide hardware-based artificial intelligence with a good foundation.Researchers at Northwestern University have fabricated a seven-terminal device fabricated on molybdenum disulfide (MoS2). One terminal controls the current between the other six.


== References ==",https://en.wikipedia.org/wiki/Memtransistor,"['Articles with short description', 'Artificial neural networks', 'Short description matches Wikidata', 'Transistors']",Data Science
133,Mathematics,"Mathematics (from Greek: μάθημα, máthēma, 'knowledge, study, learning') includes the study of such topics as quantity (number theory), structure (algebra), space (geometry), and change (analysis). It has no generally accepted definition.Mathematicians seek and use patterns to formulate new conjectures; they resolve the truth or falsity of such by mathematical proof. When mathematical structures are good models of real phenomena, mathematical reasoning can be used to provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity from as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.
Rigorous arguments first appeared in Greek mathematics, most notably in Euclid's Elements. Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.Mathematics is essential in many fields, including natural science, engineering, medicine, finance, and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians engage in pure mathematics (mathematics for its own sake) without having any application in mind, but practical applications for what began as pure mathematics are often discovered later.

History
The history of mathematics can be seen as an ever-increasing series of abstractions. The first abstraction, which is shared by many animals, was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely the quantity of their members.
As evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like time—days, seasons, or years.

Evidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra and geometry for taxation and other financial calculations, for building and construction, and for astronomy. The oldest mathematical texts from Mesopotamia and Egypt are from 2000 to 1800 BC. Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication and division) first appear in the archaeological record. The Babylonians also possessed a place-value system and used a sexagesimal numeral system  which is still in use today for measuring angles and time.

Beginning in the 6th century BC with the Pythagoreans, with Greek mathematics the Ancient Greeks began a systematic study of mathematics as a subject in its own right. Around 300 BC, Euclid introduced the axiomatic method still used in mathematics today, consisting of definition, axiom, theorem, and proof. His book, Elements, is widely considered the most successful and influential textbook of all time. The greatest mathematician of antiquity is often held to be Archimedes (c. 287–212 BC) of Syracuse. He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC), trigonometry (Hipparchus of Nicaea, 2nd century BC), and the beginnings of algebra (Diophantus, 3rd century AD).

The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics. Other notable developments of Indian mathematics include the modern definition and approximation of sine and cosine, and an early form of infinite series.

During the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other achievements of the Islamic period include advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system. Many notable mathematicians from this period were Persian, such as Al-Khwarismi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī.
During the early modern period, mathematics began to develop at an accelerating pace in Western Europe. The development of calculus by Newton and Leibniz in the 17th century revolutionized mathematics. Leonhard Euler was the most notable mathematician of the 18th century, contributing numerous theorems and discoveries. Perhaps the foremost mathematician of the 19th century was the German mathematician Carl Friedrich Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory, number theory, and statistics. In the early 20th century, Kurt Gödel transformed mathematics by publishing his incompleteness theorems, which show in part that any consistent axiomatic system—if powerful enough to describe arithmetic—will contain true propositions that cannot be proved.Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made today. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, ""The number of papers and books included in the Mathematical Reviews database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs.""

Etymology
The word mathematics comes from Ancient Greek máthēma (μάθημα), meaning ""that which is learnt,"" ""what one gets to know,"" hence also ""study"" and ""science"". The word for ""mathematics"" came to have the narrower and more technical meaning ""mathematical study"" even in Classical times. Its adjective is mathēmatikós (μαθηματικός), meaning ""related to learning"" or ""studious,"" which likewise further came to mean ""mathematical."" In particular, mathēmatikḗ tékhnē (μαθηματικὴ τέχνη; Latin: ars mathematica) meant ""the mathematical art.""
Similarly, one of the two main schools of thought in Pythagoreanism was known as the mathēmatikoi (μαθηματικοί)—which at the time meant ""learners"" rather than ""mathematicians"" in the modern sense.In Latin, and in English until around 1700, the term mathematics more commonly meant ""astrology"" (or sometimes ""astronomy"") rather than ""mathematics""; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations. For example, Saint Augustine's warning that Christians should beware of mathematici, meaning astrologers, is sometimes mistranslated as a condemnation of mathematicians.The apparent plural form in English, like the French plural form les mathématiques (and the less commonly used singular derivative la mathématique), goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural ta mathēmatiká (τὰ μαθηματικά), used by Aristotle (384–322 BC), and meaning roughly ""all things mathematical"", although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, which were inherited from Greek. In English, the noun mathematics takes a singular verb. It is often shortened to maths or, in North America, math.

Definitions of mathematics
Mathematics has no generally accepted definition. Aristotle defined mathematics as ""the science of quantity"" and this definition prevailed until the 18th century. However, Aristotle also noted a focus on quantity alone may not distinguish mathematics from sciences like physics; in his view, abstraction and studying quantity as a property ""separable in thought"" from real instances set mathematics apart.In the 19th century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions.A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable. There is not even consensus on whether mathematics is an art or a science. Some just say, ""Mathematics is what mathematicians do.""

Three leading types
Three leading types of definition of mathematics today are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought. All have severe flaws, none has widespread acceptance, and no reconciliation seems possible.

Logicist definitions
An early definition of mathematics in terms of logic was that of Benjamin Peirce (1870): ""the science that draws necessary conclusions."" In the Principia Mathematica, Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proved entirely in terms of symbolic logic. A logicist definition of mathematics is Russell's (1903) ""All Mathematics is Symbolic Logic.""

Intuitionist definitions
Intuitionist definitions, developing from the philosophy of mathematician L. E. J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is ""Mathematics is the mental activity which consists in carrying out constructs one after the other."" A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proved to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct. Intuitionists also reject the law of excluded middle (i.e., 
  
    
      
        P
        ∨
        ¬
        P
      
    
    {\displaystyle P\vee \neg P}
  ). While this stance does force them to reject one common version of proof by contradiction as a viable proof method, namely the inference of 
  
    
      
        P
      
    
    {\displaystyle P}
   from 
  
    
      
        ¬
        P
        →
        ⊥
      
    
    {\displaystyle \neg P\to \bot }
  , they are still able to infer 
  
    
      
        ¬
        P
      
    
    {\displaystyle \neg P}
   from 
  
    
      
        P
        →
        ⊥
      
    
    {\displaystyle P\to \bot }
  . For them, 
  
    
      
        ¬
        (
        ¬
        P
        )
      
    
    {\displaystyle \neg (\neg P)}
   is a strictly weaker statement than 
  
    
      
        P
      
    
    {\displaystyle P}
  .

Formalist definitions
Formalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as ""the science of formal systems"". A formal system is a set of symbols, or tokens, and some rules on how the tokens are to be combined into formulas. In formal systems, the word axiom has a special meaning different from the ordinary meaning of ""a self-evident truth"", and is used to refer to a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.

Mathematics as science
The German mathematician Carl Friedrich Gauss referred to mathematics as ""the Queen of the Sciences"". More recently, Marcus du Sautoy has called mathematics ""the Queen of Science ... the main driving force behind scientific discovery"". The philosopher Karl Popper observed that ""most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently."" Popper also noted that ""I shall certainly admit a system as empirical or scientific only if it is capable of being tested by experience.""Several authors consider that mathematics is not a science because it does not rely on empirical evidence.Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics.
The opinions of mathematicians on this matter are varied. Many mathematicians feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is created (as in art) or discovered (as in science). In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in the philosophy of mathematics.

Inspiration, pure and applied mathematics, and aesthetics
Mathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences suggest problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics.Some mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography. 
This remarkable fact, that even the ""purest"" mathematics often turns out to have practical applications, is what the physicist Eugene Wigner has named ""the unreasonable effectiveness of mathematics"". The philosopher of mathematics Mark Steiner has written extensively on this matter and acknowledges that the applicability of mathematics constitutes “a challenge to naturalism.” For the philosopher of mathematics Mary Leng, the fact that the physical world acts in accordance with the dictates of non-causal mathematical entities existing beyond the universe is ""a happy coincidence"". On the other hand, for some  anti-realists, connections, which are acquired among mathematical things, just mirror the connections acquiring among objects in the universe, so that there is no ""happy coincidence"".As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46 pages. Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science.
For those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the elegance of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid's proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds calculation, such as the fast Fourier transform. G. H. Hardy in A Mathematician's Apology expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic. Mathematical research often seeks critical features of a mathematical object. A theorem expressed as a characterization of the object by these features is the prize. Examples of particularly succinct and revelatory mathematical arguments have been published in Proofs from THE BOOK.
The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions. And at the other social extreme, philosophers continue to find problems in philosophy of mathematics, such as the nature of mathematical proof.

Notation, language, and rigor
Most of the mathematical notation in use today was not invented until the 16th century. Before that, mathematics was written out in words, limiting mathematical discovery. Euler (1707–1783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. According to Barbara Oakley, this can be attributed to the fact that mathematical ideas are both more abstract and more encrypted than those of natural language. Unlike natural language, where people can often equate a word (such as cow) with the physical object it corresponds to, mathematical symbols are abstract, lacking any physical analog. Mathematical symbols are also more highly encrypted than regular words, meaning a single symbol can encode a number of different operations or ideas.Mathematical language can be difficult to understand for beginners because even common terms, such as or and only, have a more precise meaning than they have in everyday speech, and other terms such as open and field refer to specific mathematical ideas, not covered by their laymen's meanings. Mathematical language also includes many technical terms such as homeomorphism and integrable that have no meaning outside of mathematics. Additionally, shorthand phrases such as iff for ""if and only if"" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as ""rigor"".
Mathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken ""theorems"", based on fallible intuitions, of which many instances have occurred in the history of the subject. The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19th century. Misunderstanding the rigor is a cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may be erroneous if the used computer program is erroneous. On the other hand, proof assistants allow verifying all details that cannot be given in a hand-written proof, and provide certainty of the correctness of long proofs such as that of the Feit–Thompson theorem.Axioms in traditional thought were ""self-evident truths"", but that conception is problematic. At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert's program to put all of mathematics on a firm axiomatic basis, but according to Gödel's incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.

Fields of mathematics
Mathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty. While some areas might seem unrelated, the Langlands program has found connections between areas previously thought unconnected, such as Galois groups, Riemann surfaces and number theory.
Discrete mathematics conventionally groups together the fields of mathematics which study mathematical structures that are fundamentally discrete rather than continuous.

Foundations and philosophy
In order to clarify the foundations of mathematics, the fields of mathematical logic and set theory were developed. Mathematical logic includes the mathematical study of logic and the applications of formal logic to other areas of mathematics; set theory is the branch of mathematics that studies sets or collections of objects. The phrase ""crisis of foundations"" describes the search for a rigorous foundation for mathematics that took place from approximately 1900 to 1930. Some disagreement about the foundations of mathematics continues to the present day. The crisis of foundations was stimulated by a number of controversies at the time, including the controversy over Cantor's set theory and the Brouwer–Hilbert controversy.
Mathematical logic is concerned with setting mathematics within a rigorous axiomatic framework, and studying the implications of such a framework. As such, it is home to Gödel's incompleteness theorems which (informally) imply that any effective formal system that contains basic arithmetic, if sound (meaning that all theorems that can be proved are true), is necessarily incomplete (meaning that there are true theorems which cannot be proved in that system). Whatever finite collection of number-theoretical axioms is taken as a foundation, Gödel showed how to construct a formal statement that is a true number-theoretical fact, but which does not follow from those axioms. Therefore, no formal system is a complete axiomatization of full number theory. Modern logic is divided into recursion theory, model theory, and proof theory, and is closely linked to theoretical computer science, as well as to category theory. In the context of recursion theory, the impossibility of a full axiomatization of number theory can also be formally demonstrated as a consequence of the MRDP theorem.
Theoretical computer science includes computability theory, computational complexity theory, and information theory. Computability theory examines the limitations of various theoretical models of the computer, including the most well-known model—the Turing machine. Complexity theory is the study of tractability by computer; some problems, although theoretically solvable by computer, are so expensive in terms of time or space that solving them is likely to remain practically unfeasible, even with the rapid advancement of computer hardware. A famous problem is the ""P = NP?"" problem, one of the Millennium Prize Problems. Finally, information theory is concerned with the amount of data that can be stored on a given medium, and hence deals with concepts such as compression and entropy.

Pure mathematics
Number systems and number theory
The study of quantity starts with numbers, first the familiar natural numbers

  
    
      
        
          N
        
      
    
    {\displaystyle \mathbb {N} }
  
and integers 

  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
  
(""whole numbers"") and arithmetical operations on them, which are characterized in arithmetic. The deeper properties of integers are studied in number theory, from which come such popular results as Fermat's Last Theorem. The twin prime conjecture and Goldbach's conjecture are two unsolved problems in number theory.
As the number system is further developed, the integers are recognized as a subset of the rational numbers

  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbb {Q} }
  
(""fractions""). These, in turn, are contained within the real numbers,

  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  
which are used to represent limits of sequences of rational numbers and continuous quantities. Real numbers are generalized to the complex numbers

  
    
      
        
          C
        
      
    
    {\displaystyle \mathbb {C} }
  .
According to the fundamental theorem of algebra, all polynomial equations in one unknown with complex coefficients have a solution in the complex numbers, regardless of degree of the polynomial.

  
    
      
        
          N
        
        ,
         
        
          Z
        
        ,
         
        
          Q
        
        ,
         
        
          R
        
      
    
    {\displaystyle \mathbb {N} ,\ \mathbb {Z} ,\ \mathbb {Q} ,\ \mathbb {R} }
   and 
  
    
      
        
          C
        
      
    
    {\displaystyle \mathbb {C} }
  
are the first steps of a hierarchy of numbers that goes on to include quaternions and octonions. Consideration of the natural numbers also leads to the transfinite numbers, which formalize the concept of ""infinity"".  Another area of study is the size of sets, which is described with the cardinal numbers. These include the aleph numbers, which allow meaningful comparison of the size of infinitely large sets.

Structure
Many mathematical objects, such as sets of numbers and functions, exhibit internal structure as a consequence of operations or relations that are defined on the set. Mathematics then studies properties of those sets that can be expressed in terms of that structure; for instance number theory studies properties of the set of integers that can be expressed in terms of arithmetic operations. Moreover, it frequently happens that different such structured sets (or structures) exhibit similar properties, which makes it possible, by a further step of abstraction, to state axioms for a class of structures, and then study at once the whole class of structures satisfying these axioms. Thus one can study groups, rings, fields and other abstract systems; together such studies (for structures defined by algebraic operations) constitute the domain of abstract algebra.
By its great generality, abstract algebra can often be applied to seemingly unrelated problems; for instance a number of ancient problems concerning compass and straightedge constructions were finally solved using Galois theory, which involves field theory and group theory. Another example of an algebraic theory is linear algebra, which is the general study of vector spaces, whose elements called vectors have both quantity and direction, and can be used to model (relations between) points in space. This is one example of the phenomenon that the originally unrelated areas of geometry and algebra have very strong interactions in modern mathematics. Combinatorics studies ways of enumerating the number of objects that fit a given structure.

Space
The study of space originates with geometry—in particular, Euclidean geometry, which combines space and numbers, and encompasses the well-known Pythagorean theorem. Trigonometry is the branch of mathematics that deals with relationships between the sides and the angles of triangles and with the trigonometric functions. The modern study of space generalizes these ideas to include higher-dimensional geometry, non-Euclidean geometries (which play a central role in general relativity) and topology. Quantity and space both play a role in analytic geometry, differential geometry, and algebraic geometry. Convex and discrete geometry were developed to solve problems in number theory and functional analysis but now are pursued with an eye on applications in optimization and computer science. Within differential geometry are the concepts of fiber bundles and calculus on manifolds, in particular, vector and tensor calculus. Within algebraic geometry is the description of geometric objects as solution sets of polynomial equations, combining the concepts of quantity and space, and also the study of topological groups, which combine structure and space. Lie groups are used to study space, structure, and change. Topology in all its many ramifications may have been the greatest growth area in 20th-century mathematics; it includes point-set topology, set-theoretic topology, algebraic topology and differential topology. In particular, instances of modern-day topology are metrizability theory, axiomatic set theory, homotopy theory, and Morse theory. Topology also includes the now solved Poincaré conjecture, and the still unsolved areas of the Hodge conjecture. Other results in geometry and topology, including the four color theorem and Kepler conjecture, have been proven only with the help of computers.

Change
Understanding and describing change is a common theme in the natural sciences, and calculus was developed as a tool to investigate it. Functions arise here as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.

Applied mathematics
Applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, ""applied mathematics"" is a mathematical science with specialized knowledge. The term applied mathematics also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, applied mathematics focuses on the ""formulation, study, and use of mathematical models"" in science, engineering, and other areas of mathematical practice.
In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.

Statistics and other decision sciences
Applied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) ""create data that makes sense"" with random sampling and with randomized experiments; the design of a statistical sample or experiment specifies the analysis of the data (before the data becomes available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians ""make sense of the data"" using the art of modelling and the theory of inference—with model selection and estimation; the estimated models and consequential predictions should be tested on new data.Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence. Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.

Computational mathematics
Computational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis includes the study of approximation and discretisation broadly with special concern for rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic matrix and graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.

Mathematical awards
Arguably the most prestigious award in mathematics is the Fields Medal, established in 1936 and awarded every four years (except around World War II) to as many as four individuals. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize.
The Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was instituted in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field.
A famous list of 23 open problems, called ""Hilbert's problems"", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the ""Millennium Prize Problems"", was published in 2000. Only one of them, the Riemann hypothesis, duplicates one of Hilbert's problems. A solution to any of these problems carries a 1 million dollar reward. Currently, only one of these problems, the Poincaré conjecture, has been solved.

See also
Notes
References
Bibliography


== Further reading ==",https://en.wikipedia.org/wiki/Mathematics,"['Articles containing Ancient Greek (to 1453)-language text', 'Articles containing French-language text', 'Articles containing Greek-language text', 'Articles containing Latin-language text', 'Articles with Encyclopædia Britannica links', 'Articles with short description', 'Formal sciences', 'Harv and Sfn no-target errors', 'Main topic articles', 'Mathematics', 'Pages that use a deprecated format of the math tags', 'Pages using Sister project links with default search', 'Pages using multiple image with manual scaled images', 'Short description is different from Wikidata', 'Use mdy dates from October 2014', 'Webarchive template wayback links', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with HDS identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NDL identifiers', 'Wikipedia articles with NLI identifiers', 'Wikipedia indefinitely move-protected pages', 'Wikipedia indefinitely semi-protected pages']",Data Science
134,Multilayer perceptron,"A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN). The term MLP is used ambiguously, sometimes loosely to any feedforward ANN, sometimes strictly to refer to networks composed of multiple layers of perceptrons (with threshold activation); see § Terminology. Multilayer perceptrons are sometimes colloquially referred to as ""vanilla"" neural networks, especially when they have a single hidden layer.An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.

Theory
Activation function
If a multilayer perceptron has a linear activation function in all neurons, that is, a linear function that maps the weighted inputs to the output of each neuron, then linear algebra shows that any number of layers can be reduced to a two-layer input-output model. In MLPs some neurons use a nonlinear activation function that was developed to model the frequency of action potentials, or firing, of biological neurons.
The two historically common activation functions are both sigmoids, and are described by

  
    
      
        y
        (
        
          v
          
            i
          
        
        )
        =
        tanh
        ⁡
        (
        
          v
          
            i
          
        
        )
         
         
        
          
            and
          
        
         
         
        y
        (
        
          v
          
            i
          
        
        )
        =
        (
        1
        +
        
          e
          
            −
            
              v
              
                i
              
            
          
        
        
          )
          
            −
            1
          
        
      
    
    {\displaystyle y(v_{i})=\tanh(v_{i})~~{\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}
  .In recent developments of deep learning the rectifier linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids.
The first is a hyperbolic tangent that ranges from -1 to 1, while the other is the logistic function, which is similar in shape but ranges from 0 to 1. Here 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
   is the output of the 
  
    
      
        i
      
    
    {\displaystyle i}
  th node (neuron) and 
  
    
      
        
          v
          
            i
          
        
      
    
    {\displaystyle v_{i}}
   is the weighted sum of the input connections. Alternative activation functions have been proposed, including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks, another class of supervised neural network models).

Layers
The MLP consists of three or more layers (an input and an output layer with one or more hidden layers) of nonlinearly-activating nodes. Since MLPs are fully connected, each node in one layer connects with a certain weight 
  
    
      
        
          w
          
            i
            j
          
        
      
    
    {\displaystyle w_{ij}}
   to every node in the following layer.

Learning
Learning occurs in the perceptron by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation, a generalization of the least mean squares algorithm in the linear perceptron.
We can represent the degree of error in an output node 
  
    
      
        j
      
    
    {\displaystyle j}
   in the 
  
    
      
        n
      
    
    {\displaystyle n}
  th data point (training example) by 
  
    
      
        
          e
          
            j
          
        
        (
        n
        )
        =
        
          d
          
            j
          
        
        (
        n
        )
        −
        
          y
          
            j
          
        
        (
        n
        )
      
    
    {\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}
  , where 
  
    
      
        d
      
    
    {\displaystyle d}
   is the target value and 
  
    
      
        y
      
    
    {\displaystyle y}
   is the value produced by the perceptron. The node weights can then be adjusted based on corrections that minimize the error in the entire output, given by

  
    
      
        
          
            E
          
        
        (
        n
        )
        =
        
          
            1
            2
          
        
        
          ∑
          
            j
          
        
        
          e
          
            j
          
          
            2
          
        
        (
        n
        )
      
    
    {\displaystyle {\mathcal {E}}(n)={\frac {1}{2}}\sum _{j}e_{j}^{2}(n)}
  .Using gradient descent, the change in each weight is

  
    
      
        Δ
        
          w
          
            j
            i
          
        
        (
        n
        )
        =
        −
        η
        
          
            
              ∂
              
                
                  E
                
              
              (
              n
              )
            
            
              ∂
              
                v
                
                  j
                
              
              (
              n
              )
            
          
        
        
          y
          
            i
          
        
        (
        n
        )
      
    
    {\displaystyle \Delta w_{ji}(n)=-\eta {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}y_{i}(n)}
  where 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
   is the output of the previous neuron and 
  
    
      
        η
      
    
    {\displaystyle \eta }
   is the learning rate, which is selected to ensure that the weights quickly converge to a response, without oscillations.
The derivative to be calculated depends on the induced local field 
  
    
      
        
          v
          
            j
          
        
      
    
    {\displaystyle v_{j}}
  , which itself varies. It is easy to prove that for an output node this derivative can be simplified to

  
    
      
        −
        
          
            
              ∂
              
                
                  E
                
              
              (
              n
              )
            
            
              ∂
              
                v
                
                  j
                
              
              (
              n
              )
            
          
        
        =
        
          e
          
            j
          
        
        (
        n
        )
        
          ϕ
          
            ′
          
        
        (
        
          v
          
            j
          
        
        (
        n
        )
        )
      
    
    {\displaystyle -{\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}=e_{j}(n)\phi ^{\prime }(v_{j}(n))}
  where 
  
    
      
        
          ϕ
          
            ′
          
        
      
    
    {\displaystyle \phi ^{\prime }}
   is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is

  
    
      
        −
        
          
            
              ∂
              
                
                  E
                
              
              (
              n
              )
            
            
              ∂
              
                v
                
                  j
                
              
              (
              n
              )
            
          
        
        =
        
          ϕ
          
            ′
          
        
        (
        
          v
          
            j
          
        
        (
        n
        )
        )
        
          ∑
          
            k
          
        
        −
        
          
            
              ∂
              
                
                  E
                
              
              (
              n
              )
            
            
              ∂
              
                v
                
                  k
                
              
              (
              n
              )
            
          
        
        
          w
          
            k
            j
          
        
        (
        n
        )
      
    
    {\displaystyle -{\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}=\phi ^{\prime }(v_{j}(n))\sum _{k}-{\frac {\partial {\mathcal {E}}(n)}{\partial v_{k}(n)}}w_{kj}(n)}
  .This depends on the change in weights of the 
  
    
      
        k
      
    
    {\displaystyle k}
  th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function.

Terminology
The term ""multilayer perceptron"" does not refer to a single perceptron that has multiple layers. Rather, it contains many perceptrons that are organized into layers. An alternative is ""multilayer perceptron network"". Moreover, MLP ""perceptrons"" are not perceptrons in the strictest possible sense. True perceptrons are formally a special case of artificial neurons that use a threshold activation function such as the Heaviside step function. MLP perceptrons can employ arbitrary activation functions. A true perceptron performs binary classification, an MLP neuron is free to either perform classification or regression, depending upon its activation function.
The term ""multilayer perceptron"" later was applied without respect to nature of the nodes/layers, which can be composed of arbitrarily defined artificial neurons, and not perceptrons specifically. This interpretation avoids the loosening of the definition of ""perceptron"" to mean an artificial neuron in general.

Applications
MLPs are useful in research for their ability to solve problems stochastically, which often allows approximate solutions for extremely complex problems like fitness approximation.
MLPs are universal function approximators as shown by Cybenko's theorem, so they can be used to create mathematical models by regression analysis. As classification is a particular case of regression when the response variable is categorical, MLPs make good classifier algorithms.
MLPs were a popular machine learning solution in the 1980s, finding applications in diverse fields such as speech recognition, image recognition, and machine translation software, but thereafter faced strong competition from much simpler (and related) support vector machines. Interest in backpropagation networks returned due to the successes of deep learning.

References
External links
A Gentle Introduction to Backpropagation - An intuitive tutorial by Shashi Sathyanarayana This is an updated PDF version of a blog article that was previously linked here. This article contains pseudocode (""Training Wheels for Training Neural Networks"") for implementing the algorithm.
Weka: Open source data mining software with multilayer perceptron implementation.
Neuroph Studio documentation, implements this algorithm and a few others.",https://en.wikipedia.org/wiki/Multilayer_perceptron,"['Artificial neural networks', 'Classification algorithms']",Data Science
135,Naive Bayes classifier,"In statistics, naive Bayes classifiers are a family of simple ""probabilistic classifiers"" based on applying Bayes' theorem with strong (naïve) independence assumptions between the features. They are among the simplest Bayesian network models, but coupled with kernel density estimation, they can achieve higher accuracy levels.Naïve Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.
In the statistics and computer science literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naïve Bayes is not (necessarily) a Bayesian method.

Introduction
Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter.  A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features.
For some types of probability models, naive Bayes classifiers can be trained very efficiently in a supervised learning setting. In many practical applications, parameter estimation for naive Bayes models uses the method of maximum likelihood; in other words, one can work with the naive Bayes model without accepting Bayesian probability or using any Bayesian methods.
Despite their naive design and apparently oversimplified assumptions, naive Bayes classifiers have worked quite well in many complex real-world situations. In 2004, an analysis of the Bayesian classification problem showed that there are sound theoretical reasons for the apparently implausible efficacy of naive Bayes classifiers. Still, a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches, such as boosted trees or random forests.An advantage of naive Bayes is that it only requires a small number of training data to estimate the parameters necessary for classification.

Probabilistic model
Abstractly, naïve Bayes is a conditional probability model: given a problem instance to be classified, represented by a vector 
  
    
      
        
          x
        
        =
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
      
    
    {\displaystyle \mathbf {x} =(x_{1},\ldots ,x_{n})}
   representing some n features (independent variables), it assigns to this instance probabilities

  
    
      
        p
        (
        
          C
          
            k
          
        
        ∣
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
        
      
    
    {\displaystyle p(C_{k}\mid x_{1},\ldots ,x_{n})\,}
  for each of K possible outcomes or classes 
  
    
      
        
          C
          
            k
          
        
      
    
    {\displaystyle C_{k}}
  .The problem with the above formulation is that if the number of features n is large or if a feature can take on a large number of values, then basing such a model on probability tables is infeasible. We therefore reformulate the model to make it more tractable. Using Bayes' theorem, the conditional probability can be decomposed as

  
    
      
        p
        (
        
          C
          
            k
          
        
        ∣
        
          x
        
        )
        =
        
          
            
              p
              (
              
                C
                
                  k
                
              
              )
               
              p
              (
              
                x
              
              ∣
              
                C
                
                  k
                
              
              )
            
            
              p
              (
              
                x
              
              )
            
          
        
        
      
    
    {\displaystyle p(C_{k}\mid \mathbf {x} )={\frac {p(C_{k})\ p(\mathbf {x} \mid C_{k})}{p(\mathbf {x} )}}\,}
  In plain English, using Bayesian probability terminology, the above equation can be written as

  
    
      
        
          posterior
        
        =
        
          
            
              
                prior
              
              ×
              
                likelihood
              
            
            evidence
          
        
        
      
    
    {\displaystyle {\text{posterior}}={\frac {{\text{prior}}\times {\text{likelihood}}}{\text{evidence}}}\,}
  In practice, there is interest only in the numerator of that fraction, because the denominator does not depend on 
  
    
      
        C
      
    
    {\displaystyle C}
   and the values of the features 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   are given, so that the denominator is effectively constant.
The numerator is equivalent to the joint probability model

  
    
      
        p
        (
        
          C
          
            k
          
        
        ,
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
        
      
    
    {\displaystyle p(C_{k},x_{1},\ldots ,x_{n})\,}
  which can be rewritten as follows, using the chain rule for repeated applications of the definition of conditional probability:

  
    
      
        
          
            
              
                p
                (
                
                  C
                  
                    k
                  
                
                ,
                
                  x
                  
                    1
                  
                
                ,
                …
                ,
                
                  x
                  
                    n
                  
                
                )
              
              
                
                =
                p
                (
                
                  x
                  
                    1
                  
                
                ,
                …
                ,
                
                  x
                  
                    n
                  
                
                ,
                
                  C
                  
                    k
                  
                
                )
              
            
            
              
              
                
                =
                p
                (
                
                  x
                  
                    1
                  
                
                ∣
                
                  x
                  
                    2
                  
                
                ,
                …
                ,
                
                  x
                  
                    n
                  
                
                ,
                
                  C
                  
                    k
                  
                
                )
                 
                p
                (
                
                  x
                  
                    2
                  
                
                ,
                …
                ,
                
                  x
                  
                    n
                  
                
                ,
                
                  C
                  
                    k
                  
                
                )
              
            
            
              
              
                
                =
                p
                (
                
                  x
                  
                    1
                  
                
                ∣
                
                  x
                  
                    2
                  
                
                ,
                …
                ,
                
                  x
                  
                    n
                  
                
                ,
                
                  C
                  
                    k
                  
                
                )
                 
                p
                (
                
                  x
                  
                    2
                  
                
                ∣
                
                  x
                  
                    3
                  
                
                ,
                …
                ,
                
                  x
                  
                    n
                  
                
                ,
                
                  C
                  
                    k
                  
                
                )
                 
                p
                (
                
                  x
                  
                    3
                  
                
                ,
                …
                ,
                
                  x
                  
                    n
                  
                
                ,
                
                  C
                  
                    k
                  
                
                )
              
            
            
              
              
                
                =
                ⋯
              
            
            
              
              
                
                =
                p
                (
                
                  x
                  
                    1
                  
                
                ∣
                
                  x
                  
                    2
                  
                
                ,
                …
                ,
                
                  x
                  
                    n
                  
                
                ,
                
                  C
                  
                    k
                  
                
                )
                 
                p
                (
                
                  x
                  
                    2
                  
                
                ∣
                
                  x
                  
                    3
                  
                
                ,
                …
                ,
                
                  x
                  
                    n
                  
                
                ,
                
                  C
                  
                    k
                  
                
                )
                ⋯
                p
                (
                
                  x
                  
                    n
                    −
                    1
                  
                
                ∣
                
                  x
                  
                    n
                  
                
                ,
                
                  C
                  
                    k
                  
                
                )
                 
                p
                (
                
                  x
                  
                    n
                  
                
                ∣
                
                  C
                  
                    k
                  
                
                )
                 
                p
                (
                
                  C
                  
                    k
                  
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}p(C_{k},x_{1},\ldots ,x_{n})&=p(x_{1},\ldots ,x_{n},C_{k})\\&=p(x_{1}\mid x_{2},\ldots ,x_{n},C_{k})\ p(x_{2},\ldots ,x_{n},C_{k})\\&=p(x_{1}\mid x_{2},\ldots ,x_{n},C_{k})\ p(x_{2}\mid x_{3},\ldots ,x_{n},C_{k})\ p(x_{3},\ldots ,x_{n},C_{k})\\&=\cdots \\&=p(x_{1}\mid x_{2},\ldots ,x_{n},C_{k})\ p(x_{2}\mid x_{3},\ldots ,x_{n},C_{k})\cdots p(x_{n-1}\mid x_{n},C_{k})\ p(x_{n}\mid C_{k})\ p(C_{k})\\\end{aligned}}}
  Now the ""naïve"" conditional independence assumptions come into play: assume that all features in 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   are mutually independent, conditional on the category 
  
    
      
        
          C
          
            k
          
        
      
    
    {\displaystyle C_{k}}
  . Under this assumption, 

  
    
      
        p
        (
        
          x
          
            i
          
        
        ∣
        
          x
          
            i
            +
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        ,
        
          C
          
            k
          
        
        )
        =
        p
        (
        
          x
          
            i
          
        
        ∣
        
          C
          
            k
          
        
        )
        
      
    
    {\displaystyle p(x_{i}\mid x_{i+1},\ldots ,x_{n},C_{k})=p(x_{i}\mid C_{k})\,}
  .Thus, the joint model can be expressed as

  
    
      
        
          
            
              
                p
                (
                
                  C
                  
                    k
                  
                
                ,
                
                  x
                  
                    1
                  
                
                ,
                …
                ,
                
                  x
                  
                    n
                  
                
                )
              
              
                
                ∝
                p
                (
                
                  C
                  
                    k
                  
                
                ,
                
                  x
                  
                    1
                  
                
                ,
                …
                ,
                
                  x
                  
                    n
                  
                
                )
              
            
            
              
              
                
                ∝
                p
                (
                
                  C
                  
                    k
                  
                
                )
                 
                p
                (
                
                  x
                  
                    1
                  
                
                ∣
                
                  C
                  
                    k
                  
                
                )
                 
                p
                (
                
                  x
                  
                    2
                  
                
                ∣
                
                  C
                  
                    k
                  
                
                )
                 
                p
                (
                
                  x
                  
                    3
                  
                
                ∣
                
                  C
                  
                    k
                  
                
                )
                 
                ⋯
              
            
            
              
              
                
                ∝
                p
                (
                
                  C
                  
                    k
                  
                
                )
                
                  ∏
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                p
                (
                
                  x
                  
                    i
                  
                
                ∣
                
                  C
                  
                    k
                  
                
                )
                
                ,
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}p(C_{k},x_{1},\ldots ,x_{n})&\varpropto p(C_{k},x_{1},\ldots ,x_{n})\\&\varpropto p(C_{k})\ p(x_{1}\mid C_{k})\ p(x_{2}\mid C_{k})\ p(x_{3}\mid C_{k})\ \cdots \\&\varpropto p(C_{k})\prod _{i=1}^{n}p(x_{i}\mid C_{k})\,,\end{aligned}}}
  where 
  
    
      
        ∝
      
    
    {\displaystyle \varpropto }
   denotes proportionality.
This means that under the above independence assumptions, the conditional distribution over the class variable 
  
    
      
        C
      
    
    {\displaystyle C}
   is:

  
    
      
        p
        (
        
          C
          
            k
          
        
        ∣
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
        =
        
          
            1
            Z
          
        
        p
        (
        
          C
          
            k
          
        
        )
        
          ∏
          
            i
            =
            1
          
          
            n
          
        
        p
        (
        
          x
          
            i
          
        
        ∣
        
          C
          
            k
          
        
        )
      
    
    {\displaystyle p(C_{k}\mid x_{1},\ldots ,x_{n})={\frac {1}{Z}}p(C_{k})\prod _{i=1}^{n}p(x_{i}\mid C_{k})}
  where the evidence 
  
    
      
        Z
        =
        p
        (
        
          x
        
        )
        =
        
          ∑
          
            k
          
        
        p
        (
        
          C
          
            k
          
        
        )
         
        p
        (
        
          x
        
        ∣
        
          C
          
            k
          
        
        )
      
    
    {\displaystyle Z=p(\mathbf {x} )=\sum _{k}p(C_{k})\ p(\mathbf {x} \mid C_{k})}
   is a scaling factor dependent only on 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},\ldots ,x_{n}}
  , that is, a constant if the values of the feature variables are known.

Constructing a classifier from the probability model
The discussion so far has derived the independent feature model, that is, the naïve Bayes probability model. The naïve Bayes classifier combines this model with a decision rule. One common rule is to pick the hypothesis that is most probable; this is known as the maximum a posteriori or MAP decision rule. The corresponding classifier, a Bayes classifier, is the function that assigns a class label 
  
    
      
        
          
            
              y
              ^
            
          
        
        =
        
          C
          
            k
          
        
      
    
    {\displaystyle {\hat {y}}=C_{k}}
   for some k as follows:

  
    
      
        
          
            
              y
              ^
            
          
        
        =
        
          
            argmax
            
              k
              ∈
              {
              1
              ,
              …
              ,
              K
              }
            
          
        
         
        p
        (
        
          C
          
            k
          
        
        )
        
          
            ∏
            
              i
              =
              1
            
            
              n
            
          
          p
          (
          
            x
            
              i
            
          
          ∣
          
            C
            
              k
            
          
          )
          .
        
      
    
    {\displaystyle {\hat {y}}={\underset {k\in \{1,\ldots ,K\}}{\operatorname {argmax} }}\ p(C_{k})\displaystyle \prod _{i=1}^{n}p(x_{i}\mid C_{k}).}

Parameter estimation and event models
A class's prior may be calculated by assuming equiprobable classes (i.e., 
  
    
      
        p
        (
        
          C
          
            k
          
        
        )
        =
        1
        
          /
        
        K
      
    
    {\displaystyle p(C_{k})=1/K}
  ), or by calculating an estimate for the class probability from the training set (i.e., <prior for a given class> = <number of samples in the class>/<total number of samples>). To estimate the parameters for a feature's distribution, one must assume a distribution or generate nonparametric models for the features from the training set.The assumptions on distributions of features are called the ""event model"" of the naïve Bayes classifier. For discrete features like the ones encountered in document classification (include spam filtering), multinomial and Bernoulli distributions are popular. These assumptions lead to two distinct models, which are often confused.

Gaussian naïve Bayes
When dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a normal (or Gaussian) distribution. For example, suppose the training data contains a continuous attribute, 
  
    
      
        x
      
    
    {\displaystyle x}
  . We first segment the data by the class, and then compute the mean and variance of 
  
    
      
        x
      
    
    {\displaystyle x}
   in each class. Let 
  
    
      
        
          μ
          
            k
          
        
      
    
    {\displaystyle \mu _{k}}
   be the mean of the values in 
  
    
      
        x
      
    
    {\displaystyle x}
   associated with class Ck, and let 
  
    
      
        
          σ
          
            k
          
          
            2
          
        
      
    
    {\displaystyle \sigma _{k}^{2}}
   be the Bessel corrected variance of the values in 
  
    
      
        x
      
    
    {\displaystyle x}
   associated with class Ck. Suppose we have collected some observation value 
  
    
      
        v
      
    
    {\displaystyle v}
  . Then, the probability density of 
  
    
      
        v
      
    
    {\displaystyle v}
   given a class 
  
    
      
        
          C
          
            k
          
        
      
    
    {\displaystyle C_{k}}
  , 
  
    
      
        p
        (
        x
        =
        v
        ∣
        
          C
          
            k
          
        
        )
      
    
    {\displaystyle p(x=v\mid C_{k})}
  , can be computed by plugging 
  
    
      
        v
      
    
    {\displaystyle v}
   into the equation for a normal distribution parameterized by 
  
    
      
        
          μ
          
            k
          
        
      
    
    {\displaystyle \mu _{k}}
   and 
  
    
      
        
          σ
          
            k
          
          
            2
          
        
      
    
    {\displaystyle \sigma _{k}^{2}}
  . That is,

  
    
      
        p
        (
        x
        =
        v
        ∣
        
          C
          
            k
          
        
        )
        =
        
          
            1
            
              2
              π
              
                σ
                
                  k
                
                
                  2
                
              
            
          
        
        
        
          e
          
            −
            
              
                
                  (
                  v
                  −
                  
                    μ
                    
                      k
                    
                  
                  
                    )
                    
                      2
                    
                  
                
                
                  2
                  
                    σ
                    
                      k
                    
                    
                      2
                    
                  
                
              
            
          
        
      
    
    {\displaystyle p(x=v\mid C_{k})={\frac {1}{\sqrt {2\pi \sigma _{k}^{2}}}}\,e^{-{\frac {(v-\mu _{k})^{2}}{2\sigma _{k}^{2}}}}}
  Another common technique for handling continuous values is to use binning to discretize the feature values, to obtain a new set of Bernoulli-distributed features; some literature in fact suggests that this is necessary to apply naive Bayes, but it is not, and the discretization may throw away discriminative information.Sometimes the distribution of class-conditional marginal densities is far from normal. In these cases, kernel density estimation can be used for a more realistic estimate of the marginal densities of each class. This method, which was introduced by John and Langley, can boost the accuracy of the classifier considerably.

Multinomial naïve Bayes
With a multinomial event model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial 
  
    
      
        (
        
          p
          
            1
          
        
        ,
        …
        ,
        
          p
          
            n
          
        
        )
      
    
    {\displaystyle (p_{1},\dots ,p_{n})}
   where 
  
    
      
        
          p
          
            i
          
        
      
    
    {\displaystyle p_{i}}
   is the probability that event i occurs (or K such multinomials in the multiclass case). A feature vector 
  
    
      
        
          x
        
        =
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
      
    
    {\displaystyle \mathbf {x} =(x_{1},\dots ,x_{n})}
   is then a histogram, with 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   counting the number of times event i was observed in a particular instance. This is the event model typically used for document classification, with events representing the occurrence of a word in a single document (see bag of words assumption). The likelihood of observing a histogram x is given by

  
    
      
        p
        (
        
          x
        
        ∣
        
          C
          
            k
          
        
        )
        =
        
          
            
              (
              
                ∑
                
                  i
                
              
              
                x
                
                  i
                
              
              )
              !
            
            
              
                ∏
                
                  i
                
              
              
                x
                
                  i
                
              
              !
            
          
        
        
          ∏
          
            i
          
        
        
          
            
              p
              
                k
                i
              
            
          
          
            
              x
              
                i
              
            
          
        
      
    
    {\displaystyle p(\mathbf {x} \mid C_{k})={\frac {(\sum _{i}x_{i})!}{\prod _{i}x_{i}!}}\prod _{i}{p_{ki}}^{x_{i}}}
  The multinomial naïve Bayes classifier becomes a linear classifier when expressed in log-space:

  
    
      
        
          
            
              
                log
                ⁡
                p
                (
                
                  C
                  
                    k
                  
                
                ∣
                
                  x
                
                )
              
              
                
                ∝
                log
                ⁡
                
                  (
                  
                    p
                    (
                    
                      C
                      
                        k
                      
                    
                    )
                    
                      ∏
                      
                        i
                        =
                        1
                      
                      
                        n
                      
                    
                    
                      
                        
                          p
                          
                            k
                            i
                          
                        
                      
                      
                        
                          x
                          
                            i
                          
                        
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                log
                ⁡
                p
                (
                
                  C
                  
                    k
                  
                
                )
                +
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  x
                  
                    i
                  
                
                ⋅
                log
                ⁡
                
                  p
                  
                    k
                    i
                  
                
              
            
            
              
              
                
                =
                b
                +
                
                  
                    w
                  
                  
                    k
                  
                  
                    ⊤
                  
                
                
                  x
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\log p(C_{k}\mid \mathbf {x} )&\varpropto \log \left(p(C_{k})\prod _{i=1}^{n}{p_{ki}}^{x_{i}}\right)\\&=\log p(C_{k})+\sum _{i=1}^{n}x_{i}\cdot \log p_{ki}\\&=b+\mathbf {w} _{k}^{\top }\mathbf {x} \end{aligned}}}
  where 
  
    
      
        b
        =
        log
        ⁡
        p
        (
        
          C
          
            k
          
        
        )
      
    
    {\displaystyle b=\log p(C_{k})}
   and 
  
    
      
        
          w
          
            k
            i
          
        
        =
        log
        ⁡
        
          p
          
            k
            i
          
        
      
    
    {\displaystyle w_{ki}=\log p_{ki}}
  .
If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero, because the probability estimate is directly proportional to the number of occurrences of a feature's value. This is problematic because it will wipe out all information in the other probabilities when they are multiplied. Therefore, it is often desirable to incorporate a small-sample correction, called pseudocount, in all probability estimates such that no probability is ever set to be exactly zero. This way of regularizing naive Bayes is called Laplace smoothing when the pseudocount is one, and Lidstone smoothing in the general case.
Rennie et al. discuss problems with the multinomial assumption in the context of document classification and possible ways to alleviate those problems, including the use of tf–idf weights instead of raw term frequencies and document length normalization, to produce a naive Bayes classifier that is competitive with support vector machines.

Bernoulli naïve Bayes
In the multivariate Bernoulli event model, features are independent Booleans (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks, where binary term occurrence features are used rather than term frequencies. If 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   is a boolean expressing the occurrence or absence of the i'th term from the vocabulary, then the likelihood of a document given a class 
  
    
      
        
          C
          
            k
          
        
      
    
    {\displaystyle C_{k}}
   is given by

  
    
      
        p
        (
        
          x
        
        ∣
        
          C
          
            k
          
        
        )
        =
        
          ∏
          
            i
            =
            1
          
          
            n
          
        
        
          p
          
            k
            i
          
          
            
              x
              
                i
              
            
          
        
        (
        1
        −
        
          p
          
            k
            i
          
        
        
          )
          
            (
            1
            −
            
              x
              
                i
              
            
            )
          
        
      
    
    {\displaystyle p(\mathbf {x} \mid C_{k})=\prod _{i=1}^{n}p_{ki}^{x_{i}}(1-p_{ki})^{(1-x_{i})}}
  where 
  
    
      
        
          p
          
            k
            i
          
        
      
    
    {\displaystyle p_{ki}}
   is the probability of class 
  
    
      
        
          C
          
            k
          
        
      
    
    {\displaystyle C_{k}}
   generating the term 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  . This event model is especially popular for classifying short texts. It has the benefit of explicitly modelling the absence of terms. Note that a naive Bayes classifier with a Bernoulli event model is not the same as a multinomial NB classifier with frequency counts truncated to one.

Semi-supervised parameter estimation
Given a way to train a naïve Bayes classifier from labeled data, it's possible to construct a semi-supervised training algorithm that can learn from a combination of labeled and unlabeled data by running the supervised learning algorithm in a loop:
Given a collection 
  
    
      
        D
        =
        L
        ⊎
        U
      
    
    {\displaystyle D=L\uplus U}
   of labeled samples L and unlabeled samples U, start by training a naïve Bayes classifier on L.
Until convergence, do:
Predict class probabilities 
  
    
      
        P
        (
        C
        ∣
        x
        )
      
    
    {\displaystyle P(C\mid x)}
   for all examples x in 
  
    
      
        D
      
    
    {\displaystyle D}
  .
Re-train the model based on the probabilities (not the labels) predicted in the previous step.Convergence is determined based on improvement to the model likelihood 
  
    
      
        P
        (
        D
        ∣
        θ
        )
      
    
    {\displaystyle P(D\mid \theta )}
  , where 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   denotes the parameters of the naïve Bayes model.
This training algorithm is an instance of the more general expectation–maximization algorithm (EM): the prediction step inside the loop is the E-step of EM, while the re-training of naïve Bayes is the M-step. The algorithm is formally justified by the assumption that the data are generated by a mixture model, and the components of this mixture model are exactly the classes of the classification problem.

Discussion
Despite the fact that the far-reaching independence assumptions are often inaccurate, the naive Bayes classifier has several properties that make it surprisingly useful in practice. In particular, the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one-dimensional distribution. This helps alleviate problems stemming from the curse of dimensionality, such as the need for data sets that scale exponentially with the number of features. While naive Bayes often fails to produce a good estimate for the correct class probabilities, this may not be a requirement for many applications. For example, the naive Bayes classifier will make the correct MAP decision rule classification so long as the correct class is more probable than any other class. This is true regardless of whether the probability estimate is slightly, or even grossly inaccurate. In this manner, the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model. Other reasons for the observed success of the naive Bayes classifier are discussed in the literature cited below.

Relation to logistic regression
In the case of discrete inputs (indicator or frequency features for discrete events), naive Bayes classifiers form a generative-discriminative pair with (multinomial) logistic regression classifiers: each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood 
  
    
      
        p
        (
        C
        ,
        
          x
        
        )
      
    
    {\displaystyle p(C,\mathbf {x} )}
  , while logistic regression fits the same probability model to optimize the conditional 
  
    
      
        p
        (
        C
        ∣
        
          x
        
        )
      
    
    {\displaystyle p(C\mid \mathbf {x} )}
  .The link between the two can be seen by observing that the decision function for naive Bayes (in the binary case) can be rewritten as ""predict class 
  
    
      
        
          C
          
            1
          
        
      
    
    {\displaystyle C_{1}}
   if the odds of 
  
    
      
        p
        (
        
          C
          
            1
          
        
        ∣
        
          x
        
        )
      
    
    {\displaystyle p(C_{1}\mid \mathbf {x} )}
   exceed those of 
  
    
      
        p
        (
        
          C
          
            2
          
        
        ∣
        
          x
        
        )
      
    
    {\displaystyle p(C_{2}\mid \mathbf {x} )}
  "". Expressing this in log-space gives:

  
    
      
        log
        ⁡
        
          
            
              p
              (
              
                C
                
                  1
                
              
              ∣
              
                x
              
              )
            
            
              p
              (
              
                C
                
                  2
                
              
              ∣
              
                x
              
              )
            
          
        
        =
        log
        ⁡
        p
        (
        
          C
          
            1
          
        
        ∣
        
          x
        
        )
        −
        log
        ⁡
        p
        (
        
          C
          
            2
          
        
        ∣
        
          x
        
        )
        >
        0
      
    
    {\displaystyle \log {\frac {p(C_{1}\mid \mathbf {x} )}{p(C_{2}\mid \mathbf {x} )}}=\log p(C_{1}\mid \mathbf {x} )-\log p(C_{2}\mid \mathbf {x} )>0}
  The left-hand side of this equation is the log-odds, or logit, the quantity predicted by the linear model that underlies logistic regression. Since naive Bayes is also a linear model for the two ""discrete"" event models, it can be reparametrised as a linear function 
  
    
      
        b
        +
        
          
            w
          
          
            ⊤
          
        
        x
        >
        0
      
    
    {\displaystyle b+\mathbf {w} ^{\top }x>0}
  . Obtaining the probabilities is then a matter of applying the logistic function to 
  
    
      
        b
        +
        
          
            w
          
          
            ⊤
          
        
        x
      
    
    {\displaystyle b+\mathbf {w} ^{\top }x}
  , or in the multiclass case, the softmax function.
Discriminative classifiers have lower asymptotic error than generative ones; however, research by Ng and Jordan has shown that in some practical cases naive Bayes can outperform logistic regression because it reaches its asymptotic error faster.

Examples
Person classification
Problem: classify whether a given person is a male or a female based on the measured features.
The features include height, weight, and foot size.

Training
Example training set below.

The classifier created from the training set using a Gaussian distribution assumption would be (given variances are unbiased sample variances):

Let's say we have equiprobable classes so P(male)= P(female) = 0.5.  This prior probability distribution might be based on our knowledge of frequencies in the larger population, or on frequency in the training set.

Testing
Below is a sample to be classified as male or female.

We wish to determine which posterior is greater, male or female. For the classification as male the posterior is given by

  
    
      
        
          posterior (male)
        
        =
        
          
            
              P
              (
              
                male
              
              )
              
              p
              (
              
                height
              
              ∣
              
                male
              
              )
              
              p
              (
              
                weight
              
              ∣
              
                male
              
              )
              
              p
              (
              
                foot size
              
              ∣
              
                male
              
              )
            
            
              e
              v
              i
              d
              e
              n
              c
              e
            
          
        
      
    
    {\displaystyle {\text{posterior (male)}}={\frac {P({\text{male}})\,p({\text{height}}\mid {\text{male}})\,p({\text{weight}}\mid {\text{male}})\,p({\text{foot size}}\mid {\text{male}})}{evidence}}}
  For the classification as female the posterior is given by

  
    
      
        
          posterior (female)
        
        =
        
          
            
              P
              (
              
                female
              
              )
              
              p
              (
              
                height
              
              ∣
              
                female
              
              )
              
              p
              (
              
                weight
              
              ∣
              
                female
              
              )
              
              p
              (
              
                foot size
              
              ∣
              
                female
              
              )
            
            
              e
              v
              i
              d
              e
              n
              c
              e
            
          
        
      
    
    {\displaystyle {\text{posterior (female)}}={\frac {P({\text{female}})\,p({\text{height}}\mid {\text{female}})\,p({\text{weight}}\mid {\text{female}})\,p({\text{foot size}}\mid {\text{female}})}{evidence}}}
  The evidence (also termed normalizing constant) may be calculated:

  
    
      
        
          
            
              
                
                  evidence
                
                =
                P
                (
                
                  male
                
                )
                
                p
                (
                
                  height
                
                ∣
                
                  male
                
                )
                
                p
                (
                
                  weight
                
                ∣
                
                  male
                
                )
                
                p
                (
                
                  foot size
                
                ∣
                
                  male
                
                )
              
            
            
              
                +
                P
                (
                
                  female
                
                )
                
                p
                (
                
                  height
                
                ∣
                
                  female
                
                )
                
                p
                (
                
                  weight
                
                ∣
                
                  female
                
                )
                
                p
                (
                
                  foot size
                
                ∣
                
                  female
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{evidence}}=P({\text{male}})\,p({\text{height}}\mid {\text{male}})\,p({\text{weight}}\mid {\text{male}})\,p({\text{foot size}}\mid {\text{male}})\\+P({\text{female}})\,p({\text{height}}\mid {\text{female}})\,p({\text{weight}}\mid {\text{female}})\,p({\text{foot size}}\mid {\text{female}})\end{aligned}}}
  However, given the sample, the evidence is a constant and thus scales both posteriors equally.  It therefore does not affect classification and can be ignored.  We now determine the probability distribution for the sex of the sample.

  
    
      
        P
        (
        
          male
        
        )
        =
        0.5
      
    
    {\displaystyle P({\text{male}})=0.5}
  

  
    
      
        p
        (
        
          height
        
        ∣
        
          male
        
        )
        =
        
          
            1
            
              2
              π
              
                σ
                
                  2
                
              
            
          
        
        exp
        ⁡
        
          (
          
            
              
                −
                (
                6
                −
                μ
                
                  )
                  
                    2
                  
                
              
              
                2
                
                  σ
                  
                    2
                  
                
              
            
          
          )
        
        ≈
        1.5789
      
    
    {\displaystyle p({\text{height}}\mid {\text{male}})={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}\exp \left({\frac {-(6-\mu )^{2}}{2\sigma ^{2}}}\right)\approx 1.5789}
  ,where 
  
    
      
        μ
        =
        5.855
      
    
    {\displaystyle \mu =5.855}
   and 
  
    
      
        
          σ
          
            2
          
        
        =
        3.5033
        ⋅
        
          10
          
            −
            2
          
        
      
    
    {\displaystyle \sigma ^{2}=3.5033\cdot 10^{-2}}
   are the parameters of normal distribution which have been previously determined from the training set. Note that a value greater than 1 is OK here – it is a probability density rather than a probability, because height is a continuous variable.

  
    
      
        p
        (
        
          weight
        
        ∣
        
          male
        
        )
        =
        
          
            1
            
              2
              π
              
                σ
                
                  2
                
              
            
          
        
        exp
        ⁡
        
          (
          
            
              
                −
                (
                130
                −
                μ
                
                  )
                  
                    2
                  
                
              
              
                2
                
                  σ
                  
                    2
                  
                
              
            
          
          )
        
        =
        5.9881
        ⋅
        
          10
          
            −
            6
          
        
      
    
    {\displaystyle p({\text{weight}}\mid {\text{male}})={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}\exp \left({\frac {-(130-\mu )^{2}}{2\sigma ^{2}}}\right)=5.9881\cdot 10^{-6}}
  

  
    
      
        p
        (
        
          foot size
        
        ∣
        
          male
        
        )
        =
        
          
            1
            
              2
              π
              
                σ
                
                  2
                
              
            
          
        
        exp
        ⁡
        
          (
          
            
              
                −
                (
                8
                −
                μ
                
                  )
                  
                    2
                  
                
              
              
                2
                
                  σ
                  
                    2
                  
                
              
            
          
          )
        
        =
        1.3112
        ⋅
        
          10
          
            −
            3
          
        
      
    
    {\displaystyle p({\text{foot size}}\mid {\text{male}})={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}\exp \left({\frac {-(8-\mu )^{2}}{2\sigma ^{2}}}\right)=1.3112\cdot 10^{-3}}
  

  
    
      
        
          posterior numerator (male)
        
        =
        
          their product
        
        =
        6.1984
        ⋅
        
          10
          
            −
            9
          
        
      
    
    {\displaystyle {\text{posterior numerator (male)}}={\text{their product}}=6.1984\cdot 10^{-9}}
  
  
    
      
        P
        (
        
          female
        
        )
        =
        0.5
      
    
    {\displaystyle P({\text{female}})=0.5}
  

  
    
      
        p
        (
        
          height
        
        ∣
        
          female
        
        )
        =
        2.2346
        ⋅
        
          10
          
            −
            1
          
        
      
    
    {\displaystyle p({\text{height}}\mid {\text{female}})=2.2346\cdot 10^{-1}}
  

  
    
      
        p
        (
        
          weight
        
        ∣
        
          female
        
        )
        =
        1.6789
        ⋅
        
          10
          
            −
            2
          
        
      
    
    {\displaystyle p({\text{weight}}\mid {\text{female}})=1.6789\cdot 10^{-2}}
  

  
    
      
        p
        (
        
          foot size
        
        ∣
        
          female
        
        )
        =
        2.8669
        ⋅
        
          10
          
            −
            1
          
        
      
    
    {\displaystyle p({\text{foot size}}\mid {\text{female}})=2.8669\cdot 10^{-1}}
  

  
    
      
        
          posterior numerator (female)
        
        =
        
          their product
        
        =
        5.3778
        ⋅
        
          10
          
            −
            4
          
        
      
    
    {\displaystyle {\text{posterior numerator (female)}}={\text{their product}}=5.3778\cdot 10^{-4}}
  Since posterior numerator is greater in the female case, we predict the sample is female.

Document classification
Here is a worked example of naive Bayesian classification to the document classification problem.
Consider the problem of classifying documents by their content, for example into spam and non-spam e-mails. Imagine that documents are drawn from a number of classes of documents which can be modeled as sets of words where the (independent) probability that the i-th word of a given document occurs in a document from class C can be written as

  
    
      
        p
        (
        
          w
          
            i
          
        
        ∣
        C
        )
        
      
    
    {\displaystyle p(w_{i}\mid C)\,}
  (For this treatment, we simplify things further by assuming that words are randomly distributed in the document - that is, words are not dependent on the length of the document, position within the document with relation to other words, or other document-context.)
Then the probability that a given document D contains all of the words 
  
    
      
        
          w
          
            i
          
        
      
    
    {\displaystyle w_{i}}
  , given a class C, is

  
    
      
        p
        (
        D
        ∣
        C
        )
        =
        
          ∏
          
            i
          
        
        p
        (
        
          w
          
            i
          
        
        ∣
        C
        )
        
      
    
    {\displaystyle p(D\mid C)=\prod _{i}p(w_{i}\mid C)\,}
  The question that we desire to answer is: ""what is the probability that a given document D belongs to a given class C?"" In other words, what is 
  
    
      
        p
        (
        C
        ∣
        D
        )
        
      
    
    {\displaystyle p(C\mid D)\,}
  ?
Now by definition

  
    
      
        p
        (
        D
        ∣
        C
        )
        =
        
          
            
              p
              (
              D
              ∩
              C
              )
            
            
              p
              (
              C
              )
            
          
        
      
    
    {\displaystyle p(D\mid C)={p(D\cap C) \over p(C)}}
  and

  
    
      
        p
        (
        C
        ∣
        D
        )
        =
        
          
            
              p
              (
              D
              ∩
              C
              )
            
            
              p
              (
              D
              )
            
          
        
      
    
    {\displaystyle p(C\mid D)={p(D\cap C) \over p(D)}}
  Bayes' theorem manipulates these into a statement of probability in terms of likelihood.

  
    
      
        p
        (
        C
        ∣
        D
        )
        =
        
          
            
              p
              (
              C
              )
              
              p
              (
              D
              ∣
              C
              )
            
            
              p
              (
              D
              )
            
          
        
      
    
    {\displaystyle p(C\mid D)={\frac {p(C)\,p(D\mid C)}{p(D)}}}
  Assume for the moment that there are only two mutually exclusive classes, S and ¬S (e.g. spam and not spam), such that every element (email) is in either one or the other;

  
    
      
        p
        (
        D
        ∣
        S
        )
        =
        
          ∏
          
            i
          
        
        p
        (
        
          w
          
            i
          
        
        ∣
        S
        )
        
      
    
    {\displaystyle p(D\mid S)=\prod _{i}p(w_{i}\mid S)\,}
  and

  
    
      
        p
        (
        D
        ∣
        ¬
        S
        )
        =
        
          ∏
          
            i
          
        
        p
        (
        
          w
          
            i
          
        
        ∣
        ¬
        S
        )
        
      
    
    {\displaystyle p(D\mid \neg S)=\prod _{i}p(w_{i}\mid \neg S)\,}
  Using the Bayesian result above, we can write:

  
    
      
        p
        (
        S
        ∣
        D
        )
        =
        
          
            
              p
              (
              S
              )
            
            
              p
              (
              D
              )
            
          
        
        
        
          ∏
          
            i
          
        
        p
        (
        
          w
          
            i
          
        
        ∣
        S
        )
      
    
    {\displaystyle p(S\mid D)={p(S) \over p(D)}\,\prod _{i}p(w_{i}\mid S)}
  
  
    
      
        p
        (
        ¬
        S
        ∣
        D
        )
        =
        
          
            
              p
              (
              ¬
              S
              )
            
            
              p
              (
              D
              )
            
          
        
        
        
          ∏
          
            i
          
        
        p
        (
        
          w
          
            i
          
        
        ∣
        ¬
        S
        )
      
    
    {\displaystyle p(\neg S\mid D)={p(\neg S) \over p(D)}\,\prod _{i}p(w_{i}\mid \neg S)}
  Dividing one by the other gives:

  
    
      
        
          
            
              p
              (
              S
              ∣
              D
              )
            
            
              p
              (
              ¬
              S
              ∣
              D
              )
            
          
        
        =
        
          
            
              p
              (
              S
              )
              
              
                ∏
                
                  i
                
              
              p
              (
              
                w
                
                  i
                
              
              ∣
              S
              )
            
            
              p
              (
              ¬
              S
              )
              
              
                ∏
                
                  i
                
              
              p
              (
              
                w
                
                  i
                
              
              ∣
              ¬
              S
              )
            
          
        
      
    
    {\displaystyle {p(S\mid D) \over p(\neg S\mid D)}={p(S)\,\prod _{i}p(w_{i}\mid S) \over p(\neg S)\,\prod _{i}p(w_{i}\mid \neg S)}}
  Which can be re-factored as:

  
    
      
        
          
            
              p
              (
              S
              ∣
              D
              )
            
            
              p
              (
              ¬
              S
              ∣
              D
              )
            
          
        
        =
        
          
            
              p
              (
              S
              )
            
            
              p
              (
              ¬
              S
              )
            
          
        
        
        
          ∏
          
            i
          
        
        
          
            
              p
              (
              
                w
                
                  i
                
              
              ∣
              S
              )
            
            
              p
              (
              
                w
                
                  i
                
              
              ∣
              ¬
              S
              )
            
          
        
      
    
    {\displaystyle {p(S\mid D) \over p(\neg S\mid D)}={p(S) \over p(\neg S)}\,\prod _{i}{p(w_{i}\mid S) \over p(w_{i}\mid \neg S)}}
  Thus, the probability ratio p(S | D) / p(¬S | D) can be expressed in terms of a series of likelihood ratios.
The actual probability p(S | D) can be easily computed from log (p(S | D) / p(¬S | D)) based on the observation that p(S | D) + p(¬S | D) = 1.
Taking the logarithm of all these ratios, we have:

  
    
      
        ln
        ⁡
        
          
            
              p
              (
              S
              ∣
              D
              )
            
            
              p
              (
              ¬
              S
              ∣
              D
              )
            
          
        
        =
        ln
        ⁡
        
          
            
              p
              (
              S
              )
            
            
              p
              (
              ¬
              S
              )
            
          
        
        +
        
          ∑
          
            i
          
        
        ln
        ⁡
        
          
            
              p
              (
              
                w
                
                  i
                
              
              ∣
              S
              )
            
            
              p
              (
              
                w
                
                  i
                
              
              ∣
              ¬
              S
              )
            
          
        
      
    
    {\displaystyle \ln {p(S\mid D) \over p(\neg S\mid D)}=\ln {p(S) \over p(\neg S)}+\sum _{i}\ln {p(w_{i}\mid S) \over p(w_{i}\mid \neg S)}}
  (This technique of ""log-likelihood ratios"" is a common technique in statistics.
In the case of two mutually exclusive alternatives (such as this example), the conversion of a log-likelihood ratio to a probability takes the form of a sigmoid curve: see logit for details.)
Finally, the document can be classified as follows.  It is spam if 
  
    
      
        p
        (
        S
        ∣
        D
        )
        >
        p
        (
        ¬
        S
        ∣
        D
        )
      
    
    {\displaystyle p(S\mid D)>p(\neg S\mid D)}
   (i. e., 
  
    
      
        ln
        ⁡
        
          
            
              p
              (
              S
              ∣
              D
              )
            
            
              p
              (
              ¬
              S
              ∣
              D
              )
            
          
        
        >
        0
      
    
    {\displaystyle \ln {p(S\mid D) \over p(\neg S\mid D)}>0}
  ), otherwise it is not spam.

See also
AODE
Bayes classifier
Bayesian spam filtering
Bayesian network
Random naive Bayes
Linear classifier
Logistic regression
Perceptron
Take-the-best heuristic

References
Further reading
Domingos, Pedro; Pazzani, Michael (1997). ""On the optimality of the simple Bayesian classifier under zero-one loss"". Machine Learning. 29 (2/3): 103–137. doi:10.1023/A:1007413511361.
Webb, G. I.; Boughton, J.; Wang, Z. (2005). ""Not So Naive Bayes: Aggregating One-Dependence Estimators"". Machine Learning. 58 (1): 5–24. doi:10.1007/s10994-005-4258-6.
Mozina, M.; Demsar, J.; Kattan, M.; Zupan, B. (2004). Nomograms for Visualization of Naive Bayesian Classifier (PDF). Proc. PKDD-2004. pp. 337–348.
Maron, M. E. (1961). ""Automatic Indexing: An Experimental Inquiry"". Journal of the ACM. 8 (3): 404–417. doi:10.1145/321075.321084. hdl:2027/uva.x030748531. S2CID 6692916.
Minsky, M. (1961). Steps toward Artificial Intelligence. Proc. IRE. 49. pp. 8–30.

External links
Book Chapter: Naive Bayes text classification, Introduction to Information Retrieval
Naive Bayes for Text Classification with Unbalanced Classes
Benchmark results of Naive Bayes implementations
Hierarchical Naive Bayes Classifiers for uncertain data (an extension of the Naive Bayes classifier).SoftwareNaive Bayes classifiers are available in many general-purpose machine learning and NLP packages, including Apache Mahout, Mallet, NLTK, Orange, scikit-learn and Weka.
IMSL Numerical Libraries Collections of math and statistical algorithms available in C/C++, Fortran, Java and C#/.NET. Data mining routines in the IMSL Libraries include a Naive Bayes classifier.
An interactive Microsoft Excel spreadsheet Naive Bayes implementation using VBA (requires enabled macros) with viewable source code.
jBNC - Bayesian Network Classifier Toolbox
Statistical Pattern Recognition Toolbox for Matlab.
ifile - the first freely available (Naive) Bayesian mail/spam filter
NClassifier - NClassifier is a .NET library that supports text classification and text summarization. It is a port of Classifier4J.
Classifier4J - Classifier4J is a Java library designed to do text classification. It comes with an implementation of a Bayesian classifier.
JNBC Naive Bayes Classifier running in-memory or using fast key-value stores (MapDB, LevelDB or RocksDB).
Blayze - Blayze is a minimal JVM library for Naive Bayes classification written in Kotlin.",https://en.wikipedia.org/wiki/Naive_Bayes_classifier,"['All articles lacking in-text citations', 'All articles to be expanded', 'All articles with unsourced statements', 'Articles lacking in-text citations from May 2009', 'Articles to be expanded from August 2014', 'Articles using small message boxes', 'Articles with short description', 'Articles with unsourced statements from December 2014', 'Bayesian statistics', 'Classification algorithms', 'Short description is different from Wikidata', 'Statistical classification']",Data Science
136,Non-negative matrix factorization,"Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.
NMF finds applications in such fields as astronomy, computer vision, document clustering, missing data imputation, chemometrics, audio signal processing, recommender systems, and bioinformatics.

History
In chemometrics non-negative matrix factorization has a long history under the name ""self modeling curve resolution"".
In this framework the vectors in the right matrix are continuous curves rather than discrete vectors.
Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the 1990s under the name positive matrix factorization.
It became more widely known as non-negative matrix factorization after Lee and Seung investigated
the properties of the algorithm and published some simple and useful
algorithms for two types of factorizations.

Background
Let matrix V be the product of the matrices W and H,

  
    
      
        
          V
        
        =
        
          W
        
        
          H
        
        
        .
      
    
    {\displaystyle \mathbf {V} =\mathbf {W} \mathbf {H} \,.}
  Matrix multiplication can be implemented as computing the column vectors of V as linear combinations of the column vectors in W using coefficients supplied by columns of H.  That is, each column of V can be computed as follows:

  
    
      
        
          
            v
          
          
            i
          
        
        =
        
          W
        
        
          
            h
          
          
            i
          
        
        
        ,
      
    
    {\displaystyle \mathbf {v} _{i}=\mathbf {W} \mathbf {h} _{i}\,,}
  where vi is the i-th column vector of the product matrix V and hi is the i-th column vector of the matrix H.
When multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if V is an m × n matrix, W is an m × p matrix, and H is a p × n matrix then p can be significantly less than both m and n.
Here is an example based on a text-mining application: 

Let the input matrix (the matrix to be factored) be V with 10000 rows and 500 columns where words are in rows and documents are in columns. That is, we have 500 documents indexed by 10000 words. It follows that a column vector v in V represents a document.
Assume we ask the algorithm to find 10 features in order to generate a features matrix W with 10000 rows and 10 columns and a coefficients matrix H with 10 rows and 500 columns.
The product of W and H is a matrix with 10000 rows and 500 columns, the same shape as the input matrix V and, if the factorization worked, it is a reasonable approximation to the input matrix V.
From the treatment of matrix multiplication above it follows that each column in the product matrix WH is a linear combination of the 10 column vectors in the features matrix W with coefficients supplied by the coefficients matrix H.This last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.
It is useful to think of each feature (column vector) in the features matrix W as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix H represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in W) where each feature is weighted by the feature's cell value from the document's column in H.

Clustering property
NMF has an inherent clustering property, i.e., it automatically clusters the columns of input data 

  
    
      
        
          V
        
        =
        (
        
          v
          
            1
          
        
        ,
        ⋯
        ,
        
          v
          
            n
          
        
        )
      
    
    {\displaystyle \mathbf {V} =(v_{1},\cdots ,v_{n})}
  .
More specifically, the approximation of 
  
    
      
        
          V
        
      
    
    {\displaystyle \mathbf {V} }
   by

  
    
      
        
          V
        
        ≃
        
          W
        
        
          H
        
      
    
    {\displaystyle \mathbf {V} \simeq \mathbf {W} \mathbf {H} }
   is achieved by finding 
  
    
      
        W
      
    
    {\displaystyle W}
   and 
  
    
      
        H
      
    
    {\displaystyle H}
   that minimize the error function

  
    
      
        
          |
        
        
          |
        
        V
        −
        W
        H
        
          |
        
        
          
            |
          
          
            F
          
        
        ,
      
    
    {\displaystyle ||V-WH||_{F},}
   subject to 
  
    
      
        W
        ≥
        0
        ,
        H
        ≥
        0.
      
    
    {\displaystyle W\geq 0,H\geq 0.}
  
If we furthermore impose an orthogonality constraint on 
  
    
      
        
          H
        
      
    
    {\displaystyle \mathbf {H} }
  , 
i.e. 
  
    
      
        
          H
        
        
          
            H
          
          
            T
          
        
        =
        I
      
    
    {\displaystyle \mathbf {H} \mathbf {H} ^{T}=I}
  , then the above minimization is mathematically equivalent to the minimization of K-means clustering.Furthermore, the computed 
  
    
      
        H
      
    
    {\displaystyle H}
   gives the cluster membership, i.e.,
if 
  
    
      
        
          
            H
          
          
            k
            j
          
        
        >
        
          
            H
          
          
            i
            j
          
        
      
    
    {\displaystyle \mathbf {H} _{kj}>\mathbf {H} _{ij}}
   for all i ≠ k, this suggests that
the input data 
  
    
      
        
          v
          
            j
          
        
      
    
    {\displaystyle v_{j}}
  
belongs to 
  
    
      
        
          k
          
            t
            h
          
        
      
    
    {\displaystyle k^{th}}
   cluster. 
The computed 
  
    
      
        W
      
    
    {\displaystyle W}
   gives the cluster centroids, i.e., 
the 
  
    
      
        
          k
          
            t
            h
          
        
      
    
    {\displaystyle k^{th}}
   column 
gives the cluster centroid of

  
    
      
        
          k
          
            t
            h
          
        
      
    
    {\displaystyle k^{th}}
   cluster. This centroid's representation can be significantly enhanced by convex NMF.
When the orthogonality constraint 
  
    
      
        
          H
        
        
          
            H
          
          
            T
          
        
        =
        I
      
    
    {\displaystyle \mathbf {H} \mathbf {H} ^{T}=I}
   is not explicitly imposed, the orthogonality holds to a large extent, and the clustering property holds too. Clustering is the main objective of most data mining applications of NMF.When the error function to be used is Kullback–Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.

Types
Approximate non-negative matrix factorization
Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V.  The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U. The elements of the residual matrix can either be negative or positive.
When W and H are smaller than V they become easier to store and manipulate. Another reason for factorizing V into smaller matrices W and H, is that if one is able to approximately represent the elements of V by significantly less data, then one has to infer some latent structure in the data.

Convex non-negative matrix factorization
In standard NMF, matrix factor W ∈ ℝ+m × k， i.e., W can be anything in that space.  Convex NMF restricts the columns of W to convex combinations of the input data vectors 
  
    
      
        (
        
          v
          
            1
          
        
        ,
        ⋯
        ,
        
          v
          
            n
          
        
        )
      
    
    {\displaystyle (v_{1},\cdots ,v_{n})}
  . This greatly improves the quality of data representation of W. Furthermore, the resulting matrix factor H becomes more sparse and orthogonal.

Nonnegative rank factorization
In case the nonnegative rank of V is equal to its actual rank, V = WH is called a nonnegative rank factorization (NRF). The problem of finding the NRF of V, if it exists, is known to be NP-hard.

Different cost functions and regularizations
There are different types of non-negative matrix factorizations.
The different types arise from using different cost functions for measuring the divergence between V and WH and possibly by regularization of the W and/or H matrices.Two simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback–Leibler divergence to positive matrices (the original Kullback–Leibler divergence is defined on probability distributions).
Each divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.
The factorization problem in the squared error version of NMF may be stated as:
Given a matrix 
  
    
      
        
          V
        
      
    
    {\displaystyle \mathbf {V} }
   find nonnegative matrices W and H that minimize the function

  
    
      
        F
        (
        
          W
        
        ,
        
          H
        
        )
        =
        ‖
        
          V
        
        −
        
          W
          H
        
        
          ‖
          
            F
          
          
            2
          
        
      
    
    {\displaystyle F(\mathbf {W} ,\mathbf {H} )=\|\mathbf {V} -\mathbf {WH} \|_{F}^{2}}
  Another type of NMF for images is based on the total variation norm.When L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,
although it may also still be referred to as NMF.

Online NMF
Many standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.

Algorithms
There are several ways in which the W and H may be found: Lee and Seung's multiplicative update rule has been a popular method due to the simplicity of implementation.  This algorithm is:

initialize: W and H non negative.
Then update the values in W and H by computing the following, with 
  
    
      
        n
      
    
    {\displaystyle n}
   as an index of the iteration.

  
    
      
        
          
            H
          
          
            [
            i
            ,
            j
            ]
          
          
            n
            +
            1
          
        
        ←
        
          
            H
          
          
            [
            i
            ,
            j
            ]
          
          
            n
          
        
        
          
            
              (
              (
              
                
                  W
                
                
                  n
                
              
              
                )
                
                  T
                
              
              
                V
              
              
                )
                
                  [
                  i
                  ,
                  j
                  ]
                
              
            
            
              (
              (
              
                
                  W
                
                
                  n
                
              
              
                )
                
                  T
                
              
              
                
                  W
                
                
                  n
                
              
              
                
                  H
                
                
                  n
                
              
              
                )
                
                  [
                  i
                  ,
                  j
                  ]
                
              
            
          
        
      
    
    {\displaystyle \mathbf {H} _{[i,j]}^{n+1}\leftarrow \mathbf {H} _{[i,j]}^{n}{\frac {((\mathbf {W} ^{n})^{T}\mathbf {V} )_{[i,j]}}{((\mathbf {W} ^{n})^{T}\mathbf {W} ^{n}\mathbf {H} ^{n})_{[i,j]}}}}
  
and

  
    
      
        
          
            W
          
          
            [
            i
            ,
            j
            ]
          
          
            n
            +
            1
          
        
        ←
        
          
            W
          
          
            [
            i
            ,
            j
            ]
          
          
            n
          
        
        
          
            
              (
              
                V
              
              (
              
                
                  H
                
                
                  n
                  +
                  1
                
              
              
                )
                
                  T
                
              
              
                )
                
                  [
                  i
                  ,
                  j
                  ]
                
              
            
            
              (
              
                
                  W
                
                
                  n
                
              
              
                
                  H
                
                
                  n
                  +
                  1
                
              
              (
              
                
                  H
                
                
                  n
                  +
                  1
                
              
              
                )
                
                  T
                
              
              
                )
                
                  [
                  i
                  ,
                  j
                  ]
                
              
            
          
        
      
    
    {\displaystyle \mathbf {W} _{[i,j]}^{n+1}\leftarrow \mathbf {W} _{[i,j]}^{n}{\frac {(\mathbf {V} (\mathbf {H} ^{n+1})^{T})_{[i,j]}}{(\mathbf {W} ^{n}\mathbf {H} ^{n+1}(\mathbf {H} ^{n+1})^{T})_{[i,j]}}}}
  
Until W and H are stable.Note that the updates are done on an element by element basis not matrix multiplication.
We note that the multiplicative factors for W and H, i.e. the 
  
    
      
        
          
            
              
                
                  W
                
                
                  
                    T
                  
                
              
              
                V
              
            
            
              
                
                  W
                
                
                  
                    T
                  
                
              
              
                W
              
              
                H
              
            
          
        
      
    
    {\textstyle {\frac {\mathbf {W} ^{\mathsf {T}}\mathbf {V} }{\mathbf {W} ^{\mathsf {T}}\mathbf {W} \mathbf {H} }}}
   and 
  
    
      
        
          
            
              
                
                  
                    V
                  
                  
                    
                      H
                    
                    
                      
                        T
                      
                    
                  
                
                
                  
                    W
                  
                  
                    H
                  
                  
                    
                      H
                    
                    
                      
                        T
                      
                    
                  
                
              
            
          
        
      
    
    {\textstyle {\textstyle {\frac {\mathbf {V} \mathbf {H} ^{\mathsf {T}}}{\mathbf {W} \mathbf {H} \mathbf {H} ^{\mathsf {T}}}}}}
   terms, are matrices of ones when 
  
    
      
        
          V
        
        =
        
          W
        
        
          H
        
      
    
    {\displaystyle \mathbf {V} =\mathbf {W} \mathbf {H} }
  .
More recently other algorithms have been developed.
Some approaches are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same or different, as some NMF variants regularize one of W and H. Specific approaches include the projected gradient descent methods, the active set method, the optimal gradient method, and the block principal pivoting method among several others.Current algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete. However, as in many other data mining applications, a local minimum may still prove to be useful.

Sequential NMF
The sequential construction of NMF components (W and H) was firstly used to relate NMF with Principal Component Analysis (PCA) in astronomy. The contribution from the PCA components are ranked by the magnitude of their corresponding eigenvalues; for NMF, its components can be ranked empirically when they are constructed one by one (sequentially), i.e., learn the 
  
    
      
        (
        n
        +
        1
        )
      
    
    {\displaystyle (n+1)}
  -th component with the first 
  
    
      
        n
      
    
    {\displaystyle n}
   components constructed.
The contribution of the sequential NMF components can be compared with the Karhunen–Loève theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the ""elbow"" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting. For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA, which is the indication of less over-fitting of sequential NMF.

Exact NMF
Exact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix V. A polynomial time algorithm for solving nonnegative rank factorization if V contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981. Kalofolias and Gallopoulos (2012) solved the symmetric counterpart of this problem, where V is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies a separability condition.

Relation to other techniques
In Learning the parts of objects by non-negative matrix factorization Lee and Seung proposed NMF mainly for parts-based decomposition of images.  It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.

It was later shown that some types of NMF are an instance of a more general probabilistic model called ""multinomial PCA"".
When NMF is obtained by minimizing the Kullback–Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,
trained by maximum likelihood estimation.
That method is commonly used for analyzing and clustering textual data and is also related to the latent class model.
NMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor W contains cluster centroids and H contains cluster membership indicators.  This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with ""semi-NMF"".NMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.NMF extends beyond matrices to tensors of arbitrary order. This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.
Other extensions of NMF include joint factorization of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.NMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.

Uniqueness
The factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,

  
    
      
        
          W
          H
        
        =
        
          
            W
            B
            B
          
          
            −
            1
          
        
        
          H
        
      
    
    {\displaystyle \mathbf {WH} =\mathbf {WBB} ^{-1}\mathbf {H} }
  If the two new matrices 
  
    
      
        
          
            
              
                W
                ~
              
            
          
          =
          W
          B
        
      
    
    {\displaystyle \mathbf {{\tilde {W}}=WB} }
   and 
  
    
      
        
          
            
              H
              ~
            
          
        
        =
        
          
            B
          
          
            −
            1
          
        
        
          H
        
      
    
    {\displaystyle \mathbf {\tilde {H}} =\mathbf {B} ^{-1}\mathbf {H} }
   are non-negative they form another parametrization of the factorization.
The non-negativity of 
  
    
      
        
          
            
              W
              ~
            
          
        
      
    
    {\displaystyle \mathbf {\tilde {W}} }
   and 
  
    
      
        
          
            
              H
              ~
            
          
        
      
    
    {\displaystyle \mathbf {\tilde {H}} }
   applies at least if B is a non-negative monomial matrix.
In this simple case it will just correspond to a scaling and a permutation.
More control over the non-uniqueness of NMF is obtained with sparsity constraints.

Applications
Astronomy
In astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations  and the direct imaging observations  as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007)  takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016)  where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018)  to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.
Ren et al. (2018)  are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.
In direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10⁵ to 10¹⁰, various statistical methods have been adopted, however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux. Forward modeling is currently optimized for point sources, however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors, rather than a computationally intensive data re-reduction on generated models.

Data imputation
To impute missing data in statistics, NMF can take missing data while minimizing its cost function, rather than treating these missing data as zeros. This makes it a mathematically proven method for data imputation in statistics. By first proving that the missing data are ignored in the cost function, then proving that the impact from missing data can be as small as a second order effect, Ren et al. (2020) studied and applied such an approach for the field of astronomy. Their work focuses on two-dimensional matrices, specifically, it includes mathematical derivation, simulated data imputation, and application to on-sky data.
The data imputation procedure with NMF can be composed of two steps. First, when the NMF components are known, Ren et al. (2020) proved that impact from missing data during data imputation (""target modeling"" in their study) is a second order effect. Second, when the NMF components are unknown, the authors proved that the impact from missing data during component construction is a first-to-second order effect. 
Depending on the way that the NMF components are obtained, the former step above can be either independent or dependent from the latter. In addition, the imputation quality can be increased when the more NMF components are used, see Figure 4 of Ren et al. (2020) for their illustration.

Text mining
NMF can be used for text mining applications.
In this process, a document-term matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents.
This matrix is factored into a term-feature and a feature-document matrix.
The features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.
One specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.
Another research group clustered parts of the Enron email dataset
with 65,033 messages and 91,133 terms into 50 clusters.
NMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.Hassani, Iranmanesh and Mansouri (2019) proposed a feature agglomeration method for term-document matrices which operates using NMF. The algorithm reduces the term-document matrix into a smaller matrix more suitable for text clustering.

Spectral data analysis
NMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.

Scalable Internet distance prediction
NMF is applied in scalable Internet distance (round-trip time) prediction. For a network with 
  
    
      
        N
      
    
    {\displaystyle N}
   hosts, with the help of NMF, the distances of all the 
  
    
      
        
          N
          
            2
          
        
      
    
    {\displaystyle N^{2}}
   end-to-end links can be predicted after conducting only 
  
    
      
        O
        (
        N
        )
      
    
    {\displaystyle O(N)}
   measurements. This kind of method was firstly introduced in Internet
Distance Estimation Service (IDES). Afterwards, as a fully decentralized approach, Phoenix network coordinate system
is proposed. It achieves better overall prediction accuracy by introducing the concept of weight.

Non-stationary speech denoising
Speech denoising has been a long lasting problem in audio signal processing. There are many algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al. use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.
The algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.

Population Genetics
Sparse NMF is used in Population genetics for estimating individual admixture coefficients, detecting genetic clusters of individuals in a population sample or evaluating genetic admixture in sampled genomes. In human genetic clustering, NMF algorithms provide estimates similar to those of the computer program STRUCTURE, but the algorithms are more efficient computationally and allow analysis of large population genomic data sets.

Bioinformatics
NMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters. In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes. NMF techniques can identify sources of variation such as cell types, disease subtypes, population stratification, tissue composition, and tumor clonality.

Nuclear imaging
NMF, also referred in this field as factor analysis, has been used since the 1980s to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.

Current research
Current research (since 2010) in nonnegative matrix factorization includes, but is not limited to,

Algorithmic: searching for global minima of the factors and factor initialization.
Scalability: how to factorize million-by-billion matrices, which are commonplace in Web-scale data mining, e.g., see Distributed Nonnegative Matrix Factorization (DNMF), Scalable Nonnegative Matrix Factorization (ScalableNMF), Distributed Stochastic Singular Value Decomposition.
Online: how to update the factorization when new data comes in without recomputing from scratch, e.g., see online CNSC
Collective (joint) factorization: factorizing multiple interrelated matrices for multiple-view learning, e.g. multi-view clustering, see CoNMF and MultiNMF
Cohen and Rothblum 1993 problem: whether a rational matrix always has an NMF of minimal inner dimension whose factors are also rational. Recently, this problem has been answered negatively.

See also
Multilinear algebra
Multilinear subspace learning
Tensor
Tensor decomposition
Tensor software

Sources and external links
Notes


=== Others ===",https://en.wikipedia.org/wiki/Non-negative_matrix_factorization,"['All articles with unsourced statements', 'Articles with unsourced statements from April 2015', 'CS1: long volume value', 'CS1 errors: missing periodical', 'CS1 maint: multiple names: authors list', 'Linear algebra', 'Machine learning algorithms', 'Matrix theory', 'Webarchive template wayback links']",Data Science
137,Montpellier 2 University,"Montpellier 2 University (Université Montpellier 2) was a French university in the académie of Montpellier. It was one of the three universities formed in 1970 from the original University of Montpellier. Its main campus neighbors the Montpellier 3 University's main campus, and for this reason the nearest tramway station is named ""Universities of Sciences and Literature"" rather than ""University of Sciences"". In January 2015, Montpellier 1 University and Montpellier 2 University merged into the Montpellier University (Université de Montpellier).

History
The creation of the imperial University by Napoleon I in 1808 stimulated the
formation of a number of faculties of Humanities and of Science in the main cities of the French Empire.
At that time, Montpellier had already a long-established medical college and a school of Pharmacy, but also a respected Royal Society of Sciences created in 1706. In 1810, a Faculty of Science started with initially seven chairs: mathematics, astronomy, physics, chemistry, zoology, botany, and mineralogy.
In 1879, the faculty created a research station of marine biology in Sète, and, twelve years later, and Institute of Botany (which is still part of University Montpellier 2). The Institute of Chemistry, created in the same period, became the Ecole Nationale Supérieure of Chemistry of Montpellier in 1941.
In 1964, the faculty left the centre of Montpellier to settle in a  30 hectare campus to the north of the city on which 146 000 m2 of buildings for teaching and research were built.

Heritage
The University Montpellier 2 retains the Institute of Botany of Montpellier (created in 1889 by Professor Charles Flahault), which is close to the botanical garden of University Montpellier 1.  Partly demolished after World War II, most of the buildings date from 1956. The building houses a prestigious herbarium, the largest in France after the national museum of natural history, with approximately 4 million samples and an important collection of botany vellums, and research laboratories in the fields of ecology and parasitology.
The station of marine biology in Sète has been part of the University since 1879.
In addition to these collections, the university's media library brings together its old collections of printed works, manuscripts, and iconography (including, for example, the library of the work of Felix Dunal, bequeathed to the Faculty of Science in 1856). These collections are publicly accessible given a reasonable request.
The university is also the seat of the Pôle universitaire de Montpellier which collectively represents the higher education establishments in the Languedoc-Roussillon region.

Overview of University Montpellier 2
Université Montpellier 2 is a research-intensive university where education and research cover most of the Scientific and Technological fields:
Biodiversity, Ecology, Evolution, Environment
Biology, Agronomy
Biology and Health
Chemistry
Education
Management
Mathematics, Informatics, Physics and Systems
Universe, Earth, WaterIt is partnered with 40 joint research units, 1 observatory and divided into 7 specialised faculties.

Education
The university curriculum follows the LMD system, which divides higher education into 3 diplomas: 
Licence (Bachelor's degree, 3 years of Higher Education) 
Master (Master's degree, 2 years after bachelor's degree)
Doctorat (PhD, 3 years after master's degree)
The university is divided into 7 specialised faculties:

(in French) The Faculty of Science
The Montpellier University Graduate Engineering School (Polytech)
3 University Institutes of Technology (IUT)
IUT Montpellier-Sète
(in French) IUT Nîmes
(in French) IUT Béziers
The Montpellier School of Management (IAE) (belongs to a Institut d'Administration des Entreprises IAEs network of 31 IAEs throughout France)
The Faculty of EducationAnd 6 doctoral schools.

Research
Université Montpellier 2 is composed of about forty research departments in:
Biodiversity, Ecology, Evolution, Environment
Biology, Agronomy
Biology and Health
Chemistry
Education
Management
Mathematics, Informatics, Physics and Systems
Universe, Earth, WaterThe whole research activity is carried out collaboratively with the leading national research organisations (CNRS, Inserm INSERM, Institut national de la recherche agronomique INRA, Cirad CIRAD, Ird IRD, Brgm, CEA, CNES, Ifremer, Inria, Irstea).

Rankings
2014 QS world university rankings
51 – 100th for the Earth and Marine Sciences field 
101 – 150th for the Agriculture & Forestry field 
385th worldwide 
2013 Academic Ranking of World Universities: 201 – 300th worldwide
Times Higher Education under 50 universities: 32nd worldwide 
National Taiwan university: 294th worldwide

See also
University of Montpellier
List of public universities in France by academy


== Notes ==",https://en.wikipedia.org/wiki/Montpellier_2_University,"['1970 establishments in France', '2015 disestablishments in France', 'Articles with French-language sources (fr)', 'Commons category link is on Wikidata', 'Coordinates on Wikidata', 'Educational institutions disestablished in 2015', 'Educational institutions established in 1970', 'Instances of Infobox university using image size', 'Pages using infobox university with the affiliations parameter', 'Pages using infobox university with the image name parameter', 'Universities and colleges in Montpellier', 'University of Montpellier', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with NKC identifiers', 'Wikipedia articles with NLG identifiers', 'Wikipedia articles with SUDOC identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers']",Data Science
138,National Science Board,"The National Science Board (NSB) of the United States establishes the policies of the National Science Foundation (NSF) within the framework of applicable national policies set forth by the President and the Congress. The NSB also serves as an independent policy advisory body to the President and Congress on science and engineering research and education issues. The Board has a statutory obligation to ""...render to the President and to the Congress reports on specific, individual policy matters related to science and engineering and education in science engineering, as Congress or  the President determines the need for such reports,"" (e.g. Science and Engineering Indicators; Report to Congress on Mid-scale Instrumentation at the National Science  Foundation). All Board members are presidential appointees. NSF's director serves as an ex officio 25th member and is appointed by the President and confirmed by the US Senate.

Mission Statement
Supporting Education and Research across all fields of Science and Technology and America's Investment in the Future

Background
The National Science Board was created through the National Science Foundation Act of 1950: ""There is established in the executive branch of the Government an independent agency to be known as the National Science Foundation (hereinafter referred to as the ""Foundation""). The Foundation shall consist of a National Science Board (hereinafter referred to as the ""Board"") and a Director.""As an independent Federal agency, NSF does not fall within a cabinet department; rather NSF's activities are guided by the National Science Board (NSB or Board). The Board was established by the Congress to serve as a national science policy body, and to oversee and guide the activities of NSF. It has dual responsibilities to: a) provide independent national science policy advice to the President and the Congress; and b) establish policies for NSF.
The Board meets five times per year to review and approve major NSF awards and new programs, provide policy direction to NSF, and address significant science- and engineering-related national policy issues. It initiates and conducts studies and reports on a broad range of policy topics, and publishes policy papers or statements on issues of importance to U.S. science and engineering research and education enterprises. The Board identifies issues that are critical to NSF's future, and approves NSF's strategic plan and the annual budget submission to the Office of Management and Budget (OMB). Specifically, the Board analyzes NSF's budget to ensure progress and consistency in keeping with the strategic direction set for NSF and to ensure balance between new investments and core programs.

Composition
The President appoints 24 Members of the National Science Board for six year terms. The NSF director serves as an ex officio 25th member. Every two years, one-third (eight) of the members rotate off of the Board and eight new members are appointed (or occasionally re-appointed) to serve six-year terms.  Board member nominations are based on distinguished service and eminence in research, education and/or public service.  Members are drawn from academia and industry, and represent a diverse range of science, technology, engineering, and education disciplines and geographic areas.

Current members
Terms expire May 10, 2022
Arthur Bienenstock – Professor Emeritus of Photon Science, Stanford University
W. Kent Fuchs – President, University of Florida
W. Carl Lineberger – Fellow of JILA and E. U. Condon Distinguished Professor of Chemistry, University of Colorado
Victor R. McCrary – NSB Vice Chair | Vice President for Research and Graduate Programs, University of the District of Columbia
Emilio F. Moran – John A. Hannah Distinguished Professor, Michigan State University
Ellen Ochoa – NSB Chair | Director (retired), Lyndon B. Johnson Space Center
Julia M. Phillips – Executive Emeritus, Sandia National Laboratories
Anneila I. Sargent – Ira S. Bowen Professor of Astronomy Emeritus, California Institute of Technology

Terms expire May 10, 2024
Maureen L. Condic – Associate Professor of Neurobiology and Anatomy, University of Utah, School of Medicine
Suresh V. Garimella – President, University of Vermont
Steven Leath – Former President, Auburn University
Daniel A. Reed – Senior Vice President for Academic Affairs (Provost) University of Utah
Geraldine L. Richmond – Presidential Chair in Science and Professor of Chemistry, University of Oregon
S. Alan Stern – Associate Vice President and Special Assistant to the President, Southwest Research Institute
Stephen H. Willard – CEO, Cellphire, Inc.
Maria T. Zuber – Vice President for Research, Massachusetts Institute of Technology

Terms expire May 10, 2026
Sudarsanam Suresh Babu – Director of Bredesen Center for Interdisciplinary Research and Graduate Education; Governor’s Chair of Advanced Manufacturing and Professor of Mechanical Engineering and Materials Science, Oak Ridge National Laboratory/University of Tennessee, Knoxville
Roger N. Beachy – Professor Emeritus of Biology, Washington University in St. Louis
Dario Gil – Director of IBM Research, IBM
Aaron Dominguez – Provost and Professor of Physics, Catholic University of America, Washington, DC
Melvyn E. Huff – Lecturer, University of Massachusetts, Dartmouth
Heather A. Wilson – President, University of Texas, El Paso

NSF Director and ex officio Member
Sethuraman Panchanathan – NSF Director; Executive Vice President and Chief Research and Innovation Officer of Knowledge Enterprise Development and Director of the Center for Cognitive Ubiquitous Computing, Arizona State UniversityA list of former NSB members can be found here.

Work of the National Science Board
The Board has two overarching roles:  1) Provide oversight and policy guidance to the National Science Foundation; and 2) Serve as an advisor to Congress and the President on matters concerning science and engineering in the U.S.

Committees
Much of the background work of the National Science Board is done through its committees. By statute, the Board has an Executive Committee (EC), which exercises such functions as are delegated to it by the Board, and such other committees as the Board deems necessary. As of January 2009, the Board has five other standing committees.

Standing committees
Executive Committee (EC)
Committee on Oversight (CO)
Committee on External Engagement (EE)
Committee on Awards and Facilities (A&F)
Committee on National Science and Engineering Policy (SEP)
Committee on Strategy (CS)

Subcommittees, task forces, and ad hoc committees
Subcommittee on Honorary Awards
Task Force on the Skilled Technical Workforce (STW)

Archived committees
Audit and Oversight (A&O)
ad hoc Committee for Nominating NSB Class of 2018–2024 (NOMS)
ad hoc Committee for Nominating NSB Class of 2014–2020 (NOMS)
ad hoc Committee on Honorary Awards 2014 (AWD)
ad hoc Committee on Honorary Awards 2010 (AWD)
ad hoc Committee for Nominating NSB class of 2006–2012
ad hoc Committee for the Vannevar Bush Award
ad hoc Committee on Nominating for NSB Elections
ad hoc Committee on Nominating for NSB Elections (NOMCOM)
ad hoc Committee for Nominating NSB Class of 2016–2022 (NOMS)
ad hoc Committee for Nominating NSB Class of 2012–2018 (NOMS)
ad hoc Committee on Honorary Awards 2009
ad hoc Committee on the 2003 Vannevar Bush Committee
ad hoc Working Group on Administrative Burdens
Committee on Communication and Outreach
CPP Subcommittee on Polar Issues (SOPI)
CSB Task Force on Data Policies (DP)
CPP Task Force on International Science
CPP Task Force on Hurricane Science and Engineering
CPP Task Force on Sustainable Energy
CPP Task Force on Transformative Research
CPP Task Force on Unsolicited Mid-Scale Research (MS)
CSB Subcommittee on Facilities (SCF)
CSB Task Force on Cost Sharing
Education and Human Resources Committee (EHR)
Education and Human Resources (CEH)
EHR Subcommittee on Science and Engineering Indicators
Honorary Awards Committee (AWD)
Programs and Plans (CPP)
Science and Engineering Indicators (SEI)
Strategy and Budget (CSB)
Task Force on Administrative Burdens (AB)
Task Force for the NSF 60th Anniversary (60ANN)
Task Force on the Environment
Task Force on International Issues in Science and Engineering
Task Force on Merit Review (MR)
ad hoc Task Force on NEON Performance and Plans
Workshops on Engineering Education

Science & Engineering Indicators (SEI)
One of the ways in which the National Science Board contributes to the U.S. science and engineering enterprise is with its biennial Science & Engineering Indicators report. Mandated by Congress, this report is produced in collaboration with NSF's statistical center and provides comprehensive federal data on a wide range of measurements that show us how the U.S. is doing. These reports include information about K-12, international comparisons of investments in R&D, workforce trends and public attitudes and understanding about science. Indicators serves as a resource for a wide range of users that can include policymakers at all levels, educators, analysts, reporters, the broad scientific community, and the general public. NSB highlights particular themes it sees emerging from Indicators—such as the increasing global role that China and other Asian nations play in the S&T landscape—and talks with a wide range of stakeholders about these.

The most comprehensive source of high quality, policy neutral data on the U.S. and international S&E enterprise
NSB is required by law to provide to Congress & the President by Jan. 15 of even numbered years
Under NSB guidance, Indicators is prepared by NSF's National Center for Science and Engineering Statistics (NCSES)
Covers global R&D investments and knowledge-intensive production, K-12 and postsecondary STEM education, workforce trends and composition, state level comparisons, and public attitudes and understanding of science and related issuesSEI includes seven chapters that follow a generally consistent pattern; an eighth chapter, on state indicators, presented in a unique format; and an overview that precedes these eight chapters. The chapter titles are:

Elementary and Secondary Education
Higher Education in Science and Engineering
Science and Engineering Labor Force
Research and Development: National Trends and International Linkages
Academic Research and Development
Industry, Technology, and the Global Marketplace
Science and Technology: Public Attitudes and Understanding
State IndicatorsAn appendix volume, available online contains detailed data tables keyed to each of the eight chapters listed.
In 2006, the Board produced a pilot ""digest"" or condensed version of SEI comprising a small selection of important indicators. The Digest serves two purposes: (1) to draw attention to important trends and data points from across the chapters and volumes of SEI and (2) to introduce readers to the data resources available in the main volumes of SEI.
While the 2012 version of ""Science and Engineering Indicators"" notes this survey data and the survey problem,  the NSB continues to minimize this data and does not draw attention to the unfavorable comparison to European and Japanese public understanding of these scientific issues.
A National Science Board policy statement, or ""companion,"" authored by the Board, draws upon the data in SEI and offers recommendations on issues of concern for national science and engineering research or education policy, in keeping with the Board's statutory responsibility to bring attention to such issues.

Recent SEI Companions
2004: An Emerging and Critical Problem of the Science and Engineering Labor Force
2006: America's Pressing Challenge — Building a Stronger Foundation
2008: Research and Development: Essential Foundation for U.S. Competitiveness in a Global Economy
2010: Globalization of Science and Engineering Research
2012a: Research & Development, Innovation, and the Science & Engineering Workforce
2012b: Diminishing Funding and Rising Expectations:  Trends and Challenges for Public Research UniversitiesSTEM educationThe NSB has produced policy guidance in the area of STEM education for several decades.  In 2007/2008 the NSB developed a national action plan for addressing the critical STEM education needs of our Nation while providing specific guidance for the role of NSF in the national STEM education enterprise (STEM Action Plan).
In January 2009, the NSB approved and transmitted a set of six recommendations to the Barack Obama Administration. These recommendations outline a series of steps to improve STEM education and foster innovation to ensure both scientific literacy among the public and ensure global competitiveness in the 21st century.
From the STEM education recommendations:

The National Science Board (Board) recommends a set of actions for the new Administration to implement starting in early 2009 to advance STEM (science, technology, engineering, and mathematics) education for all American students, to nurture innovation, and to ensure the long-term economic prosperity of the Nation. The urgency of this task is underscored by the need to ensure that the United States continues to excel in science and technology in the 21st century. It must develop the ideas that could transform and strengthen the economy, ensure a skilled workforce for American industry, and guarantee that all American students are provided the educational resources and tools needed to participate fully in the science and technology based economy of the 21st century. The solutions we offer here are derived from studies by the Board over the past decade and reflect our continued commitment to a high quality STEM education system for America.

Honors
Each year, the Board honors achievement and public service in science, engineering, and technology through its two honorary awards, the Vannevar Bush Award and the NSB Public Service Award.
Awards are presented during a ceremony held in Washington, DC. Several hundred members of the science and education communities—including White House, congressional, scientific society, higher education, and industry officials gather to celebrate the achievements of those awarded during this event.
The Vannevar Bush Award recognizes life-time contributions to science and public service.
The NSB Public Service Award recognizes those who foster public understanding of science and engineering.
The Board opens nominations for its honorary awards from June to early October.

See also
The White House
National Science Foundation
Office of Science and Technology Policy
House Committee on Science, Space, and Technology
Senate Committee on Commerce, Science, and Transportation

References
External links
Official Website",https://en.wikipedia.org/wiki/National_Science_Board,"['1950 establishments in the United States', 'All articles with a promotional tone', 'Articles with a promotional tone from April 2012', 'Committees', 'National Science Foundation', 'Science policy', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers']",Data Science
139,OCLC (identifier),"OCLC, Inc., doing business as OCLC, is an American nonprofit cooperative organization ""dedicated to the public purposes of furthering access to the world's information and reducing information costs"". It was founded in 1967 as the Ohio College Library Center, then became the Online Computer Library Center as it expanded. In 2017, the name was formally changed to OCLC, Inc. OCLC and its member libraries cooperatively produce and maintain WorldCat, the largest online public access catalog (OPAC) in the world. OCLC is funded mainly by the fees that libraries pay (around $200 million annually in total as of 2016) for the many different services it offers. OCLC also maintains the Dewey Decimal Classification system.

History
OCLC began in 1967, as the Ohio College Library Center, through a collaboration of university presidents, vice presidents, and library directors who wanted to create a cooperative, computerized network for libraries in the state of Ohio. The group first met on July 5, 1967, on the campus of the Ohio State University to sign the articles of incorporation for the nonprofit organization and hired Frederick G. Kilgour, a former Yale University medical school librarian, to design the shared cataloging system. Kilgour wished to merge the latest information storage and retrieval system of the time, the computer, with the oldest, the library. The plan was to merge the catalogs of Ohio libraries electronically through a computer network and database to streamline operations, control costs, and increase efficiency in library management, bringing libraries together to cooperatively keep track of the world's information in order to best serve researchers and scholars. The first library to do online cataloging through OCLC was the Alden Library at Ohio University on August 26, 1971. This was the first online cataloging by any library worldwide.Membership in OCLC is based on use of services and contribution of data. Between 1967 and 1977, OCLC membership was limited to institutions in Ohio, but in 1978, a new governance structure was established that allowed institutions from other states to join. In 2002, the governance structure was again modified to accommodate participation from outside the United States.As OCLC expanded services in the United States outside Ohio, it relied on establishing strategic partnerships with ""networks"", organizations that provided training, support and marketing services. By 2008, there were 15 independent United States regional service providers. OCLC networks played a key role in OCLC governance, with networks electing delegates to serve on the OCLC Members Council. During 2008, OCLC commissioned two studies to look at distribution channels; at the same time, the council approved governance changes that had been recommended by the Board of Trustees severing the tie between the networks and governance. In early 2009, OCLC negotiated new contracts with the former networks and opened a centralized support center.

Services
OCLC provides bibliographic, abstract and full-text information to anyone.
OCLC and its member libraries cooperatively produce and maintain WorldCat—the OCLC Online Union Catalog, the largest online public access catalog (OPAC) in the world. WorldCat has holding records from public and private libraries worldwide.
The Online Computer Library Center acquired the trademark and copyrights associated with the Dewey Decimal Classification System when it bought Forest Press in 1988. A browser for books with their Dewey Decimal Classifications was available until July 2013; it was replaced by the Classify Service.
Until August 2009, when it was sold to Backstage Library Works, OCLC owned a preservation microfilm and digitization operation called the OCLC Preservation Service Center, with its principal office in Bethlehem, Pennsylvania.
Starting in 1971, OCLC produced catalog cards for members alongside its shared online catalog; the company printed its last catalog cards on October 1, 2015.

QuestionPoint
QuestionPoint, an around-the-clock reference service provided to users by a cooperative of participating global libraries, was acquired by Springshare from OCLC in 2019 and migrated to Springshare's LibAnswers platform.

Software
OCLC commercially sells software, such as:

CONTENTdm for managing digital collections
Wise, an integrated library system and ""community engagement system""
WorldCat Discovery, a bibliographic discovery system that allows library patrons to use a single search interface to access an institution's catalog, ebooks, database subscriptions and more
WorldShare Management Services, an electronic resource management system

Research
OCLC has been conducting research for the library community for more than 30 years. In accordance with its mission, OCLC makes its research outcomes known through various publications. These publications, including journal articles, reports, newsletters, and presentations, are available through the organization's website.

OCLC Publications – Research articles from various journals including The Code4Lib Journal, OCLC Research, Reference and User Services Quarterly, College & Research Libraries News, Art Libraries Journal, and National Education Association Newsletter. The most recent publications are displayed first, and all archived resources, starting in 1970, are also available.
Membership Reports – A number of significant reports on topics ranging from virtual reference in libraries to perceptions about library funding.
Newsletters – Current and archived newsletters for the library and archive community.
Presentations – Presentations from both guest speakers and OCLC research from conferences, webcasts, and other events. The presentations are organized into five categories: Conference presentations, Dewey presentations, Distinguished Seminar Series, Guest presentations, and Research staff presentations.

Advocacy
Advocacy has been a part of OCLC's mission since its founding in 1967. OCLC staff members meet and work regularly with library leaders, information professionals, researchers, entrepreneurs, political leaders, trustees, students and patrons to advocate ""advancing research, scholarship, education, community development, information access, and global cooperation"".WebJunction, which provides training services to librarians, is a division of OCLC funded by grants from the Bill & Melinda Gates Foundation beginning in 2003.OCLC partnered with search engine providers in 2003 to advocate for libraries and share information across the Internet landscape. Google, Yahoo!, and Ask.com all collaborated with OCLC to make WorldCat records searchable through those search engines.OCLC's advocacy campaign ""Geek the Library"", started in 2009, highlights the role of public libraries. The campaign, funded by a grant from the Bill & Melinda Gates Foundation, uses a strategy based on the findings of the 2008 OCLC report, ""From Awareness to Funding: A study of library support in America"".Other past advocacy campaigns have focused on sharing the knowledge gained from library and information research. Such projects have included communities such as the Society of American Archivists, the Open Archives Initiative, the Institute for Museum and Library Services, the International Organization for Standardization, the National Information Standards Organization, the World Wide Web Consortium, the Internet Engineering Task Force, and Internet2. One of the most successful contributions to this effort was the Dublin Core Metadata Initiative, ""an open forum of libraries, archives, museums, technology organizations, and software companies who work together to develop interoperable online metadata standards that support a broad range of purposes and business models.""OCLC has collaborated with the Wikimedia Foundation and the Wikimedia volunteer community, through integrating library metadata with Wikimedia projects, hosting a Wikipedian in residence, and doing a national training program through WebJunction called ""Wikipedia + Libraries: Better Together"".

Online database: WorldCat
OCLC's WorldCat database is used by the general public and by librarians for cataloging and research. WorldCat is available to the public for searching via a subscription web-based service called FirstSearch, as well as through the publicly available WorldCat.org.

Identifiers and linked data
OCLC assigns a unique control number (referred to as an ""OCN"" for ""OCLC Control Number"") to each new bibliographic record in the WorldCat. Numbers are assigned serially, and as of mid-2013 over a billion OCNs had been created. In September 2013, the OCLC declared these numbers to be in the public domain, removing a perceived barrier to widespread use of OCNs outside OCLC itself.  The control numbers link WorldCat's records to local library system records by providing a common reference key for a record across libraries.OCNs are particularly useful as identifiers for books and other bibliographic materials that do not have ISBNs (e.g., books published before 1970). OCNs are used as identifiers often in Wikipedia and Wikidata. In October 2013, it was reported that out of 29,673 instances of book infoboxes in Wikipedia, ""there were 23,304 ISBNs and 15,226 OCNs"", and regarding Wikidata: ""of around 14 million Wikidata items, 28,741 were books. 5403 Wikidata items have an ISBN associated with them, and 12,262 have OCNs.""OCLC also runs the Virtual International Authority File (VIAF), an international name authority file, with oversight from the VIAF Council composed of representatives of institutions that contribute data to VIAF. VIAF numbers are broadly used as standard identifiers, including in Wikipedia.

Company acquisitions
OCLC acquired NetLibrary, a provider of electronic books and textbooks, in 2002 and sold it in 2010 to EBSCO Industries. OCLC owns 100% of the shares of OCLC PICA, a library automation systems and services company which has its headquarters in Leiden in the Netherlands and which was renamed ""OCLC"" at the end of 2007. In July 2006, the Research Libraries Group (RLG) merged with OCLC.On January 11, 2008, OCLC announced that it had purchased EZproxy. It has also acquired OAIster. The process started in January 2009 and from October 31, 2009, OAIster records are freely available via WorldCat.org.
In 2013 OCLC acquired the Dutch library automation company HKA and its integrated library system Wise, which OCLC calls a ""community engagement system"" that ""combines the power of customer relationship management, marketing, and analytics with ILS functions"". OCLC began offering Wise to libraries in the United States in 2019.In January 2015, OCLC acquired Sustainable Collection Services (SCS). SCS offered consulting services based on analyzing library print collection data to help libraries manage and share materials. In 2017, OCLC acquired Relais International, a library interlibrary loan service provider based in Ottawa, Canada.

Criticism
In May 2008, OCLC was criticized by Jeffrey Beall for monopolistic practices, among other faults. Library blogger Rick Mason responded that although he thought Beall had some ""valid criticisms"" of OCLC, he demurred from some of Beall's statements and warned readers to ""beware the hyperbole and the personal nature of his criticism, for they strongly overshadow that which is worth stating"".In November 2008, the Board of Directors of OCLC unilaterally issued a new Policy for Use and Transfer of WorldCat Records that would have required member libraries to include an OCLC policy note on their bibliographic records; the policy caused an uproar among librarian bloggers. Among those who protested the policy was the non-librarian activist Aaron Swartz, who believed the policy would threaten projects such as the Open Library, Zotero, and Wikipedia, and who started a petition to ""Stop the OCLC powergrab"". Swartz's petition garnered 858 signatures, but the details of his proposed actions went largely unheeded. Within a few months, the library community had forced OCLC to retract its policy and to create a Review Board to consult with member libraries more transparently. In August 2012, OCLC recommended that member libraries adopt the Open Data Commons Attribution (ODC-BY) license when sharing library catalog data, although some member libraries have explicit agreements with OCLC that they can publish catalog data using the CC0 Public Domain Dedication.In July 2010, the company was sued by SkyRiver, a rival startup, in an antitrust suit. Library automation company Innovative Interfaces joined SkyRiver in the suit. The suit was dropped in March 2013, however, following the acquisition of SkyRiver by Innovative Interfaces.
Innovative Interfaces was later bought by ExLibris, therefore passing OCLC as the dominant supplier of ILS services in the USA (over 70% market share for academic libraries and over 50% for public libraries for ExLibris, versus OCLC's 10% market share of both types of libraries in 2019).

See also
Dynix (software)
Public library advocacy

References
Further reading
External links
Official website 
""Hanging Together – the OCLC Research blog"".
""OCLC Annual Reports collection"". OCLC Archives.
""WorldCat.org"".
Works by or about OCLC in libraries (WorldCat catalog)",https://en.wikipedia.org/wiki/OCLC,"['1967 establishments in Ohio', 'All Wikipedia articles written in American English', 'All articles containing potentially dated statements', 'Articles containing potentially dated statements from 2016', 'Articles with short description', 'Bibliographic database providers', 'Commons category link is on Wikidata', 'Companies based in Dublin, Ohio', 'Cooperatives in the United States', 'Library-related organizations', 'Library automation', 'Library cataloging and classification', 'Library centers', 'OCLC', 'Organizations established in 1967', 'Short description matches Wikidata', 'Use American English from May 2017', 'Use mdy dates from September 2017', 'Wikipedia articles with BIBSYS identifiers', 'Wikipedia articles with BNE identifiers', 'Wikipedia articles with CINII identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with LNB identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NKC identifiers', 'Wikipedia articles with NLA identifiers', 'Wikipedia articles with NLI identifiers', 'Wikipedia articles with NLP identifiers', 'Wikipedia articles with NSK identifiers', 'Wikipedia articles with SELIBR identifiers', 'Wikipedia articles with SNAC-ID identifiers', 'Wikipedia articles with Trove identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with VcBA identifiers', 'Wikipedia articles with WORLDCATID identifiers', 'Wikipedia articles with multiple identifiers']",Data Science
140,Nathan Yau,"Nathan Yau is an American statistician and data visualization expert.

Early life
Nathan Chun-Yin Yau grew up in Fresno, California.He received a Bachelor of Science in electrical engineering and computer science from the University of California, Berkeley. He graduated in 2007 with a Master of Science and in 2013 with a PhD in statistics from the University of California, Los Angeles.His dissertation was titled ""An Online Tool for Personal Data Collection and Exploration"" and focused on self-surveillance techniques. Yau's earlier self-surveillance work on the ""Personal Environmental Impact Report"" was featured in Yau's chapter of the book Beautiful Data, published in 2009.

Career
Yau is known for his blog FlowingData in which he publishes writing and tutorials on information design and analytics, as well as visualizations and data science-related projects created by other professionals.He is the author of books ""Visualize This: The FlowingData Guide to Design, Visualization, and Statistics"" (2011) and ""Data Points: Visualization That Means Something"" (2013).Since 2014, Yau has worked at the U.S. Census as a research mathematical statistican.

Selected publications
Yau, Nathan (2011). Visualize This (First ed.). John Wiley and Sons. ISBN 9780470944882. OCLC 729943780.
Yau, Nathan (2013). Data points : visualization that means something (First ed.). John Wiley and Sons. ISBN 9781118462195. OCLC 871319880.


== References ==",https://en.wikipedia.org/wiki/Nathan_Yau,"['AC with 0 elements', 'Articles with hCards', 'Data scientists', 'Digital artists', 'Information graphic designers', 'Living people', 'People from Fresno, California', 'University of California, Berkeley alumni', 'University of California, Los Angeles alumni', 'Use mdy dates from April 2020', 'Year of birth missing (living people)']",Data Science
141,OPTICS algorithm,"Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander.
Its basic idea is similar to DBSCAN, but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. This is represented as a dendrogram.

Basic idea
Like DBSCAN, OPTICS requires two parameters: ε, which describes the maximum distance (radius) to consider, and MinPts, describing the number of points required to form a cluster. A point p is a core point if at least MinPts points are found within its ε-neighborhood 
  
    
      
        
          N
          
            ε
          
        
        (
        p
        )
      
    
    {\displaystyle N_{\varepsilon }(p)}
   (including point p itself). In contrast to DBSCAN, OPTICS also considers points that are part of a more densely packed cluster, so each point is assigned a core distance that describes the distance to the MinPtsth closest point:

  
    
      
        
          
            core-dist
          
          
            
              ε
              ,
              M
              i
              n
              P
              t
              s
            
          
        
        (
        p
        )
        =
        
          
            {
            
              
                
                  
                    UNDEFINED
                  
                
                
                  
                    if 
                  
                  
                    |
                  
                  
                    N
                    
                      ε
                    
                  
                  (
                  p
                  )
                  
                    |
                  
                  <
                  
                    
                      M
                      i
                      n
                      P
                      t
                      s
                    
                  
                
              
              
                
                  
                    
                      M
                      i
                      n
                      P
                      t
                      s
                    
                  
                  
                    -th smallest distance in 
                  
                  
                    N
                    
                      ε
                    
                  
                  (
                  p
                  )
                
                
                  
                    otherwise
                  
                
              
            
            
          
        
      
    
    {\displaystyle {\text{core-dist}}_{\mathit {\varepsilon ,MinPts}}(p)={\begin{cases}{\text{UNDEFINED}}&{\text{if }}|N_{\varepsilon }(p)|<{\mathit {MinPts}}\\{\mathit {MinPts}}{\text{-th smallest distance in }}N_{\varepsilon }(p)&{\text{otherwise}}\end{cases}}}
  The reachability-distance of another point o from a point p is either the distance between o and p, or the core distance of p, whichever is bigger:

  
    
      
        
          
            reachability-dist
          
          
            
              ε
              ,
              M
              i
              n
              P
              t
              s
            
          
        
        (
        o
        ,
        p
        )
        =
        
          
            {
            
              
                
                  
                    UNDEFINED
                  
                
                
                  
                    if 
                  
                  
                    |
                  
                  
                    N
                    
                      ε
                    
                  
                  (
                  p
                  )
                  
                    |
                  
                  <
                  
                    
                      M
                      i
                      n
                      P
                      t
                      s
                    
                  
                
              
              
                
                  max
                  (
                  
                    
                      core-dist
                    
                    
                      
                        ε
                        ,
                        M
                        i
                        n
                        P
                        t
                        s
                      
                    
                  
                  (
                  p
                  )
                  ,
                  
                    dist
                  
                  (
                  p
                  ,
                  o
                  )
                  )
                
                
                  
                    otherwise
                  
                
              
            
            
          
        
      
    
    {\displaystyle {\text{reachability-dist}}_{\mathit {\varepsilon ,MinPts}}(o,p)={\begin{cases}{\text{UNDEFINED}}&{\text{if }}|N_{\varepsilon }(p)|<{\mathit {MinPts}}\\\max({\text{core-dist}}_{\mathit {\varepsilon ,MinPts}}(p),{\text{dist}}(p,o))&{\text{otherwise}}\end{cases}}}
  If p and o are nearest neighbors, this is the 
  
    
      
        
          ε
          ′
        
        <
        ε
      
    
    {\displaystyle \varepsilon '<\varepsilon }
   we need to assume to have p and o belong to the same cluster.
Both core-distance and reachability-distance are undefined if no sufficiently dense cluster (w.r.t. ε) is available. Given a sufficiently large ε, this never happens, but then every ε-neighborhood query returns the entire database, resulting in 
  
    
      
        O
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle O(n^{2})}
   runtime. Hence, the ε parameter is required to cut off the density of clusters that are no longer interesting, and to speed up the algorithm.
The parameter ε is, strictly speaking, not necessary. It can simply be set to the maximum possible value. When a spatial index is available, however, it does play a practical role with regards to complexity. OPTICS abstracts from DBSCAN by removing this parameter, at least to the extent of only having to give the maximum value.

Pseudocode
The basic approach of OPTICS is similar to DBSCAN, but instead of maintaining known, but so far unprocessed cluster members in a set, they are maintained in a priority queue (e.g. using an indexed heap).

function OPTICS(DB, eps, MinPts) is
    for each point p of DB do
        p.reachability-distance = UNDEFINED
    for each unprocessed point p of DB do
        N = getNeighbors(p, eps)
        mark p as processed
        output p to the ordered list
        if core-distance(p, eps, MinPts) != UNDEFINED then
            Seeds = empty priority queue
            update(N, p, Seeds, eps, MinPts)
            for each next q in Seeds do
                N' = getNeighbors(q, eps)
                mark q as processed
                output q to the ordered list
                if core-distance(q, eps, MinPts) != UNDEFINED do
                    update(N', q, Seeds, eps, MinPts)

In update(), the priority queue Seeds is updated with the 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  -neighborhood of 
  
    
      
        p
      
    
    {\displaystyle p}
   and 
  
    
      
        q
      
    
    {\displaystyle q}
  , respectively:

function update(N, p, Seeds, eps, MinPts) is
    coredist = core-distance(p, eps, MinPts)
    for each o in N
        if o is not processed then
            new-reach-dist = max(coredist, dist(p,o))
            if o.reachability-distance == UNDEFINED then // o is not in Seeds
                o.reachability-distance = new-reach-dist
                Seeds.insert(o, new-reach-dist)
            else               // o in Seeds, check for improvement
                if new-reach-dist < o.reachability-distance then
                    o.reachability-distance = new-reach-dist
                    Seeds.move-up(o, new-reach-dist)

OPTICS hence outputs the points in a particular ordering, annotated with their smallest reachability distance (in the original algorithm, the core distance is also exported, but this is not required for further processing).

Extracting the clusters
Using a reachability-plot (a special kind of dendrogram), the hierarchical structure of the clusters can be obtained easily. It is a 2D plot, with the ordering of the points as processed by OPTICS on the x-axis and the reachability distance on the y-axis. Since points belonging to a cluster have a low reachability distance to their nearest neighbor, the clusters show up as valleys in the reachability plot. The deeper the valley, the denser the cluster.
The image above illustrates this concept. In its upper left area, a synthetic example data set is shown. The upper right part visualizes the spanning tree produced by OPTICS, and the lower part shows the reachability plot as computed by OPTICS. Colors in this plot are labels, and not computed by the algorithm; but it is well visible how the valleys in the plot correspond to the clusters in above data set. The yellow points in this image are considered noise, and no valley is found in their reachability plot. They are usually not assigned to clusters, except the omnipresent ""all data"" cluster in a hierarchical result.
Extracting clusters from this plot can be done manually by selecting a range on the x-axis after visual inspection, by selecting a threshold on the y-axis (the result is then similar to a DBSCAN clustering result with the same 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   and minPts parameters; here a value of 0.1 may yield good results), or by different algorithms that try to detect the valleys by steepness, knee detection, or local maxima. Clusterings obtained this way usually are hierarchical, and cannot be achieved by a single DBSCAN run.

Complexity
Like DBSCAN, OPTICS processes each point once, and performs one 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  -neighborhood query during this processing. Given a spatial index that grants a neighborhood query in 
  
    
      
        O
        (
        log
        ⁡
        n
        )
      
    
    {\displaystyle O(\log n)}
   runtime, an overall runtime of 
  
    
      
        O
        (
        n
        ⋅
        log
        ⁡
        n
        )
      
    
    {\displaystyle O(n\cdot \log n)}
   is obtained. The authors of the original OPTICS paper report an actual constant slowdown factor of 1.6 compared to DBSCAN. Note that the value of 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   might heavily influence the cost of the algorithm, since a value too large might raise the cost of a neighborhood query to linear complexity.
In particular, choosing 
  
    
      
        ε
        >
        
          max
          
            x
            ,
            y
          
        
        d
        (
        x
        ,
        y
        )
      
    
    {\displaystyle \varepsilon >\max _{x,y}d(x,y)}
   (larger than the maximum distance in the data set) is possible, but leads to quadratic complexity, since every neighborhood query returns the full data set. Even when no spatial index is available, this comes at additional cost in managing the heap. Therefore, 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   should be chosen appropriately for the data set.

Extensions
OPTICS-OF is an outlier detection algorithm based on OPTICS. The main use is the extraction of outliers from an existing run of OPTICS at low cost compared to using a different outlier detection method. The better known version LOF is based on the same concepts.
DeLi-Clu, Density-Link-Clustering combines ideas from single-linkage clustering and OPTICS, eliminating the 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   parameter and offering performance improvements over OPTICS.
HiSC is a hierarchical subspace clustering (axis-parallel) method based on OPTICS.
HiCO is a hierarchical correlation clustering algorithm based on OPTICS.
DiSH is an improvement over HiSC that can find more complex hierarchies.
FOPTICS is a faster implementation using random projections.
HDBSCAN* is based on a refinement of DBSCAN, excluding border-points from the clusters and thus following more strictly the basic definition of density-levels by Hartigan.

Availability
Java implementations of OPTICS, OPTICS-OF, DeLi-Clu, HiSC, HiCO and DiSH are available in the ELKI data mining framework (with index acceleration for several distance functions, and with automatic cluster extraction using the ξ extraction method). Other Java implementations include the Weka extension (no support for ξ cluster extraction).
The R package ""dbscan"" includes a C++ implementation of OPTICS (with both traditional dbscan-like and ξ cluster extraction) using a k-d tree for index acceleration for Euclidean distance only.
Python implementations of OPTICS are available in the PyClustering library and in scikit-learn. HDBSCAN* is available in the hdbscan library.


== References ==",https://en.wikipedia.org/wiki/OPTICS_algorithm,"['Articles with short description', 'Cluster analysis algorithms', 'Short description matches Wikidata']",Data Science
142,Occam learning,"In computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set.
Occam learnability implies PAC learning, and for a wide variety of concept classes, the converse is also true: PAC learnability implies Occam learnability.

Introduction
Occam Learning is named after Occam's razor, which is a principle stating that, given all other things being equal, a shorter explanation for observed data should be favored over a lengthier explanation. The theory of Occam learning is a formal and mathematical justification for this principle. It was first shown by Blumer, et al. that Occam learning implies PAC learning, which is the standard model of learning in computational learning theory. In other words, parsimony (of the output hypothesis) implies predictive power.

Definition of Occam learning
The succinctness of a concept 
  
    
      
        c
      
    
    {\displaystyle c}
   in concept class 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   can be expressed by the length 
  
    
      
        s
        i
        z
        e
        (
        c
        )
      
    
    {\displaystyle size(c)}
   of the shortest bit string that can represent 
  
    
      
        c
      
    
    {\displaystyle c}
   in 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
  . Occam learning connects the succinctness of a learning algorithm's output to its predictive power on unseen data.
Let 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   and 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
   be concept classes containing target concepts and hypotheses respectively. Then, for constants 
  
    
      
        α
        ≥
        0
      
    
    {\displaystyle \alpha \geq 0}
   and 
  
    
      
        0
        ≤
        β
        <
        1
      
    
    {\displaystyle 0\leq \beta <1}
  , a learning algorithm 
  
    
      
        L
      
    
    {\displaystyle L}
   is an 
  
    
      
        (
        α
        ,
        β
        )
      
    
    {\displaystyle (\alpha ,\beta )}
  -Occam algorithm for 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   using 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
   iff, given a set 
  
    
      
        S
        =
        {
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            m
          
        
        }
      
    
    {\displaystyle S=\{x_{1},\dots ,x_{m}\}}
   of 
  
    
      
        m
      
    
    {\displaystyle m}
   samples labeled according to a concept 
  
    
      
        c
        ∈
        
          
            C
          
        
      
    
    {\displaystyle c\in {\mathcal {C}}}
  , 
  
    
      
        L
      
    
    {\displaystyle L}
   outputs a hypothesis 
  
    
      
        h
        ∈
        
          
            H
          
        
      
    
    {\displaystyle h\in {\mathcal {H}}}
   such that

  
    
      
        h
      
    
    {\displaystyle h}
   is consistent with 
  
    
      
        c
      
    
    {\displaystyle c}
   on 
  
    
      
        S
      
    
    {\displaystyle S}
   (that is, 
  
    
      
        h
        (
        x
        )
        =
        c
        (
        x
        )
        ,
        ∀
        x
        ∈
        S
      
    
    {\displaystyle h(x)=c(x),\forall x\in S}
  ), and

  
    
      
        s
        i
        z
        e
        (
        h
        )
        ≤
        (
        n
        ⋅
        s
        i
        z
        e
        (
        c
        )
        
          )
          
            α
          
        
        
          m
          
            β
          
        
      
    
    {\displaystyle size(h)\leq (n\cdot size(c))^{\alpha }m^{\beta }}
   where 
  
    
      
        n
      
    
    {\displaystyle n}
   is the maximum length of any sample 
  
    
      
        x
        ∈
        S
      
    
    {\displaystyle x\in S}
  . An Occam algorithm is called efficient if it runs in time polynomial in 
  
    
      
        n
      
    
    {\displaystyle n}
  , 
  
    
      
        m
      
    
    {\displaystyle m}
  , and 
  
    
      
        s
        i
        z
        e
        (
        c
        )
        .
      
    
    {\displaystyle size(c).}
   We say a concept class 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   is Occam learnable with respect to a hypothesis class 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
   if there exists an efficient Occam algorithm for  
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   using 
  
    
      
        
          
            H
          
        
        .
      
    
    {\displaystyle {\mathcal {H}}.}

The relation between Occam and PAC learning
Occam learnability implies PAC learnability, as the following theorem of Blumer, et al. shows:

Theorem (Occam learning implies PAC learning)
Let 
  
    
      
        L
      
    
    {\displaystyle L}
   be an efficient  
  
    
      
        (
        α
        ,
        β
        )
      
    
    {\displaystyle (\alpha ,\beta )}
  -Occam algorithm for 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   using 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
  . Then there exists a constant 
  
    
      
        a
        >
        0
      
    
    {\displaystyle a>0}
   such that for any 
  
    
      
        0
        <
        ϵ
        ,
        δ
        <
        1
      
    
    {\displaystyle 0<\epsilon ,\delta <1}
  , for any distribution 
  
    
      
        
          
            D
          
        
      
    
    {\displaystyle {\mathcal {D}}}
  , given 
  
    
      
        m
        ≥
        a
        
          (
          
            
              
                1
                ϵ
              
            
            log
            ⁡
            
              
                1
                δ
              
            
            +
            
              
                (
                
                  
                    
                      (
                      n
                      ⋅
                      s
                      i
                      z
                      e
                      (
                      c
                      )
                      
                        )
                        
                          α
                        
                      
                      )
                    
                    ϵ
                  
                
                )
              
              
                
                  1
                  
                    1
                    −
                    β
                  
                
              
            
          
          )
        
      
    
    {\displaystyle m\geq a\left({\frac {1}{\epsilon }}\log {\frac {1}{\delta }}+\left({\frac {(n\cdot size(c))^{\alpha })}{\epsilon }}\right)^{\frac {1}{1-\beta }}\right)}
    samples drawn from 
  
    
      
        
          
            D
          
        
      
    
    {\displaystyle {\mathcal {D}}}
   and labelled according to a concept 
  
    
      
        c
        ∈
        
          
            C
          
        
      
    
    {\displaystyle c\in {\mathcal {C}}}
   of length 
  
    
      
        n
      
    
    {\displaystyle n}
   bits each, the algorithm  
  
    
      
        L
      
    
    {\displaystyle L}
   will output a hypothesis  
  
    
      
        h
        ∈
        
          
            H
          
        
      
    
    {\displaystyle h\in {\mathcal {H}}}
   such that 
  
    
      
        e
        r
        r
        o
        r
        (
        h
        )
        ≤
        ϵ
      
    
    {\displaystyle error(h)\leq \epsilon }
   with probability at least 
  
    
      
        1
        −
        δ
      
    
    {\displaystyle 1-\delta }
   .Here, 
  
    
      
        e
        r
        r
        o
        r
        (
        h
        )
      
    
    {\displaystyle error(h)}
   is with respect to the concept 
  
    
      
        c
      
    
    {\displaystyle c}
   and distribution 
  
    
      
        
          
            D
          
        
      
    
    {\displaystyle {\mathcal {D}}}
  . This implies that the algorithm 
  
    
      
        L
      
    
    {\displaystyle L}
   is also a PAC learner for the concept class 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   using hypothesis class 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
  .  A slightly more general formulation is as follows:

Theorem (Occam learning implies PAC learning, cardinality version)
Let 
  
    
      
        0
        <
        ϵ
        ,
        δ
        <
        1
      
    
    {\displaystyle 0<\epsilon ,\delta <1}
  . Let 
  
    
      
        L
      
    
    {\displaystyle L}
   be an algorithm such that, given 
  
    
      
        m
      
    
    {\displaystyle m}
   samples drawn from a fixed but unknown distribution 
  
    
      
        
          
            D
          
        
      
    
    {\displaystyle {\mathcal {D}}}
   and labeled according to a concept 
  
    
      
        c
        ∈
        
          
            C
          
        
      
    
    {\displaystyle c\in {\mathcal {C}}}
   of length 
  
    
      
        n
      
    
    {\displaystyle n}
   bits each, outputs a hypothesis 
  
    
      
        h
        ∈
        
          
            
              H
            
          
          
            n
            ,
            m
          
        
      
    
    {\displaystyle h\in {\mathcal {H}}_{n,m}}
   that is consistent with the labeled samples. Then, there exists a constant 
  
    
      
        b
      
    
    {\displaystyle b}
   such that if 
  
    
      
        log
        ⁡
        
          |
        
        
          
            
              H
            
          
          
            n
            ,
            m
          
        
        
          |
        
        ≤
        b
        ϵ
        m
        −
        log
        ⁡
        
          
            1
            δ
          
        
      
    
    {\displaystyle \log |{\mathcal {H}}_{n,m}|\leq b\epsilon m-\log {\frac {1}{\delta }}}
  , then 
  
    
      
        L
      
    
    {\displaystyle L}
    is guaranteed to output a hypothesis 
  
    
      
        h
        ∈
        
          
            
              H
            
          
          
            n
            ,
            m
          
        
      
    
    {\displaystyle h\in {\mathcal {H}}_{n,m}}
   such that 
  
    
      
        e
        r
        r
        o
        r
        (
        h
        )
        ≤
        ϵ
      
    
    {\displaystyle error(h)\leq \epsilon }
   with probability at least 
  
    
      
        1
        −
        δ
      
    
    {\displaystyle 1-\delta }
  .
While the above theorems show that Occam learning is sufficient for PAC learning, it doesn't say anything about necessity. Board and Pitt show that, for a wide variety of concept classes, Occam learning is in fact necessary for PAC learning. They proved that for any concept class that is polynomially closed under exception lists, PAC learnability implies the existence of an Occam algorithm for that concept class. Concept classes that are polynomially closed under exception lists include Boolean formulas, circuits, deterministic finite automata, decision-lists, decision-trees, and other geometrically-defined concept classes.
A concept class 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   is polynomially closed under exception lists if there exists a polynomial-time algorithm 
  
    
      
        A
      
    
    {\displaystyle A}
   such that, when given the representation of a concept 
  
    
      
        c
        ∈
        
          
            C
          
        
      
    
    {\displaystyle c\in {\mathcal {C}}}
   and a finite list 
  
    
      
        E
      
    
    {\displaystyle E}
   of exceptions, outputs a representation of a concept 
  
    
      
        
          c
          ′
        
        ∈
        
          
            C
          
        
      
    
    {\displaystyle c'\in {\mathcal {C}}}
   such that the concepts 
  
    
      
        c
      
    
    {\displaystyle c}
   and 
  
    
      
        
          c
          ′
        
      
    
    {\displaystyle c'}
   agree except on the set 
  
    
      
        E
      
    
    {\displaystyle E}
  .

Proof that Occam learning implies PAC learning
We first prove the Cardinality version. Call a hypothesis 
  
    
      
        h
        ∈
        
          
            H
          
        
      
    
    {\displaystyle h\in {\mathcal {H}}}
   bad if 
  
    
      
        e
        r
        r
        o
        r
        (
        h
        )
        ≥
        ϵ
      
    
    {\displaystyle error(h)\geq \epsilon }
  , where again 
  
    
      
        e
        r
        r
        o
        r
        (
        h
        )
      
    
    {\displaystyle error(h)}
   is with respect to the true concept 
  
    
      
        c
      
    
    {\displaystyle c}
   and the underlying distribution 
  
    
      
        
          
            D
          
        
      
    
    {\displaystyle {\mathcal {D}}}
  . The probability that a set of samples 
  
    
      
        S
      
    
    {\displaystyle S}
   is consistent with 
  
    
      
        h
      
    
    {\displaystyle h}
   is at most 
  
    
      
        (
        1
        −
        ϵ
        
          )
          
            m
          
        
      
    
    {\displaystyle (1-\epsilon )^{m}}
  , by the independence of the samples. By the union bound, the probability that there exists a bad hypothesis in 
  
    
      
        
          
            
              H
            
          
          
            n
            ,
            m
          
        
      
    
    {\displaystyle {\mathcal {H}}_{n,m}}
   is at most 
  
    
      
        
          |
        
        
          
            
              H
            
          
          
            n
            ,
            m
          
        
        
          |
        
        (
        1
        −
        ϵ
        
          )
          
            m
          
        
      
    
    {\displaystyle |{\mathcal {H}}_{n,m}|(1-\epsilon )^{m}}
  , which is less than 
  
    
      
        δ
      
    
    {\displaystyle \delta }
   if 
  
    
      
        log
        ⁡
        
          |
        
        
          
            
              H
            
          
          
            n
            ,
            m
          
        
        
          |
        
        ≤
        O
        (
        ϵ
        m
        )
        −
        log
        ⁡
        
          
            1
            δ
          
        
      
    
    {\displaystyle \log |{\mathcal {H}}_{n,m}|\leq O(\epsilon m)-\log {\frac {1}{\delta }}}
  . This concludes the proof of the second theorem above.
Using the second theorem, we can prove the first theorem. Since we have a 
  
    
      
        (
        α
        ,
        β
        )
      
    
    {\displaystyle (\alpha ,\beta )}
  -Occam algorithm, this means that any hypothesis output by 
  
    
      
        L
      
    
    {\displaystyle L}
   can be represented by at most 
  
    
      
        (
        n
        ⋅
        s
        i
        z
        e
        (
        c
        )
        
          )
          
            α
          
        
        
          m
          
            β
          
        
      
    
    {\displaystyle (n\cdot size(c))^{\alpha }m^{\beta }}
   bits, and thus 
  
    
      
        log
        ⁡
        
          |
        
        
          
            
              H
            
          
          
            n
            ,
            m
          
        
        
          |
        
        ≤
        (
        n
        ⋅
        s
        i
        z
        e
        (
        c
        )
        
          )
          
            α
          
        
        
          m
          
            β
          
        
      
    
    {\displaystyle \log |{\mathcal {H}}_{n,m}|\leq (n\cdot size(c))^{\alpha }m^{\beta }}
  . This is less than 
  
    
      
        O
        (
        ϵ
        m
        )
        −
        log
        ⁡
        
          
            1
            δ
          
        
      
    
    {\displaystyle O(\epsilon m)-\log {\frac {1}{\delta }}}
   if we set 
  
    
      
        m
        ≥
        a
        
          (
          
            
              
                1
                ϵ
              
            
            log
            ⁡
            
              
                1
                δ
              
            
            +
            
              
                (
                
                  
                    
                      (
                      n
                      ⋅
                      s
                      i
                      z
                      e
                      (
                      c
                      )
                      
                        )
                        
                          α
                        
                      
                      )
                    
                    ϵ
                  
                
                )
              
              
                
                  1
                  
                    1
                    −
                    β
                  
                
              
            
          
          )
        
      
    
    {\displaystyle m\geq a\left({\frac {1}{\epsilon }}\log {\frac {1}{\delta }}+\left({\frac {(n\cdot size(c))^{\alpha })}{\epsilon }}\right)^{\frac {1}{1-\beta }}\right)}
   for some constant 
  
    
      
        a
        >
        0
      
    
    {\displaystyle a>0}
  . Thus, by the Cardinality version Theorem, 
  
    
      
        L
      
    
    {\displaystyle L}
   will output a consistent hypothesis 
  
    
      
        h
      
    
    {\displaystyle h}
   with probability at least 
  
    
      
        1
        −
        δ
      
    
    {\displaystyle 1-\delta }
  . This concludes the proof of the first theorem above.

Improving sample complexity for common problems
Though Occam and PAC learnability are equivalent, the Occam framework can be used to produce tighter bounds on the sample complexity of classical problems including conjunctions, conjunctions with few relevant variables, and decision lists.

Extensions
Occam algorithms have also been shown to be successful for PAC learning in the presence of errors, probabilistic concepts, function learning and Markovian non-independent examples.

See also
Structural Risk Minimization
Computational learning theory


== References ==",https://en.wikipedia.org/wiki/Occam_learning,"['Computational learning theory', 'Machine learning', 'Theoretical computer science', 'Webarchive template wayback links']",Data Science
143,Online machine learning,"In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction.
Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.

Introduction
In the setting of supervised learning, a function of 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
   is to be learned, where 
  
    
      
        X
      
    
    {\displaystyle X}
   is thought of as a space of inputs and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   as a space of outputs, that predicts well on instances that are drawn from a joint probability distribution 
  
    
      
        p
        (
        x
        ,
        y
        )
      
    
    {\displaystyle p(x,y)}
   on 
  
    
      
        X
        ×
        Y
      
    
    {\displaystyle X\times Y}
  . In reality, the learner never knows the true distribution 
  
    
      
        p
        (
        x
        ,
        y
        )
      
    
    {\displaystyle p(x,y)}
   over instances. Instead, the learner usually has access to a training set of examples 
  
    
      
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        ,
        …
        ,
        (
        
          x
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
      
    
    {\displaystyle (x_{1},y_{1}),\ldots ,(x_{n},y_{n})}
  . In this setting, the loss function is given as 
  
    
      
        V
        :
        Y
        ×
        Y
        →
        
          R
        
      
    
    {\displaystyle V:Y\times Y\to \mathbb {R} }
  , such that 
  
    
      
        V
        (
        f
        (
        x
        )
        ,
        y
        )
      
    
    {\displaystyle V(f(x),y)}
   measures the difference between the predicted value 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   and the true value 
  
    
      
        y
      
    
    {\displaystyle y}
  . The ideal goal is to select a function 
  
    
      
        f
        ∈
        
          
            H
          
        
      
    
    {\displaystyle f\in {\mathcal {H}}}
  , where 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
   is a space of functions called a hypothesis space, so that some notion of total loss is minimised. Depending on the type of model (statistical or adversarial), one can devise different notions of loss, which lead to different learning algorithms.

Statistical view of online learning
In statistical learning models, the training sample 
  
    
      
        (
        
          x
          
            i
          
        
        ,
        
          y
          
            i
          
        
        )
      
    
    {\displaystyle (x_{i},y_{i})}
   are assumed to have been drawn from the true distribution 
  
    
      
        p
        (
        x
        ,
        y
        )
      
    
    {\displaystyle p(x,y)}
   and the objective is to minimize the expected ""risk""

  
    
      
        I
        [
        f
        ]
        =
        
          E
        
        [
        V
        (
        f
        (
        x
        )
        ,
        y
        )
        ]
        =
        ∫
        V
        (
        f
        (
        x
        )
        ,
        y
        )
        
        d
        p
        (
        x
        ,
        y
        )
         
        .
      
    
    {\displaystyle I[f]=\mathbb {E} [V(f(x),y)]=\int V(f(x),y)\,dp(x,y)\ .}
  A common paradigm in this situation is to estimate a function 
  
    
      
        
          
            
              f
              ^
            
          
        
      
    
    {\displaystyle {\hat {f}}}
   through empirical risk minimization or regularized empirical risk minimization (usually Tikhonov regularization). The choice of loss function here gives rise to several well-known learning algorithms such as regularized least squares and support vector machines.
A purely online model in this category would learn based on just the new input 
  
    
      
        (
        
          x
          
            t
            +
            1
          
        
        ,
        
          y
          
            t
            +
            1
          
        
        )
      
    
    {\displaystyle (x_{t+1},y_{t+1})}
  , the current best predictor 
  
    
      
        
          f
          
            t
          
        
      
    
    {\displaystyle f_{t}}
   and some extra stored information (which is usually expected to have storage requirements independent of training data size). For many formulations, for example nonlinear kernel methods, true online learning is not possible, though a form of hybrid online learning with recursive algorithms can be used where 
  
    
      
        
          f
          
            t
            +
            1
          
        
      
    
    {\displaystyle f_{t+1}}
   is permitted to depend on 
  
    
      
        
          f
          
            t
          
        
      
    
    {\displaystyle f_{t}}
   and all previous data points 
  
    
      
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        ,
        …
        ,
        (
        
          x
          
            t
          
        
        ,
        
          y
          
            t
          
        
        )
      
    
    {\displaystyle (x_{1},y_{1}),\ldots ,(x_{t},y_{t})}
  . In this case, the space requirements are no longer guaranteed to be constant since it requires storing all previous data points, but the solution may take less time to compute with the addition of a new data point, as compared to batch learning techniques.
A common strategy to overcome the above issues is to learn using mini-batches, which process a small batch of 
  
    
      
        b
        ≥
        1
      
    
    {\displaystyle b\geq 1}
   data points at a time, this can be considered as pseudo-online learning for 
  
    
      
        b
      
    
    {\displaystyle b}
   much smaller than the total number of training points. Mini-batch techniques are used with repeated passing over the training data to obtain optimized out-of-core versions of machine learning algorithms, for example, stochastic gradient descent. When combined with backpropagation, this is currently the de facto training method for training artificial neural networks.

Example: linear least squares
The simple example of linear least squares is used to explain a variety of ideas in online learning. The ideas are general enough to be applied to other settings, for example, with other convex loss functions.

Batch learning
Consider the setting of supervised learning with 
  
    
      
        f
      
    
    {\displaystyle f}
   being a linear function to be learned:

  
    
      
        f
        (
        
          x
          
            j
          
        
        )
        =
        ⟨
        w
        ,
        
          x
          
            j
          
        
        ⟩
        =
        w
        ⋅
        
          x
          
            j
          
        
      
    
    {\displaystyle f(x_{j})=\langle w,x_{j}\rangle =w\cdot x_{j}}
  where 
  
    
      
        
          x
          
            j
          
        
        ∈
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle x_{j}\in \mathbb {R} ^{d}}
   is a vector of inputs (data points) and 
  
    
      
        w
        ∈
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle w\in \mathbb {R} ^{d}}
   is a linear filter vector.
The goal is to compute the filter vector 
  
    
      
        w
      
    
    {\displaystyle w}
  .
To this end, a square loss function 

  
    
      
        V
        (
        f
        (
        
          x
          
            j
          
        
        )
        ,
        
          y
          
            j
          
        
        )
        =
        (
        f
        (
        
          x
          
            j
          
        
        )
        −
        
          y
          
            j
          
        
        
          )
          
            2
          
        
        =
        (
        ⟨
        w
        ,
        
          x
          
            j
          
        
        ⟩
        −
        
          y
          
            j
          
        
        
          )
          
            2
          
        
      
    
    {\displaystyle V(f(x_{j}),y_{j})=(f(x_{j})-y_{j})^{2}=(\langle w,x_{j}\rangle -y_{j})^{2}}
  is used to compute the vector 
  
    
      
        w
      
    
    {\displaystyle w}
   that minimizes the empirical loss

  
    
      
        
          I
          
            n
          
        
        [
        w
        ]
        =
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        V
        (
        ⟨
        w
        ,
        
          x
          
            j
          
        
        ⟩
        ,
        
          y
          
            j
          
        
        )
        =
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        (
        
          x
          
            j
          
          
            T
          
        
        w
        −
        
          y
          
            j
          
        
        
          )
          
            2
          
        
      
    
    {\displaystyle I_{n}[w]=\sum _{j=1}^{n}V(\langle w,x_{j}\rangle ,y_{j})=\sum _{j=1}^{n}(x_{j}^{T}w-y_{j})^{2}}
  where

  
    
      
        
          y
          
            j
          
        
        ∈
        
          R
        
      
    
    {\displaystyle y_{j}\in \mathbb {R} }
  .Let  
  
    
      
        X
      
    
    {\displaystyle X}
   be the 
  
    
      
        i
        ×
        d
      
    
    {\displaystyle i\times d}
   data matrix and 
  
    
      
        y
        ∈
        
          
            R
          
          
            i
          
        
      
    
    {\displaystyle y\in \mathbb {R} ^{i}}
   is the column vector of target values after the arrival of the first 
  
    
      
        i
      
    
    {\displaystyle i}
   data points.
Assuming that the covariance matrix 
  
    
      
        
          Σ
          
            i
          
        
        =
        
          X
          
            T
          
        
        X
      
    
    {\displaystyle \Sigma _{i}=X^{T}X}
   is invertible (otherwise it is preferential to proceed in a similar fashion with Tikhonov regularization), the best solution 
  
    
      
        
          f
          
            ∗
          
        
        (
        x
        )
        =
        ⟨
        
          w
          
            ∗
          
        
        ,
        x
        ⟩
      
    
    {\displaystyle f^{*}(x)=\langle w^{*},x\rangle }
   to the linear least squares problem is given by

  
    
      
        
          w
          
            ∗
          
        
        =
        (
        
          X
          
            T
          
        
        X
        
          )
          
            −
            1
          
        
        
          X
          
            T
          
        
        y
        =
        
          Σ
          
            i
          
          
            −
            1
          
        
        
          ∑
          
            j
            =
            1
          
          
            i
          
        
        
          x
          
            j
          
        
        
          y
          
            j
          
        
      
    
    {\displaystyle w^{*}=(X^{T}X)^{-1}X^{T}y=\Sigma _{i}^{-1}\sum _{j=1}^{i}x_{j}y_{j}}
  .Now, calculating the covariance matrix 
  
    
      
        
          Σ
          
            i
          
        
        =
        
          ∑
          
            j
            =
            1
          
          
            i
          
        
        
          x
          
            j
          
        
        
          x
          
            j
          
          
            T
          
        
      
    
    {\displaystyle \Sigma _{i}=\sum _{j=1}^{i}x_{j}x_{j}^{T}}
   takes time 
  
    
      
        O
        (
        i
        
          d
          
            2
          
        
        )
      
    
    {\displaystyle O(id^{2})}
  , inverting the 
  
    
      
        d
        ×
        d
      
    
    {\displaystyle d\times d}
   matrix takes time 
  
    
      
        O
        (
        
          d
          
            3
          
        
        )
      
    
    {\displaystyle O(d^{3})}
  , while the rest of the multiplication takes time 
  
    
      
        O
        (
        
          d
          
            2
          
        
        )
      
    
    {\displaystyle O(d^{2})}
  , giving a total time of 
  
    
      
        O
        (
        i
        
          d
          
            2
          
        
        +
        
          d
          
            3
          
        
        )
      
    
    {\displaystyle O(id^{2}+d^{3})}
  . When there are 
  
    
      
        n
      
    
    {\displaystyle n}
   total points in the dataset, to recompute the solution after the arrival of every datapoint 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=1,\ldots ,n}
  , the naive approach will have a total complexity 
  
    
      
        O
        (
        
          n
          
            2
          
        
        
          d
          
            2
          
        
        +
        n
        
          d
          
            3
          
        
        )
      
    
    {\displaystyle O(n^{2}d^{2}+nd^{3})}
  . Note that when storing the matrix 
  
    
      
        
          Σ
          
            i
          
        
      
    
    {\displaystyle \Sigma _{i}}
  , then updating it at each step needs only adding 
  
    
      
        
          x
          
            i
            +
            1
          
        
        
          x
          
            i
            +
            1
          
          
            T
          
        
      
    
    {\displaystyle x_{i+1}x_{i+1}^{T}}
  , which takes 
  
    
      
        O
        (
        
          d
          
            2
          
        
        )
      
    
    {\displaystyle O(d^{2})}
   time, reducing the total time to 
  
    
      
        O
        (
        n
        
          d
          
            2
          
        
        +
        n
        
          d
          
            3
          
        
        )
        =
        O
        (
        n
        
          d
          
            3
          
        
        )
      
    
    {\displaystyle O(nd^{2}+nd^{3})=O(nd^{3})}
  , but with an additional storage space of 
  
    
      
        O
        (
        
          d
          
            2
          
        
        )
      
    
    {\displaystyle O(d^{2})}
   to store 
  
    
      
        
          Σ
          
            i
          
        
      
    
    {\displaystyle \Sigma _{i}}
  .

Online learning: recursive least squares
The recursive least squares (RLS) algorithm considers an online approach to the least squares problem. It can be shown that by initialising 
  
    
      
        
          
            w
            
              0
            
          
          =
          0
          ∈
          
            
              R
            
            
              d
            
          
        
      
    
    {\displaystyle \textstyle w_{0}=0\in \mathbb {R} ^{d}}
   and 
  
    
      
        
          
            Γ
            
              0
            
          
          =
          I
          ∈
          
            
              R
            
            
              d
              ×
              d
            
          
        
      
    
    {\displaystyle \textstyle \Gamma _{0}=I\in \mathbb {R} ^{d\times d}}
  , the solution of the linear least squares problem given in the previous section can be computed by the following iteration:

  
    
      
        
          Γ
          
            i
          
        
        =
        
          Γ
          
            i
            −
            1
          
        
        −
        
          
            
              
                Γ
                
                  i
                  −
                  1
                
              
              
                x
                
                  i
                
              
              
                x
                
                  i
                
                
                  T
                
              
              
                Γ
                
                  i
                  −
                  1
                
              
            
            
              1
              +
              
                x
                
                  i
                
                
                  T
                
              
              
                Γ
                
                  i
                  −
                  1
                
              
              
                x
                
                  i
                
              
            
          
        
      
    
    {\displaystyle \Gamma _{i}=\Gamma _{i-1}-{\frac {\Gamma _{i-1}x_{i}x_{i}^{T}\Gamma _{i-1}}{1+x_{i}^{T}\Gamma _{i-1}x_{i}}}}
  

  
    
      
        
          w
          
            i
          
        
        =
        
          w
          
            i
            −
            1
          
        
        −
        
          Γ
          
            i
          
        
        
          x
          
            i
          
        
        (
        
          x
          
            i
          
          
            T
          
        
        
          w
          
            i
            −
            1
          
        
        −
        
          y
          
            i
          
        
        )
      
    
    {\displaystyle w_{i}=w_{i-1}-\Gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})}
  The above iteration algorithm can be proved using induction on 
  
    
      
        i
      
    
    {\displaystyle i}
  . The proof also shows that 
  
    
      
        
          Γ
          
            i
          
        
        =
        
          Σ
          
            i
          
          
            −
            1
          
        
      
    
    {\displaystyle \Gamma _{i}=\Sigma _{i}^{-1}}
  . 
One can look at RLS also in the context of adaptive filters (see RLS).
The complexity for 
  
    
      
        n
      
    
    {\displaystyle n}
   steps of this algorithm is 
  
    
      
        O
        (
        n
        
          d
          
            2
          
        
        )
      
    
    {\displaystyle O(nd^{2})}
  , which is an order of magnitude faster than the corresponding batch learning complexity. The storage requirements at every step 
  
    
      
        i
      
    
    {\displaystyle i}
   here are to store the matrix 
  
    
      
        
          Γ
          
            i
          
        
      
    
    {\displaystyle \Gamma _{i}}
  , which is constant at 
  
    
      
        O
        (
        
          d
          
            2
          
        
        )
      
    
    {\displaystyle O(d^{2})}
  . For the case when 
  
    
      
        
          Σ
          
            i
          
        
      
    
    {\displaystyle \Sigma _{i}}
   is not invertible, consider the regularised version of the problem 
loss function 
  
    
      
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        (
        
          x
          
            j
          
          
            T
          
        
        w
        −
        
          y
          
            j
          
        
        
          )
          
            2
          
        
        +
        λ
        
          |
        
        
          |
        
        w
        
          |
        
        
          
            |
          
          
            2
          
          
            2
          
        
      
    
    {\displaystyle \sum _{j=1}^{n}(x_{j}^{T}w-y_{j})^{2}+\lambda ||w||_{2}^{2}}
  . Then, it's easy to show that the same algorithm works with 
  
    
      
        
          Γ
          
            0
          
        
        =
        (
        I
        +
        λ
        I
        
          )
          
            −
            1
          
        
      
    
    {\displaystyle \Gamma _{0}=(I+\lambda I)^{-1}}
  , and the iterations proceed to give 
  
    
      
        
          Γ
          
            i
          
        
        =
        (
        
          Σ
          
            i
          
        
        +
        λ
        I
        
          )
          
            −
            1
          
        
      
    
    {\displaystyle \Gamma _{i}=(\Sigma _{i}+\lambda I)^{-1}}
  .

Stochastic gradient descent
When this  

  
    
      
        
          
            w
            
              i
            
          
          =
          
            w
            
              i
              −
              1
            
          
          −
          
            Γ
            
              i
            
          
          
            x
            
              i
            
          
          (
          
            x
            
              i
            
            
              T
            
          
          
            w
            
              i
              −
              1
            
          
          −
          
            y
            
              i
            
          
          )
        
      
    
    {\displaystyle \textstyle w_{i}=w_{i-1}-\Gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})}
  is replaced by

  
    
      
        
          
            w
            
              i
            
          
          =
          
            w
            
              i
              −
              1
            
          
          −
          
            γ
            
              i
            
          
          
            x
            
              i
            
          
          (
          
            x
            
              i
            
            
              T
            
          
          
            w
            
              i
              −
              1
            
          
          −
          
            y
            
              i
            
          
          )
          =
          
            w
            
              i
              −
              1
            
          
          −
          
            γ
            
              i
            
          
          ∇
          V
          (
          ⟨
          
            w
            
              i
              −
              1
            
          
          ,
          
            x
            
              i
            
          
          ⟩
          ,
          
            y
            
              i
            
          
          )
        
      
    
    {\displaystyle \textstyle w_{i}=w_{i-1}-\gamma _{i}x_{i}(x_{i}^{T}w_{i-1}-y_{i})=w_{i-1}-\gamma _{i}\nabla V(\langle w_{i-1},x_{i}\rangle ,y_{i})}
  or 
  
    
      
        
          Γ
          
            i
          
        
        ∈
        
          
            R
          
          
            d
            ×
            d
          
        
      
    
    {\displaystyle \Gamma _{i}\in \mathbb {R} ^{d\times d}}
   by 
  
    
      
        
          γ
          
            i
          
        
        ∈
        
          R
        
      
    
    {\displaystyle \gamma _{i}\in \mathbb {R} }
  , this becomes the stochastic gradient descent algorithm. In this case, the complexity for 
  
    
      
        n
      
    
    {\displaystyle n}
   steps of this algorithm reduces to 
  
    
      
        O
        (
        n
        d
        )
      
    
    {\displaystyle O(nd)}
  . The storage requirements at every step 
  
    
      
        i
      
    
    {\displaystyle i}
   are constant at 
  
    
      
        O
        (
        d
        )
      
    
    {\displaystyle O(d)}
  .
However, the stepsize 
  
    
      
        
          γ
          
            i
          
        
      
    
    {\displaystyle \gamma _{i}}
   needs to be chosen carefully to solve the expected risk minimization problem, as detailed above. By choosing a decaying step size 
  
    
      
        
          γ
          
            i
          
        
        ≈
        
          
            1
            
              i
            
          
        
        ,
      
    
    {\displaystyle \gamma _{i}\approx {\frac {1}{\sqrt {i}}},}
   one can prove the convergence of the average iterate 
  
    
      
        
          
            
              w
              ¯
            
          
          
            n
          
        
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          w
          
            i
          
        
      
    
    {\displaystyle {\overline {w}}_{n}={\frac {1}{n}}\sum _{i=1}^{n}w_{i}}
  . This setting is a special case of stochastic optimization, a well known problem in optimization.

Incremental stochastic gradient descent
In practice, one can perform multiple stochastic gradient passes (also called cycles or epochs) over the data. The algorithm thus obtained is
called incremental gradient method and corresponds to an iteration

  
    
      
        
          
            w
            
              i
            
          
          =
          
            w
            
              i
              −
              1
            
          
          −
          
            γ
            
              i
            
          
          ∇
          V
          (
          ⟨
          
            w
            
              i
              −
              1
            
          
          ,
          
            x
            
              
                t
                
                  i
                
              
            
          
          ⟩
          ,
          
            y
            
              
                t
                
                  i
                
              
            
          
          )
        
      
    
    {\displaystyle \textstyle w_{i}=w_{i-1}-\gamma _{i}\nabla V(\langle w_{i-1},x_{t_{i}}\rangle ,y_{t_{i}})}
  The main difference with the stochastic gradient method is that here a sequence 
  
    
      
        
          t
          
            i
          
        
      
    
    {\displaystyle t_{i}}
   is chosen to decide which training point is visited in the 
  
    
      
        i
      
    
    {\displaystyle i}
  -th step. Such a sequence can be stochastic or deterministic. The number of iterations is then decoupled to the number of points (each point can be considered more than once). The incremental gradient method can be shown to provide a minimizer to the empirical risk. Incremental techniques can be advantageous when considering objective functions made up of a sum of many terms e.g. an empirical error corresponding to a very large dataset.

Kernel methods
Kernels can be used to extend the above algorithms to non-parametric models (or models where the parameters form an infinite dimensional space). The corresponding procedure will no longer be truly online and instead involve storing all the data points, but is still faster than the brute force method.
This discussion is restricted to the case of the square loss, though it can be extended to any convex loss. It can be shown by an easy induction  that if 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
   is the data matrix and 
  
    
      
        
          w
          
            i
          
        
      
    
    {\displaystyle w_{i}}
   is the output after 
  
    
      
        i
      
    
    {\displaystyle i}
   steps of the SGD algorithm, then,

  
    
      
        
          w
          
            i
          
        
        =
        
          X
          
            i
          
          
            T
          
        
        
          c
          
            i
          
        
      
    
    {\displaystyle w_{i}=X_{i}^{T}c_{i}}
  where 
  
    
      
        
          
            c
            
              i
            
          
          =
          (
          (
          
            c
            
              i
            
          
          
            )
            
              1
            
          
          ,
          (
          
            c
            
              i
            
          
          
            )
            
              2
            
          
          ,
          .
          .
          .
          ,
          (
          
            c
            
              i
            
          
          
            )
            
              i
            
          
          )
          ∈
          
            
              R
            
            
              i
            
          
        
      
    
    {\displaystyle \textstyle c_{i}=((c_{i})_{1},(c_{i})_{2},...,(c_{i})_{i})\in \mathbb {R} ^{i}}
   and the sequence 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
   satisfies the recursion:

  
    
      
        
          c
          
            0
          
        
        =
        0
      
    
    {\displaystyle c_{0}=0}
  

  
    
      
        (
        
          c
          
            i
          
        
        
          )
          
            j
          
        
        =
        (
        
          c
          
            i
            −
            1
          
        
        
          )
          
            j
          
        
        ,
        j
        =
        1
        ,
        2
        ,
        .
        .
        .
        ,
        i
        −
        1
      
    
    {\displaystyle (c_{i})_{j}=(c_{i-1})_{j},j=1,2,...,i-1}
   and

  
    
      
        (
        
          c
          
            i
          
        
        
          )
          
            i
          
        
        =
        
          γ
          
            i
          
        
        
          
            (
          
        
        
          y
          
            i
          
        
        −
        
          ∑
          
            j
            =
            1
          
          
            i
            −
            1
          
        
        (
        
          c
          
            i
            −
            1
          
        
        
          )
          
            j
          
        
        ⟨
        
          x
          
            j
          
        
        ,
        
          x
          
            i
          
        
        ⟩
        
          
            )
          
        
      
    
    {\displaystyle (c_{i})_{i}=\gamma _{i}{\Big (}y_{i}-\sum _{j=1}^{i-1}(c_{i-1})_{j}\langle x_{j},x_{i}\rangle {\Big )}}
  Notice that here 
  
    
      
        ⟨
        
          x
          
            j
          
        
        ,
        
          x
          
            i
          
        
        ⟩
      
    
    {\displaystyle \langle x_{j},x_{i}\rangle }
   is just the standard Kernel on 
  
    
      
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle \mathbb {R} ^{d}}
  , and the predictor is of the form 

  
    
      
        
          f
          
            i
          
        
        (
        x
        )
        =
        ⟨
        
          w
          
            i
            −
            1
          
        
        ,
        x
        ⟩
        =
        
          ∑
          
            j
            =
            1
          
          
            i
            −
            1
          
        
        (
        
          c
          
            i
            −
            1
          
        
        
          )
          
            j
          
        
        ⟨
        
          x
          
            j
          
        
        ,
        x
        ⟩
      
    
    {\displaystyle f_{i}(x)=\langle w_{i-1},x\rangle =\sum _{j=1}^{i-1}(c_{i-1})_{j}\langle x_{j},x\rangle }
  .Now, if  a general kernel 
  
    
      
        K
      
    
    {\displaystyle K}
   is introduced instead and let the predictor be 

  
    
      
        
          f
          
            i
          
        
        (
        x
        )
        =
        
          ∑
          
            j
            =
            1
          
          
            i
            −
            1
          
        
        (
        
          c
          
            i
            −
            1
          
        
        
          )
          
            j
          
        
        K
        (
        
          x
          
            j
          
        
        ,
        x
        )
      
    
    {\displaystyle f_{i}(x)=\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x)}
  then the same proof will also show that predictor minimising the least squares loss is obtained by changing the above recursion to

  
    
      
        (
        
          c
          
            i
          
        
        
          )
          
            i
          
        
        =
        
          γ
          
            i
          
        
        
          
            (
          
        
        
          y
          
            i
          
        
        −
        
          ∑
          
            j
            =
            1
          
          
            i
            −
            1
          
        
        (
        
          c
          
            i
            −
            1
          
        
        
          )
          
            j
          
        
        K
        (
        
          x
          
            j
          
        
        ,
        
          x
          
            i
          
        
        )
        
          
            )
          
        
      
    
    {\displaystyle (c_{i})_{i}=\gamma _{i}{\Big (}y_{i}-\sum _{j=1}^{i-1}(c_{i-1})_{j}K(x_{j},x_{i}){\Big )}}
  The above expression requires storing all the data for updating 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
  . The total time complexity for the recursion when evaluating for the 
  
    
      
        n
      
    
    {\displaystyle n}
  -th datapoint is 
  
    
      
        O
        (
        
          n
          
            2
          
        
        d
        k
        )
      
    
    {\displaystyle O(n^{2}dk)}
  , where 
  
    
      
        k
      
    
    {\displaystyle k}
   is the cost of evaluating the kernel on a single pair of points.
Thus, the use of the kernel has allowed the movement from a finite dimensional parameter space 
  
    
      
        
          
            w
            
              i
            
          
          ∈
          
            
              R
            
            
              d
            
          
        
      
    
    {\displaystyle \textstyle w_{i}\in \mathbb {R} ^{d}}
   to a possibly infinite dimensional feature represented by a kernel 
  
    
      
        K
      
    
    {\displaystyle K}
   by instead performing the recursion on the space of parameters 
  
    
      
        
          
            c
            
              i
            
          
          ∈
          
            
              R
            
            
              i
            
          
        
      
    
    {\displaystyle \textstyle c_{i}\in \mathbb {R} ^{i}}
  , whose dimension is the same as the size of the training dataset. In general, this is a consequence of the representer theorem.

Online convex optimization
Online convex optimization (OCO)  is a general framework for decision making which leverages convex optimization to allow for efficient algorithms. The framework is that of repeated game playing as follows:
For 
  
    
      
        t
        =
        1
        ,
        2
        ,
        .
        .
        .
        ,
        T
      
    
    {\displaystyle t=1,2,...,T}
  

Learner receives input 
  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  
Learner outputs 
  
    
      
        
          w
          
            t
          
        
      
    
    {\displaystyle w_{t}}
   from a fixed convex set 
  
    
      
        S
      
    
    {\displaystyle S}
  
Nature sends back a convex loss function 
  
    
      
        
          v
          
            t
          
        
        :
        S
        →
        
          R
        
      
    
    {\displaystyle v_{t}:S\rightarrow \mathbb {R} }
  .
Learner suffers loss 
  
    
      
        
          v
          
            t
          
        
        (
        
          w
          
            t
          
        
        )
      
    
    {\displaystyle v_{t}(w_{t})}
   and updates its modelThe goal is to minimize regret, or the difference between cumulative loss and the loss of the best fixed point  
  
    
      
        u
        ∈
        S
      
    
    {\displaystyle u\in S}
   in hindsight.
As an example, consider the case of online least squares linear regression. Here, the weight vectors come from the convex set 
  
    
      
        S
        =
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle S=\mathbb {R} ^{d}}
  , and nature sends back the convex loss function 
  
    
      
        
          v
          
            t
          
        
        (
        w
        )
        =
        (
        ⟨
        w
        ,
        
          x
          
            t
          
        
        ⟩
        −
        
          y
          
            t
          
        
        
          )
          
            2
          
        
      
    
    {\displaystyle v_{t}(w)=(\langle w,x_{t}\rangle -y_{t})^{2}}
  . Note here that 
  
    
      
        
          y
          
            t
          
        
      
    
    {\displaystyle y_{t}}
   is implicitly sent with 
  
    
      
        
          v
          
            t
          
        
      
    
    {\displaystyle v_{t}}
  .
Some online prediction problems however cannot fit in the framework of OCO. For example, in online classification, the prediction domain and the loss functions are not convex. In such scenarios, two simple techniques for convexification are used: randomisation and surrogate loss functions.
Some simple online convex optimisation algorithms are:

Follow the leader (FTL)
The simplest learning rule to try is to select (at the current step) the hypothesis that has the least loss over all past rounds. This algorithm is called Follow the leader, and is simply given round 
  
    
      
        t
      
    
    {\displaystyle t}
   by:

  
    
      
        
          w
          
            t
          
        
        =
        
          
            a
            r
            g
            
            m
            i
            n
          
          
            w
            ∈
            S
          
        
        ⁡
        
          ∑
          
            i
            =
            1
          
          
            t
            −
            1
          
        
        
          v
          
            i
          
        
        (
        w
        )
      
    
    {\displaystyle w_{t}=\operatorname {arg\,min} _{w\in S}\sum _{i=1}^{t-1}v_{i}(w)}
  This method can thus be looked as a greedy algorithm. For the case of online quadratic optimization (where the loss function is 
  
    
      
        
          v
          
            t
          
        
        (
        w
        )
        =
        
          |
        
        
          |
        
        w
        −
        
          x
          
            t
          
        
        
          |
        
        
          
            |
          
          
            2
          
          
            2
          
        
      
    
    {\displaystyle v_{t}(w)=||w-x_{t}||_{2}^{2}}
  ), one can show a regret bound that grows as 
  
    
      
        log
        ⁡
        (
        T
        )
      
    
    {\displaystyle \log(T)}
  . However, similar bounds cannot be obtained for the FTL algorithm for other important families of models like online linear optimization. To do so, one modifies FTL by adding regularisation.

Follow the regularised leader (FTRL)
This is a natural modification of FTL that is used to stabilise the FTL solutions and obtain better regret bounds. A regularisation function 
  
    
      
        R
        :
        S
        →
        
          R
        
      
    
    {\displaystyle R:S\rightarrow \mathbb {R} }
    is chosen and learning performed in round t as follows:

  
    
      
        
          w
          
            t
          
        
        =
        
          
            a
            r
            g
            
            m
            i
            n
          
          
            w
            ∈
            S
          
        
        ⁡
        
          ∑
          
            i
            =
            1
          
          
            t
            −
            1
          
        
        
          v
          
            i
          
        
        (
        w
        )
        +
        R
        (
        w
        )
      
    
    {\displaystyle w_{t}=\operatorname {arg\,min} _{w\in S}\sum _{i=1}^{t-1}v_{i}(w)+R(w)}
  As a special example, consider the case of online linear optimisation i.e. where nature sends back loss functions of the form 
  
    
      
        
          v
          
            t
          
        
        (
        w
        )
        =
        ⟨
        w
        ,
        
          z
          
            t
          
        
        ⟩
      
    
    {\displaystyle v_{t}(w)=\langle w,z_{t}\rangle }
  . Also, let 
  
    
      
        S
        =
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle S=\mathbb {R} ^{d}}
  . Suppose the regularisation function 
  
    
      
        R
        (
        w
        )
        =
        
          
            1
            
              2
              η
            
          
        
        
          |
        
        
          |
        
        w
        
          |
        
        
          
            |
          
          
            2
          
          
            2
          
        
      
    
    {\displaystyle R(w)={\frac {1}{2\eta }}||w||_{2}^{2}}
   is chosen for some positive number 
  
    
      
        η
      
    
    {\displaystyle \eta }
  . Then, one can show that the regret minimising iteration becomes 

  
    
      
        
          w
          
            t
            +
            1
          
        
        =
        −
        η
        
          ∑
          
            i
            =
            1
          
          
            t
          
        
        
          z
          
            i
          
        
        =
        
          w
          
            t
          
        
        −
        η
        
          z
          
            t
          
        
      
    
    {\displaystyle w_{t+1}=-\eta \sum _{i=1}^{t}z_{i}=w_{t}-\eta z_{t}}
  Note that this can be rewritten as 
  
    
      
        
          w
          
            t
            +
            1
          
        
        =
        
          w
          
            t
          
        
        −
        η
        ∇
        
          v
          
            t
          
        
        (
        
          w
          
            t
          
        
        )
      
    
    {\displaystyle w_{t+1}=w_{t}-\eta \nabla v_{t}(w_{t})}
  , which looks exactly like online gradient descent.
If S is instead some convex subspace of 
  
    
      
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle \mathbb {R} ^{d}}
  , S would need to be projected onto, leading to the modified update rule

  
    
      
        
          w
          
            t
            +
            1
          
        
        =
        
          Π
          
            S
          
        
        (
        −
        η
        
          ∑
          
            i
            =
            1
          
          
            t
          
        
        
          z
          
            i
          
        
        )
        =
        
          Π
          
            S
          
        
        (
        η
        
          θ
          
            t
            +
            1
          
        
        )
      
    
    {\displaystyle w_{t+1}=\Pi _{S}(-\eta \sum _{i=1}^{t}z_{i})=\Pi _{S}(\eta \theta _{t+1})}
  This algorithm is known as lazy projection, as the vector 
  
    
      
        
          θ
          
            t
            +
            1
          
        
      
    
    {\displaystyle \theta _{t+1}}
   accumulates the gradients. It is also known as Nesterov's dual averaging algorithm. In this scenario of linear loss functions and quadratic regularisation, the regret is bounded by 
  
    
      
        O
        (
        
          
            T
          
        
        )
      
    
    {\displaystyle O({\sqrt {T}})}
  , and thus the average regret goes to 0 as desired.

Online subgradient descent (OSD)
The above proved a regret bound for linear loss functions 
  
    
      
        
          v
          
            t
          
        
        (
        w
        )
        =
        ⟨
        w
        ,
        
          z
          
            t
          
        
        ⟩
      
    
    {\displaystyle v_{t}(w)=\langle w,z_{t}\rangle }
  . To generalise the algorithm to any convex loss function, the subgradient 
  
    
      
        ∂
        
          v
          
            t
          
        
        (
        
          w
          
            t
          
        
        )
      
    
    {\displaystyle \partial v_{t}(w_{t})}
   of 
  
    
      
        
          v
          
            t
          
        
      
    
    {\displaystyle v_{t}}
   is used as a linear approximation to 
  
    
      
        
          v
          
            t
          
        
      
    
    {\displaystyle v_{t}}
   near 
  
    
      
        
          w
          
            t
          
        
      
    
    {\displaystyle w_{t}}
  , leading to the online subgradient descent algorithm:
Initialise parameter 
  
    
      
        η
        ,
        
          w
          
            1
          
        
        =
        0
      
    
    {\displaystyle \eta ,w_{1}=0}
  
For 
  
    
      
        t
        =
        1
        ,
        2
        ,
        .
        .
        .
        ,
        T
      
    
    {\displaystyle t=1,2,...,T}
  

Predict using 
  
    
      
        
          w
          
            t
          
        
      
    
    {\displaystyle w_{t}}
  , receive 
  
    
      
        
          f
          
            t
          
        
      
    
    {\displaystyle f_{t}}
   from nature.
Choose 
  
    
      
        
          z
          
            t
          
        
        ∈
        ∂
        
          v
          
            t
          
        
        (
        
          w
          
            t
          
        
        )
      
    
    {\displaystyle z_{t}\in \partial v_{t}(w_{t})}
  
If 
  
    
      
        S
        =
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle S=\mathbb {R} ^{d}}
  , update as 
  
    
      
        
          w
          
            t
            +
            1
          
        
        =
        
          w
          
            t
          
        
        −
        η
        
          z
          
            t
          
        
      
    
    {\displaystyle w_{t+1}=w_{t}-\eta z_{t}}
  
If 
  
    
      
        S
        ⊂
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle S\subset \mathbb {R} ^{d}}
  , project cumulative gradients onto 
  
    
      
        S
      
    
    {\displaystyle S}
   i.e. 
  
    
      
        
          w
          
            t
            +
            1
          
        
        =
        
          Π
          
            S
          
        
        (
        η
        
          θ
          
            t
            +
            1
          
        
        )
        ,
        
          θ
          
            t
            +
            1
          
        
        =
        
          θ
          
            t
          
        
        +
        
          z
          
            t
          
        
      
    
    {\displaystyle w_{t+1}=\Pi _{S}(\eta \theta _{t+1}),\theta _{t+1}=\theta _{t}+z_{t}}
  One can use the OSD algorithm to derive 
  
    
      
        O
        (
        
          
            T
          
        
        )
      
    
    {\displaystyle O({\sqrt {T}})}
   regret bounds for the online version of SVM's for classification, which use the hinge loss
  
    
      
        
          v
          
            t
          
        
        (
        w
        )
        =
        max
        {
        0
        ,
        1
        −
        
          y
          
            t
          
        
        (
        w
        ⋅
        
          x
          
            t
          
        
        )
        }
      
    
    {\displaystyle v_{t}(w)=\max\{0,1-y_{t}(w\cdot x_{t})\}}

Other algorithms
Quadratically regularised FTRL algorithms lead to lazily projected gradient algorithms as described above. To use the above for arbitrary convex functions and regularisers, one uses online mirror descent.  The optimal regularization in hindsight can be derived for linear loss functions, this leads to the AdaGrad algorithm.
For the Euclidean regularisation, one can show a regret bound of 
  
    
      
        O
        (
        
          
            T
          
        
        )
      
    
    {\displaystyle O({\sqrt {T}})}
  , which can be improved further to a 
  
    
      
        O
        (
        log
        ⁡
        T
        )
      
    
    {\displaystyle O(\log T)}
   for strongly convex and exp-concave loss functions.

Interpretations of online learning
The paradigm of online learning has different interpretations depending on the choice of the learning model, each of which has distinct implications about the predictive quality of the sequence of functions 
  
    
      
        
          f
          
            1
          
        
        ,
        
          f
          
            2
          
        
        ,
        …
        ,
        
          f
          
            n
          
        
      
    
    {\displaystyle f_{1},f_{2},\ldots ,f_{n}}
  . The prototypical stochastic gradient descent algorithm is used for this discussion. As noted above, its recursion is given by

  
    
      
        
          
            w
            
              t
            
          
          =
          
            w
            
              t
              −
              1
            
          
          −
          
            γ
            
              t
            
          
          ∇
          V
          (
          ⟨
          
            w
            
              t
              −
              1
            
          
          ,
          
            x
            
              t
            
          
          ⟩
          ,
          
            y
            
              t
            
          
          )
        
      
    
    {\displaystyle \textstyle w_{t}=w_{t-1}-\gamma _{t}\nabla V(\langle w_{t-1},x_{t}\rangle ,y_{t})}
  The first interpretation consider the stochastic gradient descent method as applied to the problem of minimizing the expected risk 
  
    
      
        I
        [
        w
        ]
      
    
    {\displaystyle I[w]}
   defined above. Indeed, in the case of an infinite stream of data, since the examples 
  
    
      
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        ,
        (
        
          x
          
            2
          
        
        ,
        
          y
          
            2
          
        
        )
        ,
        …
      
    
    {\displaystyle (x_{1},y_{1}),(x_{2},y_{2}),\ldots }
   are assumed to be drawn i.i.d. from the distribution 
  
    
      
        p
        (
        x
        ,
        y
        )
      
    
    {\displaystyle p(x,y)}
  , the sequence of gradients of 
  
    
      
        V
        (
        ⋅
        ,
        ⋅
        )
      
    
    {\displaystyle V(\cdot ,\cdot )}
   in the above iteration are an i.i.d. sample of stochastic estimates of the gradient of the expected risk 
  
    
      
        I
        [
        w
        ]
      
    
    {\displaystyle I[w]}
   and therefore one can apply complexity results for the stochastic gradient descent method to bound the deviation 
  
    
      
        I
        [
        
          w
          
            t
          
        
        ]
        −
        I
        [
        
          w
          
            ∗
          
        
        ]
      
    
    {\displaystyle I[w_{t}]-I[w^{\ast }]}
  , where 
  
    
      
        
          w
          
            ∗
          
        
      
    
    {\displaystyle w^{\ast }}
   is the minimizer of 
  
    
      
        I
        [
        w
        ]
      
    
    {\displaystyle I[w]}
  . This interpretation is also valid in the case of a finite training set; although with multiple passes through the data the gradients are no longer independent, still complexity results can be obtained in special cases.
The second interpretation applies to the case of a finite training set and considers the SGD algorithm as an instance of incremental gradient descent method. In this case, one instead looks at the empirical risk:

  
    
      
        
          I
          
            n
          
        
        [
        w
        ]
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        V
        (
        ⟨
        w
        ,
        
          x
          
            i
          
        
        ⟩
        ,
        
          y
          
            i
          
        
        )
         
        .
      
    
    {\displaystyle I_{n}[w]={\frac {1}{n}}\sum _{i=1}^{n}V(\langle w,x_{i}\rangle ,y_{i})\ .}
  Since the gradients of 
  
    
      
        V
        (
        ⋅
        ,
        ⋅
        )
      
    
    {\displaystyle V(\cdot ,\cdot )}
   in the incremental gradient descent iterations are also stochastic estimates of the gradient of 
  
    
      
        
          I
          
            n
          
        
        [
        w
        ]
      
    
    {\displaystyle I_{n}[w]}
  , this interpretation is also related to the stochastic gradient descent method, but applied to minimize the empirical risk as opposed to the expected risk. Since this interpretation concerns the empirical risk and not the expected risk, multiple passes through the data are readily allowed and actually lead to tighter bounds on the deviations 
  
    
      
        
          I
          
            n
          
        
        [
        
          w
          
            t
          
        
        ]
        −
        
          I
          
            n
          
        
        [
        
          w
          
            n
          
          
            ∗
          
        
        ]
      
    
    {\displaystyle I_{n}[w_{t}]-I_{n}[w_{n}^{\ast }]}
  , where 
  
    
      
        
          w
          
            n
          
          
            ∗
          
        
      
    
    {\displaystyle w_{n}^{\ast }}
   is the minimizer of 
  
    
      
        
          I
          
            n
          
        
        [
        w
        ]
      
    
    {\displaystyle I_{n}[w]}
  .

Implementations
Vowpal Wabbit: Open-source fast out-of-core online learning system which is notable for supporting a number of machine learning reductions, importance weighting and a selection of different loss functions and optimisation algorithms. It uses the hashing trick for bounding the size of the set of features independent of the amount of training data.
scikit-learn: Provides out-of-core implementations of algorithms for
Classification: Perceptron, SGD classifier, Naive bayes classifier.
Regression: SGD Regressor, Passive Aggressive regressor.
Clustering: Mini-batch k-means.
Feature extraction: Mini-batch dictionary learning, Incremental PCA.

See also
Learning paradigms

Incremental learning
Lazy learning
Offline learning, the opposite model
Reinforcement learning
Supervised learningGeneral algorithms

Online algorithm
Online optimization
Streaming algorithm
Stochastic gradient descentLearning models

Adaptive Resonance Theory
Hierarchical temporal memory
k-nearest neighbor algorithm
Learning vector quantization
Perceptron

References
External links
http://onlineprediction.net/, Wiki for On-Line Prediction.
6.883: Online Methods in Machine Learning: Theory and Applications. Alexander Rakhlin. MIT",https://en.wikipedia.org/wiki/Online_machine_learning,"['All articles with unsourced statements', 'Articles with short description', 'Articles with unsourced statements from September 2019', 'Machine learning algorithms', 'Short description matches Wikidata', 'Wikipedia articles needing clarification from September 2019']",Data Science
144,Outline of machine learning,"The following outline is provided as an overview of and topical guide to machine learning. Machine learning is a subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a ""field of study that gives computers the ability to learn without being explicitly programmed"". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.

What type of thing is machine learning?
An academic discipline
A branch of science
An applied science
A subfield of computer science
A branch of artificial intelligence
A subfield of soft computing
Application of statistics

Branches of machine learning
Subfields of machine learning
Subfields of machine learning

Computational learning theory – studying the design and analysis of machine learning algorithms.
Grammar induction
Meta learning

Cross-disciplinary fields involving machine learning
Cross-disciplinary fields involving machine learning

Adversarial machine learning
Predictive analytics
Quantum machine learning
Robot learning
Developmental robotics

Applications of machine learning
Applications of machine learning

Bioinformatics
Biomedical informatics
Computer vision
Customer relationship management –
Data mining
Email filtering
Inverted pendulum – balance and equilibrium system.
Natural language processing (NLP)
Automatic summarization
Automatic taxonomy construction
Dialog system
Grammar checker
Language recognition
Handwriting recognition
Optical character recognition
Speech recognition
Machine translation
Question answering
Speech synthesis
Text mining
Term frequency–inverse document frequency (tf–idf)
Text simplification
Pattern recognition
Facial recognition system
Handwriting recognition
Image recognition
Optical character recognition
Speech recognition
Recommendation system
Collaborative filtering
Content-based filtering
Hybrid recommender systems (Collaborative and content-based filtering)
Search engine
Search engine optimization
Social Engineering

Machine learning hardware
Machine learning hardware

Graphics processing unit
Tensor processing unit
Vision processing unit

Machine learning tools
Machine learning tools   (list)

Comparison of deep learning software
Comparison of deep learning software/Resources

Machine learning frameworks
Machine learning framework

Proprietary machine learning frameworks
Proprietary machine learning frameworks

Amazon Machine Learning
Microsoft Azure Machine Learning Studio
DistBelief – replaced by TensorFlow

Open source machine learning frameworks
Open source machine learning frameworks

Apache Singa
Apache MXNet
Caffe
PyTorch
mlpack
TensorFlow
Torch
CNTK
Accord.Net

Machine learning libraries
Machine learning library   

Deeplearning4j
Theano
Scikit-learn
Keras

Machine learning algorithms
Machine learning algorithm

Types of machine learning algorithms
Almeida–Pineda recurrent backpropagation
ALOPEX
Backpropagation
Bootstrap aggregating
CN2 algorithm
Constructing skill trees
Dehaene–Changeux model
Diffusion map
Dominance-based rough set approach
Dynamic time warping
Error-driven learning
Evolutionary multimodal optimization
Expectation–maximization algorithm
FastICA
Forward–backward algorithm
GeneRec
Genetic Algorithm for Rule Set Production
Growing self-organizing map
Hyper basis function network
IDistance
K-nearest neighbors algorithm
Kernel methods for vector output
Kernel principal component analysis
Leabra
Linde–Buzo–Gray algorithm
Local outlier factor
Logic learning machine
LogitBoost
Manifold alignment
Markov chain Monte Carlo (MCMC)
Minimum redundancy feature selection
Mixture of experts
Multiple kernel learning
Non-negative matrix factorization
Online machine learning
Out-of-bag error
Prefrontal cortex basal ganglia working memory
PVLV
Q-learning
Quadratic unconstrained binary optimization
Query-level feature
Quickprop
Radial basis function network
Randomized weighted majority algorithm
Reinforcement learning
Repeated incremental pruning to produce error reduction (RIPPER)
Rprop
Rule-based machine learning
Skill chaining
Sparse PCA
State–action–reward–state–action
Stochastic gradient descent
Structured kNN
T-distributed stochastic neighbor embedding
Temporal difference learning
Wake-sleep algorithm
Weighted majority algorithm (machine learning)

Machine learning methods
Machine learning method   (list)

Instance-based algorithm
K-nearest neighbors algorithm (KNN)
Learning vector quantization (LVQ)
Self-organizing map (SOM)

Regression analysis
Logistic regression
Ordinary least squares regression (OLSR)
Linear regression
Stepwise regression
Multivariate adaptive regression splines (MARS)Regularization algorithm
Ridge regression
Least Absolute Shrinkage and Selection Operator (LASSO)
Elastic net
Least-angle regression (LARS)
Classifiers
Probabilistic classifier
Naive Bayes classifier
Binary classifier
Linear classifier
Hierarchical classifier

Dimensionality reduction
Dimensionality reduction

Canonical correlation analysis (CCA)
Factor analysis
Feature extraction
Feature selection
Independent component analysis (ICA)
Linear discriminant analysis (LDA)
Multidimensional scaling (MDS)
Non-negative matrix factorization (NMF)
Partial least squares regression (PLSR)
Principal component analysis (PCA)
Principal component regression (PCR)
Projection pursuit
Sammon mapping
t-distributed stochastic neighbor embedding (t-SNE)

Ensemble learning
Ensemble learning

AdaBoost
Boosting
Bootstrap aggregating (Bagging)
Ensemble averaging – process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models ""average out.""
Gradient boosted decision tree (GBDT)
Gradient boosting machine (GBM)
Random Forest
Stacked Generalization (blending)

Meta learning
Meta learning

Inductive bias
Metadata

Reinforcement learning
Reinforcement learning

Q-learning
State–action–reward–state–action (SARSA)
Temporal difference learning (TD)
Learning Automata

Supervised learning
Supervised learning

AODE
Artificial neural network
Association rule learning algorithms
Apriori algorithm
Eclat algorithm
Case-based reasoning
Gaussian process regression
Gene expression programming
Group method of data handling (GMDH)
Inductive logic programming
Instance-based learning
Lazy learning
Learning Automata
Learning Vector Quantization
Logistic Model Tree
Minimum message length (decision trees, decision graphs, etc.)
Nearest Neighbor Algorithm
Analogical modeling
Probably approximately correct learning (PAC) learning
Ripple down rules, a knowledge acquisition methodology
Symbolic machine learning algorithms
Support vector machines
Random Forests
Ensembles of classifiers
Bootstrap aggregating (bagging)
Boosting (meta-algorithm)
Ordinal classification
Information fuzzy networks (IFN)
Conditional Random Field
ANOVA
Quadratic classifiers
k-nearest neighbor
Boosting
SPRINT
Bayesian networks
Naive Bayes
Hidden Markov models
Hierarchical hidden Markov model

Bayesian
Bayesian statistics

Bayesian knowledge base
Naive Bayes
Gaussian Naive Bayes
Multinomial Naive Bayes
Averaged One-Dependence Estimators (AODE)
Bayesian Belief Network (BBN)
Bayesian Network (BN)

Decision tree algorithms
Decision tree algorithm

Decision tree
Classification and regression tree (CART)
Iterative Dichotomiser 3 (ID3)
C4.5 algorithm
C5.0 algorithm
Chi-squared Automatic Interaction Detection (CHAID)
Decision stump
Conditional decision tree
ID3 algorithm
Random forest
SLIQ

Linear classifier
Linear classifier

Fisher's linear discriminant
Linear regression
Logistic regression
Multinomial logistic regression
Naive Bayes classifier
Perceptron
Support vector machine

Unsupervised learning
Unsupervised learning

Expectation-maximization algorithm
Vector Quantization
Generative topographic map
Information bottleneck method

Artificial neural networks
Artificial neural network

Feedforward neural network
Extreme learning machine
Convolutional neural network
Recurrent neural network
Long short-term memory (LSTM)
Logic learning machine
Self-organizing map

Association rule learning
Association rule learning

Apriori algorithm
Eclat algorithm
FP-growth algorithm

Hierarchical clustering
Hierarchical clustering

Single-linkage clustering
Conceptual clustering

Cluster analysis
Cluster analysis

BIRCH
DBSCAN
Expectation-maximization (EM)
Fuzzy clustering
Hierarchical Clustering
K-means clustering
K-medians
Mean-shift
OPTICS algorithm

Anomaly detection
Anomaly detection

k-nearest neighbors classification (k-NN)
Local outlier factor

Semi-supervised learning
Semi-supervised learning

Active learning – special case of semi-supervised learning in which a learning algorithm is able to interactively query the user (or some other information source) to obtain the desired outputs at new data points.
Generative models
Low-density separation
Graph-based methods
Co-training
Transduction

Deep learning
Deep learning

Deep belief networks
Deep Boltzmann machines
Deep Convolutional neural networks
Deep Recurrent neural networks
Hierarchical temporal memory
Generative Adversarial Networks
Deep Boltzmann Machine (DBM)
Stacked Auto-Encoders

Other machine learning methods and problems
Anomaly detection
Association rules
Bias-variance dilemma
Classification
Multi-label classification
Clustering
Data Pre-processing
Empirical risk minimization
Feature engineering
Feature learning
Learning to rank
Occam learning
Online machine learning
PAC learning
Regression
Reinforcement Learning
Semi-supervised learning
Statistical learning
Structured prediction
Graphical models
Bayesian network
Conditional random field (CRF)
Hidden Markov model (HMM)
Unsupervised learning
VC theory

Machine learning research
List of artificial intelligence projects
List of datasets for machine learning research

History of machine learning
History of machine learning

Timeline of machine learning

Machine learning projects
Machine learning projects

DeepMind
Google Brain

Machine learning organizations
Machine learning organizations

Knowledge Engineering and Machine Learning Group

Machine learning conferences and workshops
Artificial Intelligence and Security (AISec) (co-located workshop with CCS)
Conference on Neural Information Processing Systems (NIPS)
ECML PKDD
International Conference on Machine Learning (ICML)
ML4ALL (Machine Learning For All)

Machine learning publications
Books on machine learning
Books about machine learning

Machine learning journals
Machine Learning
Journal of Machine Learning Research (JMLR)
Neural Computation

Persons influential in machine learning
Alberto Broggi
Andrei Knyazev
Andrew McCallum
Andrew Ng
Anuraag Jain
Armin B. Cremers
Ayanna Howard
Barney Pell
Ben Goertzel
Ben Taskar
Bernhard Schölkopf
Brian D. Ripley
Christopher G. Atkeson
Corinna Cortes
Demis Hassabis
Douglas Lenat
Eric Xing
Ernst Dickmanns
Geoffrey Hinton – co-inventor of the backpropagation and contrastive divergence training algorithms
Hans-Peter Kriegel
Hartmut Neven
Heikki Mannila
Ian Goodfellow – Father of Generative & adversarial networks
Jacek M. Zurada
Jaime Carbonell
Jeremy Slovak
Jerome H. Friedman
John D. Lafferty
John Platt – invented SMO and Platt scaling
Julie Beth Lovins
Jürgen Schmidhuber
Karl Steinbuch
Katia Sycara
Leo Breiman – invented bagging and random forests
Lise Getoor
Luca Maria Gambardella
Léon Bottou
Marcus Hutter
Mehryar Mohri
Michael Collins
Michael I. Jordan
Michael L. Littman
Nando de Freitas
Ofer Dekel
Oren Etzioni
Pedro Domingos
Peter Flach
Pierre Baldi
Pushmeet Kohli
Ray Kurzweil
Rayid Ghani
Ross Quinlan
Salvatore J. Stolfo
Sebastian Thrun
Selmer Bringsjord
Sepp Hochreiter
Shane Legg
Stephen Muggleton
Steve Omohundro
Tom M. Mitchell
Trevor Hastie
Vasant Honavar
Vladimir Vapnik – co-inventor of the SVM and VC theory
Yann LeCun – invented convolutional neural networks
Yasuo Matsuyama
Yoshua Bengio
Zoubin Ghahramani

See also
Outline of artificial intelligence
Outline of computer vision
Outline of robotics
Accuracy paradox
Action model learning
Activation function
Activity recognition
ADALINE
Adaptive neuro fuzzy inference system
Adaptive resonance theory
Additive smoothing
Adjusted mutual information
AIVA
AIXI
AlchemyAPI
AlexNet
Algorithm selection
Algorithmic inference
Algorithmic learning theory
AlphaGo
AlphaGo Zero
Alternating decision tree
Apprenticeship learning
Causal Markov condition
Competitive learning
Concept learning
Decision tree learning
Differentiable programming
Distribution learning theory
Eager learning
End-to-end reinforcement learning
Error tolerance (PAC learning)
Explanation-based learning
Feature
GloVe
Hyperparameter
IBM Machine Learning Hub
Inferential theory of learning
Learning automata
Learning classifier system
Learning rule
Learning with errors
M-Theory (learning framework)
Machine learning control
Machine learning in bioinformatics
Margin
Markov chain geostatistics
Markov chain Monte Carlo (MCMC)
Markov information source
Markov logic network
Markov model
Markov random field
Markovian discrimination
Maximum-entropy Markov model
Multi-armed bandit
Multi-task learning
Multilinear subspace learning
Multimodal learning
Multiple instance learning
Multiple-instance learning
Never-Ending Language Learning
Offline learning
Parity learning
Population-based incremental learning
Predictive learning
Preference learning
Proactive learning
Proximal gradient methods for learning
Semantic analysis
Similarity learning
Sparse dictionary learning
Stability (learning theory)
Statistical learning theory
Statistical relational learning
Tanagra
Transfer learning
Variable-order Markov model
Version space learning
Waffles
Weka
Loss function
Loss functions for classification
Mean squared error (MSE)
Mean squared prediction error (MSPE)
Taguchi loss function
Low-energy adaptive clustering hierarchy

Other
Anne O'Tate
Ant colony optimization algorithms
Anthony Levandowski
Anti-unification (computer science)
Apache Flume
Apache Giraph
Apache Mahout
Apache SINGA
Apache Spark
Apache SystemML
Aphelion (software)
Arabic Speech Corpus
Archetypal analysis
Arthur Zimek
Artificial ants
Artificial bee colony algorithm
Artificial development
Artificial immune system
Astrostatistics
Averaged one-dependence estimators
Bag-of-words model
Balanced clustering
Ball tree
Base rate
Bat algorithm
Baum–Welch algorithm
Bayesian hierarchical modeling
Bayesian interpretation of kernel regularization
Bayesian optimization
Bayesian structural time series
Bees algorithm
Behavioral clustering
Bernoulli scheme
Bias–variance tradeoff
Biclustering
BigML
Binary classification
Bing Predicts
Bio-inspired computing
Biogeography-based optimization
Biplot
Bondy's theorem
Bongard problem
Bradley–Terry model
BrownBoost
Brown clustering
Burst error
CBCL (MIT)
CIML community portal
CMA-ES
CURE data clustering algorithm
Cache language model
Calibration (statistics)
Canonical correspondence analysis
Canopy clustering algorithm
Cascading classifiers
Category utility
CellCognition
Cellular evolutionary algorithm
Chi-square automatic interaction detection
Chromosome (genetic algorithm)
Classifier chains
Cleverbot
Clonal selection algorithm
Cluster-weighted modeling
Clustering high-dimensional data
Clustering illusion
CoBoosting
Cobweb (clustering)
Cognitive computer
Cognitive robotics
Collostructional analysis
Common-method variance
Complete-linkage clustering
Computer-automated design
Concept class
Concept drift
Conference on Artificial General Intelligence
Conference on Knowledge Discovery and Data Mining
Confirmatory factor analysis
Confusion matrix
Congruence coefficient
Connect (computer system)
Consensus clustering
Constrained clustering
Constrained conditional model
Constructive cooperative coevolution
Correlation clustering
Correspondence analysis
Cortica
Coupled pattern learner
Cross-entropy method
Cross-validation (statistics)
Crossover (genetic algorithm)
Cuckoo search
Cultural algorithm
Cultural consensus theory
Curse of dimensionality
DADiSP
DARPA LAGR Program
Darkforest
Dartmouth workshop
DarwinTunes
Data Mining Extensions
Data exploration
Data pre-processing
Data stream clustering
Dataiku
Davies–Bouldin index
Decision boundary
Decision list
Decision tree model
Deductive classifier
DeepArt
DeepDream
Deep Web Technologies
Defining length
Dendrogram
Dependability state model
Detailed balance
Determining the number of clusters in a data set
Detrended correspondence analysis
Developmental robotics
Diffbot
Differential evolution
Discrete phase-type distribution
Discriminative model
Dissociated press
Distributed R
Dlib
Document classification
Documenting Hate
Domain adaptation
Doubly stochastic model
Dual-phase evolution
Dunn index
Dynamic Bayesian network
Dynamic Markov compression
Dynamic topic model
Dynamic unobserved effects model
EDLUT
ELKI
Edge recombination operator
Effective fitness
Elastic map
Elastic matching
Elbow method (clustering)
Emergent (software)
Encog
Entropy rate
Erkki Oja
Eurisko
European Conference on Artificial Intelligence
Evaluation of binary classifiers
Evolution strategy
Evolution window
Evolutionary Algorithm for Landmark Detection
Evolutionary algorithm
Evolutionary art
Evolutionary music
Evolutionary programming
Evolvability (computer science)
Evolved antenna
Evolver (software)
Evolving classification function
Expectation propagation
Exploratory factor analysis
F1 score
FLAME clustering
Factor analysis of mixed data
Factor graph
Factor regression model
Factored language model
Farthest-first traversal
Fast-and-frugal trees
Feature Selection Toolbox
Feature hashing
Feature scaling
Feature vector
Firefly algorithm
First-difference estimator
First-order inductive learner
Fish School Search
Fisher kernel
Fitness approximation
Fitness function
Fitness proportionate selection
Fluentd
Folding@home
Formal concept analysis
Forward algorithm
Fowlkes–Mallows index
Frederick Jelinek
Frrole
Functional principal component analysis
GATTO
GLIMMER
Gary Bryce Fogel
Gaussian adaptation
Gaussian process
Gaussian process emulator
Gene prediction
General Architecture for Text Engineering
Generalization error
Generalized canonical correlation
Generalized filtering
Generalized iterative scaling
Generalized multidimensional scaling
Generative adversarial network
Generative model
Genetic algorithm
Genetic algorithm scheduling
Genetic algorithms in economics
Genetic fuzzy systems
Genetic memory (computer science)
Genetic operator
Genetic programming
Genetic representation
Geographical cluster
Gesture Description Language
Geworkbench
Glossary of artificial intelligence
Glottochronology
Golem (ILP)
Google matrix
Grafting (decision trees)
Gramian matrix
Grammatical evolution
Granular computing
GraphLab
Graph kernel
Gremlin (programming language)
Growth function
HUMANT (HUManoid ANT) algorithm
Hammersley–Clifford theorem
Harmony search
Hebbian theory
Hidden Markov random field
Hidden semi-Markov model
Hierarchical hidden Markov model
Higher-order factor analysis
Highway network
Hinge loss
Holland's schema theorem
Hopkins statistic
Hoshen–Kopelman algorithm
Huber loss
IRCF360
Ian Goodfellow
Ilastik
Ilya Sutskever
Immunocomputing
Imperialist competitive algorithm
Inauthentic text
Incremental decision tree
Induction of regular languages
Inductive bias
Inductive probability
Inductive programming
Influence diagram
Information Harvesting
Information fuzzy networks
Information gain in decision trees
Information gain ratio
Inheritance (genetic algorithm)
Instance selection
Intel RealSense
Interacting particle system
Interactive machine translation
International Joint Conference on Artificial Intelligence
International Meeting on Computational Intelligence Methods for Bioinformatics and Biostatistics
International Semantic Web Conference
Iris flower data set
Island algorithm
Isotropic position
Item response theory
Iterative Viterbi decoding
JOONE
Jabberwacky
Jaccard index
Jackknife variance estimates for random forest
Java Grammatical Evolution
Joseph Nechvatal
Jubatus
Julia (programming language)
Junction tree algorithm
K-SVD
K-means++
K-medians clustering
K-medoids
KNIME
KXEN Inc.
K q-flats
Kaggle
Kalman filter
Katz's back-off model
Kernel adaptive filter
Kernel density estimation
Kernel eigenvoice
Kernel embedding of distributions
Kernel method
Kernel perceptron
Kernel random forest
Kinect
Klaus-Robert Müller
Kneser–Ney smoothing
Knowledge Vault
Knowledge integration
LIBSVM
LPBoost
Labeled data
LanguageWare
Language Acquisition Device (computer)
Language identification in the limit
Language model
Large margin nearest neighbor
Latent Dirichlet allocation
Latent class model
Latent semantic analysis
Latent variable
Latent variable model
Lattice Miner
Layered hidden Markov model
Learnable function class
Least squares support vector machine
Leave-one-out error
Leslie P. Kaelbling
Linear genetic programming
Linear predictor function
Linear separability
Lingyun Gu
Linkurious
Lior Ron (business executive)
List of genetic algorithm applications
List of metaphor-based metaheuristics
List of text mining software
Local case-control sampling
Local independence
Local tangent space alignment
Locality-sensitive hashing
Log-linear model
Logistic model tree
Low-rank approximation
Low-rank matrix approximations
MATLAB
MIMIC (immunology)
MXNet
Mallet (software project)
Manifold regularization
Margin-infused relaxed algorithm
Margin classifier
Mark V. Shaney
Massive Online Analysis
Matrix regularization
Matthews correlation coefficient
Mean shift
Mean squared error
Mean squared prediction error
Measurement invariance
Medoid
MeeMix
Melomics
Memetic algorithm
Meta-optimization
Mexican International Conference on Artificial Intelligence
Michael Kearns (computer scientist)
MinHash
Mixture model
Mlpy
Models of DNA evolution
Moral graph
Mountain car problem
Movidius
Multi-armed bandit
Multi-label classification
Multi expression programming
Multiclass classification
Multidimensional analysis
Multifactor dimensionality reduction
Multilinear principal component analysis
Multiple correspondence analysis
Multiple discriminant analysis
Multiple factor analysis
Multiple sequence alignment
Multiplicative weight update method
Multispectral pattern recognition
Mutation (genetic algorithm)
MysteryVibe
N-gram
NOMINATE (scaling method)
Native-language identification
Natural Language Toolkit
Natural evolution strategy
Nearest-neighbor chain algorithm
Nearest centroid classifier
Nearest neighbor search
Neighbor joining
Nest Labs
NetMiner
NetOwl
Neural Designer
Neural Engineering Object
Neural Lab
Neural modeling fields
Neural network software
NeuroSolutions
Neuro Laboratory
Neuroevolution
Neuroph
Niki.ai
Noisy channel model
Noisy text analytics
Nonlinear dimensionality reduction
Novelty detection
Nuisance variable
Numenta
One-class classification
Onnx
OpenNLP
Optimal discriminant analysis
Oracle Data Mining
Orange (software)
Ordination (statistics)
Overfitting
PROGOL
PSIPRED
Pachinko allocation
PageRank
Parallel metaheuristic
Parity benchmark
Part-of-speech tagging
Particle swarm optimization
Path dependence
Pattern language (formal languages)
Peltarion Synapse
Perplexity
Persian Speech Corpus
Picas (app)
Pietro Perona
Pipeline Pilot
Piranha (software)
Pitman–Yor process
Plate notation
Polynomial kernel
Pop music automation
Population process
Portable Format for Analytics
Predictive Model Markup Language
Predictive state representation
Preference regression
Premature convergence
Principal geodesic analysis
Prior knowledge for pattern recognition
Prisma (app)
Probabilistic Action Cores
Probabilistic context-free grammar
Probabilistic latent semantic analysis
Probabilistic soft logic
Probability matching
Probit model
Product of experts
Programming with Big Data in R
Proper generalized decomposition
Pruning (decision trees)
Pushpak Bhattacharyya
Q methodology
Qloo
Quality control and genetic algorithms
Quantum Artificial Intelligence Lab
Queueing theory
Quick, Draw!
R (programming language)
Rada Mihalcea
Rademacher complexity
Radial basis function kernel
Rand index
Random indexing
Random projection
Random subspace method
Ranking SVM
RapidMiner
Rattle GUI
Raymond Cattell
Reasoning system
Regularization perspectives on support vector machines
Relational data mining
Relationship square
Relevance vector machine
Relief (feature selection)
Renjin
Repertory grid
Representer theorem
Reward-based selection
Richard Zemel
Right to explanation
RoboEarth
Robust principal component analysis
RuleML Symposium
Rule induction
Rules extraction system family
SAS (software)
SNNS
SPSS Modeler
SUBCLU
Sample complexity
Sample exclusion dimension
Santa Fe Trail problem
Savi Technology
Schema (genetic algorithms)
Search-based software engineering
Selection (genetic algorithm)
Self-Service Semantic Suite
Semantic folding
Semantic mapping (statistics)
Semidefinite embedding
Sense Networks
Sensorium Project
Sequence labeling
Sequential minimal optimization
Shattered set
Shogun (toolbox)
Silhouette (clustering)
SimHash
SimRank
Similarity measure
Simple matching coefficient
Simultaneous localization and mapping
Sinkov statistic
Sliced inverse regression
Snakes and Ladders
Soft independent modelling of class analogies
Soft output Viterbi algorithm
Solomonoff's theory of inductive inference
SolveIT Software
Spectral clustering
Spike-and-slab variable selection
Statistical machine translation
Statistical parsing
Statistical semantics
Stefano Soatto
Stephen Wolfram
Stochastic block model
Stochastic cellular automaton
Stochastic diffusion search
Stochastic grammar
Stochastic matrix
Stochastic universal sampling
Stress majorization
String kernel
Structural equation modeling
Structural risk minimization
Structured sparsity regularization
Structured support vector machine
Subclass reachability
Sufficient dimension reduction
Sukhotin's algorithm
Sum of absolute differences
Sum of absolute transformed differences
Swarm intelligence
Switching Kalman filter
Symbolic regression
Synchronous context-free grammar
Syntactic pattern recognition
TD-Gammon
TIMIT
Teaching dimension
Teuvo Kohonen
Textual case-based reasoning
Theory of conjoint measurement
Thomas G. Dietterich
Thurstonian model
Topic model
Tournament selection
Training, test, and validation sets
Transiogram
Trax Image Recognition
Trigram tagger
Truncation selection
Tucker decomposition
UIMA
UPGMA
Ugly duckling theorem
Uncertain data
Uniform convergence in probability
Unique negative dimension
Universal portfolio algorithm
User behavior analytics
VC dimension
VIGRA
Validation set
Vapnik–Chervonenkis theory
Variable-order Bayesian network
Variable kernel density estimation
Variable rules analysis
Variational message passing
Varimax rotation
Vector quantization
Vicarious (company)
Viterbi algorithm
Vowpal Wabbit
WACA clustering algorithm
WPGMA
Ward's method
Weasel program
Whitening transformation
Winnow (algorithm)
Win–stay, lose–switch
Witness set
Wolfram Language
Wolfram Mathematica
Writer invariant
Xgboost
Yooreeka
Zeroth (software)

Further reading
Trevor Hastie, Robert Tibshirani and Jerome H. Friedman (2001). The Elements of Statistical Learning, Springer. ISBN 0-387-95284-5.
Pedro Domingos (September 2015), The Master Algorithm, Basic Books, ISBN 978-0-465-06570-7
Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar (2012). Foundations of Machine Learning, The MIT Press. ISBN 978-0-262-01825-8.
Ian H. Witten and Eibe Frank (2011). Data Mining: Practical machine learning tools and techniques Morgan Kaufmann, 664pp., ISBN 978-0-12-374856-0.
David J. C. MacKay. Information Theory, Inference, and Learning Algorithms Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1
Richard O. Duda, Peter E. Hart, David G. Stork (2001) Pattern classification (2nd edition), Wiley, New York, ISBN 0-471-05669-3.
Christopher Bishop (1995). Neural Networks for Pattern Recognition, Oxford University Press. ISBN 0-19-853864-2.
Vladimir Vapnik (1998). Statistical Learning Theory. Wiley-Interscience, ISBN 0-471-03003-1.
Ray Solomonoff, An Inductive Inference Machine, IRE Convention Record, Section on Information Theory, Part 2, pp., 56–62, 1957.
Ray Solomonoff, ""An Inductive Inference Machine"" A privately circulated report from the 1956 Dartmouth Summer Research Conference on AI.

References
External links

Data Science: Data to Insights from MIT (machine learning)
Popular online course by Andrew Ng, at Coursera. It uses GNU Octave. The course is a free version of Stanford University's actual course taught by Ng, see.stanford.edu/Course/CS229 available for free].
mloss is an academic database of open-source machine learning software.",https://en.wikipedia.org/wiki/Outline_of_machine_learning,"['All articles to be expanded', 'Articles to be expanded from November 2018', 'Articles using small message boxes', 'Articles with short description', 'Artificial intelligence', 'Computing-related lists', 'Data mining', 'Machine learning', 'Outlines of applied sciences', 'Pages using Sister project links with default search', 'Short description is different from Wikidata', 'Wikipedia outlines']",Data Science
145,PMID (identifier),"PubMed is a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine (NLM) at the National Institutes of Health maintain the database as part of the Entrez system of information retrieval.From 1971 to 1997, online access to the MEDLINE database had been primarily through institutional facilities, such as university libraries. PubMed, first released in January 1996, ushered in the era of private, free, home- and office-based MEDLINE searching. The PubMed system was offered free to the public starting in June 1997.

Content
In addition to MEDLINE, PubMed provides access to:

older references from the print version of Index Medicus, back to 1951 and earlier
references to some journals before they were indexed in Index Medicus and MEDLINE, for instance Science, BMJ, and Annals of Surgery
very recent entries to records for an article before it is indexed with Medical Subject Headings (MeSH) and added to MEDLINE
a collection of books available full-text and other subsets of NLM records
PMC citations
NCBI BookshelfMany PubMed records contain links to full text articles, some of which are freely available, often in PubMed Central and local mirrors, such as Europe PubMed Central.Information about the journals indexed in MEDLINE, and available through PubMed, is found in the NLM Catalog.As of 27 January 2020, PubMed has more than 30 million citations and abstracts dating back to 1966, selectively to the year 1865, and very selectively to 1809. As of the same date, 20 million of PubMed's records are listed with their abstracts, and 21.5 million records have links to full-text versions (of which 7.5 million articles are available, full-text for free). Over the last 10 years (ending 31 December 2019), an average of nearly 1 million new records were added each year. Approximately 12% of the records in PubMed correspond to cancer-related entries, which have grown from 6% in the 1950s to 16% in 2016.
Other significant proportion of records correspond to ""chemistry"" (8.69%), ""therapy"" (8.39%), and ""infection"" (5%).In 2016, NLM changed the indexing system so that publishers are able to directly correct typos and errors in PubMed indexed articles.PubMed has been reported to include some articles published in predatory journals. MEDLINE and PubMed policies for the selection of journals for database inclusion are slightly different. Weaknesses in the criteria and procedures for indexing journals in PubMed Central may allow publications from predatory journals to leak into PubMed.

Characteristics
Website design
A new PubMed interface was launched in October 2009 and encouraged the use of such quick, Google-like search formulations; they have also been described as 'telegram' searches. By default the results are sorted by Most Recent, but this can be changed to Best Match, Publication Date, First Author, Last Author, Journal, or Title.The PubMed website design and domain was updated in January 2020 and became default on 15 May 2020, with the updated and new features. There was a critical reaction from many researchers who frequently use the site.

PubMed for handhelds/mobiles
PubMed/MEDLINE can be accessed via handheld devices, using for instance the ""PICO"" option (for focused clinical questions) created by the NLM. A ""PubMed Mobile"" option, providing access to a mobile friendly, simplified PubMed version, is also available.

Search
Standard search
Simple searches on PubMed can be carried out by entering key aspects of a subject into PubMed's search window.
PubMed translates this initial search formulation and automatically adds field names, relevant MeSH (Medical Subject Headings) terms, synonyms, Boolean operators, and 'nests' the resulting terms appropriately, enhancing the search formulation significantly, in particular by routinely combining (using the OR operator) textwords and MeSH terms.
The examples given in a PubMed tutorial demonstrate how this automatic process works:

Causes Sleep Walking is translated as (""etiology""[Subheading] OR ""etiology""[All Fields] OR ""causes""[All Fields] OR ""causality""[MeSH Terms] OR ""causality""[All Fields]) AND (""somnambulism""[MeSH Terms] OR ""somnambulism""[All Fields] OR (""sleep""[All Fields] AND ""walking""[All Fields]) OR ""sleep walking""[All Fields])
Likewise,

soft
 Attack Aspirin Prevention is translated as (""myocardial infarction""[MeSH Terms] OR (""myocardial""[All Fields] AND ""infarction""[All Fields]) OR ""myocardial infarction""[All Fields] OR (""heart""[All Fields] AND ""attack""[All Fields]) OR ""heart attack""[All Fields]) AND (""aspirin""[MeSH Terms] OR ""aspirin""[All Fields]) AND (""prevention and control""[Subheading] OR (""prevention""[All Fields] AND ""control""[All Fields]) OR ""prevention and control""[All Fields] OR ""prevention""[All Fields])

Comprehensive search
For optimal searches in PubMed, it is necessary to understand its core component, MEDLINE, and especially of the MeSH (Medical Subject Headings) controlled vocabulary used to index MEDLINE articles. They may also require complex search strategies, use of field names (tags), proper use of limits and other features; reference librarians and search specialists offer search services.The search into PubMed's search window is only recommended for the search of unequivocal topics or new interventions that do not yet have a MeSH heading created, as well as for the search for commercial brands of medicines and proper nouns. It is also useful when there is no suitable heading or the descriptor represents a partial aspect. The search using the thesaurus MeSH is more accurate and will give fewer irrelevant results. In addition, it saves the disadvantage of the free text search in which the spelling, singular/plural or abbreviated differences have to be taken into consideration. On the other side, articles more recently incorporated into the database to which descriptors have not yet been assigned will not be found. Therefore, to guarantee an exhaustive search, a combination of controlled language headings and free text terms must be used.

Journal article parameters
When a journal article is indexed, numerous article parameters are extracted and stored as structured information. Such parameters are: Article Type (MeSH terms, e.g., ""Clinical Trial""), Secondary identifiers, (MeSH terms), Language, Country of the Journal or publication history (e-publication date, print journal publication date).

Publication Type: Clinical queries/systematic reviews
Publication type parameter allows searching by the type of publication, including reports of various kinds of clinical research.

Secondary ID
Since July 2005, the MEDLINE article indexing process extracts identifiers from the article abstract and puts those in a field called Secondary Identifier (SI). The secondary identifier field is to store accession numbers to various databases of molecular sequence data, gene expression or chemical compounds and clinical trial IDs. For clinical trials, PubMed extracts trial IDs for the two largest trial registries: ClinicalTrials.gov (NCT identifier) and the International Standard Randomized Controlled Trial Number Register (IRCTN identifier).

See also
A reference which is judged particularly relevant can be marked and ""related articles"" can be identified. If relevant, several studies can be selected and related articles to all of them can be generated (on PubMed or any of the other NCBI Entrez databases) using the 'Find related data' option. The related articles are then listed in order of ""relatedness"". To create these lists of related articles, PubMed compares words from the title and abstract of each citation, as well as the MeSH headings assigned, using a powerful word-weighted algorithm. The 'related articles' function has been judged to be so precise that the authors of a paper suggested it can be used instead of a full search.

Mapping to MeSH
PubMed automatically links to MeSH terms and subheadings. Examples would be: ""bad breath"" links to (and includes in the search) ""halitosis"", ""heart attack"" to ""myocardial infarction"", ""breast cancer"" to ""breast neoplasms"". Where appropriate, these MeSH terms are automatically ""expanded"", that is, include more specific terms. Terms like ""nursing"" are automatically linked to ""Nursing [MeSH]"" or ""Nursing [Subheading]"". This feature is called Auto Term Mapping and is enacted, by default, in free text searching but not exact phrase searching (i.e. enclosing the search query with double quotes). This feature makes PubMed searches more sensitive and avoids false-negative (missed) hits by compensating for the diversity of medical terminology.PubMed does not apply automatic mapping of the term in the following circumstances: by writing the quoted phrase (e.g., ""kidney allograft""), when truncated on the asterisk (e.g., kidney allograft*), and when looking with field labels (e.g., Cancer [ti]).

My NCBI
The PubMed optional facility ""My NCBI"" (with free registration) provides tools for

saving searches
filtering search results
setting up automatic updates sent by e-mail
saving sets of references retrieved as part of a PubMed search
configuring display formats or highlighting search termsand a wide range of other options. The ""My NCBI"" area can be accessed from any computer with web-access.
An earlier version of ""My NCBI"" was called ""PubMed Cubby"".

LinkOut
LinkOut is an NLM facility to link and make available full-text local journal holdings. Some 3,200 sites (mainly academic institutions) participate in this NLM facility (as of March 2010), from Aalborg University in Denmark to ZymoGenetics in Seattle. Users at these institutions see their institution's logo within the PubMed search result (if the journal is held at that institution) and can access the full-text. Link out is being consolidated with Outside Tool as of the major platform update coming in the Summer of 2019.

PubMed Commons
In 2016, PubMed allows authors of articles to comment on articles indexed by PubMed. This feature was initially tested in a pilot mode (since 2013) and was made permanent in 2016. In February 2018, PubMed Commons was discontinued due to the fact that ""usage has remained minimal"".

askMEDLINE
askMEDLINE, a free-text, natural language query tool for MEDLINE/PubMed, developed by the NLM, also suitable for handhelds.

PubMed identifier
A PMID (PubMed identifier or PubMed unique identifier) is a unique integer value, starting at 1, assigned to each PubMed record. A PMID is not the same as a PMCID (PubMed Central identifier) which is the identifier for all works published in the free-to-access PubMed Central.The assignment of a PMID or PMCID to a publication tells the reader nothing about the type or quality of the content. PMIDs are assigned to letters to the editor, editorial opinions, op-ed columns, and any other piece that the editor chooses to include in the journal, as well as peer-reviewed papers. The existence of the identification number is also not proof that the papers have not been retracted for fraud, incompetence, or misconduct. The announcement about any corrections to original papers may be assigned a PMID.
Each number that is entered in the PubMed search window is treated by default as if it were a PMID. Therefore, any reference in PubMed can be located using the PMID.

Alternative interfaces
The National Library of Medicine leases the MEDLINE information to a number of private vendors such as Embase, Ovid, Dialog, EBSCO, Knowledge Finder and many other commercial, non-commercial, and academic providers. As of October 2008, more than 500 licenses had been issued, more than 200 of them to providers outside the United States. As licenses to use MEDLINE data are available for free, the NLM in effect provides a free testing ground for a wide range of alternative interfaces and 3rd party additions to PubMed, one of a very few large, professionally curated databases which offers this option.
Lu identifies a sample of 28 current and free Web-based PubMed versions, requiring no installation or registration, which are grouped into four categories:

Ranking search results, for instance: eTBLAST; MedlineRanker; MiSearch;
Clustering results by topics, authors, journals etc., for instance: Anne O'Tate; ClusterMed;
Enhancing semantics and visualization, for instance: EBIMed; MedEvi.
Improved search interface and retrieval experience, for instance, askMEDLINE BabelMeSH; and PubCrawler.As most of these and other alternatives rely essentially on PubMed/MEDLINE data leased under license from the NLM/PubMed, the term ""PubMed derivatives"" has been suggested. Without the need to store about 90 GB of original PubMed Datasets, anybody can write PubMed applications using the eutils-application program interface as described in ""The E-utilities In-Depth: Parameters, Syntax and More"", by Eric Sayers, PhD. Various citation format generators, taking PMID numbers as input, are examples of web applications making use of the eutils-application program interface. Sample web pages include Citation Generator - Mick Schroeder, Pubmed Citation Generator - Ultrasound of the Week, PMID2cite, and Cite this for me.

Data mining of PubMed
Alternative methods to mine the data in PubMed use programming environments such as Matlab, Python or R. In these cases, queries of PubMed are written as lines of code and passed to PubMed and the response is then processed directly in the programming environment.  Code can be automated to systematically queries with different keywords such as disease, year, organs, etc. A recent publication (2017) found that the proportion of cancer-related entries in PubMed has risen from 6% in the 1950s to 16% in 2016.The data accessible by PubMed can be mirrored locally using an unofficial tool such as MEDOC.Millions of PubMed records augment various open data datasets about open access, like Unpaywall. Data analysis tools like Unpaywall Journals are used by libraries to assist with big deal cancellations: libraries can avoid subscriptions for materials already served by instant open access via open archives like PubMed Central.

See also
PubMed Central
Europe PubMed Central
PubMed Central Canada
JournalReview.org
List of academic databases and search engines

References
External links
Official website
PubMed search tags & field qualifiers",https://en.wikipedia.org/wiki/PubMed,"['All articles containing potentially dated statements', 'All articles with unsourced statements', 'American medical websites', 'Articles containing potentially dated statements from February 2013', 'Articles containing potentially dated statements from January 2020', 'Articles containing potentially dated statements from March 2010', 'Articles containing potentially dated statements from October 2008', 'Articles with short description', 'Articles with unsourced statements from December 2018', 'Bibliographic databases and indexes', 'Biological databases', 'CS1 Spanish-language sources (es)', 'Databases in the United States', 'Medical search engines', 'National Institutes of Health', 'Online databases', 'Short description matches Wikidata', 'United States National Library of Medicine', 'Use dmy dates from September 2020']",Data Science
146,Peter Naur,"Peter Naur (25 October 1928 – 3 January 2016) was a Danish computer science pioneer and Turing award winner. He is best known as a contributor, with John Backus, to the Backus–Naur form (BNF) notation used in describing the syntax for most programming languages. He also contributed to creating the language ALGOL 60.

Biography
Naur began his career as an astronomer for which he received his Doctor of Philosophy (Ph.D.) degree in 1957, but his encounter with computers led to a change of profession. From 1959 to 1969, he was employed at Regnecentralen, the Danish computing company, while at the same time giving lectures at the Niels Bohr Institute and the Technical University of Denmark. From 1969 to 1998, Naur was a professor of computer science at University of Copenhagen.
He was a member of the International Federation for Information Processing (IFIP) IFIP Working Group 2.1 on Algorithmic Languages and Calculi, which specified, supports, and maintains the languages ALGOL 60 and ALGOL 68.Naur's main areas of inquiry were design, structure, and performance of computer programs and algorithms. He also pioneered in software engineering and software architecture. In his book Computing: A Human Activity (1992), which is a collection of his contributions to computer science, he rejected the formalist school of programming that views programming as a branch of mathematics. He did not like being associated with the Backus–Naur form (attributed to him by Donald Knuth) and said that he would prefer it to be called the Backus normal form.
Naur was married to computer scientist Christiane Floyd.
Naur disliked the term computer science and suggested it be called datalogy or data science. The former term has been adopted in Denmark and Sweden as datalogi, while the latter term is now used for data analysis, including statistics and databases.
Since the middle 1960s, computer science has been practiced in Denmark under Peter Naur's term datalogy, the science of data processes. Starting at Regnecentralen and the University of Copenhagen, the Copenhagen Tradition of Computer Science has developed its own special characteristics by means of a close connection with applications and other fields of knowledge. The tradition is not least visible in the area of education. Comprehensive project activity is an integral part of the curriculum, thus presenting theory as an aspect of realistic solutions known primarily through actual experience. Peter Naur early recognized the particular educational challenges presented by computer science. His innovations have shown their quality and vitality also at other universities. There is a close connection between computer science training as it has been formed at Copenhagen University, and the view of computer science which characterized Peter Naur's research.In later years, he was quite outspoken of the pursuit of science as a whole: Naur can possibly be identified with the empiricist school, that tells that one shall not seek deeper connections between things that manifest themselves in the world, but keep to the observable facts. He has attacked both certain strands of philosophy and psychology from this viewpoint. He was also developing a theory of human thinking which he called ""Synapse-State Theory of Mental Life"".Naur won the 2005 Association for Computing Machinery (ACM) A.M. Turing Award for his work on defining the programming language ALGOL 60. In particular, his role as editor of the influential Report on the Algorithmic Language ALGOL 60 with its pioneering use of BNF was recognized. Naur is the only Dane to have won the Turing Award.
Naur died on 3 January 2016 after a short illness.

Bibliography
Numbers refer to the published bibliography. Naur published a large number of articles and chapters on astronomy, computer science, issues in society, classical music, psychology, and education.

66. Minor planet 51 Nemausa and the fundamental system of declinations, PhD thesis, 1957
95. (editor) Backus, J. W.; Wegstein, J. H.; van Wijngaarden, A.; Woodger, M.; Bauer, F. L.; Green, J.; Katz, C.; McCarthy, J.; Perlis, A. J.; Rutishauser, H.; Samelson, K.; Vauquois, B. (May 1960). ""Report on the algorithmic language ALGOL 60"". Comm. ACM. 3 (5): 299–314. doi:10.1145/367236.367262. S2CID 278290. and several other journals.
128. (editor) Backus, J. W.; Wegstein, J. H.; van Wijngaarden, A.; Woodger, M.; Nauer, P.; Bauer, F. L.; Green, J.; Katz, C.; McCarthy, J.; Perlis, A. J.; Rutishauser, H.; Samelson, K.; Vauquois, B. (January 1963). ""Revised report on the algorithmic language ALGOL 60"". Comm. ACM. 6 (1): 1–17. doi:10.1145/366193.366201. S2CID 7853511.
144. ""Go to statements and good Algol style"" (PNG). BIT Numerical Mathematics. 3 (3): 204–5. 1963. doi:10.1007/BF01939987. S2CID 189784279.
212. —; Randell, Brian; Buxton, J.N. (1976) [1969]. The Conference on Software Engineering, 7–11 October 1968. Garmisch, Germany. ISBN 978-0884053347. OCLC 610836679.
213. —; Gram, C.; Hald, J.; Hansen, H. B.; Wessel, A. (1969). Datamatik – Studentlitteratur.
247, 249. (with B. Pedersen) Matematik 4 kursusbog, 2 volumes, Copenhagen University, 1971, 2nd ed. 1972
264. Concise Survey of Computer Methods, 397 p., Studentlitteratur, 1974
274. Datalogi 2 1975/76, 102 p., Copenhagen University, 1975, new edition 1976
333. — (1992). Computing: A Human Activity. ACM Press. ISBN 978-0201580693.
347. — (1995). Knowing and the Mystique of Logic and Rules: Including True Statements in Knowing and Action * Computer Modelling of Human Knowing Activity * Coherent Description as the Core of Scholarship and Science. Springer. ISBN 978-0-7923-3680-8.
363. Naur, Peter (1999). Antifilosofisk leksikon: Tænkning – sproglighed – videnskabelighed. ISBN 87-987221-0-7.; English translation 2001, ISBN 87-987221-1-5
382. Naur, Peter (2002). Psykologi i videnskabelig rekonstruktion. ISBN 978-87-987221-2-0.
— (January 2007). ""Computing versus human thinking"". Comm. ACM. 50 (1): 85–94. doi:10.1145/1188913.1188922.
Daylight, E.G.; P., Naur (2011). Pluralism in Software Engineering: Turing Award Winner Peter Naur Explains. Lonely Scholar. ISBN 978-94-91386-00-8.

See also
List of pioneers in computer science

References
External links
Personal website with a detailed bibliography
Talk at UIST, 2006",https://en.wikipedia.org/wiki/Peter_Naur,"['1928 births', '2016 deaths', 'All articles with dead external links', 'Articles with dead external links from January 2018', 'Articles with hCards', 'Articles with permanently dead external links', 'CS1 Danish-language sources (da)', 'Danish astronomers', 'Danish computer programmers', 'Danish computer scientists', 'People from Frederiksberg', 'Programming language designers', 'Technical University of Denmark faculty', 'Turing Award laureates', 'University of Copenhagen faculty', 'Use dmy dates from January 2016', 'Wikipedia articles with ACM-DL identifiers', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with DBLP identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MGP identifiers', 'Wikipedia articles with NKC identifiers', 'Wikipedia articles with NLP identifiers', 'Wikipedia articles with NTA identifiers', 'Wikipedia articles with PLWABN identifiers', 'Wikipedia articles with SNAC-ID identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers']",Data Science
147,Perceptron,"In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.

History
The perceptron algorithm was invented in 1958 at the Cornell Aeronautical Laboratory by Frank Rosenblatt, funded by the United States Office of Naval Research.The perceptron was intended to be a machine, rather than a program, and while its first implementation was in software for the IBM 704, it was subsequently implemented in custom-built hardware as the ""Mark 1 perceptron"". This machine was designed for image recognition: it had an array of 400 photocells, randomly connected to the ""neurons"". Weights were encoded in potentiometers, and weight updates during learning were performed by electric motors.In a 1958 press conference organized by the US Navy, Rosenblatt made statements about the perceptron that caused a heated controversy among the fledgling AI community; based on Rosenblatt's statements, The New York Times reported the perceptron to be ""the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.""Although the perceptron initially seemed promising, it was quickly proved that perceptrons could not be trained to recognise many classes of patterns. This caused the field of neural network research to stagnate for many years, before it was recognised that a feedforward neural network with two or more layers (also called a multilayer perceptron) had greater processing power than perceptrons with one layer (also called a single-layer perceptron).
Single-layer perceptrons are only capable of learning linearly separable patterns. For a classification task with some step activation function, a single node will have a single line dividing the data points forming the patterns. More nodes can create more dividing lines, but those lines must somehow be combined to form more complex classifications. A second layer of perceptrons, or even linear nodes, are sufficient to solve a lot of otherwise non-separable problems.
In 1969, a famous book entitled Perceptrons by Marvin Minsky and Seymour Papert showed that it was impossible for these classes of network to learn an XOR function. It is often believed (incorrectly) that they also conjectured that a similar result would hold for a multi-layer perceptron network. However, this is not true, as both Minsky and Papert already knew that multi-layer perceptrons were capable of producing an XOR function. (See the page on Perceptrons (book) for more information.)  Nevertheless, the often-miscited Minsky/Papert text caused a significant decline in interest and funding of neural network research. It took ten more years until neural network research experienced a resurgence in the 1980s.  This text was reprinted in 1987 as ""Perceptrons - Expanded Edition"" where some errors in the original text are shown and corrected.
The kernel perceptron algorithm was already introduced in 1964 by Aizerman et al. Margin bounds guarantees were given for the Perceptron algorithm in the general non-separable case first by Freund and Schapire (1998), and more recently by Mohri and Rostamizadeh (2013) who extend previous results and give new L1 bounds.The perceptron is a simplified model of a biological neuron. While the complexity of biological neuron models is often required to fully understand neural behavior, research suggests a perceptron-like linear model can produce some behavior seen in real neurons.

Definition
In the modern sense, the perceptron is an algorithm for learning a binary classifier called a threshold function: a function that maps its input 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   (a real-valued vector) to an output value 
  
    
      
        f
        (
        
          x
        
        )
      
    
    {\displaystyle f(\mathbf {x} )}
   (a single binary value):

  
    
      
        f
        (
        
          x
        
        )
        =
        
          
            {
            
              
                
                  1
                
                
                  
                    if 
                  
                   
                  
                    w
                  
                  ⋅
                  
                    x
                  
                  +
                  b
                  >
                  0
                  ,
                
              
              
                
                  0
                
                
                  
                    otherwise
                  
                
              
            
            
          
        
      
    
    {\displaystyle f(\mathbf {x} )={\begin{cases}1&{\text{if }}\ \mathbf {w} \cdot \mathbf {x} +b>0,\\0&{\text{otherwise}}\end{cases}}}
  where 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   is a vector of real-valued weights, 
  
    
      
        
          w
        
        ⋅
        
          x
        
      
    
    {\displaystyle \mathbf {w} \cdot \mathbf {x} }
   is the dot product 
  
    
      
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          w
          
            i
          
        
        
          x
          
            i
          
        
      
    
    {\displaystyle \sum _{i=1}^{m}w_{i}x_{i}}
  , where m is the number of inputs to the perceptron, and b is the bias. The bias shifts the decision boundary away from the origin and does not depend on any input value.
The value of 
  
    
      
        f
        (
        
          x
        
        )
      
    
    {\displaystyle f(\mathbf {x} )}
   (0 or 1) is used to classify 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   as either a positive or a negative instance, in the case of a binary classification problem. If b is negative, then the weighted combination of inputs must produce a positive value greater than 
  
    
      
        
          |
        
        b
        
          |
        
      
    
    {\displaystyle |b|}
   in order to push the classifier neuron over the 0 threshold. Spatially, the bias alters the position (though not the orientation) of the decision boundary. The perceptron learning algorithm does not terminate if the learning set is not linearly separable. If the vectors are not linearly separable learning will never reach a point where all vectors are classified properly. The most famous example of the perceptron's inability to solve problems with linearly nonseparable vectors is the Boolean exclusive-or problem. The solution spaces of decision boundaries for all binary functions and learning behaviors are studied in the reference.In the context of neural networks, a perceptron is an artificial neuron using the Heaviside step function as the activation function. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from a multilayer perceptron, which is a misnomer for a more complicated neural network.  As a linear classifier, the single-layer perceptron is the simplest feedforward neural network.

Learning algorithm
Below is an example of a learning algorithm for a single-layer perceptron. For multilayer perceptrons, where a hidden layer exists, more sophisticated algorithms such as backpropagation must be used. If the activation function or the underlying process being modeled by the perceptron is nonlinear, alternative learning algorithms such as the delta rule can be used as long as the activation function is differentiable. Nonetheless, the learning algorithm described in the steps below will often work, even for multilayer perceptrons with nonlinear activation functions.
When multiple perceptrons are combined in an artificial neural network, each output neuron operates independently of all the others; thus, learning each output can be considered in isolation.

Definitions
We first define some variables:

r is the learning rate of the perceptron. Learning rate is between 0 and 1, larger values make the weight changes more volatile.

  
    
      
        y
        =
        f
        (
        
          z
        
        )
      
    
    {\displaystyle y=f(\mathbf {z} )}
   denotes the output from the perceptron for an input vector 
  
    
      
        
          z
        
      
    
    {\displaystyle \mathbf {z} }
  .

  
    
      
        D
        =
        {
        (
        
          
            x
          
          
            1
          
        
        ,
        
          d
          
            1
          
        
        )
        ,
        …
        ,
        (
        
          
            x
          
          
            s
          
        
        ,
        
          d
          
            s
          
        
        )
        }
      
    
    {\displaystyle D=\{(\mathbf {x} _{1},d_{1}),\dots ,(\mathbf {x} _{s},d_{s})\}}
   is the training set of 
  
    
      
        s
      
    
    {\displaystyle s}
   samples, where:

  
    
      
        
          
            x
          
          
            j
          
        
      
    
    {\displaystyle \mathbf {x} _{j}}
   is the 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional input vector.

  
    
      
        
          d
          
            j
          
        
      
    
    {\displaystyle d_{j}}
   is the desired output value of the perceptron for that input.We show the values of the features as follows:

  
    
      
        
          x
          
            j
            ,
            i
          
        
      
    
    {\displaystyle x_{j,i}}
   is the value of the 
  
    
      
        i
      
    
    {\displaystyle i}
  th feature of the 
  
    
      
        j
      
    
    {\displaystyle j}
  th training input vector.

  
    
      
        
          x
          
            j
            ,
            0
          
        
        =
        1
      
    
    {\displaystyle x_{j,0}=1}
  .To represent the weights: 

  
    
      
        
          w
          
            i
          
        
      
    
    {\displaystyle w_{i}}
   is the 
  
    
      
        i
      
    
    {\displaystyle i}
  th value in the weight vector, to be multiplied by the value of the 
  
    
      
        i
      
    
    {\displaystyle i}
  th input feature.
Because 
  
    
      
        
          x
          
            j
            ,
            0
          
        
        =
        1
      
    
    {\displaystyle x_{j,0}=1}
  , the 
  
    
      
        
          w
          
            0
          
        
      
    
    {\displaystyle w_{0}}
   is effectively a bias that we use instead of the bias constant 
  
    
      
        b
      
    
    {\displaystyle b}
  .To show the time-dependence of 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
  , we use:

  
    
      
        
          w
          
            i
          
        
        (
        t
        )
      
    
    {\displaystyle w_{i}(t)}
   is the weight 
  
    
      
        i
      
    
    {\displaystyle i}
   at time 
  
    
      
        t
      
    
    {\displaystyle t}
  .

Steps
The algorithm updates the weights after steps 2a and 2b. These weights are immediately applied to a pair in the training set, and subsequently updated, rather than waiting until all pairs in the training set have undergone these steps.

Convergence
The perceptron is a linear classifier, therefore it will never get to the state with all the input vectors classified correctly if the training set D is not linearly separable, i.e. if the positive examples cannot be separated from the negative examples by a hyperplane. In this case, no ""approximate"" solution will be gradually approached under the standard learning algorithm, but instead, learning will fail completely. Hence, if linear separability of the training set is not known a priori, one of the training variants below should be used.
If the training set is linearly separable, then the perceptron is guaranteed to converge. Furthermore, there is an upper bound on the number of times the perceptron will adjust its weights during the training.
Suppose that the input vectors from the two classes can be separated by a hyperplane with a margin 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  , i.e. there exists a weight vector 
  
    
      
        
          w
        
        ,
        
          |
        
        
          |
        
        
          w
        
        
          |
        
        
          |
        
        =
        1
      
    
    {\displaystyle \mathbf {w} ,||\mathbf {w} ||=1}
  , and a bias term b such that 
  
    
      
        
          w
        
        ⋅
        
          
            x
          
          
            j
          
        
        >
        γ
      
    
    {\displaystyle \mathbf {w} \cdot \mathbf {x} _{j}>\gamma }
   for all 
  
    
      
        j
      
    
    {\displaystyle j}
   with 
  
    
      
        
          d
          
            j
          
        
        =
        1
      
    
    {\displaystyle d_{j}=1}
   and 
  
    
      
        
          w
        
        ⋅
        
          
            x
          
          
            j
          
        
        <
        −
        γ
      
    
    {\displaystyle \mathbf {w} \cdot \mathbf {x} _{j}<-\gamma }
   for all 
  
    
      
        j
      
    
    {\displaystyle j}
   with 
  
    
      
        
          d
          
            j
          
        
        =
        0
      
    
    {\displaystyle d_{j}=0}
  , where 
  
    
      
        
          d
          
            j
          
        
      
    
    {\displaystyle d_{j}}
   is the desired output value of the perceptron for input 
  
    
      
        j
      
    
    {\displaystyle j}
  . Also, let R denote the maximum norm of an input vector. Novikoff (1962) proved that in this case the perceptron algorithm converges after making 
  
    
      
        O
        (
        
          R
          
            2
          
        
        
          /
        
        
          γ
          
            2
          
        
        )
      
    
    {\displaystyle O(R^{2}/\gamma ^{2})}
   updates. The idea of the proof is that the weight vector is always adjusted by a bounded amount in a direction with which it has a negative dot product, and thus can be bounded above by O(√t), where t is the number of changes to the weight vector. However, it can also be bounded below by O(t) because if there exists an (unknown) satisfactory weight vector, then every change makes progress in this (unknown) direction by a positive amount that depends only on the input vector.

While the perceptron algorithm is guaranteed to converge on some solution in the case of a linearly separable training set, it may still pick any solution and problems may admit many solutions of varying quality. The perceptron of optimal stability, nowadays better known as the linear support-vector machine, was designed to solve this problem (Krauth and Mezard, 1987).

Variants
The pocket algorithm with ratchet (Gallant, 1990) solves the stability problem of perceptron learning by keeping the best solution seen so far ""in its pocket"". The pocket algorithm then returns the solution in the pocket, rather than the last solution. It can be used also for non-separable data sets, where the aim is to find a perceptron with a small number of misclassifications. However, these solutions appear purely stochastically and hence the pocket algorithm neither approaches them gradually in the course of learning, nor are they guaranteed to show up within a given number of learning steps.
The Maxover algorithm (Wendemuth, 1995) is ""robust"" in the sense that it will converge regardless of (prior) knowledge of linear separability of the data set. In the linearly separable case, it will solve the training problem – if desired, even with optimal stability (maximum margin between the classes). For non-separable data sets, it will return a solution with a small number of misclassifications. In all cases, the algorithm gradually approaches the solution in the course of learning, without memorizing previous states and without stochastic jumps. Convergence is to global optimality for separable data sets and to local optimality for non-separable data sets.
The Voted Perceptron (Freund and Schapire, 1999), is a variant using multiple weighted perceptrons. The algorithm starts a new perceptron every time an example is wrongly classified, initializing the weights vector with the final weights of the last perceptron. Each perceptron will also be given another weight corresponding to how many examples do they correctly classify before wrongly classifying one, and at the end the output will be a weighted vote on all perceptrons.
In separable problems, perceptron training can also aim at finding the largest separating margin between the classes. The so-called perceptron of optimal stability can be determined by means of iterative training and optimization schemes, such as the Min-Over algorithm (Krauth and Mezard, 1987)  or the AdaTron (Anlauf and Biehl, 1989)). AdaTron uses the fact that the corresponding quadratic optimization problem is convex. The perceptron of optimal stability, together with the kernel trick, are the conceptual foundations of the support-vector machine.
The 
  
    
      
        α
      
    
    {\displaystyle \alpha }
  -perceptron further used a pre-processing layer of fixed random weights, with thresholded output units. This enabled the perceptron to classify analogue patterns, by projecting them into a binary space. In fact, for a projection space of sufficiently high dimension, patterns can become linearly separable.
Another way to solve nonlinear problems without using multiple layers is to use higher order networks (sigma-pi unit). In this type of network, each element in the input vector is extended with each pairwise combination of multiplied inputs (second order). This can be extended to an n-order network.
It should be kept in mind, however, that the best classifier is not necessarily that which classifies all the training data perfectly. Indeed, if we had the prior constraint that the data come from equi-variant Gaussian distributions, the linear separation in the input space is optimal, and the nonlinear solution is overfitted.
Other linear classification algorithms include Winnow, support-vector machine, and logistic regression.

Multiclass perceptron
Like most other techniques for training linear classifiers, the perceptron generalizes naturally to multiclass classification.  Here, the input 
  
    
      
        x
      
    
    {\displaystyle x}
   and the output 
  
    
      
        y
      
    
    {\displaystyle y}
   are drawn from arbitrary sets. A feature representation function 
  
    
      
        f
        (
        x
        ,
        y
        )
      
    
    {\displaystyle f(x,y)}
   maps each possible input/output pair to a finite-dimensional real-valued feature vector.  As before, the feature vector is multiplied by a weight vector 
  
    
      
        w
      
    
    {\displaystyle w}
  , but now the resulting score is used to choose among many possible outputs:

  
    
      
        
          
            
              y
              ^
            
          
        
        =
        
          argmax
          
            y
          
        
        ⁡
        f
        (
        x
        ,
        y
        )
        ⋅
        w
        .
      
    
    {\displaystyle {\hat {y}}=\operatorname {argmax} _{y}f(x,y)\cdot w.}
  Learning again iterates over the examples, predicting an output for each, leaving the weights unchanged when the predicted output matches the target, and changing them when it does not.  The update becomes:

  
    
      
        
          w
          
            t
            +
            1
          
        
        =
        
          w
          
            t
          
        
        +
        f
        (
        x
        ,
        y
        )
        −
        f
        (
        x
        ,
        
          
            
              y
              ^
            
          
        
        )
        .
      
    
    {\displaystyle w_{t+1}=w_{t}+f(x,y)-f(x,{\hat {y}}).}
  This multiclass feedback formulation reduces to the original perceptron when 
  
    
      
        x
      
    
    {\displaystyle x}
   is a real-valued vector, 
  
    
      
        y
      
    
    {\displaystyle y}
   is chosen from 
  
    
      
        {
        0
        ,
        1
        }
      
    
    {\displaystyle \{0,1\}}
  , and 
  
    
      
        f
        (
        x
        ,
        y
        )
        =
        y
        x
      
    
    {\displaystyle f(x,y)=yx}
  .
For certain problems, input/output representations and features can be chosen so that 
  
    
      
        
          
            a
            r
            g
            m
            a
            x
          
          
            y
          
        
        f
        (
        x
        ,
        y
        )
        ⋅
        w
      
    
    {\displaystyle \mathrm {argmax} _{y}f(x,y)\cdot w}
   can be found efficiently even though 
  
    
      
        y
      
    
    {\displaystyle y}
   is chosen from a very large or even infinite set.
Since 2002, perceptron training has become popular in the field of natural language processing for such tasks as part-of-speech tagging and syntactic parsing (Collins, 2002). It has also been applied to large-scale machine learning problems in a distributed computing setting.

References
Further reading
Aizerman, M. A. and Braverman, E. M. and Lev I. Rozonoer. Theoretical foundations of the potential function method in pattern recognition learning. Automation and Remote Control, 25:821–837, 1964.
Rosenblatt, Frank (1958), The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain, Cornell Aeronautical Laboratory, Psychological Review, v65, No. 6, pp. 386–408. doi:10.1037/h0042519.
Rosenblatt, Frank (1962), Principles of Neurodynamics. Washington, DC:Spartan Books.
Minsky M. L. and Papert S. A. 1969. Perceptrons. Cambridge, MA: MIT Press.
Gallant, S. I. (1990). Perceptron-based learning algorithms. IEEE Transactions on Neural Networks, vol. 1, no. 2, pp. 179–191.
Mohri, Mehryar and Rostamizadeh, Afshin (2013). Perceptron Mistake Bounds arXiv:1305.0208, 2013.
Novikoff, A. B. (1962). On convergence proofs on perceptrons. Symposium on the Mathematical Theory of Automata, 12, 615–622. Polytechnic Institute of Brooklyn.
Widrow, B., Lehr, M.A., ""30 years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation,"" Proc. IEEE, vol 78, no 9, pp. 1415–1442, (1990).
Collins, M. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with the perceptron algorithm in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP '02).
Yin, Hongfeng (1996), Perceptron-Based Algorithms and Analysis, Spectrum Library, Concordia University, Canada

External links
A Perceptron implemented in MATLAB to learn binary NAND function
Chapter 3 Weighted networks - the perceptron and chapter 4 Perceptron learning of Neural Networks - A Systematic Introduction by Raúl Rojas (ISBN 978-3-540-60505-8)
History of perceptrons
Mathematics of multilayer perceptrons
Visualize several perceptron variants learning in browser",https://en.wikipedia.org/wiki/Perceptron,"['Articles with example Python (programming language) code', 'Artificial neural networks', 'Classification algorithms', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NDL identifiers']",Data Science
148,Principal component analysis,"The principal components of a collection of points in a real p-space are a sequence of 
  
    
      
        p
      
    
    {\displaystyle p}
   direction vectors, where the 
  
    
      
        
          i
          
            th
          
        
      
    
    {\displaystyle i^{\text{th}}}
   vector is the direction of a line that best fits the data while being orthogonal to the first 
  
    
      
        i
        −
        1
      
    
    {\displaystyle i-1}
   vectors. Here, a best-fitting line is defined as one that minimizes the average squared distance from the points to the line. These directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Principal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest.
PCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The 
  
    
      
        
          i
          
            th
          
        
      
    
    {\displaystyle i^{\text{th}}}
   principal component can be taken as a direction orthogonal to the first 
  
    
      
        i
        −
        1
      
    
    {\displaystyle i-1}
   principal components that maximizes the variance of the projected data.
From either objective, it can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset. Robust and L1-norm-based variants of standard PCA have also been proposed.

History
PCA was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s. Depending on the field of application, it is also named the discrete Karhunen–Loève transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (invented in the last quarter of the 19th century), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch. 7 of Jolliffe's Principal Component Analysis), Eckart–Young theorem (Harman, 1960), or empirical orthogonal functions (EOF) in meteorological science, empirical eigenfunction decomposition (Sirovich, 1987), empirical component analysis (Lorenz, 1956), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics.

Intuition
PCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small.
To find the axes of the ellipsoid, we must first subtract the mean of each variable from the dataset to center the data around the origin. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to turn them into unit vectors. Once this is done, each of the mutually orthogonal, unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform our covariance matrix into a diagonalised form with the diagonal elements representing the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.

Details
PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.Consider an 
  
    
      
        n
        ×
        p
      
    
    {\displaystyle n\times p}
   data matrix, X, with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), where each of the n rows represents a different repetition of the experiment, and each of the p columns gives a particular kind of feature (say, the results from a particular sensor).
Mathematically, the transformation is defined by a set of size 
  
    
      
        l
      
    
    {\displaystyle l}
   of p-dimensional vectors of weights or coefficients 
  
    
      
        
          
            w
          
          
            (
            k
            )
          
        
        =
        (
        
          w
          
            1
          
        
        ,
        …
        ,
        
          w
          
            p
          
        
        
          )
          
            (
            k
            )
          
        
      
    
    {\displaystyle \mathbf {w} _{(k)}=(w_{1},\dots ,w_{p})_{(k)}}
   that map each row vector 
  
    
      
        
          
            x
          
          
            (
            i
            )
          
        
      
    
    {\displaystyle \mathbf {x} _{(i)}}
   of X to a new vector of principal component scores 
  
    
      
        
          
            t
          
          
            (
            i
            )
          
        
        =
        (
        
          t
          
            1
          
        
        ,
        …
        ,
        
          t
          
            l
          
        
        
          )
          
            (
            i
            )
          
        
      
    
    {\displaystyle \mathbf {t} _{(i)}=(t_{1},\dots ,t_{l})_{(i)}}
  , given by

  
    
      
        
          
            
              t
              
                k
              
            
          
          
            (
            i
            )
          
        
        =
        
          
            x
          
          
            (
            i
            )
          
        
        ⋅
        
          
            w
          
          
            (
            k
            )
          
        
        
        
          f
          o
          r
        
        
        i
        =
        1
        ,
        …
        ,
        n
        
        k
        =
        1
        ,
        …
        ,
        l
      
    
    {\displaystyle {t_{k}}_{(i)}=\mathbf {x} _{(i)}\cdot \mathbf {w} _{(k)}\qquad \mathrm {for} \qquad i=1,\dots ,n\qquad k=1,\dots ,l}
  in such a way that the individual variables 
  
    
      
        
          t
          
            1
          
        
        ,
        …
        ,
        
          t
          
            l
          
        
      
    
    {\displaystyle t_{1},\dots ,t_{l}}
   of t considered over the data set successively inherit the maximum possible variance from X, with each coefficient vector w constrained to be a unit vector (where 
  
    
      
        l
      
    
    {\displaystyle l}
   is usually selected to be less than 
  
    
      
        p
      
    
    {\displaystyle p}
   to reduce dimensionality).

First component
In order to maximize variance, the first weight vector w(1) thus has to satisfy

  
    
      
        
          
            w
          
          
            (
            1
            )
          
        
        =
        
          
            
              arg
              
              m
              a
              x
            
            
              ‖
              
                w
              
              ‖
              =
              1
            
          
        
        
        
          {
          
            
              ∑
              
                i
              
            
            (
            
              t
              
                1
              
            
            
              )
              
                (
                i
                )
              
              
                2
              
            
          
          }
        
        =
        
          
            
              arg
              
              m
              a
              x
            
            
              ‖
              
                w
              
              ‖
              =
              1
            
          
        
        
        
          {
          
            
              ∑
              
                i
              
            
            
              
                (
                
                  
                    
                      x
                    
                    
                      (
                      i
                      )
                    
                  
                  ⋅
                  
                    w
                  
                
                )
              
              
                2
              
            
          
          }
        
      
    
    {\displaystyle \mathbf {w} _{(1)}={\underset {\Vert \mathbf {w} \Vert =1}{\operatorname {\arg \,max} }}\,\left\{\sum _{i}(t_{1})_{(i)}^{2}\right\}={\underset {\Vert \mathbf {w} \Vert =1}{\operatorname {\arg \,max} }}\,\left\{\sum _{i}\left(\mathbf {x} _{(i)}\cdot \mathbf {w} \right)^{2}\right\}}
  Equivalently, writing this in matrix form gives

  
    
      
        
          
            w
          
          
            (
            1
            )
          
        
        =
        
          
            
              arg
              
              m
              a
              x
            
            
              ‖
              
                w
              
              ‖
              =
              1
            
          
        
        
        {
        ‖
        
          X
          w
        
        
          ‖
          
            2
          
        
        }
        =
        
          
            
              arg
              
              m
              a
              x
            
            
              ‖
              
                w
              
              ‖
              =
              1
            
          
        
        
        
          {
          
            
              
                w
              
              
                T
              
            
            
              
                X
                
                  T
                
              
            
            
              X
              w
            
          
          }
        
      
    
    {\displaystyle \mathbf {w} _{(1)}={\underset {\Vert \mathbf {w} \Vert =1}{\operatorname {\arg \,max} }}\,\{\Vert \mathbf {Xw} \Vert ^{2}\}={\underset {\Vert \mathbf {w} \Vert =1}{\operatorname {\arg \,max} }}\,\left\{\mathbf {w} ^{T}\mathbf {X^{T}} \mathbf {Xw} \right\}}
  Since w(1) has been defined to be a unit vector, it equivalently also satisfies

  
    
      
        
          
            w
          
          
            (
            1
            )
          
        
        =
        
          
            arg
            
            m
            a
            x
          
        
        
        
          {
          
            
              
                
                  
                    w
                  
                  
                    T
                  
                
                
                  
                    X
                    
                      T
                    
                  
                
                
                  X
                  w
                
              
              
                
                  
                    w
                  
                  
                    T
                  
                
                
                  w
                
              
            
          
          }
        
      
    
    {\displaystyle \mathbf {w} _{(1)}={\operatorname {\arg \,max} }\,\left\{{\frac {\mathbf {w} ^{T}\mathbf {X^{T}} \mathbf {Xw} }{\mathbf {w} ^{T}\mathbf {w} }}\right\}}
  The quantity to be maximised can be recognised as a Rayleigh quotient. A standard result for a positive semidefinite matrix such as XTX is that the quotient's maximum possible value is the largest eigenvalue of the matrix, which occurs when w is the corresponding eigenvector.
With w(1) found, the first principal component of a data vector x(i) can then be given as a score t1(i) = x(i) ⋅ w(1) in the transformed co-ordinates, or as the corresponding vector in the original variables, {x(i) ⋅ w(1)} w(1).

Further components
The kth component can be found by subtracting the first k − 1 principal components from X:

  
    
      
        
          
            
              
                X
                ^
              
            
          
          
            k
          
        
        =
        
          X
        
        −
        
          ∑
          
            s
            =
            1
          
          
            k
            −
            1
          
        
        
          X
        
        
          
            w
          
          
            (
            s
            )
          
        
        
          
            w
          
          
            (
            s
            )
          
          
            
              T
            
          
        
      
    
    {\displaystyle \mathbf {\hat {X}} _{k}=\mathbf {X} -\sum _{s=1}^{k-1}\mathbf {X} \mathbf {w} _{(s)}\mathbf {w} _{(s)}^{\rm {T}}}
  and then finding the weight vector which extracts the maximum variance from this new data matrix

  
    
      
        
          
            w
          
          
            (
            k
            )
          
        
        =
        
          
            
              a
              r
              g
              
              m
              a
              x
            
            
              ‖
              
                w
              
              ‖
              =
              1
            
          
        
        
          {
          
            ‖
            
              
                
                  
                    X
                    ^
                  
                
              
              
                k
              
            
            
              w
            
            
              ‖
              
                2
              
            
          
          }
        
        =
        
          
            arg
            
            m
            a
            x
          
        
        
        
          {
          
            
              
                
                  
                    
                      w
                    
                    
                      T
                    
                  
                  
                    
                      
                        
                          X
                          ^
                        
                      
                    
                    
                      k
                    
                    
                      T
                    
                  
                  
                    
                      
                        
                          X
                          ^
                        
                      
                    
                    
                      k
                    
                  
                  
                    w
                  
                
                
                  
                    
                      w
                    
                    
                      T
                    
                  
                  
                    w
                  
                
              
            
          
          }
        
      
    
    {\displaystyle \mathbf {w} _{(k)}={\underset {\Vert \mathbf {w} \Vert =1}{\operatorname {arg\,max} }}\left\{\Vert \mathbf {\hat {X}} _{k}\mathbf {w} \Vert ^{2}\right\}={\operatorname {\arg \,max} }\,\left\{{\tfrac {\mathbf {w} ^{T}\mathbf {\hat {X}} _{k}^{T}\mathbf {\hat {X}} _{k}\mathbf {w} }{\mathbf {w} ^{T}\mathbf {w} }}\right\}}
  It turns out that this gives the remaining eigenvectors of XTX, with the maximum values for the quantity in brackets given by their corresponding eigenvalues. Thus the weight vectors are eigenvectors of XTX.
The kth principal component of a data vector x(i) can therefore be given as a score tk(i) = x(i) ⋅ w(k) in the transformed co-ordinates, or as the corresponding vector in the space of the original variables, {x(i) ⋅ w(k)} w(k), where w(k) is the kth eigenvector of XTX.
The full principal components decomposition of X can therefore be given as

  
    
      
        
          T
        
        =
        
          X
        
        
          W
        
      
    
    {\displaystyle \mathbf {T} =\mathbf {X} \mathbf {W} }
  where W is a p-by-p matrix of weights whose columns are the eigenvectors of XTX. The transpose of W is sometimes called the whitening or sphering transformation. Columns of W multiplied by the square root of corresponding eigenvalues, that is, eigenvectors scaled up by the variances, are called loadings in PCA or in Factor analysis.

Covariances
XTX itself can be recognised as proportional to the empirical sample covariance matrix of the dataset XT.The sample covariance Q between two of the different principal components over the dataset is given by:

  
    
      
        
          
            
              
                Q
                (
                
                  
                    P
                    C
                  
                  
                    (
                    j
                    )
                  
                
                ,
                
                  
                    P
                    C
                  
                  
                    (
                    k
                    )
                  
                
                )
              
              
                
                ∝
                (
                
                  X
                
                
                  
                    w
                  
                  
                    (
                    j
                    )
                  
                
                
                  )
                  
                    T
                  
                
                (
                
                  X
                
                
                  
                    w
                  
                  
                    (
                    k
                    )
                  
                
                )
              
            
            
              
              
                
                =
                
                  
                    w
                  
                  
                    (
                    j
                    )
                  
                  
                    T
                  
                
                
                  
                    X
                  
                  
                    T
                  
                
                
                  X
                
                
                  
                    w
                  
                  
                    (
                    k
                    )
                  
                
              
            
            
              
              
                
                =
                
                  
                    w
                  
                  
                    (
                    j
                    )
                  
                  
                    T
                  
                
                
                  λ
                  
                    (
                    k
                    )
                  
                
                
                  
                    w
                  
                  
                    (
                    k
                    )
                  
                
              
            
            
              
              
                
                =
                
                  λ
                  
                    (
                    k
                    )
                  
                
                
                  
                    w
                  
                  
                    (
                    j
                    )
                  
                  
                    T
                  
                
                
                  
                    w
                  
                  
                    (
                    k
                    )
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}Q(\mathrm {PC} _{(j)},\mathrm {PC} _{(k)})&\propto (\mathbf {X} \mathbf {w} _{(j)})^{T}(\mathbf {X} \mathbf {w} _{(k)})\\&=\mathbf {w} _{(j)}^{T}\mathbf {X} ^{T}\mathbf {X} \mathbf {w} _{(k)}\\&=\mathbf {w} _{(j)}^{T}\lambda _{(k)}\mathbf {w} _{(k)}\\&=\lambda _{(k)}\mathbf {w} _{(j)}^{T}\mathbf {w} _{(k)}\end{aligned}}}
  where the eigenvalue property of w(k) has been used to move from line 2 to line 3. However eigenvectors w(j) and w(k) corresponding to eigenvalues of a symmetric matrix are orthogonal (if the eigenvalues are different), or can be orthogonalised (if the vectors happen to share an equal repeated value). The product in the final line is therefore zero; there is no sample covariance between different principal components over the dataset.
Another way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix.
In matrix form, the empirical covariance matrix for the original variables can be written

  
    
      
        
          Q
        
        ∝
        
          
            X
          
          
            T
          
        
        
          X
        
        =
        
          W
        
        
          Λ
        
        
          
            W
          
          
            T
          
        
      
    
    {\displaystyle \mathbf {Q} \propto \mathbf {X} ^{T}\mathbf {X} =\mathbf {W} \mathbf {\Lambda } \mathbf {W} ^{T}}
  The empirical covariance matrix between the principal components becomes

  
    
      
        
          
            W
          
          
            T
          
        
        
          Q
        
        
          W
        
        ∝
        
          
            W
          
          
            T
          
        
        
          W
        
        
        
          Λ
        
        
        
          
            W
          
          
            T
          
        
        
          W
        
        =
        
          Λ
        
      
    
    {\displaystyle \mathbf {W} ^{T}\mathbf {Q} \mathbf {W} \propto \mathbf {W} ^{T}\mathbf {W} \,\mathbf {\Lambda } \,\mathbf {W} ^{T}\mathbf {W} =\mathbf {\Lambda } }
  where Λ is the diagonal matrix of eigenvalues λ(k) of XTX. λ(k) is equal to the sum of the squares over the dataset associated with each component k, that is, λ(k) = Σi tk2(i) = Σi (x(i) ⋅ w(k))2.

Dimensionality reduction
The transformation T = X W maps a data vector x(i) from an original space of p variables to a new space of p variables which are uncorrelated over the dataset. However, not all the principal components need to be kept. Keeping only the first L principal components, produced by using only the first L eigenvectors, gives the truncated transformation

  
    
      
        
          
            T
          
          
            L
          
        
        =
        
          X
        
        
          
            W
          
          
            L
          
        
      
    
    {\displaystyle \mathbf {T} _{L}=\mathbf {X} \mathbf {W} _{L}}
  where the matrix TL now has n rows but only L columns. In other words, PCA learns a linear transformation 
  
    
      
        t
        =
        
          W
          
            T
          
        
        x
        ,
        x
        ∈
        
          R
          
            p
          
        
        ,
        t
        ∈
        
          R
          
            L
          
        
        ,
      
    
    {\displaystyle t=W^{T}x,x\in R^{p},t\in R^{L},}
   where the columns of p × L matrix W form an orthogonal basis for the L features (the components of representation t) that are decorrelated. By construction, of all the transformed data matrices with only L columns, this score matrix maximises the variance in the original data that has been preserved, while minimising the total squared reconstruction error 
  
    
      
        ‖
        
          T
        
        
          
            W
          
          
            T
          
        
        −
        
          
            T
          
          
            L
          
        
        
          
            W
          
          
            L
          
          
            T
          
        
        
          ‖
          
            2
          
          
            2
          
        
      
    
    {\displaystyle \|\mathbf {T} \mathbf {W} ^{T}-\mathbf {T} _{L}\mathbf {W} _{L}^{T}\|_{2}^{2}}
   or 
  
    
      
        ‖
        
          X
        
        −
        
          
            X
          
          
            L
          
        
        
          ‖
          
            2
          
          
            2
          
        
      
    
    {\displaystyle \|\mathbf {X} -\mathbf {X} _{L}\|_{2}^{2}}
  .

Such dimensionality reduction can be a very useful step for visualising and processing high-dimensional datasets, while still retaining as much of the variance in the dataset as possible. For example, selecting L = 2 and keeping only the first two principal components finds the two-dimensional plane through the high-dimensional dataset in which the data is most spread out, so if the data contains clusters these too may be most spread out, and therefore most visible to be plotted out in a two-dimensional diagram; whereas if two directions through the data (or two of the original variables) are chosen at random, the clusters may be much less spread apart from each other, and may in fact be much more likely to substantially overlay each other, making them indistinguishable.
Similarly, in regression analysis, the larger the number of explanatory variables allowed, the greater is the chance of overfitting the model, producing conclusions that fail to generalise to other datasets. One approach, especially when there are strong correlations between different possible explanatory variables, is to reduce them to a few principal components and then run the regression against them, a method called principal component regression.
Dimensionality reduction may also be appropriate when the variables in a dataset are noisy. If each column of the dataset contains independent identically distributed Gaussian noise, then the columns of T will also contain similarly identically distributed Gaussian noise (such a distribution is invariant under the effects of the matrix W, which can be thought of as a high-dimensional rotation of the co-ordinate axes). However, with more of the total variance concentrated in the first few principal components compared to the same noise variance, the proportionate effect of the noise is less—the first few components achieve a higher signal-to-noise ratio. PCA thus can have the effect of concentrating much of the signal into the first few principal components, which can usefully be captured by dimensionality reduction; while the later principal components may be dominated by noise, and so disposed of without great loss. If the dataset is not too large, the significance of the principal components can be tested using parametric bootstrap, as an aid in determining how many principal components to retain.

Singular value decomposition
The principal components transformation can also be associated with another matrix factorization, the singular value decomposition (SVD) of X,

  
    
      
        
          X
        
        =
        
          U
        
        
          Σ
        
        
          
            W
          
          
            T
          
        
      
    
    {\displaystyle \mathbf {X} =\mathbf {U} \mathbf {\Sigma } \mathbf {W} ^{T}}
  Here Σ is an n-by-p rectangular diagonal matrix of positive numbers σ(k), called the singular values of X; U is an n-by-n matrix, the columns of which are orthogonal unit vectors of length n called the left singular vectors of X; and W is a p-by-p whose columns are orthogonal unit vectors of length p and called the right singular vectors of X.
In terms of this factorization, the matrix XTX can be written

  
    
      
        
          
            
              
                
                  
                    X
                  
                  
                    T
                  
                
                
                  X
                
              
              
                
                =
                
                  W
                
                
                  
                    Σ
                  
                  
                    T
                  
                
                
                  
                    U
                  
                  
                    T
                  
                
                
                  U
                
                
                  Σ
                
                
                  
                    W
                  
                  
                    T
                  
                
              
            
            
              
              
                
                =
                
                  W
                
                
                  
                    Σ
                  
                  
                    T
                  
                
                
                  Σ
                
                
                  
                    W
                  
                  
                    T
                  
                
              
            
            
              
              
                
                =
                
                  W
                
                
                  
                    
                      
                        Σ
                        ^
                      
                    
                  
                  
                    2
                  
                
                
                  
                    W
                  
                  
                    T
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\mathbf {X} ^{T}\mathbf {X} &=\mathbf {W} \mathbf {\Sigma } ^{T}\mathbf {U} ^{T}\mathbf {U} \mathbf {\Sigma } \mathbf {W} ^{T}\\&=\mathbf {W} \mathbf {\Sigma } ^{T}\mathbf {\Sigma } \mathbf {W} ^{T}\\&=\mathbf {W} \mathbf {\hat {\Sigma }} ^{2}\mathbf {W} ^{T}\end{aligned}}}
  where  
  
    
      
        
          
            
              Σ
              ^
            
          
        
      
    
    {\displaystyle \mathbf {\hat {\Sigma }} }
   is the square diagonal matrix with the singular values of  X  and the excess zeros chopped off that satisfies  
  
    
      
        
          
            
              
                
                  Σ
                  ^
                
              
            
            
              2
            
          
        
        =
        
          
            Σ
          
          
            T
          
        
        
          Σ
        
      
    
    {\displaystyle \mathbf {{\hat {\Sigma }}^{2}} =\mathbf {\Sigma } ^{T}\mathbf {\Sigma } }
  . Comparison with the eigenvector factorization of XTX establishes that the right singular vectors W of X are equivalent to the eigenvectors of XTX, while the singular values σ(k) of  
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
   are equal to the square-root of the eigenvalues λ(k) of XTX.
Using the singular value decomposition the score matrix T can be written

  
    
      
        
          
            
              
                
                  T
                
              
              
                
                =
                
                  X
                
                
                  W
                
              
            
            
              
              
                
                =
                
                  U
                
                
                  Σ
                
                
                  
                    W
                  
                  
                    T
                  
                
                
                  W
                
              
            
            
              
              
                
                =
                
                  U
                
                
                  Σ
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\mathbf {T} &=\mathbf {X} \mathbf {W} \\&=\mathbf {U} \mathbf {\Sigma } \mathbf {W} ^{T}\mathbf {W} \\&=\mathbf {U} \mathbf {\Sigma } \end{aligned}}}
  so each column of T is given by one of the left singular vectors of X multiplied by the corresponding singular value. This form is also the polar decomposition of T.
Efficient algorithms exist to calculate the SVD of X without having to form the matrix XTX, so computing the SVD is now the standard way to calculate a principal components analysis from a data matrix, unless only a handful of components are required.
As with the eigen-decomposition, a truncated n × L score matrix TL can be obtained by considering only the first L largest singular values and their singular vectors:

  
    
      
        
          
            T
          
          
            L
          
        
        =
        
          
            U
          
          
            L
          
        
        
          
            Σ
          
          
            L
          
        
        =
        
          X
        
        
          
            W
          
          
            L
          
        
      
    
    {\displaystyle \mathbf {T} _{L}=\mathbf {U} _{L}\mathbf {\Sigma } _{L}=\mathbf {X} \mathbf {W} _{L}}
  The truncation of a matrix M or T using a truncated singular value decomposition in this way produces a truncated matrix that is the nearest possible matrix of rank L to the original matrix, in the sense of the difference between the two having the smallest possible Frobenius norm, a result known as the Eckart–Young theorem [1936].

Further considerations
Given a set of points in Euclidean space, the first principal component corresponds to a line that passes through the multidimensional mean and minimizes the sum of squares of the distances of the points from the line. The second principal component corresponds to the same concept after all correlation with the first principal component has been subtracted from the points. The singular values (in Σ) are the square roots of the eigenvalues of the matrix XTX. Each eigenvalue is proportional to the portion of the ""variance"" (more correctly of the sum of the squared distances of the points from their multidimensional mean) that is associated with each eigenvector. The sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean. PCA essentially rotates the set of points around their mean in order to align with the principal components. This moves as much of the variance as possible (using an orthogonal transformation) into the first few dimensions. The values in the remaining dimensions, therefore, tend to be small and may be dropped with minimal loss of information (see below). PCA is often used in this manner for dimensionality reduction. PCA has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest ""variance"" (as defined above). This advantage, however, comes at the price of greater computational requirements if compared, for example, and when applicable, to the discrete cosine transform, and in particular to the DCT-II which is simply known as the ""DCT"". Nonlinear dimensionality reduction techniques tend to be more computationally demanding than PCA.
PCA is sensitive to the scaling of the variables. If we have just two variables and they have the same sample variance and are positively correlated, then the PCA will entail a rotation by 45° and the ""weights"" (they are the cosines of rotation) for the two variables with respect to the principal component will be equal. But if we multiply all values of the first variable by 100, then the first principal component will be almost the same as that variable, with a small contribution from the other variable, whereas the second component will be almost aligned with the second original variable. This means that whenever the different variables have different units (like temperature and mass), PCA is a somewhat arbitrary method of analysis. (Different results would be obtained if one used Fahrenheit rather than Celsius for example.) Pearson's original paper was entitled ""On Lines and Planes of Closest Fit to Systems of Points in Space"" – ""in space"" implies physical Euclidean space where such concerns do not arise. One way of making the PCA less arbitrary is to use variables scaled so as to have unit variance, by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as a basis for PCA. However, this compresses (or expands) the fluctuations in all dimensions of the signal space to unit variance.
Mean subtraction (a.k.a. ""mean centering"") is necessary for performing classical PCA to ensure that the first principal component describes the direction of maximum variance. If mean subtraction is not performed, the first principal component might instead correspond more or less to the mean of the data. A mean of zero is needed for finding a basis that minimizes the mean square error of the approximation of the data.Mean-centering is unnecessary if performing a principal components analysis on a correlation matrix, as the data are already centered after calculating correlations. Correlations are derived from the cross-product of two standard scores (Z-scores) or statistical moments (hence the name: Pearson Product-Moment Correlation). Also see the article by Kromrey & Foster-Johnson (1998) on ""Mean-centering in Moderated Regression: Much Ado About Nothing"".
PCA is a popular primary technique in pattern recognition. It is not, however, optimized for class separability. However, it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting Euclidean distance between center of mass of two or more classes. The linear discriminant analysis is an alternative which is optimized for class separability.

Table of symbols and abbreviations
Properties and limitations of PCA
Properties
Some properties of PCA include:
Property 1: For any integer q, 1 ≤ q ≤ p, consider the orthogonal linear transformation

  
    
      
        y
        =
        
          
            B
            ′
          
        
        x
      
    
    {\displaystyle y=\mathbf {B'} x}
  
where 
  
    
      
        y
      
    
    {\displaystyle y}
   is a q-element vector and 
  
    
      
        
          
            B
            ′
          
        
      
    
    {\displaystyle \mathbf {B'} }
   is a (q × p) matrix, and let 
  
    
      
        
          
            Σ
          
          
            y
          
        
        =
        
          
            B
            ′
          
        
        
          Σ
        
        
          B
        
      
    
    {\displaystyle \mathbf {\Sigma } _{y}=\mathbf {B'} \mathbf {\Sigma } \mathbf {B} }
   be the variance-covariance matrix for 
  
    
      
        y
      
    
    {\displaystyle y}
  . Then the trace of 
  
    
      
        
          
            Σ
          
          
            y
          
        
      
    
    {\displaystyle \mathbf {\Sigma } _{y}}
  , denoted 
  
    
      
        tr
        ⁡
        (
        
          
            Σ
          
          
            y
          
        
        )
      
    
    {\displaystyle \operatorname {tr} (\mathbf {\Sigma } _{y})}
  , is maximized by taking 
  
    
      
        
          B
        
        =
        
          
            A
          
          
            q
          
        
      
    
    {\displaystyle \mathbf {B} =\mathbf {A} _{q}}
  , where 
  
    
      
        
          
            A
          
          
            q
          
        
      
    
    {\displaystyle \mathbf {A} _{q}}
   consists of the first q columns of 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbf {A} }
   
  
    
      
        (
        
          
            B
            ′
          
        
      
    
    {\displaystyle (\mathbf {B'} }
   is the transposition of 
  
    
      
        
          B
        
        )
      
    
    {\displaystyle \mathbf {B} )}
  .Property 2: Consider again the orthonormal transformation

  
    
      
        y
        =
        
          
            B
            ′
          
        
        x
      
    
    {\displaystyle y=\mathbf {B'} x}
  
with 
  
    
      
        x
        ,
        
          B
        
        ,
        
          A
        
      
    
    {\displaystyle x,\mathbf {B} ,\mathbf {A} }
   and 
  
    
      
        
          
            Σ
          
          
            y
          
        
      
    
    {\displaystyle \mathbf {\Sigma } _{y}}
   defined as before. Then 
  
    
      
        tr
        ⁡
        (
        
          
            Σ
          
          
            y
          
        
        )
      
    
    {\displaystyle \operatorname {tr} (\mathbf {\Sigma } _{y})}
   is minimized by taking 
  
    
      
        
          B
        
        =
        
          
            A
          
          
            q
          
          
            ∗
          
        
        ,
      
    
    {\displaystyle \mathbf {B} =\mathbf {A} _{q}^{*},}
   where 
  
    
      
        
          
            A
          
          
            q
          
          
            ∗
          
        
      
    
    {\displaystyle \mathbf {A} _{q}^{*}}
   consists of the last q columns of 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbf {A} }
  .The statistical implication of this property is that the last few PCs are not simply unstructured left-overs after removing the important PCs. Because these last PCs have variances as small as possible they are useful in their own right. They can help to detect unsuspected near-constant linear relationships between the elements of x, and they may also be useful in regression, in selecting a subset of variables from x, and in outlier detection.

Property 3: (Spectral decomposition of Σ)

  
    
      
        
          Σ
        
        =
        
          λ
          
            1
          
        
        
          α
          
            1
          
        
        
          α
          
            1
          
          ′
        
        +
        ⋯
        +
        
          λ
          
            p
          
        
        
          α
          
            p
          
        
        
          α
          
            p
          
          ′
        
      
    
    {\displaystyle \mathbf {\Sigma } =\lambda _{1}\alpha _{1}\alpha _{1}'+\cdots +\lambda _{p}\alpha _{p}\alpha _{p}'}
  Before we look at its usage, we first look at diagonal elements,

  
    
      
        Var
        ⁡
        (
        
          x
          
            j
          
        
        )
        =
        
          ∑
          
            k
            =
            1
          
          
            P
          
        
        
          λ
          
            k
          
        
        
          α
          
            k
            j
          
          
            2
          
        
      
    
    {\displaystyle \operatorname {Var} (x_{j})=\sum _{k=1}^{P}\lambda _{k}\alpha _{kj}^{2}}
  Then, perhaps the main statistical implication of the result is that not only can we decompose the combined variances of all the elements of x into decreasing contributions due to each PC, but we can also decompose the whole covariance matrix into contributions 
  
    
      
        
          λ
          
            k
          
        
        
          α
          
            k
          
        
        
          α
          
            k
          
          ′
        
      
    
    {\displaystyle \lambda _{k}\alpha _{k}\alpha _{k}'}
   from each PC. Although not strictly decreasing, the elements of 
  
    
      
        
          λ
          
            k
          
        
        
          α
          
            k
          
        
        
          α
          
            k
          
          ′
        
      
    
    {\displaystyle \lambda _{k}\alpha _{k}\alpha _{k}'}
   will tend to become smaller as 
  
    
      
        k
      
    
    {\displaystyle k}
   increases, as 
  
    
      
        
          λ
          
            k
          
        
        
          α
          
            k
          
        
        
          α
          
            k
          
          ′
        
      
    
    {\displaystyle \lambda _{k}\alpha _{k}\alpha _{k}'}
   is nonincreasing for increasing 
  
    
      
        k
      
    
    {\displaystyle k}
  , whereas the elements of 
  
    
      
        
          α
          
            k
          
        
      
    
    {\displaystyle \alpha _{k}}
   tend to stay about the same size because of the normalization constraints: 
  
    
      
        
          α
          
            k
          
          ′
        
        
          α
          
            k
          
        
        =
        1
        ,
        k
        =
        1
        ,
        …
        ,
        p
      
    
    {\displaystyle \alpha _{k}'\alpha _{k}=1,k=1,\dots ,p}
  .

Limitations
As noted above, the results of PCA depend on the scaling of the variables. This can be cured by scaling each feature by its standard deviation, so that one ends up with dimensionless features with unital variance.The applicability of PCA as described above is limited by certain (tacit) assumptions made in its derivation. In particular, PCA can capture linear correlations between the features but fails when this assumption is violated (see Figure 6a in the reference). In some cases, coordinate transformations can restore the linearity assumption and PCA can then be applied (see kernel PCA).
Another limitation is the mean-removal process before constructing the covariance matrix for PCA. In fields such as astronomy, all the signals are non-negative, and the mean-removal process will force the mean of some astrophysical exposures to be zero, which consequently creates unphysical negative fluxes, and forward modeling has to be performed to recover the true magnitude of the signals. As an alternative method, non-negative matrix factorization focusing only on the non-negative elements in the matrices, which is well-suited for astrophysical observations. See more at Relation between PCA and Non-negative Matrix Factorization.

PCA and information theory
Dimensionality reduction loses information, in general. PCA-based dimensionality reduction tends to minimize that information loss, under certain signal and noise models.
Under the assumption that

  
    
      
        
          x
        
        =
        
          s
        
        +
        
          n
        
        ,
      
    
    {\displaystyle \mathbf {x} =\mathbf {s} +\mathbf {n} ,}
  that is, that the data vector 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   is the sum of the desired information-bearing signal 
  
    
      
        
          s
        
      
    
    {\displaystyle \mathbf {s} }
   and a noise signal 
  
    
      
        
          n
        
      
    
    {\displaystyle \mathbf {n} }
   one can show that PCA can be optimal for dimensionality reduction, from an information-theoretic point-of-view.
In particular, Linsker showed that if 
  
    
      
        
          s
        
      
    
    {\displaystyle \mathbf {s} }
   is Gaussian and 
  
    
      
        
          n
        
      
    
    {\displaystyle \mathbf {n} }
   is Gaussian noise with a covariance matrix proportional to the identity matrix, the PCA maximizes the mutual information 
  
    
      
        I
        (
        
          y
        
        ;
        
          s
        
        )
      
    
    {\displaystyle I(\mathbf {y} ;\mathbf {s} )}
   between the desired information 
  
    
      
        
          s
        
      
    
    {\displaystyle \mathbf {s} }
   and the dimensionality-reduced output 
  
    
      
        
          y
        
        =
        
          
            W
          
          
            L
          
          
            T
          
        
        
          x
        
      
    
    {\displaystyle \mathbf {y} =\mathbf {W} _{L}^{T}\mathbf {x} }
  .If the noise is still Gaussian and has a covariance matrix proportional to the identity matrix (that is, the components of the vector 
  
    
      
        
          n
        
      
    
    {\displaystyle \mathbf {n} }
   are iid), but the information-bearing signal 
  
    
      
        
          s
        
      
    
    {\displaystyle \mathbf {s} }
   is non-Gaussian (which is a common scenario), PCA at least minimizes an upper bound on the information loss, which is defined as

  
    
      
        I
        (
        
          x
        
        ;
        
          s
        
        )
        −
        I
        (
        
          y
        
        ;
        
          s
        
        )
        .
      
    
    {\displaystyle I(\mathbf {x} ;\mathbf {s} )-I(\mathbf {y} ;\mathbf {s} ).}
  The optimality of PCA is also preserved if the noise 
  
    
      
        
          n
        
      
    
    {\displaystyle \mathbf {n} }
   is iid and at least more Gaussian (in terms of the Kullback–Leibler divergence) than the information-bearing signal 
  
    
      
        
          s
        
      
    
    {\displaystyle \mathbf {s} }
  . In general, even if the above signal model holds, PCA loses its information-theoretic optimality as soon as the noise 
  
    
      
        
          n
        
      
    
    {\displaystyle \mathbf {n} }
   becomes dependent.

Computing PCA using the covariance method
The following is a detailed description of PCA using the covariance method (see also here) as opposed to the correlation method.The goal is to transform a given data set X of dimension p to an alternative data set Y of smaller dimension L. Equivalently, we are seeking to find the matrix Y, where Y is the Karhunen–Loève transform (KLT) of matrix X:

  
    
      
        
          Y
        
        =
        
          K
          L
          T
        
        {
        
          X
        
        }
      
    
    {\displaystyle \mathbf {Y} =\mathbb {KLT} \{\mathbf {X} \}}

Organize the data set
Suppose you have data comprising a set of observations of p variables, and you want to reduce the data so that each observation can be described with only L variables, L < p. Suppose further, that the data are arranged as a set of n data vectors 
  
    
      
        
          
            x
          
          
            1
          
        
        …
        
          
            x
          
          
            n
          
        
      
    
    {\displaystyle \mathbf {x} _{1}\ldots \mathbf {x} _{n}}
   with each 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   representing a single grouped observation of the p variables.

Write 
  
    
      
        
          
            x
          
          
            1
          
        
        …
        
          
            x
          
          
            n
          
        
      
    
    {\displaystyle \mathbf {x} _{1}\ldots \mathbf {x} _{n}}
   as row vectors, each of which has p columns.
Place the row vectors into a single matrix X of dimensions n × p.

Calculate the empirical mean
Find the empirical mean along each column j = 1, ..., p.
Place the calculated mean values into an empirical mean vector u of dimensions p × 1.
  
    
      
        
          u
          
            j
          
        
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          X
          
            i
            j
          
        
      
    
    {\displaystyle u_{j}={1 \over n}\sum _{i=1}^{n}X_{ij}}

Calculate the deviations from the mean
Mean subtraction is an integral part of the solution towards finding a principal component basis that minimizes the mean square error of approximating the data. Hence we proceed by centering the data as follows:

Subtract the empirical mean vector 
  
    
      
        
          
            u
          
          
            T
          
        
      
    
    {\displaystyle \mathbf {u} ^{T}}
   from each row of the data matrix X.
Store mean-subtracted data in the n × p matrix B.
  
    
      
        
          B
        
        =
        
          X
        
        −
        
          h
        
        
          
            u
          
          
            T
          
        
      
    
    {\displaystyle \mathbf {B} =\mathbf {X} -\mathbf {h} \mathbf {u} ^{T}}
  
where h is an n × 1 column vector of all 1s:
  
    
      
        
          h
          
            i
          
        
        =
        1
        
        
        
        
          for 
        
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle h_{i}=1\,\qquad \qquad {\text{for }}i=1,\ldots ,n}
  In some applications, each variable (column of B) may also be scaled to have a variance equal to 1 (see Z-score).  This step affects the calculated principal components, but makes them independent of the units used to measure the different variables.

Find the covariance matrix
Find the p × p empirical covariance matrix C from matrix B:
  
    
      
        
          C
        
        =
        
          
            1
            
              n
              −
              1
            
          
        
        
          
            B
          
          
            ∗
          
        
        
          B
        
      
    
    {\displaystyle \mathbf {C} ={1 \over {n-1}}\mathbf {B} ^{*}\mathbf {B} }
  
where 
  
    
      
        ∗
      
    
    {\displaystyle *}
   is the conjugate transpose operator. If B consists entirely of real numbers, which is the case in many applications, the ""conjugate transpose"" is the same as the regular transpose.The reasoning behind using n − 1 instead of n to calculate the covariance is Bessel's correction.

Find the eigenvectors and eigenvalues of the covariance matrix
Compute the matrix V of eigenvectors which diagonalizes the covariance matrix C:
  
    
      
        
          
            V
          
          
            −
            1
          
        
        
          C
        
        
          V
        
        =
        
          D
        
      
    
    {\displaystyle \mathbf {V} ^{-1}\mathbf {C} \mathbf {V} =\mathbf {D} }
  where D is the diagonal matrix of eigenvalues of C. This step will typically involve the use of a computer-based algorithm for computing eigenvectors and eigenvalues. These algorithms are readily available as sub-components of most matrix algebra systems, such as SAS, R, MATLAB, Mathematica, SciPy, IDL (Interactive Data Language), or GNU Octave as well as OpenCV.Matrix D will take the form of an p × p diagonal matrix, where
  
    
      
        
          D
          
            k
            ℓ
          
        
        =
        
          λ
          
            k
          
        
        
        
          for 
        
        k
        =
        ℓ
      
    
    {\displaystyle D_{k\ell }=\lambda _{k}\qquad {\text{for }}k=\ell }
  
is the jth eigenvalue of the covariance matrix C, and

  
    
      
        
          D
          
            k
            ℓ
          
        
        =
        0
        
        
          for 
        
        k
        ≠
        ℓ
        .
      
    
    {\displaystyle D_{k\ell }=0\qquad {\text{for }}k\neq \ell .}
  Matrix V, also of dimension p × p, contains p column vectors, each of length p, which represent the p eigenvectors of the covariance matrix C.
The eigenvalues and eigenvectors are ordered and paired. The jth eigenvalue corresponds to the jth eigenvector.
Matrix V denotes the matrix of right eigenvectors (as opposed to left eigenvectors). In general, the matrix of right eigenvectors need not be the (conjugate) transpose of the matrix of left eigenvectors.

Rearrange the eigenvectors and eigenvalues
Sort the columns of the eigenvector matrix V and eigenvalue matrix D in order of decreasing eigenvalue.
Make sure to maintain the correct pairings between the columns in each matrix.

Compute the cumulative energy content for each eigenvector
The eigenvalues represent the distribution of the source data's energy among each of the eigenvectors, where the eigenvectors form a basis for the data. The cumulative energy content g for the jth eigenvector is the sum of the energy content across all of the eigenvalues from 1 through j:
  
    
      
        
          g
          
            j
          
        
        =
        
          ∑
          
            k
            =
            1
          
          
            j
          
        
        
          D
          
            k
            k
          
        
        
        
          for 
        
        j
        =
        1
        ,
        …
        ,
        p
      
    
    {\displaystyle g_{j}=\sum _{k=1}^{j}D_{kk}\qquad {\text{for }}j=1,\dots ,p}

Select a subset of the eigenvectors as basis vectors
Save the first L columns of V as the p × L matrix W:
  
    
      
        
          W
          
            k
            l
          
        
        =
        
          V
          
            k
            ℓ
          
        
        
        
          for 
        
        k
        =
        1
        ,
        …
        ,
        p
        
        ℓ
        =
        1
        ,
        …
        ,
        L
      
    
    {\displaystyle W_{kl}=V_{k\ell }\qquad {\text{for }}k=1,\dots ,p\qquad \ell =1,\dots ,L}
  where
  
    
      
        1
        ≤
        L
        ≤
        p
        .
      
    
    {\displaystyle 1\leq L\leq p.}
  Use the vector g as a guide in choosing an appropriate value for L. The goal is to choose a value of L as small as possible while achieving a reasonably high value of g on a percentage basis. For example, you may want to choose L so that the cumulative energy g is above a certain threshold, like 90 percent. In this case, choose the smallest value of L such that
  
    
      
        
          
            
              g
              
                L
              
            
            
              g
              
                p
              
            
          
        
        ≥
        0.9
        
      
    
    {\displaystyle {\frac {g_{L}}{g_{p}}}\geq 0.9\,}

Project the data onto the new basis
The projected data points are the rows of the matrix
  
    
      
        
          T
        
        =
        
          B
        
        ⋅
        
          W
        
      
    
    {\displaystyle \mathbf {T} =\mathbf {B} \cdot \mathbf {W} }
  That is, the first column of 
  
    
      
        
          T
        
      
    
    {\displaystyle \mathbf {T} }
   is the projection of the data points onto the first principal component, the second column is the projection onto the second principal component, etc.

Derivation of PCA using the covariance method
Let X be a d-dimensional random vector expressed as column vector. Without loss of generality, assume X has zero mean.
We want to find 
  
    
      
        (
        ∗
        )
        
      
    
    {\displaystyle (\ast )\,}
   a d × d orthonormal transformation matrix P so that PX has a diagonal covariance matrix (that is, PX is a random vector with all its distinct components pairwise uncorrelated).
A quick computation assuming 
  
    
      
        P
      
    
    {\displaystyle P}
   were unitary yields:

  
    
      
        
          
            
              
                cov
                ⁡
                (
                P
                X
                )
              
              
                
                =
                E
                ⁡
                [
                P
                X
                 
                (
                P
                X
                
                  )
                  
                    ∗
                  
                
                ]
              
            
            
              
              
                
                =
                E
                ⁡
                [
                P
                X
                 
                
                  X
                  
                    ∗
                  
                
                
                  P
                  
                    ∗
                  
                
                ]
              
            
            
              
              
                
                =
                P
                E
                ⁡
                [
                X
                
                  X
                  
                    ∗
                  
                
                ]
                
                  P
                  
                    ∗
                  
                
              
            
            
              
              
                
                =
                P
                cov
                ⁡
                (
                X
                )
                
                  P
                  
                    −
                    1
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\operatorname {cov} (PX)&=\operatorname {E} [PX~(PX)^{*}]\\&=\operatorname {E} [PX~X^{*}P^{*}]\\&=P\operatorname {E} [XX^{*}]P^{*}\\&=P\operatorname {cov} (X)P^{-1}\\\end{aligned}}}
  Hence 
  
    
      
        (
        ∗
        )
        
      
    
    {\displaystyle (\ast )\,}
   holds if and only if 
  
    
      
        cov
        ⁡
        (
        X
        )
      
    
    {\displaystyle \operatorname {cov} (X)}
   were diagonalisable by 
  
    
      
        P
      
    
    {\displaystyle P}
  .
This is very constructive, as cov(X) is guaranteed to be a non-negative definite matrix and thus is guaranteed to be diagonalisable by some unitary matrix.

Covariance-free computation
In practical implementations, especially with high dimensional data (large p), the naive covariance method is rarely used because it is not efficient due to high computational and memory costs of explicitly determining the covariance matrix. The covariance-free approach avoids the np2 operations of explicitly calculating and storing the covariance matrix XTX, instead utilizing one of matrix-free methods, for example, based on the function evaluating the product XT(X r) at the cost of 2np operations.

Iterative computation
One way to compute the first principal component efficiently is shown in the following pseudo-code, for a data matrix X with zero mean, without ever computing its covariance matrix.

r = a random vector of length p

  
    
      
        
          r
        
        =
        
          
            
              r
            
            
              
                |
              
              
                r
              
              
                |
              
            
          
        
      
    
    {\displaystyle \mathbf {r} ={\frac {\mathbf {r} }{|\mathbf {r} |}}}
  
do c times:
      s = 0 (a vector of length p)
      for each row 
  
    
      
        
          x
        
        ∈
        
          X
        
      
    
    {\displaystyle \mathbf {x} \in \mathbf {X} }
  
            
  
    
      
        
          s
        
        =
        
          s
        
        +
        (
        
          x
        
        ⋅
        
          r
        
        )
        
          x
        
      
    
    {\displaystyle \mathbf {s} =\mathbf {s} +(\mathbf {x} \cdot \mathbf {r} )\mathbf {x} }
  
      
  
    
      
        
          eigenvalue
        
        =
        
          
            r
          
          
            T
          
        
        
          s
        
      
    
    {\displaystyle {\text{eigenvalue}}=\mathbf {r} ^{T}\mathbf {s} }
  
      
  
    
      
        
          error
        
        =
        
          |
        
        
          eigenvalue
        
        ⋅
        
          r
        
        −
        
          s
        
        
          |
        
      
    
    {\displaystyle {\text{error}}=|{\text{eigenvalue}}\cdot \mathbf {r} -\mathbf {s} |}
  
      
  
    
      
        
          r
        
        =
        
          
            
              s
            
            
              
                |
              
              
                s
              
              
                |
              
            
          
        
      
    
    {\displaystyle \mathbf {r} ={\frac {\mathbf {s} }{|\mathbf {s} |}}}
  
      exit if 
  
    
      
        
          error
        
        <
        
          tolerance
        
      
    
    {\displaystyle {\text{error}}<{\text{tolerance}}}
  
return 
  
    
      
        
          eigenvalue
        
        ,
        
          r
        
      
    
    {\displaystyle {\text{eigenvalue}},\mathbf {r} }
  

This power iteration algorithm simply calculates the vector XT(X r), normalizes, and places the result back in r. The eigenvalue is approximated by rT (XTX) r, which is the Rayleigh quotient on the unit vector r for the covariance matrix XTX . If the largest singular value is well separated from the next largest one, the vector r gets close to the first principal component of X within the number of iterations c, which is small relative to p, at the total cost 2cnp. The power iteration convergence can be accelerated without noticeably sacrificing the small cost per iteration using more advanced matrix-free methods, such as the Lanczos algorithm or the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method.
Subsequent principal components can be computed one-by-one via deflation or simultaneously as a block. In the former approach, imprecisions in already computed approximate principal components additively affect the accuracy of the subsequently computed principal components, thus increasing the error with every new computation. The latter approach in the block power method replaces single-vectors r and s with block-vectors, matrices R and S. Every column of R approximates one of the leading principal components, while all columns are iterated simultaneously. The main calculation is evaluation of the product XT(X R). Implemented, for example, in LOBPCG, efficient blocking eliminates the accumulation of the errors, allows using high-level BLAS matrix-matrix product functions, and typically leads to faster convergence, compared to the single-vector one-by-one technique.

The NIPALS method
Non-linear iterative partial least squares (NIPALS) is a variant the classical power iteration with matrix deflation by subtraction implemented for computing the first few components in a principal component or partial least squares analysis. For very-high-dimensional datasets, such as those generated in the *omics sciences (for example, genomics, metabolomics) it is usually only necessary to compute the first few PCs. The non-linear iterative partial least squares (NIPALS) algorithm updates iterative approximations to the leading scores and loadings t1 and r1T by the power iteration multiplying on every iteration by X on the left and on the right, that is, calculation of the covariance matrix is avoided, just as in the matrix-free implementation of the power iterations to XTX, based on the function evaluating the product XT(X r) = ((X r)TX)T.
The matrix deflation by subtraction is performed by subtracting the outer product, t1r1T from X leaving the deflated residual matrix used to calculate the subsequent leading PCs.
For large data matrices, or matrices that have a high degree of column collinearity, NIPALS suffers from loss of orthogonality of PCs due to machine precision round-off errors accumulated in each iteration and matrix deflation by subtraction. A Gram–Schmidt re-orthogonalization algorithm is applied to both the scores and the loadings at each iteration step to eliminate this loss of orthogonality. NIPALS reliance on single-vector multiplications cannot take advantage of high-level BLAS and results in slow convergence for clustered leading singular values—both these deficiencies are resolved in more sophisticated matrix-free block solvers, such as the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method.

Online/sequential estimation
In an ""online"" or ""streaming"" situation with data arriving piece by piece rather than being stored in a single batch, it is useful to make an estimate of the PCA projection that can be updated sequentially. This can be done efficiently, but requires different algorithms.

PCA and qualitative variables
In PCA, it is common that we want to introduce qualitative variables as supplementary elements. For example, many quantitative variables have been measured on plants. For these plants, some qualitative variables are available as, for example, the species to which the plant belongs. These data were subjected to PCA for quantitative variables. When analyzing the results, it is natural to connect the principal components to the qualitative variable species.
For this, the following results are produced.

Identification, on the factorial planes, of the different species, for example, using different colors.
Representation, on the factorial planes, of the centers of gravity of plants belonging to the same species.
For each center of gravity and each axis, p-value to judge the significance of the difference between the center of gravity and origin.These results are what is called introducing a qualitative variable as supplementary element. This procedure is detailed in and Husson, Lê & Pagès 2009 and Pagès 2013.
Few software offer this option in an ""automatic"" way. This is the case of SPAD that historically, following the work of Ludovic Lebart, was the first to propose this option, and the R package FactoMineR.

Applications
Quantitative finance
In quantitative finance, principal component analysis can be directly applied to the risk management of interest rate derivative portfolios. Trading multiple swap instruments which are usually a function of 30–500 other market quotable swap instruments is sought to be reduced to usually 3 or 4 principal components, representing the path of interest rates on a macro basis. Converting risks to be represented as those to factor loadings (or multipliers) provides assessments and understanding beyond that available to simply collectively viewing risks to individual 30–500 buckets.
PCA has also been applied to equity portfolios in a similar fashion, both to portfolio risk and to risk return. One application is to reduce portfolio risk, where allocation strategies are applied to the ""principal portfolios"" instead of the underlying stocks. A second is to enhance portfolio return, using the principal components to select stocks with upside potential.

Neuroscience
A variant of principal components analysis is used in neuroscience to identify the specific properties of a stimulus that increase a neuron's probability of generating an action potential. This technique is known as spike-triggered covariance analysis. In a typical application an experimenter presents a white noise process as a stimulus (usually either as a sensory input to a test subject, or as a current injected directly into the neuron) and records a train of action potentials, or spikes, produced by the neuron as a result. Presumably, certain features of the stimulus make the neuron more likely to spike. In order to extract these features, the experimenter calculates the covariance matrix of the spike-triggered ensemble, the set of all stimuli (defined and discretized over a finite time window, typically on the order of 100 ms) that immediately preceded a spike. The eigenvectors of the difference between the spike-triggered covariance matrix and the covariance matrix of the prior stimulus ensemble (the set of all stimuli, defined over the same length time window) then indicate the directions in the space of stimuli along which the variance of the spike-triggered ensemble differed the most from that of the prior stimulus ensemble. Specifically, the eigenvectors with the largest positive eigenvalues correspond to the directions along which the variance of the spike-triggered ensemble showed the largest positive change compared to the variance of the prior. Since these were the directions in which varying the stimulus led to a spike, they are often good approximations of the sought after relevant stimulus features.
In neuroscience, PCA is also used to discern the identity of a neuron from the shape of its action potential. Spike sorting is an important procedure because extracellular recording techniques often pick up signals from more than one neuron. In spike sorting, one first uses PCA to reduce the dimensionality of the space of action potential waveforms, and then performs clustering analysis to associate specific action potentials with individual neurons.
PCA as a dimension reduction technique is particularly suited to detect coordinated activities of large neuronal ensembles. It has been used in determining collective variables, that is, order parameters, during phase transitions in the brain.

Relation with other methods
Correspondence analysis
Correspondence analysis (CA)
was developed by Jean-Paul Benzécri
and is conceptually similar to PCA, but scales the data (which should be non-negative) so that rows and columns are treated equivalently. It is traditionally applied to contingency tables.
CA decomposes the chi-squared statistic associated to this table into orthogonal factors.
Because CA is a descriptive technique, it can be applied to tables for which the chi-squared statistic is appropriate or not.
Several variants of CA are available including detrended correspondence analysis and canonical correspondence analysis. One special extension is multiple correspondence analysis, which may be seen as the counterpart of principal component analysis for categorical data.

Factor analysis
Principal component analysis creates variables that are linear combinations of the original variables. The new variables have the property that the variables are all orthogonal. The PCA transformation can be helpful as a pre-processing step before clustering. PCA is a variance-focused approach seeking to reproduce the total variable variance, in which components reflect both common and unique variance of the variable. PCA is generally preferred for purposes of data reduction (that is, translating variable space into optimal factor space) but not when the goal is to detect the latent construct or factors.
Factor analysis is similar to principal component analysis, in that factor analysis also involves linear combinations of variables. Different from PCA, factor analysis is a correlation-focused approach seeking to reproduce the inter-correlations among variables, in which the factors ""represent the common variance of variables, excluding unique variance"". In terms of the correlation matrix, this corresponds with focusing on explaining the off-diagonal terms (that is, shared co-variance), while PCA focuses on explaining the terms that sit on the diagonal. However, as a side result, when trying to reproduce the on-diagonal terms, PCA also tends to fit relatively well the off-diagonal correlations. Results given by PCA and factor analysis are very similar in most situations, but this is not always the case, and there are some problems where the results are significantly different. Factor analysis is generally used when the research purpose is detecting data structure (that is, latent constructs or factors) or causal modeling. If the factor model is incorrectly formulated or the assumptions are not met, then factor analysis will give erroneous results.

K-means clustering
It has been asserted that the relaxed solution of k-means clustering, specified by the cluster indicators, is given by the principal components, and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace. However, that PCA is a useful relaxation of k-means clustering was not a new result, and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.

Non-negative matrix factorization
Non-negative matrix factorization (NMF) is a dimension reduction method where only non-negative elements in the matrices are used, which is therefore a promising method in astronomy, in the sense that astrophysical signals are non-negative. The PCA components are orthogonal to each other, while the NMF components are all non-negative and therefore constructs a non-orthogonal basis.
In PCA, the contribution of each component is ranked based on the magnitude of its corresponding eigenvalue, which is equivalent to the fractional residual variance (FRV) in analyzing empirical data. For NMF, its components are ranked based only on the empirical FRV curves. The residual fractional eigenvalue plots, that is, 
  
    
      
        1
        −
        
          ∑
          
            i
            =
            1
          
          
            k
          
        
        
          λ
          
            i
          
        
        
          
            /
          
        
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        
          λ
          
            j
          
        
      
    
    {\displaystyle 1-\sum _{i=1}^{k}\lambda _{i}{\Big /}\sum _{j=1}^{n}\lambda _{j}}
   as a function of component number 
  
    
      
        k
      
    
    {\displaystyle k}
   given a total of 
  
    
      
        n
      
    
    {\displaystyle n}
   components, for PCA has a flat plateau, where no data is captured to remove the quasi-static noise, then the curves dropped quickly as an indication of over-fitting and captures random noise. The FRV curves for NMF is decreasing continuously  when the NMF components are constructed sequentially, indicating the continuous capturing of quasi-static noise; then converge to higher levels than PCA, indicating the less over-fitting property of NMF.

Generalizations
Sparse PCA
A particular disadvantage of PCA is that the principal components are usually linear combinations of all input variables. Sparse PCA overcomes this disadvantage by finding linear combinations that contain just a few input variables. It extends the classic method of principal component analysis (PCA) for the reduction of dimensionality of data by adding sparsity constraint on the input variables.
Several approaches have been proposed, including

a regression framework,
a convex relaxation/semidefinite programming framework,
a generalized power method framework
an alternating maximization framework
forward-backward greedy search and exact methods using branch-and-bound techniques,
Bayesian formulation framework.The methodological and theoretical developments of Sparse PCA as well as its applications in scientific studies were recently reviewed in a survey paper.

Nonlinear PCA
Most of the modern methods for nonlinear dimensionality reduction find their theoretical and algorithmic roots in PCA or K-means. Pearson's original idea was to take a straight line (or plane) which will be ""the best fit"" to a set of data points. Principal curves and manifolds give the natural geometric framework for PCA generalization and extend the geometric interpretation of PCA by explicitly constructing an embedded manifold for data approximation, and by encoding using standard geometric projection onto the manifold, as it is illustrated by Fig.
See also the elastic map algorithm and principal geodesic analysis. Another popular generalization is kernel PCA, which corresponds to PCA performed in a reproducing kernel Hilbert space associated with a positive definite kernel.
In multilinear subspace learning, PCA is generalized to multilinear PCA (MPCA) that extracts features directly from tensor representations. MPCA is solved by performing PCA in each mode of the tensor iteratively. MPCA has been applied to face recognition, gait recognition, etc. MPCA is further extended to uncorrelated MPCA, non-negative MPCA and robust MPCA.
N-way principal component analysis may be performed with models such as Tucker decomposition, PARAFAC, multiple factor analysis, co-inertia analysis, STATIS, and DISTATIS.

Robust PCA
While PCA finds the mathematically optimal method (as in minimizing the squared error), it is still sensitive to outliers in the data that produce large errors, something that the method tries to avoid in the first place. It is therefore common practice to remove outliers before computing PCA. However, in some contexts, outliers can be difficult to identify. For example, in data mining algorithms like correlation clustering, the assignment of points to clusters and outliers is not known beforehand.
A recently proposed generalization of PCA based on a weighted PCA increases robustness by assigning different weights to data objects based on their estimated relevancy.
Outlier-resistant variants of PCA have also been proposed, based on L1-norm formulations (L1-PCA).Robust principal component analysis (RPCA) via decomposition in low-rank and sparse matrices is a modification of PCA that works well with respect to grossly corrupted observations.

Similar techniques
Independent component analysis
Independent component analysis (ICA) is directed to similar problems as principal component analysis, but finds additively separable components rather than successive approximations.

Network component analysis
Given a matrix 
  
    
      
        E
      
    
    {\displaystyle E}
  , it tries to decompose it into two matrices such that 
  
    
      
        E
        =
        A
        P
      
    
    {\displaystyle E=AP}
  . A key difference from techniques such as PCA and ICA is that some of the entries of 
  
    
      
        A
      
    
    {\displaystyle A}
   are constrained to be 0. Here 
  
    
      
        P
      
    
    {\displaystyle P}
   is termed the regulatory layer. While in general such a decomposition can have multiple solutions, they prove that if the following conditions are satisfied :

  
    
      
        A
      
    
    {\displaystyle A}
   has full column rank
Each column of 
  
    
      
        A
      
    
    {\displaystyle A}
   must have at least 
  
    
      
        L
        −
        1
      
    
    {\displaystyle L-1}
   zeroes where 
  
    
      
        L
      
    
    {\displaystyle L}
   is the number of columns of 
  
    
      
        A
      
    
    {\displaystyle A}
   (or alternatively the number of rows of 
  
    
      
        P
      
    
    {\displaystyle P}
  ). The justification for this criterion is that if a node is removed from the regulatory layer along with all the output nodes connected to it, the result must still be characterized by a connectivity matrix with full column rank.

  
    
      
        P
      
    
    {\displaystyle P}
   must have full row rank.then the decomposition is unique up to multiplication by a scalar.

Discriminant analysis component analysis
Discriminant analysis of principal components (DAPC) is a multivariate method used to identify and describe clusters of genetically related individuals. Genetic variation is partitioned into two components: variation between groups and within groups, and it maximizes the former. Linear discriminants are linear combinations of alleles which best separate the clusters. Alleles that most contribute to this discrimination are therefore those that are the most markedly different across groups. The contributions of alleles to the groupings identified by DAPC can allow identifying regions of the genome driving the genetic divergence among groups 
In DAPC, data is first transformed using a principal components analysis (PCA) and subsequently clusters are identified using discriminant analysis (DA).
A DAPC can be realized on R using the package Adegenet. (more info: https://adegenet.r-forge.r-project.org/)

Software/source code
ALGLIB - a C++ and C# library that implements PCA and truncated PCA
Analytica – The built-in EigenDecomp function computes principal components.
ELKI – includes PCA for projection, including robust variants of PCA, as well as PCA-based clustering algorithms.
Gretl – principal component analysis can be performed either via the pca command or via the princomp() function.
Julia – Supports PCA with the pca function in the MultivariateStats package
KNIME – A java based nodal arranging software for Analysis, in this the nodes called PCA, PCA compute, PCA Apply, PCA inverse make it easily.
Mathematica – Implements principal component analysis with the PrincipalComponents command using both covariance and correlation methods.
MathPHP – PHP mathematics library with support for PCA.
MATLAB Statistics Toolbox – The functions princomp and pca (R2012b) give the principal components, while the function pcares gives the residuals and reconstructed matrix for a low-rank PCA approximation.
Matplotlib – Python library have a PCA package in the .mlab module.
mlpack – Provides an implementation of principal component analysis in C++.
NAG Library – Principal components analysis is implemented via the g03aa routine (available in both the Fortran versions of the Library).
NMath – Proprietary numerical library containing PCA for the .NET Framework.
GNU Octave – Free software computational environment mostly compatible with MATLAB, the function princomp gives the principal component.
OpenCV
Oracle Database 12c – Implemented via DBMS_DATA_MINING.SVDS_SCORING_MODE by specifying setting value SVDS_SCORING_PCA
Orange (software) – Integrates PCA in its visual programming environment. PCA displays a scree plot (degree of explained variance) where user can interactively select the number of principal components.
Origin – Contains PCA in its Pro version.
Qlucore – Commercial software for analyzing multivariate data with instant response using PCA.
R – Free statistical package, the functions princomp and prcomp can be used for principal component analysis; prcomp uses singular value decomposition which generally gives better numerical accuracy. Some packages that implement PCA in R, include, but are not limited to: ade4, vegan, ExPosition, dimRed, and FactoMineR.
SAS – Proprietary software; for example, see 
Scikit-learn – Python library for machine learning which contains PCA, Probabilistic PCA, Kernel PCA, Sparse PCA and other techniques in the decomposition module.
Weka – Java library for machine learning which contains modules for computing principal components.

See also
References
Further reading
Jackson, J.E. (1991). A User's Guide to Principal Components (Wiley).
Jolliffe, I. T. (1986). Principal Component Analysis. Springer Series in Statistics. Springer-Verlag. pp. 487. CiteSeerX 10.1.1.149.8828. doi:10.1007/b98835. ISBN 978-0-387-95442-4.
Jolliffe, I. T. (2002). Principal Component Analysis. Springer Series in Statistics. New York: Springer-Verlag. doi:10.1007/b98835. ISBN 978-0-387-95442-4.
Husson François, Lê Sébastien & Pagès Jérôme (2009). Exploratory Multivariate Analysis by Example Using R. Chapman & Hall/CRC The R Series, London. 224p. ISBN 978-2-7535-0938-2
Pagès Jérôme (2014). Multiple Factor Analysis by Example Using R. Chapman & Hall/CRC The R Series London 272 p

External links
University of Copenhagen video by Rasmus Bro on YouTube
Stanford University video by Andrew Ng on YouTube
A Tutorial on Principal Component Analysis
A layman's introduction to principal component analysis on YouTube (a video of less than 100 seconds.)
StatQuest: Principal Component Analysis (PCA) clearly explained on YouTube
See also the list of Software implementations",https://en.wikipedia.org/wiki/Principal_component_analysis,"['All articles with unsourced statements', 'Articles with long short description', 'Articles with short description', 'Articles with unsourced statements from August 2014', 'Articles with unsourced statements from March 2011', 'Articles with unsourced statements from November 2019', 'CS1 maint: multiple names: authors list', 'Commons category link from Wikidata', 'Dimension reduction', 'Matrix decompositions', 'Short description matches Wikidata', 'Wikipedia articles needing clarification from March 2011', 'Wikipedia articles needing page number citations from November 2020', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with multiple identifiers']",Data Science
149,Probably approximately correct learning,"In computational learning theory, probably approximately correct (PAC) learning is a framework for mathematical analysis of machine learning. It was proposed in 1984 by Leslie Valiant.In this framework, the learner receives samples and must select a generalization function (called the hypothesis) from a certain class of possible functions. The goal is that, with high probability (the ""probably"" part), the selected function will have low generalization error (the ""approximately correct"" part). The learner must be able to learn the concept given any arbitrary approximation ratio, probability of success, or distribution of the samples.
The model was later extended to treat noise (misclassified samples).
An important innovation of the PAC framework is the introduction of computational complexity theory concepts to machine learning. In particular, the learner is expected to find efficient functions (time and space requirements bounded to a polynomial of the example size), and the learner itself must implement an efficient procedure (requiring an example count bounded to a polynomial of the concept size, modified by the approximation and likelihood bounds).

Definitions and terminology
In order to give the definition for something that is PAC-learnable, we first have to introduce some terminology.For the following definitions, two examples will be used.  The first is the problem of character recognition given an array of 
  
    
      
        n
      
    
    {\displaystyle n}
   bits encoding a binary-valued image.  The other example is the problem of finding an interval that will correctly classify points within the interval as positive and the points outside of the range as negative.
Let 
  
    
      
        X
      
    
    {\displaystyle X}
   be a set called the instance space or the encoding of all the samples.  In the character recognition problem, the instance space is 
  
    
      
        X
        =
        {
        0
        ,
        1
        
          }
          
            n
          
        
      
    
    {\displaystyle X=\{0,1\}^{n}}
  .  In the interval problem the instance space, 
  
    
      
        X
      
    
    {\displaystyle X}
  , is the set of all bounded intervals in 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  , where 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   denotes the set of all real numbers.
A concept is a subset 
  
    
      
        c
        ⊂
        X
      
    
    {\displaystyle c\subset X}
  .  One concept is the set of all patterns of bits in 
  
    
      
        X
        =
        {
        0
        ,
        1
        
          }
          
            n
          
        
      
    
    {\displaystyle X=\{0,1\}^{n}}
   that encode a picture of the letter ""P"".  An example concept from the second example is the set of open intervals, 
  
    
      
        {
        (
        a
        ,
        b
        )
        ∣
        0
        ≤
        a
        ≤
        π
        
          /
        
        2
        ,
        π
        ≤
        b
        ≤
        
          
            13
          
        
        }
      
    
    {\displaystyle \{(a,b)\mid 0\leq a\leq \pi /2,\pi \leq b\leq {\sqrt {13}}\}}
  , each of which contains only the positive points.  A concept class  
  
    
      
        C
      
    
    {\displaystyle C}
   is a collection of concepts over 
  
    
      
        X
      
    
    {\displaystyle X}
  .  This could be the set of all subsets of the array of bits that are skeletonized 4-connected (width of the font is 1).
Let 
  
    
      
        E
        X
        (
        c
        ,
        D
        )
      
    
    {\displaystyle EX(c,D)}
   be a procedure that draws an example, 
  
    
      
        x
      
    
    {\displaystyle x}
  , using a probability distribution 
  
    
      
        D
      
    
    {\displaystyle D}
   and gives the correct label 
  
    
      
        c
        (
        x
        )
      
    
    {\displaystyle c(x)}
  , that is 1 if 
  
    
      
        x
        ∈
        c
      
    
    {\displaystyle x\in c}
   and 0 otherwise.
Now, given 
  
    
      
        0
        <
        ϵ
        ,
        δ
        <
        1
      
    
    {\displaystyle 0<\epsilon ,\delta <1}
  , assume there is an algorithm 
  
    
      
        A
      
    
    {\displaystyle A}
   and a polynomial 
  
    
      
        p
      
    
    {\displaystyle p}
   in 
  
    
      
        1
        
          /
        
        ϵ
        ,
        1
        
          /
        
        δ
      
    
    {\displaystyle 1/\epsilon ,1/\delta }
   (and other relevant parameters of the class 
  
    
      
        C
      
    
    {\displaystyle C}
  ) such that, given a sample of size 
  
    
      
        p
      
    
    {\displaystyle p}
   drawn according to 
  
    
      
        E
        X
        (
        c
        ,
        D
        )
      
    
    {\displaystyle EX(c,D)}
  , then, with probability of at least 
  
    
      
        1
        −
        δ
      
    
    {\displaystyle 1-\delta }
  , 
  
    
      
        A
      
    
    {\displaystyle A}
   outputs a hypothesis 
  
    
      
        h
        ∈
        C
      
    
    {\displaystyle h\in C}
   that has an average error less than or equal to 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
   on 
  
    
      
        X
      
    
    {\displaystyle X}
   with the same distribution 
  
    
      
        D
      
    
    {\displaystyle D}
  .  Further if the above statement for algorithm 
  
    
      
        A
      
    
    {\displaystyle A}
   is true for every concept 
  
    
      
        c
        ∈
        C
      
    
    {\displaystyle c\in C}
   and for every distribution 
  
    
      
        D
      
    
    {\displaystyle D}
   over 
  
    
      
        X
      
    
    {\displaystyle X}
  , and for all 
  
    
      
        0
        <
        ϵ
        ,
        δ
        <
        1
      
    
    {\displaystyle 0<\epsilon ,\delta <1}
    then 
  
    
      
        C
      
    
    {\displaystyle C}
   is (efficiently) PAC learnable (or distribution-free PAC learnable).  We can also say that 
  
    
      
        A
      
    
    {\displaystyle A}
   is a PAC learning algorithm for 
  
    
      
        C
      
    
    {\displaystyle C}
  .

Equivalence
Under some regularity conditions these conditions are equivalent: 
The concept class C is PAC learnable.
The VC dimension of C is finite.
C is a uniform Glivenko–Cantelli class.
C is compressible in the sense of Littlestone and Warmuth

See also
Machine learning
Data mining
Error tolerance (PAC learning)
Sample complexity

References
https://users.soe.ucsc.edu/~manfred/pubs/lrnk-olivier.pdf
Moran, Shay; Yehudayoff, Amir (2015). ""Sample compression schemes for VC classes"". arXiv:1503.06960 [cs.LG].

Further reading
M. Kearns, U. Vazirani. An Introduction to Computational Learning Theory. MIT Press, 1994. A textbook.
M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. MIT Press, 2018. Chapter 2 contains a detailed treatment of PAC-learnability. Readable through open access from the publisher.
D. Haussler. Overview of the Probably Approximately Correct (PAC) Learning Framework. An introduction to the topic.
L. Valiant. Probably Approximately Correct. Basic Books, 2013. In which Valiant argues that PAC learning describes how organisms evolve and learn.",https://en.wikipedia.org/wiki/Probably_approximately_correct_learning,"['Articles with short description', 'Computational learning theory', 'Short description matches Wikidata', 'Wikipedia articles needing clarification from March 2018']",Data Science
150,Nate Silver,"Nathaniel Read Silver (born January 13, 1978) is an American statistician and writer who analyzes baseball (see sabermetrics), basketball, and elections (see psephology). He is the founder and editor-in-chief of FiveThirtyEight and a Special Correspondent for ABC News.
Silver first gained public recognition for developing PECOTA, a system for forecasting the performance and career development of Major League Baseball players, which he sold to and then managed for Baseball Prospectus from 2003 to 2009.Silver was named one of The World's 100 Most Influential People by Time in 2009 after an election forecasting system he developed successfully predicted the outcomes in 49 of the 50 states in the 2008 U.S. Presidential election. In the 2012 United States presidential election, the forecasting system correctly predicted the winner of all 50 states and the District of Columbia. FiveThirtyEight, like many  outlets, failed to anticipate Donald Trump's victory in the 2016 presidential election. However, the 28% chance of victory they gave Trump going into election day was significantly higher than that of most other analysts. In 2020, FiveThirtyEight correctly predicted Joe Biden's victory in the election, forecasting 48 out of 50 states correctly. However, just like many pollsters and experts, he significantly overestimated Biden's margins in battleground states such as Wisconsin, Michigan and Pennsylvania. It is worth noting that his forecasts are known for having a high degree of uncertainty, so the actual results were still considered a highly possible scenario by Silver.
In 2010, the FiveThirtyEight blog was licensed for publication by The New York Times. In 2012 and 2013, FiveThirtyEight won Webby Awards as the ""Best Political Blog"" from the International Academy of Digital Arts and Sciences.
In July 2013, Silver sold FiveThirtyEight to ESPN, and Silver became its Editor in Chief. The ESPN-owned FiveThirtyEight launched on March 17, 2014. The site focused on a broad range of subjects under the rubric of ""data journalism"".Silver's book, The Signal and the Noise, was published in September 2012. It subsequently reached The New York Times best seller list for nonfiction, and was named by Amazon.com as the No. 1 best nonfiction book of 2012. The Signal and the Noise won the 2013 Phi Beta Kappa Award in Science. The book has been translated into eleven languages: Chinese (separate editions in traditional and simplified characters), Czech, Finnish, German, Italian, Polish, Portuguese, Romanian, Russian, and Spanish.

Early life
Silver was born in East Lansing, Michigan, the son of Sally (née Thrun), a community activist, and Brian David Silver, a former chair of the political science department at Michigan State University. Silver's mother's family, of English and German descent, includes several distinguished men and women, including his maternal great-grandfather, Harmon Lewis, who was president of the Alcoa Steamship Company, Inc. Silver's father's family includes two uncles -- Leon Silver and Caswell Silver -- who were distinguished geologists. Silver has described himself as ""half-Jewish"".Silver showed a proficiency in math from a young age. According to journalist William Hageman, ""Silver caught the baseball bug when he was 6.... It was 1984, the year the Detroit Tigers won the World Series. The Tigers became his team and baseball his sport. And if there's anything that goes hand in glove with baseball, it's numbers, another of Silver's childhood interests (""It's always more interesting to apply it to batting averages than algebra class"")"".As a student at East Lansing High School, in 1996 Silver won first place in the State of Michigan in the 49th annual John S. Knight Scholarship Contest for senior high school debaters.Silver first showed his journalism skills as a writer and opinion page editor for The Portrait, East Lansing High School's student newspaper, from 1993–1996.In 2000, Silver graduated with Honors with a Bachelor of Arts degree in economics from the University of Chicago. He also wrote for the Chicago Weekly News and the Chicago Maroon. He spent his third year at the London School of Economics.

Career
Economic consultant: 2000–2004
After college graduation in 2000, Silver worked for three and a half years as a transfer pricing consultant with KPMG in Chicago. When asked in 2009, ""What is your biggest regret in life?"" Silver responded, ""Spending four years of my life at a job I didn't like"". While employed at KPMG, Silver continued to nurture his lifelong interest in baseball and statistics, and on the side he began to work on his PECOTA system for projecting player performance and careers. He quit his job at KPMG in April 2004 and for a time earned his living mainly by playing online poker. According to Sports Illustrated writer Alexander Wolff, over a three-year period Silver earned $400,000 from online poker.

Baseball analyst: 2003–2008
In 2003, Silver became a writer for Baseball Prospectus (BP), after having sold PECOTA to BP in return for a partnership interest. After resigning from KPMG in 2004, he took the position of Executive Vice-President, later renamed Managing Partner of BP. Silver further developed PECOTA and wrote a weekly column under the heading ""Lies, Damned Lies"". He applied sabermetric techniques to a broad range of topics including forecasting the performance of individual players, the economics of baseball, metrics for the valuation of players, and developing an Elo rating system for Major League baseball.Between 2003 and 2009, Silver co-authored the Baseball Prospectus annual book of Major League Baseball forecasts, as well as other books, including Mind Game: How the Boston Red Sox Got Smart, Won a World Series, and Created a New Blueprint for Winning, Baseball Between the Numbers, and It Ain't Over 'til It's Over: The Baseball Prospectus Pennant Race Book.He contributed articles about baseball to ESPN.com, Sports Illustrated, Slate, the New York Sun, and The New York Times.Silver has authored more than 200 articles for Baseball Prospectus.

PECOTA
PECOTA (Player Empirical Comparison and Optimization Test Algorithm) is a statistical system that projects the future performance of hitters and pitchers. It is designed primarily for two users: fans interested in fantasy baseball, and professionals in the baseball business trying to predict the performance and valuation of major league players. Unlike most other baseball projection systems, PECOTA relies on matching a given current player to a set of ""comparable"" players whose past performance can serve as a guide to how the given current player is likely to perform in the future. Unlike most other such systems, PECOTA also calculates a range of probable performance levels rather than a single predicted value on a given measure such as earned run average or batting average.
PECOTA projections were first published by Baseball Prospectus in the 2003 edition of its annual book as well as online by BaseballProspectus.com. Silver produced the PECOTA forecasts for each Major League Baseball season from 2003 through 2009.

Political analyst and blogger: 2008–present
FiveThirtyEight blog
Creation and motivation
On November 1, 2007, while still employed by Baseball Prospectus, Silver began publishing a diary under the pseudonym ""Poblano"" on the progressive political blog Daily Kos. Silver set out to analyze quantitative aspects of the political game to enlighten a broader audience. Silver reports that ""he was stranded in a New Orleans airport when the idea of FiveThirtyEight.com came to him. 'I was just frustrated with the analysis. ... I saw a lot of discussion about strategy that was not all that sophisticated, especially when it came to quantitative things like polls and demographics'"". His forecasts of the 2008 United States presidential primary elections drew a lot of attention, including being cited by The New York Times op-ed columnist William Kristol.On March 7, 2008, while still writing as ""Poblano"", Silver established his own blog, FiveThirtyEight.com. Often colloquially referred to as just 538, the website takes its name from the number of electors in the United States electoral college.On May 30, 2008, Poblano revealed his identity to FiveThirtyEight.com readers. On June 1, 2008, Silver published a two-page op-ed in the New York Post outlining the rationale underlying his focus on the statistical aspects of politics. He first appeared on national television on CNN's American Morning on June 13, 2008.Silver described his partisan orientation as follows in the FAQ on his website: ""My state [Illinois] has non-partisan registration, so I am not registered as anything. I vote for Democratic candidates the majority of the time (though by no means always). This year, I have been a supporter of Barack Obama"". With respect to the impartiality of his electoral projections, Silver stated, ""Are [my] results biased toward [my] preferred candidates? I hope not, but that is for you to decide. I have tried to disclose as much about my methodology as possible"".

2008 election and aftermath
Shortly after the November 4 election, ESPN writer Jim Caple observed, ""Forget Cole Hamels and the Phillies. No one in baseball had a more impressive fall than Nate Silver....  [R]ight now Silver is exhausted. He barely slept the last couple weeks of the campaign—'By the end, it was full-time plus'—and for that matter, he says he couldn't have kept it up had the campaign lasted two days longer. Plus, he has his Baseball Prospectus duties. 'We write our [Baseball Prospectus 2009] book from now through the first of the year,' [Silver] said. 'I have a week to relax and then it gets just as busy again. In February 2009 I will just have to find an island in the Caribbean and throw my BlackBerry in the ocean'"".Later in November 2008, Silver signed a contract with Penguin Group USA to write two books, reportedly for a $700,000 advance.Silver was invited to be a speaker at TED 2009 in February 2009, and keynote speaker at the 2009 South by Southwest (SXSW) Interactive conference (March 2009).While maintaining his FiveThirtyEight.com website, in January 2009 Silver began a monthly feature column, ""The Data"", in Esquire as well as contributed occasional articles to other media such as The New York Times and The Wall Street Journal.  He also tried his luck in the 2009 World Series of Poker.The success of his FiveThirtyEight.com blog marked the effective end of Silver's career as baseball analyst, though he continued to devote some attention to sports statistics and sports economics in his blog. In March 2009, he stepped down as Managing Partner of Baseball Prospectus and handed over responsibility for producing future PECOTA projections to other Baseball Prospectus staff members. In April 2009, he appeared as an analyst on ESPN's Baseball Tonight. After March 2009, he published only two ""Lies, Damned Lies"" columns on BaseballProspectus.com.
In November 2009, ESPN introduced a new Soccer Power Index (SPi), designed by Nate Silver, for predicting the outcome of the 2010 FIFA World Cup. He published a post-mortem after the tournament, comparing his predictions to those of alternative rating systems.In April 2010, in an assignment for New York magazine, Silver created a quantitative index of ""The Most Livable Neighborhoods in New York"".

Transition to The New York Times
On June 3, 2010, Silver announced on FiveThirtyEight

In the near future, the blog will ""re-launch"" under a NYTimes.com domain. It will retain its own identity (akin to other Times blogs like DealBook), but will be organized under the News:Politics section. Once this occurs, content will no longer be posted at FiveThirtyEight.com on an ongoing basis, and the blog will re-direct to the new URL. In addition, I will be contributing content to the print edition of The New York Times, and to the Sunday Magazine. The partnership agreement, which is structured as a license, has a term of three years.
The New York Times ""FiveThirtyEight: Nate Silver's Political Calculus"" commenced on August 25, 2010, with the publication of ""New Forecast Shows Democrats Losing 6 to 7 Senate Seats"". From that date the blog focused almost exclusively on forecasting the outcomes of the 2010 U.S. Senate and U.S. House of Representatives elections as well as state gubernatorial contests. Silver's Times Sunday Magazine feature first appeared on November 19, 2010, under the heading ""Go Figure"". It was later titled ""Acts of Mild Subversion"".While blogging for The Times, Silver also worked on his book about prediction, which was published in September 2012. At that time, Silver began to drop hints that after 2012 he would turn his attention to matters other than detailed statistical forecasting of elections. As reported in New York magazine: "" 'I view my role now as providing more of a macro-level skepticism, rather than saying this poll is good or this poll is evil,' he says. And in four [years], he might be even more macro, as he turns his forecasting talents to other fields. 'I'm 97 percent sure that the FiveThirtyEight model will exist in 2016,' he says, 'but it could be someone else who's running it or licensing it.'""During the last year of FiveThirtyEight's license to The New York Times, it drew a very large volume of online traffic to the paper:

The Times does not release traffic figures, but a spokesperson said yesterday that Silver's blog provided a significant—and significantly growing, over the past year—percentage of Times pageviews. This fall, visits to the Times' political coverage (including FiveThirtyEight) have increased, both absolutely and as a percentage of site visits. But FiveThirtyEight's growth is staggering: where earlier this year, somewhere between 10 and 20 percent of politics visits included a stop at FiveThirtyEight, last week that figure was 71 percent.
But Silver's blog has buoyed more than just the politics coverage, becoming a significant traffic-driver for the site as a whole. Earlier this year, approximately 1 percent of visits to the New York Times included FiveThirtyEight. Last week, that number was 13 percent. Yesterday, it was 20 percent. That is, one in five visitors to the sixth-most-trafficked U.S. news site took a look at Silver's blog.

Departure from The Times
In an online chat session a week after the 2012 election Silver commented: ""As tempting as it might be to pull a Jim Brown/Sandy Koufax and just mic-drop/retire from elections forecasting, I expect that we'll be making forecasts in 2014 and 2016. Midterm elections can be dreadfully boring, unfortunately. But the 2016 G.O.P. primary seems almost certain to be epic."" In late November 2012, Times executive editor Jill Abramson declared her wish to keep Silver and his blog: ""We would love to have Nate continue to be part of the New York Times family, and to expand on what he does"", she said. ""We know he began in sports anyway, so it is not an exclusively political product. I am excited to talk to Nate when he finishes his book tour about ways to expand that kind of reporting.""On July 22, 2013, ESPN announced that it had acquired ownership of the FiveThirtyEight website and brand, and that ""Silver will serve as editor-in-chief of the site and will build a team of journalists, editors, analysts and contributors in the coming months.""The New York Times public editor Margaret Sullivan wrote upon Silver's decision to leave for ESPN:

I don't think Nate Silver ever really fit into the Times culture and I think he was aware of that. He was, in a word, disruptive. Much like the Brad Pitt character in the movie ""Moneyball"" disrupted the old model of how to scout baseball players, Nate disrupted the traditional model of how to cover politics.
She added, ""A number of traditional and well-respected Times journalists disliked his work."" Later, Sullivan wrote in The Times that ""I don't feel so good about not being able to investigate every complaint from every individual reader fully, or about making some misjudgments in individual posts — my Nate Silver commentary, among others, has probably been off-base..."".New York magazine reported that executive editor Jill Abramson ""put on a full-court press"" to keep Silver at The Times and that ""for Abramson, Silver was a tentpole attraction for her favorite subject, national politics, and brought the kind of buzz she thought valuable"", but the company's CEO and President Mark Thompson ""confirmed that keeping Silver was not at the top of his agenda."" The article stated that ""the major reason Silver left was because he felt it was Thompson who had not committed to building his franchise. The mixed signals from Thompson and Abramson—his lack of enthusiasm for committing resources to Silver, her desire to keep a major star—frustrated Silver and his lawyer.""

FiveThirtyEight under ESPN ownership
When FiveThirtyEight was relaunched under ESPN's ownership on March 17, 2014, Silver outlined the scope of topics that would be covered under the rubric of ""data journalism"":
We've expanded our staff from two full-time journalists to 20 and counting. Few of them will focus on politics exclusively; instead, our coverage will span five major subject areas — politics, economics, science, life and sports.
Our team also has a broad set of skills and experience in methods that fall under the rubric of data journalism. These include statistical analysis, but also data visualization, computer programming and data-literate reporting. So in addition to written stories, we'll have interactive graphics and features. Within a couple of months we'll launch a podcast, and we'll be collaborating with ESPN Films and Grantland to produce original documentary films.

FiveThirtyEight's election forecasts
2008 U.S. elections
In March 2008, Silver established his blog FiveThirtyEight.com, in which he developed a system for tracking polls and forecasting the outcome of the 2008 general election. At the same time, he continued making forecasts of the 2008 Democratic primary elections. That several of his forecasts based on demographic analysis proved to be substantially more accurate than those of the professional pollsters gained visibility and professional credibility for ""Poblano"", the pseudonym that Silver was then using.After the North Carolina and Indiana primaries on May 6, the popularity of FiveThirtyEight.com surged. Silver recalls the scenario: 'I know the polls show it's really tight in NC, but we think Obama is going to win by thirteen, fourteen points, and he did. ... Any time you make a prediction like that people give you probably too much credit for it.... But after that [Silver's and the website's popularity] started to really take off. It's pretty nonlinear, once you get one mention in the mainstream media, other people [quickly follow suit]'"".As a CNET reporter wrote on election eve, ""Even though Silver launched the site as recently as March, its straightforward approach, daring predictions, and short but impressive track record has put it on the map of political sites to follow. The Washington Post featured Silver in its 14th annual election prediction contest this year, and he'll be reporting on Tuesday night's results with Dan Rather on HDNet"".Silver's final 2008 presidential election forecast accurately predicted the winner of 49 of the 50 states as well as the District of Columbia (missing only the prediction for Indiana). As his model predicted, the races in Missouri and North Carolina were particularly close. He also correctly predicted the winners of every U.S. Senate race.  The accuracy of his predictions won him further acclaim, including abroad, and added to his reputation as a leading political prognosticator.Barack Obama's 2008 presidential campaign signed off on a proposal to share all of its private polling with Silver. After signing a confidentiality agreement, Silver was granted access to hundreds of polls the campaign had conducted.

2010 U.S. elections
Shortly after FiveThirtyEight relocated to The New York Times, Silver introduced his prediction models for the 2010 elections to the U.S. Senate, the U.S. House of Representatives, and state governorships. Each of these models relied initially on a combination of electoral history, demographics, and polling. Silver eventually published detailed forecasts and analyses of the results for all three sets of elections. He correctly predicted the winner in 34 of the 37 contested Senate races.  His 2010 congressional mid-term predictions were not as accurate as those made in 2008, but were still within the reported confidence interval. Silver predicted a Republican pickup of 54 seats in the House of Representatives; the GOP won 63 seats. Of the 37 gubernatorial races, FiveThirtyEight correctly predicted the winner of 36.

2012 U.S. elections
Although throughout 2011 Silver devoted a lot of attention on his blog to the 2012 Republican party primaries, his first effort to handicap the 2012 Presidential general election appeared as the cover story in The New York Times Magazine a year prior to the election: ""Is Obama Toast? Handicapping the 2012 Election"". Accompanying the online release of this article, Silver also published ""Choose Obama's Re-Election Adventure"", an interactive toy that allowed readers to predict the outcome of the election based on their assumptions about three variables: President Obama's favorability ratings, the rate of GDP growth, and how conservative the Republican opponent would be. This analysis stimulated a lot of critical discussion.While publishing numerous stories on the Republican primary elections, in mid-February 2012 Silver reprised and updated his previous Magazine story with another one, ""What Obama Should Do Next"". This story painted a more optimistic picture of President Obama's re-election chances. A companion article on his FiveThirtyEight blog, ""The Fundamentals Now Favor Obama"", explained how the model and the facts on the ground had changed between November and February.Silver published the first iteration of his 2012 general election forecasts on June 7, 2012. According to the model, at that time Barack Obama was projected to win 291 electoral votes—21 more than the 270 required for a majority. Obama then had an estimated 61.8% chance of winning a majority.On the morning of the November 6, 2012, presidential election, the final update of Silver's model at 10:10 A.M. gave President Barack Obama a 90.9% chance of winning a majority of the 538 electoral votes. Both in summary tables and in an electoral map, Silver forecast the winner of each state. At the conclusion of that day, when Mitt Romney had conceded to Barack Obama, Silver's model had correctly predicted the winner of every one of the 50 states and the District of Columbia. Silver, along with at least three academic-based analysts—Drew Linzer, Simon Jackman, and Josh Putnam—who also aggregated polls from multiple pollsters—thus was not only broadly correct about the election outcome, but also specifically predicted the outcomes for the nine swing states. In contrast, individual pollsters were less successful. For example, Rasmussen Reports ""missed on six of its nine swing-state polls"".

2016 U.S. elections
In the week leading up to the 2016 U.S. presidential election, the FiveThirtyEight team predicted that Hillary Clinton had a 64.5% chance of winning the election. Their final prediction on November 8, 2016, gave Clinton a 71% chance to win the 2016 United States presidential election, while other major forecasters had predicted Clinton to win with at least an 85% to 99% probability. Donald Trump won the election.  FiveThirtyEight argued it projected a much higher chance (29%) of Donald Trump winning the presidency than other modelers, a projection which was criticized days before the election by Ryan Grim of The Huffington Post as ""unskewing"" too much in favor of Trump.

Reception
Silver has been criticized for inaccurate predictions. In January 2010, journalist and blogger Colby Cosh criticized Silver's performance during the Massachusetts special Senate election, saying he was ""still arguing as late as Thursday afternoon that [Martha] Coakley was the clear favourite; he changed his mind at midnight that evening and acknowledged that Scott Brown had a puncher's chance."" (Brown won the election.)Silver's quantitative focus on polling data, without insight from experience in political organizing or journalism, has been a recurring critique from experienced commentators. Huffington Post columnist Geoffrey Dunn described Silver as someone who ""has never organized a precinct in his life, much less walked one, pontificating about the dynamics in the electoral processes as if he actually understood them.""Considerable criticism during the 2012 elections came from political conservatives, who argued that Silver's election projections were politically biased against Mitt Romney, the Republican candidate for president. For example, Silver was accused of applying a double standard to his treatment of Rasmussen Reports polls, such as a 2010 analysis asserting a statistical bias in its methodology. Josh Jordan wrote in National Review that Silver clearly favored Obama and adjusted the weight he gave polls ""based on what [he] think[s] of the pollster and the results and not based on what is actually inside the poll"".On MSNBC's Morning Joe, host Joe Scarborough stated that Silver's prediction that day of a 73.6% chance of a win for Obama greatly exceeded the confidence of the Obama campaign itself, which Scarborough equated to that of the Romney campaign, both believing ""they have a 50.1 percent chance of winning"", and calling Silver an ""ideologue"" and a ""joke"". Silver responded with the offer of a $1,000 wager (for charity) over the outcome of the election. The New York Times public editor Margaret Sullivan, while defending Silver's analysis, characterized the wager as ""a bad idea"" as it gave the appearance of a partisan motive for Silver, and ""inappropriate"" for someone perceived as a Times journalist (although Silver was not a member of the newspaper's staff).After a post-election appearance by Silver on Joe Scarborough's Morning Joe, Scarborough published what he called a ""(semi) apology"", in which he concluded:
{{Quote|I won't apologize to Mr. Silver for predicting an outcome that I had also been predicting for a year. But I do need to tell Nate I'm sorry for leaning in too hard and lumping him with pollsters whose methodology is as rigorous as the Simpsons' strip mall physician, Dr. Nick. For those sins (and a multitude of others that I'm sure I don't even know about), I am sorry.
Politics is a messy sport. And just as ball players who drink beer and eat fried chicken in dugouts across America can screw up the smartest sabermetrician's forecast, Nate Silver's formula is sure to let his fervent admirers down from time to time. But judging from what I saw of him this morning, Nate is a grounded guy who admits as much in his book. I was too tough on him and there's a 84.398264% chance I will be less dismissive of his good work in the future"".Silver's nondisclosure of the details of his analytical model has resulted in some skepticism. Washington Post journalist Ezra Klein wrote: ""There are good criticisms to make of Silver's model, not the least of which is that, while Silver is almost tediously detailed about what's going on in the model, he won't give out the code, and without the code, we can't say with certainty how the model works."" Colby Cosh wrote that the model ""is proprietary and irreproducible. That last feature makes it unwise to use Silver's model as a straw stand-in for ""science"", as if the model had been fully specified in a peer-reviewed journal"".

The Signal and the Noise
The Signal and The Noise was published in the United States on September 27, 2012. It reached the New York Times Best Sellers List as #12 for non-fiction hardback books after its first week in print. It dropped to #20 in the second week, before rising to #13 in the third, and remaining on the non-fiction hardback top 15 list for the following 13 weeks, with a highest weekly ranking of #4. Sales increased after the election on November 6, jumping 800% and becoming the second best seller on Amazon.com.The book describes methods of mathematical model-building using probability and statistics. Silver takes a big-picture approach to using statistical tools, combining sources of unique data (e.g., timing a minor league ball player's fastball using a radar gun), with historical data and principles of sound statistical analysis; Silver argues that many of these are violated by many pollsters and pundits who nonetheless have important media roles. Case studies in the book include baseball, elections, climate change, the financial crash, poker, and weather forecasting. These different topics illustrate different statistical principles. As a reviewer in The New York Times notes: ""It's largely about evaluating predictions in a variety of fields, from finance to weather to epidemiology. We learn about a handful of successes: when, for instance, meteorologists predict a hurricane's landfall 72 hours in advance.... But mostly we learn about failures. It turns out we're not even close to predicting the next catastrophic earthquake or the spread of the next killer bird flu, despite the enormous amounts of brainpower trained on these questions in the past few decades"".

Blogs and other publications
BlogsFiveThirtyEight blog, now owned by ABC
Nate Silver's Baseball Prospectus article archive (2003–2009)
Nate Silver's The Burrito Bracket (2007)Other publicationsNate Silver, ""The Most Livable Neighborhoods in New York: A Quantitative Index of the 50 Most Satisfying Places to Live"", New York, April 11, 2010.
Nate Silver, ""The Influence Index"", Time, April 29, 2010.
Nate Silver and Walter Hickey, ""Best Picture Math"", Vanity Fair, March 2014.
Gareth Cook (Editor), Nate Silver (Introduction). The Best American Infographics 2014, Houghton Mifflin Harcourt. ISBN 978-0547974514.
Review of two children's books, with an autobiographical comment: ""Beautiful Minds: The Boy Who Loved Math and On a Beam of Light"", The New York Times, July 12, 2013.
Andrew Gelman, Nate Silver, Aaron S. Edlin, ""What Is the Probability Your Vote Will Make a Difference"", Economic Inquiry, 2012, 50(2): 321–26.
In addition to chapters in several issues of the Baseball Prospectus annual, Silver contributed chapters to one-off monographs edited by Baseball Prospectus, including:
Mind Game: How the Boston Red Sox Got Smart, Won a World Series, and Created a New Blueprint for Winning. Steven Goldman, Ed. New York: Workman Publishing Co., 2005. ISBN 0-7611-4018-2.
Baseball Between the Numbers: Why Everything You Know about the Game Is Wrong. Jonah Keri, Ed. New York: Basic Books, 2006. ISBN 0-465-00596-9 (hardback) and ISBN 0-465-00547-0 (paperback).
It Ain't over 'til It's over: The Baseball Prospectus Pennant Race Book. Steven Goldman, Ed. New York: Basic Books. Hardback 2007. ISBN 0-465-00284-6; paperback 2008. ISBN 0-465-00285-4.

Media
Silver's self-unmasking at the end of May 2008 brought him a lot of publicity focused on his combined skill as both baseball statistician-forecaster and political statistician-forecaster, including articles about him in The Wall Street Journal, Newsweek, Science News, and his hometown Lansing State Journal.In early June he began to cross-post his daily ""Today's Polls"" updates on ""The Plank"" in The New Republic. Also, Rasmussen Reports began to use the FiveThirtyEight.com poll averages for its own tracking of the 2008 state-by-state races.In 2009 through 2012, Silver appeared as a political analyst on MSNBC, CNN and Bloomberg Television, PBS, NPR, Democracy Now!, The Charlie Rose Show, ABC News, and Current TV.Silver also appeared on The Colbert Report (October 7, 2008 and November 5, 2012), The Daily Show (October 17, 2012 and November 7, 2012), and Real Time with Bill Maher (October 26, 2012).That Silver accurately predicted the outcome of the 2012 presidential race, in the face of numerous public attacks on his forecasts by critics, inspired many articles in the press, ranging from Gizmodo, to online and mainstream newspapers, news and commentary magazines, business media, trade journals, media about media, and Scientific American, as well as a feature interview on The Today Show, a return appearance on The Daily Show, and an appearance on Morning JoeSilver's first appearance on ABC News as Editor-in-Chief of the new FiveThirtyEight.com was on George Stephanopoulos's This Week on November 3, 2013.Silver is referenced in the Syfy channel show The Magicians as an earth wizard who uses polling spells.In 2015, Silver appeared on the podcast Employee of the Month, where he criticized Vox Media for ""recycling Wikipedia entries"" in their content.

Selected recognition and awards
April 30, 2009: Silver was named one of ""The World's 100 Most Influential People"" by TIME magazine.
May 12, 2013: Silver received an honorary Doctor of Science degree (Scientiæ Doctor honoris causa – D.Sc. h.c.) and gave the commencement address at Ripon College.
May 24, 2013: Silver received an honorary Doctor of Literature degree (Doctor of Literature honoris causa) and presented a commencement address at The New School.
October 2013: Silver's The Signal and the Noise won the 2013 Phi Beta Kappa Award in Science, which recognizes ""outstanding contributions by scientists to the literature of science"".
December 2013: The University of Leuven (KU Leuven) (Belgium) and the Leuven Statistics Research Centre awarded Silver an honorary doctoral degree ""for his prominent role in the development, application, and dissemination of proper prediction methods in sports and in political sciences"".
May 25, 2014: Silver received a Doctorate of Humane Letters, honoris causa, from Amherst College.
May 2017: Georgetown University awarded Silver a degree of Doctor of Humane Letters honoris causa.
May 2018: Kenyon College awarded Silver a degree, Doctor of Humane Letters.

Personal life
In a 2012 interview with Charlie Rose he said, ""I'd say I’m somewhere in between being a libertarian and a liberal. So if I were to vote, it would be kind of a Gary Johnson versus Mitt Romney decision, I suppose.""Silver is a great-nephew of geologists Caswell Silver and Leon Silver. He is a great-grandson of Harmon Lewis, the President of Alcoa Steamship Company, as well as a great-great-nephew of the embryologist Warren Harmon Lewis and his wife, biologist Margaret Reed Lewis.Silver is openly gay. ""I've always felt like something of an outsider. I've always had friends but I've always come from an outside point of view. I think that's important. If you grow up gay, or in a household that's agnostic, when most people are religious, then from the get-go, you are saying there are things that the majority of society believes that I don't believe"", he told an interviewer in 2012. ""When asked what made you feel more of a misfit, being gay or being a geek, he replied, 'Probably the numbers stuff since I had that from when I was six.'"" When asked in 2008 if he had noticed people looking at him as a ""gay icon"", he responded, ""I've started to notice it a little bit, although so far it seems like I'm more a subject of geek affection than gay affection"".Silver discussed his sexuality in the context of growing up in East Lansing, in an article about the Supreme Court ruling Obergefell v. Hodges in favor of recognizing same-sex marriage on the date of its announcement. He analyzed the speed of the change of public sentiment, pointing out that the change over only several decades has been palpable to the current generations.Silver has long been interested in fantasy baseball, especially Scoresheet Baseball. While in college he served as an expert on Scoresheet Baseball for BaseballHQ. When he took up political writing, Silver abandoned his blog, The Burrito Bracket, in which he ran a one-and-done competition among the taquerias in his Wicker Park neighborhood in Chicago.Silver plays poker semi-professionally.

See also
Computational statistics
Editorial board
List of self-identified LGBTQ New Yorkers
New Yorkers in journalism
Political analysis
Sports betting

References
Further reading
Calderone, Michael (August 15, 2019). ""Behind Nate Silver's war with The New York Times"". Politico. Retrieved August 18, 2019.

External links
Appearances on C-SPAN  
Nate Silver at IMDb",https://en.wikipedia.org/wiki/Nate_Silver,"['1978 births', '21st-century American journalists', '21st-century American mathematicians', 'ABC News personalities', 'All articles with dead external links', 'All articles with unsourced statements', 'American bloggers', 'American male bloggers', 'American male journalists', 'American people of English descent', 'American people of German descent', 'American people of Jewish descent', 'American poker players', 'American political commentators', 'American political writers', 'American statisticians', 'Articles with dead external links from February 2018', 'Articles with hCards', 'Articles with permanently dead external links', 'Articles with short description', 'Articles with unsourced statements from April 2017', 'Baseball statisticians', 'Commons category link from Wikidata', 'Data journalists', 'ESPN people', 'Gay writers', 'Journalists from Michigan', 'Journalists from New York City', 'KPMG people', 'LGBT journalists from the United States', 'LGBT people from Michigan', 'LGBT scientists', 'LGBT scientists from the United States', 'LGBT writers from the United States', 'Living people', 'Mathematicians from Michigan', 'Mathematicians from New York (state)', 'Online journalists', 'People from East Lansing, Michigan', 'Psephologists', 'Science bloggers', 'Short description is different from Wikidata', 'Sportswriters from New York (state)', 'The New York Times writers', 'University of Chicago alumni', 'Use mdy dates from January 2019', 'Webarchive template archiveis links', 'Webarchive template wayback links', 'Wikipedia articles with BNE identifiers', 'Wikipedia articles with CANTIC identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with ICCU identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with NDL identifiers', 'Wikipedia articles with NKC identifiers', 'Wikipedia articles with NLI identifiers', 'Wikipedia articles with NLK identifiers', 'Wikipedia articles with NTA identifiers', 'Wikipedia articles with PLWABN identifiers', 'Wikipedia articles with SUDOC identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers']",Data Science
151,Q-learning,"Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence ""model-free""), and it can handle problems with stochastic transitions and rewards without requiring adaptations.
For any finite Markov decision process (FMDP), Q-learning finds an optimal policy in the sense of maximizing the expected value of the total reward over any and all successive steps, starting from the current state. Q-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy. ""Q"" refers to the function that the algorithm computes - the expected rewards for an action taken in a given state.

Reinforcement learning
Reinforcement learning involves an agent, a set of states 
  
    
      
        S
      
    
    {\displaystyle S}
  , and a set 
  
    
      
        A
      
    
    {\displaystyle A}
   of actions per state. By performing an action 
  
    
      
        a
        ∈
        A
      
    
    {\displaystyle a\in A}
  , the agent transitions from state to state. Executing an action in a specific state provides the agent with a reward (a numerical score).
The goal of the agent is to maximize its total reward. It does this by adding the maximum reward attainable from future states to the reward for achieving its current state, effectively influencing the current action by the potential future reward. This potential reward is a weighted sum of the expected values of the rewards of all future steps starting from the current state.
As an example, consider the process of boarding a train, in which the reward is measured by the negative of the total time spent boarding (alternatively, the cost of boarding the train is equal to the boarding time). One strategy is to enter the train door as soon as they open, minimizing the initial wait time for yourself. If the train is crowded, however, then you will have a slow entry after the initial action of entering the door as people are fighting you to depart the train as you attempt to board. The total boarding time, or cost, is then:

0 seconds wait time + 15 seconds fight timeOn the next day, by random chance (exploration), you decide to wait and let other people depart first. This initially results in a longer wait time. However, time fighting other passengers is less. Overall, this path has a higher reward than that of the previous day, since the total boarding time is now:

5 second wait time + 0 second fight timeThrough exploration, despite the initial (patient) action resulting in a larger cost (or negative reward) than in the forceful strategy, the overall cost is lower, thus revealing a more rewarding strategy.

Algorithm
After 
  
    
      
        Δ
        t
      
    
    {\displaystyle \Delta t}
   steps into the future the agent will decide some next step. The weight for this step is calculated as 
  
    
      
        
          γ
          
            Δ
            t
          
        
      
    
    {\displaystyle \gamma ^{\Delta t}}
  , where 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
   (the discount factor) is a number between 0 and 1 (
  
    
      
        0
        ≤
        γ
        ≤
        1
      
    
    {\displaystyle 0\leq \gamma \leq 1}
  ) and has the effect of valuing rewards received earlier higher than those received later (reflecting the value of a ""good start""). 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
   may also be interpreted as the probability to succeed (or survive) at every step 
  
    
      
        Δ
        t
      
    
    {\displaystyle \Delta t}
  .
The algorithm, therefore, has a function that calculates the quality of a state–action combination:

  
    
      
        Q
        :
        S
        ×
        A
        →
        
          R
        
      
    
    {\displaystyle Q:S\times A\to \mathbb {R} }
  .Before learning begins, 
  
    
      
        Q
      
    
    {\displaystyle Q}
   is initialized to a possibly arbitrary fixed value (chosen by the programmer). Then, at each time 
  
    
      
        t
      
    
    {\displaystyle t}
   the agent selects an action 
  
    
      
        
          a
          
            t
          
        
      
    
    {\displaystyle a_{t}}
  , observes a reward 
  
    
      
        
          r
          
            t
          
        
      
    
    {\displaystyle r_{t}}
  , enters a new state 
  
    
      
        
          s
          
            t
            +
            1
          
        
      
    
    {\displaystyle s_{t+1}}
   (that may depend on both the previous state 
  
    
      
        
          s
          
            t
          
        
      
    
    {\displaystyle s_{t}}
   and the selected action), and 
  
    
      
        Q
      
    
    {\displaystyle Q}
   is updated. The core of the algorithm is a Bellman equation as a simple value iteration update, using the weighted average of the old value and the new information:

  
    
      
        
          Q
          
            n
            e
            w
          
        
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
        ←
        
          
            
              
                Q
                (
                
                  s
                  
                    t
                  
                
                ,
                
                  a
                  
                    t
                  
                
                )
              
              ⏟
            
          
          
            old value
          
        
        +
        
          
            
              α
              ⏟
            
          
          
            learning rate
          
        
        ⋅
        
          
            
              
                
                  
                    (
                  
                
                
                  
                    
                      
                        
                          
                            
                              
                                r
                                
                                  t
                                
                              
                              ⏟
                            
                          
                          
                            reward
                          
                        
                        +
                        
                          
                            
                              γ
                              ⏟
                            
                          
                          
                            discount factor
                          
                        
                        ⋅
                        
                          
                            
                              
                                
                                  max
                                  
                                    a
                                  
                                
                                Q
                                (
                                
                                  s
                                  
                                    t
                                    +
                                    1
                                  
                                
                                ,
                                a
                                )
                              
                              ⏟
                            
                          
                          
                            estimate of optimal future value
                          
                        
                      
                      ⏟
                    
                  
                  
                    new value (temporal difference target)
                  
                
                −
                
                  
                    
                      
                        Q
                        (
                        
                          s
                          
                            t
                          
                        
                        ,
                        
                          a
                          
                            t
                          
                        
                        )
                      
                      ⏟
                    
                  
                  
                    old value
                  
                
                
                  
                    )
                  
                
              
              ⏞
            
          
          
            temporal difference
          
        
      
    
    {\displaystyle Q^{new}(s_{t},a_{t})\leftarrow \underbrace {Q(s_{t},a_{t})} _{\text{old value}}+\underbrace {\alpha } _{\text{learning rate}}\cdot \overbrace {{\bigg (}\underbrace {\underbrace {r_{t}} _{\text{reward}}+\underbrace {\gamma } _{\text{discount factor}}\cdot \underbrace {\max _{a}Q(s_{t+1},a)} _{\text{estimate of optimal future value}}} _{\text{new value (temporal difference target)}}-\underbrace {Q(s_{t},a_{t})} _{\text{old value}}{\bigg )}} ^{\text{temporal difference}}}
  where 
  
    
      
        
          r
          
            t
          
        
      
    
    {\displaystyle r_{t}}
   is the reward received when moving from the state 
  
    
      
        
          s
          
            t
          
        
      
    
    {\displaystyle s_{t}}
   to the state 
  
    
      
        
          s
          
            t
            +
            1
          
        
      
    
    {\displaystyle s_{t+1}}
  , and 
  
    
      
        α
      
    
    {\displaystyle \alpha }
   is the learning rate 
  
    
      
        (
        0
        <
        α
        ≤
        1
        )
      
    
    {\displaystyle (0<\alpha \leq 1)}
  .
Note that 
  
    
      
        
          Q
          
            n
            e
            w
          
        
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
      
    
    {\displaystyle Q^{new}(s_{t},a_{t})}
   is the sum of three factors:

  
    
      
        (
        1
        −
        α
        )
        Q
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
      
    
    {\displaystyle (1-\alpha )Q(s_{t},a_{t})}
  : the current value weighted by the learning rate. Values of the learning rate near to 1 made faster the changes in Q.

  
    
      
        α
        
        
          r
          
            t
          
        
      
    
    {\displaystyle \alpha \,r_{t}}
  : the reward 
  
    
      
        
          r
          
            t
          
        
        =
        r
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
      
    
    {\displaystyle r_{t}=r(s_{t},a_{t})}
   to obtain if action 
  
    
      
        
          a
          
            t
          
        
      
    
    {\displaystyle a_{t}}
   is taken when in state 
  
    
      
        
          s
          
            t
          
        
      
    
    {\displaystyle s_{t}}
   (weighted by learning rate)

  
    
      
        α
        γ
        
          max
          
            a
          
        
        Q
        (
        
          s
          
            t
            +
            1
          
        
        ,
        a
        )
      
    
    {\displaystyle \alpha \gamma \max _{a}Q(s_{t+1},a)}
  : the maximum reward that can be obtained from state 
  
    
      
        
          s
          
            t
            +
            1
          
        
      
    
    {\displaystyle s_{t+1}}
  (weighted by learning rate and discount factor)An episode of the algorithm ends when state 
  
    
      
        
          s
          
            t
            +
            1
          
        
      
    
    {\displaystyle s_{t+1}}
   is a final or terminal state. However, Q-learning can also learn in non-episodic tasks. If the discount factor is lower than 1, the action values are finite even if the problem can contain infinite loops.
For all final states 
  
    
      
        
          s
          
            f
          
        
      
    
    {\displaystyle s_{f}}
  , 
  
    
      
        Q
        (
        
          s
          
            f
          
        
        ,
        a
        )
      
    
    {\displaystyle Q(s_{f},a)}
   is never updated, but is set to the reward value 
  
    
      
        r
      
    
    {\displaystyle r}
   observed for state 
  
    
      
        
          s
          
            f
          
        
      
    
    {\displaystyle s_{f}}
  . In most cases, 
  
    
      
        Q
        (
        
          s
          
            f
          
        
        ,
        a
        )
      
    
    {\displaystyle Q(s_{f},a)}
   can be taken to equal zero.

Influence of variables
Learning Rate
The learning rate or step size determines to what extent newly acquired information overrides old information. A factor of 0 makes the agent learn nothing (exclusively exploiting prior knowledge), while a factor of 1 makes the agent consider only the most recent information (ignoring prior knowledge to explore possibilities). In fully deterministic environments, a learning rate of 
  
    
      
        
          α
          
            t
          
        
        =
        1
      
    
    {\displaystyle \alpha _{t}=1}
   is optimal. When the problem is stochastic, the algorithm converges under some technical conditions on the learning rate that require it to decrease to zero. In practice, often a constant learning rate is used, such as 
  
    
      
        
          α
          
            t
          
        
        =
        0.1
      
    
    {\displaystyle \alpha _{t}=0.1}
   for all 
  
    
      
        t
      
    
    {\displaystyle t}
  .

Discount factor
The discount factor 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
   determines the importance of future rewards. A factor of 0 will make the agent ""myopic"" (or short-sighted) by only considering current rewards, i.e. 
  
    
      
        
          r
          
            t
          
        
      
    
    {\displaystyle r_{t}}
   (in the update rule above), while a factor approaching 1 will make it strive for a long-term high reward. If the discount factor meets or exceeds 1, the action values may diverge. For 
  
    
      
        γ
        =
        1
      
    
    {\displaystyle \gamma =1}
  , without a terminal state, or if the agent never reaches one, all environment histories become infinitely long, and utilities with additive, undiscounted rewards generally become infinite. Even with a discount factor only slightly lower than 1, Q-function learning leads to propagation of errors and instabilities when the value function is approximated with an artificial neural network. In that case, starting with a lower discount factor and increasing it towards its final value accelerates learning.

Initial conditions (Q0)
Since Q-learning is an iterative algorithm, it implicitly assumes an initial condition before the first update occurs. High initial values, also known as ""optimistic initial conditions"", can encourage exploration: no matter what action is selected, the update rule will cause it to have lower values than the other alternative, thus increasing their choice probability. The first reward 
  
    
      
        r
      
    
    {\displaystyle r}
   can be used to reset the initial conditions. According to this idea, the first time an action is taken the reward is used to set the value of 
  
    
      
        Q
      
    
    {\displaystyle Q}
  . This allows immediate learning in case of fixed deterministic rewards. A model that incorporates reset of initial conditions (RIC) is expected to predict participants' behavior better than a model that assumes any arbitrary initial condition (AIC). RIC seems to be consistent with human behaviour in repeated binary choice experiments.

Implementation
Q-learning at its simplest stores data in tables. This approach falters with increasing numbers of states/actions since the likelihood of the agent visiting a particular state and performing a particular action is increasingly small.

Function approximation
Q-learning can be combined with function approximation. This makes it possible to apply the algorithm to larger problems, even when the state space is continuous.
One solution is to use an (adapted) artificial neural network as a function approximator. Function approximation may speed up learning in finite problems, due to the fact that the algorithm can generalize earlier experiences to previously unseen states.

Quantization
Another technique to decrease the state/action space quantizes possible values. Consider the example of learning to balance a stick on a finger. To describe a state at a certain point in time involves the position of the finger in space, its velocity, the angle of the stick and the angular velocity of the stick. This yields a four-element vector that describes one state, i.e. a snapshot of one state encoded into four values. The problem is that infinitely many possible states are present. To shrink the possible space of valid actions multiple values can be assigned to a bucket. The exact distance of the finger from its starting position (-Infinity to Infinity) is not known, but rather whether it is far away or not (Near, Far).

History
Q-learning was introduced by Chris Watkins in 1989. A convergence proof was presented by Watkins and Dayan in 1992.
Watkins was addressing “Learning from delayed rewards”, the title of his PhD thesis. Eight years earlier in 1981 the same problem, under the name of “Delayed reinforcement learning”, was solved by Bozinovski's Crossbar Adaptive Array (CAA). The memory matrix 
  
    
      
        W
        =
        ‖
        w
        (
        a
        ,
        s
        )
        ‖
      
    
    {\displaystyle W=\|w(a,s)\|}
   was the same as the eight years later Q-table of Q-learning. The architecture introduced the term “state evaluation” in reinforcement learning. The crossbar learning algorithm, written in mathematical pseudocode in the paper, in each iteration performs the following computation:

In state s perform action a;
Receive consequence state s';
Compute state evaluation 
  
    
      
        v
        (
        
          s
          ′
        
        )
      
    
    {\displaystyle v(s')}
  ;
Update crossbar value 
  
    
      
        
          w
          ′
        
        (
        a
        ,
        s
        )
        =
        w
        (
        a
        ,
        s
        )
        +
        v
        (
        
          s
          ′
        
        )
      
    
    {\displaystyle w'(a,s)=w(a,s)+v(s')}
  .The term “secondary reinforcement” is borrowed from animal learning theory, to model state values via backpropagation: the state value 
  
    
      
        v
        (
        
          s
          ′
        
        )
      
    
    {\displaystyle v(s')}
   of the consequence situation is backpropagated to the previously encountered situations. CAA computes state values vertically and actions horizontally (the ""crossbar""). Demonstration graphs showing delayed reinforcement learning contained states (desirable, undesirable, and neutral states), which were computed by the state evaluation function. This learning system was a forerunner of the Q-learning algorithm.In 2014, Google DeepMind patented an application of Q-learning to deep learning, titled ""deep reinforcement learning"" or ""deep Q-learning"" that can play Atari 2600 games at expert human levels.

Variants
Deep Q-learning
The DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy of the agent and the data distribution, and the correlations between Q and the target values.
The technique used experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed. This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative updates adjust Q towards target values that are only periodically updated, further reducing correlations with the target.

Double Q-learning
Because the future maximum approximated action value in Q-learning is evaluated using the same Q function as in current action selection policy, in noisy environments Q-learning can sometimes overestimate the action values, slowing the learning. A variant called Double Q-learning was proposed to correct this. Double Q-learning is an off-policy reinforcement learning algorithm, where a different policy is used for value evaluation than what is used to select the next action.
In practice, two separate value functions are trained in a mutually symmetric fashion using separate experiences, 
  
    
      
        
          Q
          
            A
          
        
      
    
    {\displaystyle Q^{A}}
   and 
  
    
      
        
          Q
          
            B
          
        
      
    
    {\displaystyle Q^{B}}
  . The double Q-learning update step is then as follows:

  
    
      
        
          Q
          
            t
            +
            1
          
          
            A
          
        
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
        =
        
          Q
          
            t
          
          
            A
          
        
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
        +
        
          α
          
            t
          
        
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
        
          (
          
            
              r
              
                t
              
            
            +
            γ
            
              Q
              
                t
              
              
                B
              
            
            
              (
              
                
                  s
                  
                    t
                    +
                    1
                  
                
                ,
                
                  
                    
                      a
                      r
                      g
                       
                      m
                      a
                      x
                    
                  
                  
                    a
                  
                
                ⁡
                
                  Q
                  
                    t
                  
                  
                    A
                  
                
                (
                
                  s
                  
                    t
                    +
                    1
                  
                
                ,
                a
                )
              
              )
            
            −
            
              Q
              
                t
              
              
                A
              
            
            (
            
              s
              
                t
              
            
            ,
            
              a
              
                t
              
            
            )
          
          )
        
      
    
    {\displaystyle Q_{t+1}^{A}(s_{t},a_{t})=Q_{t}^{A}(s_{t},a_{t})+\alpha _{t}(s_{t},a_{t})\left(r_{t}+\gamma Q_{t}^{B}\left(s_{t+1},\mathop {\operatorname {arg~max} } _{a}Q_{t}^{A}(s_{t+1},a)\right)-Q_{t}^{A}(s_{t},a_{t})\right)}
  , and

  
    
      
        
          Q
          
            t
            +
            1
          
          
            B
          
        
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
        =
        
          Q
          
            t
          
          
            B
          
        
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
        +
        
          α
          
            t
          
        
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
        
          (
          
            
              r
              
                t
              
            
            +
            γ
            
              Q
              
                t
              
              
                A
              
            
            
              (
              
                
                  s
                  
                    t
                    +
                    1
                  
                
                ,
                
                  
                    
                      a
                      r
                      g
                       
                      m
                      a
                      x
                    
                  
                  
                    a
                  
                
                ⁡
                
                  Q
                  
                    t
                  
                  
                    B
                  
                
                (
                
                  s
                  
                    t
                    +
                    1
                  
                
                ,
                a
                )
              
              )
            
            −
            
              Q
              
                t
              
              
                B
              
            
            (
            
              s
              
                t
              
            
            ,
            
              a
              
                t
              
            
            )
          
          )
        
        .
      
    
    {\displaystyle Q_{t+1}^{B}(s_{t},a_{t})=Q_{t}^{B}(s_{t},a_{t})+\alpha _{t}(s_{t},a_{t})\left(r_{t}+\gamma Q_{t}^{A}\left(s_{t+1},\mathop {\operatorname {arg~max} } _{a}Q_{t}^{B}(s_{t+1},a)\right)-Q_{t}^{B}(s_{t},a_{t})\right).}
  Now the estimated value of the discounted future is evaluated using a different policy, which solves the overestimation issue.
This algorithm was later modified in 2015 and combined with deep learning, as in the DQN algorithm, resulting in Double DQN, which outperforms the original DQN algorithm.

Others
Delayed Q-learning is an alternative implementation of the online Q-learning algorithm, with probably approximately correct (PAC) learning.Greedy GQ is a variant of Q-learning to use in combination with (linear) function approximation. The advantage of Greedy GQ is that convergence is guaranteed even when function approximation is used to estimate the action values.

Limitations
The standard Q-learning algorithm (using a 
  
    
      
        Q
      
    
    {\displaystyle Q}
   table) applies only to discrete action and state spaces. Discretization of these values leads to inefficient learning, largely due to the curse of dimensionality. However, there are adaptations of Q-learning that attempt solve this problem such as Wire-fitted Neural Network Q-Learning.

See also
Reinforcement learning
Temporal difference learning
SARSA
Iterated prisoner's dilemma
Game theory

References
External links
Watkins, C.J.C.H. (1989). Learning from Delayed Rewards. PhD thesis, Cambridge University, Cambridge, England.
Strehl, Li, Wiewiora, Langford, Littman (2006). PAC model-free reinforcement learning
Reinforcement Learning: An Introduction by Richard Sutton and Andrew S. Barto, an online textbook. See ""6.5 Q-Learning: Off-Policy TD Control"".
Piqle: a Generic Java Platform for Reinforcement Learning
Reinforcement Learning Maze, a demonstration of guiding an ant through a maze using Q-learning
Q-learning work by Gerald Tesauro",https://en.wikipedia.org/wiki/Q-learning,"['All articles with unsourced statements', 'Articles with unsourced statements from December 2017', 'CS1 errors: missing periodical', 'Machine learning algorithms', 'Reinforcement learning', 'Wikipedia articles needing clarification from January 2020']",Data Science
152,Proper generalized decomposition,"The proper generalized decomposition (PGD) is an iterative numerical method for solving boundary value problems (BVPs), that is, partial differential equations constrained by a set of boundary conditions.
The PGD algorithm computes an approximation of the solution of the BVP by successive enrichment. This means that, in each iteration, a new component (or mode) is computed and added to the approximation. The more modes obtained, the closer the approximation is to its theoretical solution. By selecting only the first PGD modes, a reduced order model of the solution is obtained. Because of this, PGD is considered a dimensionality reduction algorithm. In addition, it is considered as a generalized form of the proper orthogonal decomposition.

Description
The proper generalized decomposition is a method characterized by (1) a variational formulation of the problem, (2) a discretization of the domain in the style of the finite element method, (3) the assumption that the solution can be approximated as a separated representation and (4) a numerical greedy algorithm to find the solution.The most implemented variational formulation in PGD is the Bubnov-Galerkin method, although other implementations exist.The discretization of the domain is a well defined set of procedures that cover (a) the creation of finite element meshes, (b) the definition of basis function on reference elements (also called shape functions) and (c) the mapping of reference elements onto the elements of the mesh.
PGD assumes that the solution u of a (multidimensional) problem can be approximated as a separated representation of the form

  
    
      
        
          u
        
        ≈
        
          
            u
          
          
            N
          
        
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            d
          
        
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        
          
            
              X
              
                1
              
            
          
          
            i
          
        
        (
        
          x
          
            1
          
        
        )
        ⋅
        
          
            
              X
              
                2
              
            
          
          
            i
          
        
        (
        
          x
          
            2
          
        
        )
        ⋯
        
          
            
              X
              
                d
              
            
          
          
            i
          
        
        (
        
          x
          
            d
          
        
        )
        ,
      
    
    {\displaystyle \mathbf {u} \approx \mathbf {u} ^{N}(x_{1},x_{2},\ldots ,x_{d})=\sum _{i=1}^{N}\mathbf {X_{1}} _{i}(x_{1})\cdot \mathbf {X_{2}} _{i}(x_{2})\cdots \mathbf {X_{d}} _{i}(x_{d}),}
  where the number of addends N and the functional products X1(x1), X2(x2), ..., Xd(xd), each depending on a variable (or variables), are unknown beforehand.
The solution is sought by applying a greedy algorithm, usually the fixed point algorithm, to the weak formulation of the problem. For each iteration i of the algorithm, a mode of the solution is computed. Each mode consists of a set of numerical values of the functional products X1(x1), ..., Xd(xd), which enrich the approximation of the solution. Note that due to the greedy nature of the algorithm, the term 'enrich' is used rather than 'improve'. The number of computed modes required to obtain an approximation of the solution below a certain error threshold depends on the stop criterium of the iterative algorithm.
Unlike POD, PGD modes are not necessarily orthogonal to each other.

Features
PGD is suitable for solving high-dimensional problems, since it overcomes the limitations of classical approaches. In particular, PGD avoids the curse of dimensionality, as solving decoupled problems is computationally much less expensive than solving multidimensional problems.
Therefore, PGD enables to re-adapt parametric problems into a multidimensional framework by setting the parameters of the problem as extra coordinates:

  
    
      
        
          u
        
        ≈
        
          
            u
          
          
            N
          
        
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            d
          
        
        ;
        
          k
          
            1
          
        
        ,
        …
        ,
        
          k
          
            p
          
        
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        
          
            
              X
              
                1
              
            
          
          
            i
          
        
        (
        
          x
          
            1
          
        
        )
        ⋯
        
          
            
              X
              
                d
              
            
          
          
            i
          
        
        (
        
          x
          
            d
          
        
        )
        ⋅
        
          
            
              K
              
                1
              
            
          
          
            i
          
        
        (
        
          k
          
            1
          
        
        )
        ⋯
        
          
            
              K
              
                p
              
            
          
          
            i
          
        
        (
        
          k
          
            p
          
        
        )
        ,
      
    
    {\displaystyle \mathbf {u} \approx \mathbf {u} ^{N}(x_{1},\ldots ,x_{d};k_{1},\ldots ,k_{p})=\sum _{i=1}^{N}\mathbf {X_{1}} _{i}(x_{1})\cdots \mathbf {X_{d}} _{i}(x_{d})\cdot \mathbf {K_{1}} _{i}(k_{1})\cdots \mathbf {K_{p}} _{i}(k_{p}),}
  where a series of functional products K1(k1), K2(k2), ..., Kp(kp), each depending on a parameter (or parameters), has been incorporated to the equation.
In this case, the obtained approximation of the solution is called computational vademecum: a general meta-model containing all the particular solutions for every possible value of the involved parameters.

Sparse Subspace Learning
The Sparse Subspace Learning (SSL) method leverages the use of hierarchical collocation to approximate the numerical solution of parametric models. With respect to traditional projection-based reduced order modeling, the use of a collocation enables non-intrusive approach based on sparse adaptive sampling of the parametric space. This allows to recover the lowdimensional structure of the parametric solution subspace while also learning the functional dependency from the parameters in explicit form. A sparse low-rank approximate tensor representation of the parametric solution can be built through an incremental strategy that only needs to have access to the output of a deterministic solver. Non-intrusiveness makes this approach straightforwardly applicable to challenging problems characterized by nonlinearity or non affine weak forms.


== References ==",https://en.wikipedia.org/wiki/Proper_generalized_decomposition,"['Boundary value problems', 'CS1 maint: multiple names: authors list', 'Dimension reduction', 'Mathematical modeling', 'Numerical analysis']",Data Science
153,Random forest,"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set. Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. However, data characteristics can affect their performance.The first algorithm for random decision forests was created by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the ""stochastic discrimination"" approach to classification proposed by Eugene Kleinberg.An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered ""Random Forests"" as a trademark (as of 2019, owned by Minitab, Inc.). The extension combines Breiman's ""bagging"" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.
Random forests are frequently used as ""blackbox"" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration in packages such as scikit-learn.

History
The general method of random decision forests was first proposed by Ho in 1995. Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected feature dimensions.  A subsequent work along the same lines concluded that other splitting methods behave similarly, as long as they are randomly forced to be insensitive to some feature dimensions.  Note that this observation of a more complex classifier (a larger forest) getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting.  The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination.The early development of Breiman's notion of random forests was influenced by the work of Amit and
Geman who introduced the idea of searching over a random subset of the
available decisions when splitting a node, in the context of growing a single
tree.  The idea of random subspace selection from Ho was also influential in the design of random forests.  In this method a forest of trees is grown,
and variation among the trees is introduced by projecting the training data
into a randomly chosen subspace before fitting each tree or each node.  Finally, the idea of
randomized node optimization, where the decision at each node is selected by a
randomized procedure, rather than a deterministic optimization was first
introduced by Dietterich.The introduction of random forests proper was first made in a paper
by Leo Breiman.  This paper describes a method of building a forest of
uncorrelated trees using a CART like procedure, combined with randomized node
optimization and bagging.  In addition, this paper combines several
ingredients, some previously known and some novel, which form the basis of the
modern practice of random forests, in particular:

Using out-of-bag error as an estimate of the generalization error.
Measuring variable importance through permutation.The report also offers the first theoretical result for random forests in the
form of a bound on the generalization error which depends on the strength of the
trees in the forest and their correlation.

Algorithm
Preliminaries: decision tree learning
Decision trees are a popular method for various machine learning tasks. Tree learning ""come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining"", say Hastie et al., ""because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate"".In particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model.
Forests are like the pulling together of decision tree algorithm efforts. Taking the teamwork of many trees thus improving the performance of a single random tree. Though not quite similar, forests give the effects of a K-fold cross validation.

Bagging
The training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set X = x1, ..., xn with responses Y = y1, ..., yn, bagging repeatedly (B times) selects a random sample with replacement of the training set and fits trees to these samples:

For b = 1, ..., B:
Sample, with replacement, n training examples from X, Y; call these Xb, Yb.
Train a classification or regression tree fb on Xb, Yb.After training, predictions for unseen samples x' can be made by averaging the predictions from all the individual regression trees on x':

  
    
      
        
          
            
              f
              ^
            
          
        
        =
        
          
            1
            B
          
        
        
          ∑
          
            b
            =
            1
          
          
            B
          
        
        
          f
          
            b
          
        
        (
        
          x
          ′
        
        )
      
    
    {\displaystyle {\hat {f}}={\frac {1}{B}}\sum _{b=1}^{B}f_{b}(x')}
  or by taking the majority vote in the case of classification trees.
This bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.
Additionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on x':

  
    
      
        σ
        =
        
          
            
              
                
                  ∑
                  
                    b
                    =
                    1
                  
                  
                    B
                  
                
                (
                
                  f
                  
                    b
                  
                
                (
                
                  x
                  ′
                
                )
                −
                
                  
                    
                      f
                      ^
                    
                  
                
                
                  )
                  
                    2
                  
                
              
              
                B
                −
                1
              
            
          
        
        .
      
    
    {\displaystyle \sigma ={\sqrt {\frac {\sum _{b=1}^{B}(f_{b}(x')-{\hat {f}})^{2}}{B-1}}}.}
  The number of samples/trees, B, is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. An optimal number of trees B can be found using cross-validation, or by observing the out-of-bag error: the mean prediction error on each training sample xi, using only the trees that did not have xi in their bootstrap sample.
The training and test error tend to level off after some number of trees have been fit.

From bagging to random forests
The above procedure describes the original bagging algorithm for trees. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called ""feature bagging"". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the B trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho.Typically, for a classification problem with p features, √p (rounded down) features are used in each split.  For regression problems the inventors recommend p/3 (rounded down) with a minimum node size of 5 as the default. In practice the best values for these parameters will depend on the problem, and they should be treated as tuning parameters.

ExtraTrees
Adding one further step of randomization yields extremely randomized trees, or ExtraTrees. While similar to ordinary random forests in that they are an ensemble of individual trees, there are two main differences: first, each tree is trained using the whole learning sample (rather than a bootstrap sample), and second, the top-down splitting in the tree learner is randomized. Instead of computing the locally optimal cut-point for each feature under consideration (based on, e.g., information gain or the Gini impurity), a random cut-point is selected. This value is selected from a uniform distribution within the feature's empirical range (in the tree's training set). Then, of all the randomly generated splits, the split that yields the highest score is chosen to split the node. Similar to ordinary random forests, the number of randomly selected features to be considered at each node can be specified. Default values for this parameter are 
  
    
      
        
          
            p
          
        
      
    
    {\displaystyle {\sqrt {p}}}
   for classification and 
  
    
      
        p
      
    
    {\displaystyle p}
   for regression, where 
  
    
      
        p
      
    
    {\displaystyle p}
   is the number of features in the model.

Properties
Variable importance
Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way.  The following technique was described in Breiman's original paper and is implemented in the R package randomForest.The first step in measuring the variable importance in a data set 
  
    
      
        
          
            
              D
            
          
          
            n
          
        
        =
        {
        (
        
          X
          
            i
          
        
        ,
        
          Y
          
            i
          
        
        )
        
          }
          
            i
            =
            1
          
          
            n
          
        
      
    
    {\displaystyle {\mathcal {D}}_{n}=\{(X_{i},Y_{i})\}_{i=1}^{n}}
   is to fit a random forest to the data.  During the fitting process the out-of-bag error for each data point is recorded and averaged over the forest (errors on an independent test set can be substituted if bagging is not used during training).
To measure the importance of the 
  
    
      
        j
      
    
    {\displaystyle j}
  -th feature after training, the values of the 
  
    
      
        j
      
    
    {\displaystyle j}
  -th feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set.  The importance score for the 
  
    
      
        j
      
    
    {\displaystyle j}
  -th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees.  The score is normalized by the standard deviation of these differences.
Features which produce large values for this score are ranked as more important than features which produce small values. The statistical definition of the variable importance measure was given and analyzed by Zhu et al.This method of determining variable importance has some drawbacks.  For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Methods such as partial permutations
and growing unbiased trees can be used to solve the problem.  If the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups.

Relationship to nearest neighbors
A relationship between random forests and the k-nearest neighbor algorithm (k-NN) was pointed out by Lin and Jeon in 2002. It turns out that both can be viewed as so-called weighted neighborhoods schemes. These are models built from a training set 
  
    
      
        {
        (
        
          x
          
            i
          
        
        ,
        
          y
          
            i
          
        
        )
        
          }
          
            i
            =
            1
          
          
            n
          
        
      
    
    {\displaystyle \{(x_{i},y_{i})\}_{i=1}^{n}}
   that make predictions 
  
    
      
        
          
            
              y
              ^
            
          
        
      
    
    {\displaystyle {\hat {y}}}
   for new points x' by looking at the ""neighborhood"" of the point, formalized by a weight function W:

  
    
      
        
          
            
              y
              ^
            
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        W
        (
        
          x
          
            i
          
        
        ,
        
          x
          ′
        
        )
        
        
          y
          
            i
          
        
        .
      
    
    {\displaystyle {\hat {y}}=\sum _{i=1}^{n}W(x_{i},x')\,y_{i}.}
  Here, 
  
    
      
        W
        (
        
          x
          
            i
          
        
        ,
        
          x
          ′
        
        )
      
    
    {\displaystyle W(x_{i},x')}
   is the non-negative weight of the i'th training point relative to the new point x' in the same tree. For any particular x', the weights for points 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   must sum to one. Weight functions are given as follows:

In k-NN, the weights are 
  
    
      
        W
        (
        
          x
          
            i
          
        
        ,
        
          x
          ′
        
        )
        =
        
          
            1
            k
          
        
      
    
    {\displaystyle W(x_{i},x')={\frac {1}{k}}}
   if xi is one of the k points closest to x', and zero otherwise.
In a tree, 
  
    
      
        W
        (
        
          x
          
            i
          
        
        ,
        
          x
          ′
        
        )
        =
        
          
            1
            
              k
              ′
            
          
        
      
    
    {\displaystyle W(x_{i},x')={\frac {1}{k'}}}
   if xi is one of the k' points in the same leaf as x', and zero otherwise.Since a forest averages the predictions of a set of m trees with individual weight functions 
  
    
      
        
          W
          
            j
          
        
      
    
    {\displaystyle W_{j}}
  , its predictions are

  
    
      
        
          
            
              y
              ^
            
          
        
        =
        
          
            1
            m
          
        
        
          ∑
          
            j
            =
            1
          
          
            m
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          W
          
            j
          
        
        (
        
          x
          
            i
          
        
        ,
        
          x
          ′
        
        )
        
        
          y
          
            i
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          (
          
            
              
                1
                m
              
            
            
              ∑
              
                j
                =
                1
              
              
                m
              
            
            
              W
              
                j
              
            
            (
            
              x
              
                i
              
            
            ,
            
              x
              ′
            
            )
          
          )
        
        
        
          y
          
            i
          
        
        .
      
    
    {\displaystyle {\hat {y}}={\frac {1}{m}}\sum _{j=1}^{m}\sum _{i=1}^{n}W_{j}(x_{i},x')\,y_{i}=\sum _{i=1}^{n}\left({\frac {1}{m}}\sum _{j=1}^{m}W_{j}(x_{i},x')\right)\,y_{i}.}
  This shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. The neighbors of x' in this interpretation are the points 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   sharing the same leaf in any tree 
  
    
      
        j
      
    
    {\displaystyle j}
  . In this way, the neighborhood of x' depends in a complex way on the structure of the trees, and thus on the structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature.

Unsupervised learning with random forests
As part of their construction, random forest predictors naturally lead to a dissimilarity measure among the observations. One can also define a random forest dissimilarity measure between unlabeled data: the idea is to construct a random forest predictor that distinguishes the ""observed"" data from suitably generated synthetic data.
The observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. A random forest dissimilarity can be attractive because it handles mixed variable types very well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. The random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the ""Addcl 1"" random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. The random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.

Variants
Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes classifiers. In cases that the relationship between the predictors and the target variable is linear, the base learners may have an equally high accuracy as the ensemble learner.

Kernel random forest
In machine learning, kernel random forests establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze.

History
Leo Breiman was the first person to notice the link between random forest and kernel methods. He pointed out that random forests which are grown using i.i.d. random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani proposed Random Forest Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet first defined KeRF estimates and gave the explicit link between KeRF estimates and random forest. He also gave explicit expressions for kernels based on centered random forest and uniform random forest, two simplified models of random forest. He named these two KeRFs Centered KeRF and Uniform KeRF, and proved upper bounds on their rates of consistency.

Notations and definitions
Preliminaries: Centered forests
Centered forest is a simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level 
  
    
      
        k
      
    
    {\displaystyle k}
   is built, where 
  
    
      
        k
        ∈
        
          N
        
      
    
    {\displaystyle k\in \mathbb {N} }
   is a parameter of the algorithm.

Uniform forest
Uniform forest is another simplified model for Breiman's original random forest, which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature.

From random forest to KeRF
Given a training sample  
  
    
      
        
          
            
              D
            
          
          
            n
          
        
        =
        {
        (
        
          
            X
          
          
            i
          
        
        ,
        
          Y
          
            i
          
        
        )
        
          }
          
            i
            =
            1
          
          
            n
          
        
      
    
    {\displaystyle {\mathcal {D}}_{n}=\{(\mathbf {X} _{i},Y_{i})\}_{i=1}^{n}}
   of 
  
    
      
        [
        0
        ,
        1
        
          ]
          
            p
          
        
        ×
        
          R
        
      
    
    {\displaystyle [0,1]^{p}\times \mathbb {R} }
  -valued independent random variables distributed as the independent prototype pair 
  
    
      
        (
        
          X
        
        ,
        Y
        )
      
    
    {\displaystyle (\mathbf {X} ,Y)}
  , where 
  
    
      
        E
        ⁡
        [
        
          Y
          
            2
          
        
        ]
        <
        ∞
      
    
    {\displaystyle \operatorname {E} [Y^{2}]<\infty }
  . We aim at predicting the response 
  
    
      
        Y
      
    
    {\displaystyle Y}
  , associated with the random variable 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
  , by estimating the regression function 
  
    
      
        m
        (
        
          x
        
        )
        =
        E
        ⁡
        [
        Y
        ∣
        
          X
        
        =
        
          x
        
        ]
      
    
    {\displaystyle m(\mathbf {x} )=\operatorname {E} [Y\mid \mathbf {X} =\mathbf {x} ]}
  . A random regression forest is an ensemble of 
  
    
      
        M
      
    
    {\displaystyle M}
   randomized regression trees. Denote 
  
    
      
        
          m
          
            n
          
        
        (
        
          x
        
        ,
        
          
            Θ
          
          
            j
          
        
        )
      
    
    {\displaystyle m_{n}(\mathbf {x} ,\mathbf {\Theta } _{j})}
   the predicted value at point 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   by the 
  
    
      
        j
      
    
    {\displaystyle j}
  -th tree, where 
  
    
      
        
          
            Θ
          
          
            1
          
        
        ,
        …
        ,
        
          
            Θ
          
          
            M
          
        
      
    
    {\displaystyle \mathbf {\Theta } _{1},\ldots ,\mathbf {\Theta } _{M}}
   are independent random variables, distributed as a generic random variable 
  
    
      
        
          Θ
        
      
    
    {\displaystyle \mathbf {\Theta } }
  , independent of the sample 
  
    
      
        
          
            
              D
            
          
          
            n
          
        
      
    
    {\displaystyle {\mathcal {D}}_{n}}
  . This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate 
  
    
      
        
          m
          
            M
            ,
            n
          
        
        (
        
          x
        
        ,
        
          Θ
          
            1
          
        
        ,
        …
        ,
        
          Θ
          
            M
          
        
        )
        =
        
          
            1
            M
          
        
        
          ∑
          
            j
            =
            1
          
          
            M
          
        
        
          m
          
            n
          
        
        (
        
          x
        
        ,
        
          Θ
          
            j
          
        
        )
      
    
    {\displaystyle m_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{M}}\sum _{j=1}^{M}m_{n}(\mathbf {x} ,\Theta _{j})}
  .
For regression trees, we have 
  
    
      
        
          m
          
            n
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          
            
              
                Y
                
                  i
                
              
              
                
                  1
                
                
                  
                    
                      X
                    
                    
                      i
                    
                  
                  ∈
                  
                    A
                    
                      n
                    
                  
                  (
                  
                    x
                  
                  ,
                  
                    Θ
                    
                      j
                    
                  
                  )
                
              
            
            
              
                N
                
                  n
                
              
              (
              
                x
              
              ,
              
                Θ
                
                  j
                
              
              )
            
          
        
      
    
    {\displaystyle m_{n}=\sum _{i=1}^{n}{\frac {Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}{N_{n}(\mathbf {x} ,\Theta _{j})}}}
  , where 
  
    
      
        
          A
          
            n
          
        
        (
        
          x
        
        ,
        
          Θ
          
            j
          
        
        )
      
    
    {\displaystyle A_{n}(\mathbf {x} ,\Theta _{j})}
   is the cell containing 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
  , designed with randomness 
  
    
      
        
          Θ
          
            j
          
        
      
    
    {\displaystyle \Theta _{j}}
   and dataset 
  
    
      
        
          
            
              D
            
          
          
            n
          
        
      
    
    {\displaystyle {\mathcal {D}}_{n}}
  , and 
  
    
      
        
          N
          
            n
          
        
        (
        
          x
        
        ,
        
          Θ
          
            j
          
        
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          
            1
          
          
            
              
                X
              
              
                i
              
            
            ∈
            
              A
              
                n
              
            
            (
            
              x
            
            ,
            
              Θ
              
                j
              
            
            )
          
        
      
    
    {\displaystyle N_{n}(\mathbf {x} ,\Theta _{j})=\sum _{i=1}^{n}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}
  .
Thus random forest estimates satisfy, for all 
  
    
      
        
          x
        
        ∈
        [
        0
        ,
        1
        
          ]
          
            d
          
        
      
    
    {\displaystyle \mathbf {x} \in [0,1]^{d}}
  , 
  
    
      
        
          m
          
            M
            ,
            n
          
        
        (
        
          x
        
        ,
        
          Θ
          
            1
          
        
        ,
        …
        ,
        
          Θ
          
            M
          
        
        )
        =
        
          
            1
            M
          
        
        
          ∑
          
            j
            =
            1
          
          
            M
          
        
        
          (
          
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            
              
                
                  
                    Y
                    
                      i
                    
                  
                  
                    
                      1
                    
                    
                      
                        
                          X
                        
                        
                          i
                        
                      
                      ∈
                      
                        A
                        
                          n
                        
                      
                      (
                      
                        x
                      
                      ,
                      
                        Θ
                        
                          j
                        
                      
                      )
                    
                  
                
                
                  
                    N
                    
                      n
                    
                  
                  (
                  
                    x
                  
                  ,
                  
                    Θ
                    
                      j
                    
                  
                  )
                
              
            
          
          )
        
      
    
    {\displaystyle m_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{M}}\sum _{j=1}^{M}\left(\sum _{i=1}^{n}{\frac {Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}{N_{n}(\mathbf {x} ,\Theta _{j})}}\right)}
  . Random regression forest has two level of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet defined KeRF by

  
    
      
        
          
            
              
                m
                ~
              
            
          
          
            M
            ,
            n
          
        
        (
        
          x
        
        ,
        
          Θ
          
            1
          
        
        ,
        …
        ,
        
          Θ
          
            M
          
        
        )
        =
        
          
            1
            
              
                ∑
                
                  j
                  =
                  1
                
                
                  M
                
              
              
                N
                
                  n
                
              
              (
              
                x
              
              ,
              
                Θ
                
                  j
                
              
              )
            
          
        
        
          ∑
          
            j
            =
            1
          
          
            M
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          Y
          
            i
          
        
        
          
            1
          
          
            
              
                X
              
              
                i
              
            
            ∈
            
              A
              
                n
              
            
            (
            
              x
            
            ,
            
              Θ
              
                j
              
            
            )
          
        
        ,
      
    
    {\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{\sum _{j=1}^{M}N_{n}(\mathbf {x} ,\Theta _{j})}}\sum _{j=1}^{M}\sum _{i=1}^{n}Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})},}
  which is equal to the mean of the 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
  's falling in the cells containing 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   in the forest. If we define the connection function of the 
  
    
      
        M
      
    
    {\displaystyle M}
   finite forest as 
  
    
      
        
          K
          
            M
            ,
            n
          
        
        (
        
          x
        
        ,
        
          z
        
        )
        =
        
          
            1
            M
          
        
        
          ∑
          
            j
            =
            1
          
          
            M
          
        
        
          
            1
          
          
            
              z
            
            ∈
            
              A
              
                n
              
            
            (
            
              x
            
            ,
            
              Θ
              
                j
              
            
            )
          
        
      
    
    {\displaystyle K_{M,n}(\mathbf {x} ,\mathbf {z} )={\frac {1}{M}}\sum _{j=1}^{M}\mathbf {1} _{\mathbf {z} \in A_{n}(\mathbf {x} ,\Theta _{j})}}
  , i.e. the proportion of cells shared between 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   and 
  
    
      
        
          z
        
      
    
    {\displaystyle \mathbf {z} }
  , then almost surely we have 
  
    
      
        
          
            
              
                m
                ~
              
            
          
          
            M
            ,
            n
          
        
        (
        
          x
        
        ,
        
          Θ
          
            1
          
        
        ,
        …
        ,
        
          Θ
          
            M
          
        
        )
        =
        
          
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                Y
                
                  i
                
              
              
                K
                
                  M
                  ,
                  n
                
              
              (
              
                x
              
              ,
              
                
                  x
                
                
                  i
                
              
              )
            
            
              
                ∑
                
                  ℓ
                  =
                  1
                
                
                  n
                
              
              
                K
                
                  M
                  ,
                  n
                
              
              (
              
                x
              
              ,
              
                
                  x
                
                
                  ℓ
                
              
              )
            
          
        
      
    
    {\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {\sum _{i=1}^{n}Y_{i}K_{M,n}(\mathbf {x} ,\mathbf {x} _{i})}{\sum _{\ell =1}^{n}K_{M,n}(\mathbf {x} ,\mathbf {x} _{\ell })}}}
  , which defines the KeRF.

Centered KeRF
The construction of Centered KeRF of level 
  
    
      
        k
      
    
    {\displaystyle k}
   is the same as for centered forest, except that predictions are made by 
  
    
      
        
          
            
              
                m
                ~
              
            
          
          
            M
            ,
            n
          
        
        (
        
          x
        
        ,
        
          Θ
          
            1
          
        
        ,
        …
        ,
        
          Θ
          
            M
          
        
        )
      
    
    {\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})}
  , the corresponding kernel function, or connection function is

  
    
      
        
          
            
              
                
                  K
                  
                    k
                  
                  
                    c
                    c
                  
                
                (
                
                  x
                
                ,
                
                  z
                
                )
                =
                
                  ∑
                  
                    
                      k
                      
                        1
                      
                    
                    ,
                    …
                    ,
                    
                      k
                      
                        d
                      
                    
                    ,
                    
                      ∑
                      
                        j
                        =
                        1
                      
                      
                        d
                      
                    
                    
                      k
                      
                        j
                      
                    
                    =
                    k
                  
                
              
              
                
                  
                    
                      k
                      !
                    
                    
                      
                        k
                        
                          1
                        
                      
                      !
                      ⋯
                      
                        k
                        
                          d
                        
                      
                      !
                    
                  
                
                
                  
                    (
                    
                      
                        1
                        d
                      
                    
                    )
                  
                  
                    k
                  
                
                
                  ∏
                  
                    j
                    =
                    1
                  
                  
                    d
                  
                
                
                  
                    1
                  
                  
                    ⌈
                    
                      2
                      
                        
                          k
                          
                            j
                          
                        
                      
                    
                    
                      x
                      
                        j
                      
                    
                    ⌉
                    =
                    ⌈
                    
                      2
                      
                        
                          k
                          
                            j
                          
                        
                      
                    
                    
                      z
                      
                        j
                      
                    
                    ⌉
                  
                
                ,
              
            
            
              
              
                
                   for all 
                
                
                  x
                
                ,
                
                  z
                
                ∈
                [
                0
                ,
                1
                
                  ]
                  
                    d
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}K_{k}^{cc}(\mathbf {x} ,\mathbf {z} )=\sum _{k_{1},\ldots ,k_{d},\sum _{j=1}^{d}k_{j}=k}&{\frac {k!}{k_{1}!\cdots k_{d}!}}\left({\frac {1}{d}}\right)^{k}\prod _{j=1}^{d}\mathbf {1} _{\lceil 2^{k_{j}}x_{j}\rceil =\lceil 2^{k_{j}}z_{j}\rceil },\\&{\text{ for all }}\mathbf {x} ,\mathbf {z} \in [0,1]^{d}.\end{aligned}}}

Uniform KeRF
Uniform KeRF is built in the same way as uniform forest, except that predictions are made by 
  
    
      
        
          
            
              
                m
                ~
              
            
          
          
            M
            ,
            n
          
        
        (
        
          x
        
        ,
        
          Θ
          
            1
          
        
        ,
        …
        ,
        
          Θ
          
            M
          
        
        )
      
    
    {\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})}
  , the corresponding kernel function, or connection function is

  
    
      
        
          K
          
            k
          
          
            u
            f
          
        
        (
        
          0
        
        ,
        
          x
        
        )
        =
        
          ∑
          
            
              k
              
                1
              
            
            ,
            …
            ,
            
              k
              
                d
              
            
            ,
            
              ∑
              
                j
                =
                1
              
              
                d
              
            
            
              k
              
                j
              
            
            =
            k
          
        
        
          
            
              k
              !
            
            
              
                k
                
                  1
                
              
              !
              …
              
                k
                
                  d
                
              
              !
            
          
        
        
          
            (
            
              
                1
                d
              
            
            )
          
          
            k
          
        
        
          ∏
          
            m
            =
            1
          
          
            d
          
        
        
          (
          
            1
            −
            
              |
            
            
              x
              
                m
              
            
            
              |
            
            
              ∑
              
                j
                =
                0
              
              
                
                  k
                  
                    m
                  
                
                −
                1
              
            
            
              
                
                  (
                  −
                  ln
                  ⁡
                  
                    |
                  
                  
                    x
                    
                      m
                    
                  
                  
                    |
                  
                  
                    )
                    
                      j
                    
                  
                
                
                  j
                  !
                
              
            
          
          )
        
        
           for all 
        
        
          x
        
        ∈
        [
        0
        ,
        1
        
          ]
          
            d
          
        
        .
      
    
    {\displaystyle K_{k}^{uf}(\mathbf {0} ,\mathbf {x} )=\sum _{k_{1},\ldots ,k_{d},\sum _{j=1}^{d}k_{j}=k}{\frac {k!}{k_{1}!\ldots k_{d}!}}\left({\frac {1}{d}}\right)^{k}\prod _{m=1}^{d}\left(1-|x_{m}|\sum _{j=0}^{k_{m}-1}{\frac {(-\ln |x_{m}|)^{j}}{j!}}\right){\text{ for all }}\mathbf {x} \in [0,1]^{d}.}

Properties
Relation between KeRF and random forest
Predictions given by KeRF and random forests are close if the number of points in each cell is controlled:

Assume that there exist sequences 
  
    
      
        (
        
          a
          
            n
          
        
        )
        ,
        (
        
          b
          
            n
          
        
        )
      
    
    {\displaystyle (a_{n}),(b_{n})}
   such that, almost surely,

  
    
      
        
          a
          
            n
          
        
        ≤
        
          N
          
            n
          
        
        (
        
          x
        
        ,
        Θ
        )
        ≤
        
          b
          
            n
          
        
        
           and 
        
        
          a
          
            n
          
        
        ≤
        
          
            1
            M
          
        
        
          ∑
          
            m
            =
            1
          
          
            M
          
        
        
          N
          
            n
          
        
        
          
            x
          
          ,
          
            Θ
            
              m
            
          
        
        ≤
        
          b
          
            n
          
        
        .
      
    
    {\displaystyle a_{n}\leq N_{n}(\mathbf {x} ,\Theta )\leq b_{n}{\text{ and }}a_{n}\leq {\frac {1}{M}}\sum _{m=1}^{M}N_{n}{\mathbf {x} ,\Theta _{m}}\leq b_{n}.}
  Then almost surely,

  
    
      
        
          |
        
        
          m
          
            M
            ,
            n
          
        
        (
        
          x
        
        )
        −
        
          
            
              
                m
                ~
              
            
          
          
            M
            ,
            n
          
        
        (
        
          x
        
        )
        
          |
        
        ≤
        
          
            
              
                b
                
                  n
                
              
              −
              
                a
                
                  n
                
              
            
            
              a
              
                n
              
            
          
        
        
          
            
              
                m
                ~
              
            
          
          
            M
            ,
            n
          
        
        (
        
          x
        
        )
        .
      
    
    {\displaystyle |m_{M,n}(\mathbf {x} )-{\tilde {m}}_{M,n}(\mathbf {x} )|\leq {\frac {b_{n}-a_{n}}{a_{n}}}{\tilde {m}}_{M,n}(\mathbf {x} ).}

Relation between infinite KeRF and infinite random forest
When the number of trees 
  
    
      
        M
      
    
    {\displaystyle M}
   goes to infinity, then we have infinite random forest and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:

Assume that there exist sequences 
  
    
      
        (
        
          ε
          
            n
          
        
        )
        ,
        (
        
          a
          
            n
          
        
        )
        ,
        (
        
          b
          
            n
          
        
        )
      
    
    {\displaystyle (\varepsilon _{n}),(a_{n}),(b_{n})}
   such that, almost surely

  
    
      
        E
        ⁡
        [
        
          N
          
            n
          
        
        (
        
          x
        
        ,
        Θ
        )
        ]
        ≥
        1
        ,
      
    
    {\displaystyle \operatorname {E} [N_{n}(\mathbf {x} ,\Theta )]\geq 1,}
  

  
    
      
        P
        ⁡
        [
        
          a
          
            n
          
        
        ≤
        
          N
          
            n
          
        
        (
        
          x
        
        ,
        Θ
        )
        ≤
        
          b
          
            n
          
        
        ∣
        
          
            
              D
            
          
          
            n
          
        
        ]
        ≥
        1
        −
        
          ε
          
            n
          
        
        
          /
        
        2
        ,
      
    
    {\displaystyle \operatorname {P} [a_{n}\leq N_{n}(\mathbf {x} ,\Theta )\leq b_{n}\mid {\mathcal {D}}_{n}]\geq 1-\varepsilon _{n}/2,}
  

  
    
      
        P
        ⁡
        [
        
          a
          
            n
          
        
        ≤
        
          E
          
            Θ
          
        
        ⁡
        [
        
          N
          
            n
          
        
        (
        
          x
        
        ,
        Θ
        )
        ]
        ≤
        
          b
          
            n
          
        
        ∣
        
          
            
              D
            
          
          
            n
          
        
        ]
        ≥
        1
        −
        
          ε
          
            n
          
        
        
          /
        
        2
        ,
      
    
    {\displaystyle \operatorname {P} [a_{n}\leq \operatorname {E} _{\Theta }[N_{n}(\mathbf {x} ,\Theta )]\leq b_{n}\mid {\mathcal {D}}_{n}]\geq 1-\varepsilon _{n}/2,}
  Then almost surely,

  
    
      
        
          |
        
        
          m
          
            ∞
            ,
            n
          
        
        (
        
          x
        
        )
        −
        
          
            
              
                m
                ~
              
            
          
          
            ∞
            ,
            n
          
        
        (
        
          x
        
        )
        
          |
        
        ≤
        
          
            
              
                b
                
                  n
                
              
              −
              
                a
                
                  n
                
              
            
            
              a
              
                n
              
            
          
        
        
          
            
              
                m
                ~
              
            
          
          
            ∞
            ,
            n
          
        
        (
        
          x
        
        )
        +
        n
        
          ε
          
            n
          
        
        
          (
          
            
              max
              
                1
                ≤
                i
                ≤
                n
              
            
            
              Y
              
                i
              
            
          
          )
        
        .
      
    
    {\displaystyle |m_{\infty ,n}(\mathbf {x} )-{\tilde {m}}_{\infty ,n}(\mathbf {x} )|\leq {\frac {b_{n}-a_{n}}{a_{n}}}{\tilde {m}}_{\infty ,n}(\mathbf {x} )+n\varepsilon _{n}\left(\max _{1\leq i\leq n}Y_{i}\right).}

Consistency results
Assume that 
  
    
      
        Y
        =
        m
        (
        
          X
        
        )
        +
        ε
      
    
    {\displaystyle Y=m(\mathbf {X} )+\varepsilon }
  , where 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   is a centered Gaussian noise, independent of 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
  , with finite variance 
  
    
      
        
          σ
          
            2
          
        
        <
        ∞
      
    
    {\displaystyle \sigma ^{2}<\infty }
  . Moreover, 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
   is uniformly distributed on 
  
    
      
        [
        0
        ,
        1
        
          ]
          
            d
          
        
      
    
    {\displaystyle [0,1]^{d}}
   and 
  
    
      
        m
      
    
    {\displaystyle m}
   is Lipschitz. Scornet proved upper bounds on the rates of consistency for centered KeRF and uniform KeRF.

Consistency of centered KeRF
Providing 
  
    
      
        k
        →
        ∞
      
    
    {\displaystyle k\rightarrow \infty }
   and 
  
    
      
        n
        
          /
        
        
          2
          
            k
          
        
        →
        ∞
      
    
    {\displaystyle n/2^{k}\rightarrow \infty }
  , there exists a constant 
  
    
      
        
          C
          
            1
          
        
        >
        0
      
    
    {\displaystyle C_{1}>0}
   such that, for all 
  
    
      
        n
      
    
    {\displaystyle n}
  ,

  
    
      
        
          E
        
        [
        
          
            
              
                m
                ~
              
            
          
          
            n
          
          
            c
            c
          
        
        (
        
          X
        
        )
        −
        m
        (
        
          X
        
        )
        
          ]
          
            2
          
        
        ≤
        
          C
          
            1
          
        
        
          n
          
            −
            1
            
              /
            
            (
            3
            +
            d
            log
            ⁡
            2
            )
          
        
        (
        log
        ⁡
        n
        
          )
          
            2
          
        
      
    
    {\displaystyle \mathbb {E} [{\tilde {m}}_{n}^{cc}(\mathbf {X} )-m(\mathbf {X} )]^{2}\leq C_{1}n^{-1/(3+d\log 2)}(\log n)^{2}}
  .

Consistency of uniform KeRF
Providing 
  
    
      
        k
        →
        ∞
      
    
    {\displaystyle k\rightarrow \infty }
   and 
  
    
      
        n
        
          /
        
        
          2
          
            k
          
        
        →
        ∞
      
    
    {\displaystyle n/2^{k}\rightarrow \infty }
  , there exists a constant 
  
    
      
        C
        >
        0
      
    
    {\displaystyle C>0}
   such that,

  
    
      
        
          E
        
        [
        
          
            
              
                m
                ~
              
            
          
          
            n
          
          
            u
            f
          
        
        (
        
          X
        
        )
        −
        m
        (
        
          X
        
        )
        
          ]
          
            2
          
        
        ≤
        C
        
          n
          
            −
            2
            
              /
            
            (
            6
            +
            3
            d
            log
            ⁡
            2
            )
          
        
        (
        log
        ⁡
        n
        
          )
          
            2
          
        
      
    
    {\displaystyle \mathbb {E} [{\tilde {m}}_{n}^{uf}(\mathbf {X} )-m(\mathbf {X} )]^{2}\leq Cn^{-2/(6+3d\log 2)}(\log n)^{2}}
  .

Disadvantages
While random forests often achieve higher accuracy than a single decision tree, they sacrifice the intrinsic interpretability present in decision trees. Decision trees are among a fairly small family of machine learning models that are easily interpretable along with linear models, rule-based models, and attention-based models. This interpretability is one of the most desirable qualities of decision trees. It allows developers to confirm that the model has learned realistic information from the data and allows end-users to have trust and confidence in the decisions made by the model. For example, following the path that a decision tree takes to make its decision is quite trivial, but following the paths of tens or hundreds of trees is much harder. To achieve both performance and interpretability, some model compression techniques allow transforming a random forest into a minimal ""born-again"" decision tree that faithfully reproduces the same decision function. If it is established that the predictive attributes are linearly correlated with the target variable, using random forest may not enhance the accuracy of the base learner. Furthermore, in problems with multiple categorical variables, random forest may not be able to increase the accuracy of the base learner.

See also
Boosting
Decision tree learning
Ensemble learning
Gradient boosting
Non-parametric statistics
Randomized algorithm

References
Further reading
External links
Random Forests classifier description (Leo Breiman's site)
Liaw, Andy & Wiener, Matthew ""Classification and Regression by randomForest"" R News (2002) Vol. 2/3 p. 18 (Discussion of the use of the random forest package for R)",https://en.wikipedia.org/wiki/Random_forest,"['All articles containing potentially dated statements', 'Articles containing potentially dated statements from 2019', 'Articles with short description', 'CS1 errors: missing periodical', 'CS1 maint: uses authors parameter', 'Classification algorithms', 'Computational statistics', 'Decision theory', 'Decision trees', 'Ensemble learning', 'Machine learning', 'Short description is different from Wikidata']",Data Science
154,Recurrent neural network,"A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.The term “recurrent neural network” is used indiscriminately to refer to two broad classes of networks with a similar general structure, where one is finite impulse and the other is infinite impulse. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.
Both finite impulse and infinite impulse recurrent networks can have additional stored states, and the storage can be under direct control by the neural network. The storage can also be replaced by another network or graph, if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN).

History
Recurrent neural networks were based on David Rumelhart's work in 1986.  Hopfield networks - a special kind of RNN - were discovered by John Hopfield in 1982. In 1993, a neural history compressor system solved a “Very Deep Learning” task that required more than 1000 subsequent layers in an RNN unfolded in time.

LSTM
Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.Around 2007, LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications. In 2009, a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014, the Chinese company Baidu used CTC-trained RNNs to break the Switchboard Hub5'00 speech recognition dataset benchmark without using any traditional speech processing methods.LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM.LSTM broke records for improved machine translation, Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. Given the computation and memory overheads of running LSTMs, there have been efforts on accelerating LSTM using hardware accelerators.

Architectures
RNNs come in many variants.

Fully recurrent
Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons.  This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack connections between those neurons.  The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance.  However, what appears to be layers are, in fact, different steps in time of the same fully recurrent neural network.  The left-most item in the illustration shows the recurrent connections as the arc labeled 'v'.  It is ""unfolded"" in time to produce the appearance of layers.

Elman networks and Jordan networks
An Elman network is a three-layer network (arranged horizontally as x, y, and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step, the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron.
Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also referred to as the state layer. They have a recurrent connection to themselves.Elman and Jordan networks are also known as “Simple recurrent networks” (SRN).

Elman network

  
    
      
        
          
            
              
                
                  h
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    h
                  
                
                (
                
                  W
                  
                    h
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    h
                  
                
                
                  h
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    h
                  
                
                )
              
            
            
              
                
                  y
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    y
                  
                
                (
                
                  W
                  
                    y
                  
                
                
                  h
                  
                    t
                  
                
                +
                
                  b
                  
                    y
                  
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}}
  
Jordan network

  
    
      
        
          
            
              
                
                  h
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    h
                  
                
                (
                
                  W
                  
                    h
                  
                
                
                  x
                  
                    t
                  
                
                +
                
                  U
                  
                    h
                  
                
                
                  y
                  
                    t
                    −
                    1
                  
                
                +
                
                  b
                  
                    h
                  
                
                )
              
            
            
              
                
                  y
                  
                    t
                  
                
              
              
                
                =
                
                  σ
                  
                    y
                  
                
                (
                
                  W
                  
                    y
                  
                
                
                  h
                  
                    t
                  
                
                +
                
                  b
                  
                    y
                  
                
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}}
  Variables and functions

  
    
      
        
          x
          
            t
          
        
      
    
    {\displaystyle x_{t}}
  : input vector

  
    
      
        
          h
          
            t
          
        
      
    
    {\displaystyle h_{t}}
  : hidden layer vector

  
    
      
        
          y
          
            t
          
        
      
    
    {\displaystyle y_{t}}
  : output vector

  
    
      
        W
      
    
    {\displaystyle W}
  , 
  
    
      
        U
      
    
    {\displaystyle U}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
  : parameter matrices and vector

  
    
      
        
          σ
          
            h
          
        
      
    
    {\displaystyle \sigma _{h}}
   and 
  
    
      
        
          σ
          
            y
          
        
      
    
    {\displaystyle \sigma _{y}}
  : Activation functions

Hopfield
The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN, as it does not process sequences of patterns. However, it guarantees that it will converge. If the connections are trained using Hebbian learning then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.

Bidirectional associative memory
Introduced by Bart Kosko, a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically, bipolar encoding is preferred to binary encoding of the associative pairs. Recently, stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.A BAM network has two layers, either of which can be driven as an input to recall an association and produce an output on the other layer.

Echo state
The echo state network (ESN) has a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine.

Independently RNN (IndRNN)
The Independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with the non-saturated nonlinear functions such as ReLU. Using skip connections, deep networks can be trained.

Recursive
A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure, such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree.

Neural history compressor
The neural history compressor is an unsupervised stack of RNNs. At the input level, it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN, which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level.
The system effectively minimises the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence, the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events.
It is possible to distill the RNN hierarchy into two RNNs: the ""conscious"" chunker (higher level) and the ""subconscious"" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer, then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate, rarely changing memories across long intervals. In turn this helps the automatizer to make many of its once unpredictable inputs predictable, such that the chunker can focus on the remaining unpredictable events.A generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993, such a system solved a “Very Deep Learning” task that required more than 1000 subsequent layers in an RNN unfolded in time.

Second order RNNs
Second order RNNs use higher order weights 
  
    
      
        w
        
          

          
          
            i
            j
            k
          
        
      
    
    {\displaystyle w{}_{ijk}}
   instead of the standard 
  
    
      
        w
        
          

          
          
            i
            j
          
        
      
    
    {\displaystyle w{}_{ij}}
   weights, and states can be a product. This allows a direct mapping to a finite state machine both in training, stability, and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability.

Long short-term memory
Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called “forget gates”. LSTM prevents backpropagated errors from vanishing or exploding. Instead, errors can flow backwards through unlimited numbers of virtual layers unfolded in space. That is, LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high frequency components.
Many applications use stacks of LSTM RNNs and train them by Connectionist Temporal Classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.
LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts.

Gated recurrent unit
Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. They are used in the full form and several simplified variants. Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory. They have fewer parameters than LSTM, as they lack an output gate.

Bi-directional
Bi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element's past and future contexts. This is done by concatenating the outputs of two RNNs, one processing the sequence from left to right, the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique has been proven to be especially useful when combined with LSTM RNNs.

Continuous-time
A continuous time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming spike train.
For a neuron 
  
    
      
        i
      
    
    {\displaystyle i}
   in the network with activation 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
  , the rate of change of activation is given by:

  
    
      
        
          τ
          
            i
          
        
        
          
            
              
                y
                ˙
              
            
          
          
            i
          
        
        =
        −
        
          y
          
            i
          
        
        +
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        
          w
          
            j
            i
          
        
        σ
        (
        
          y
          
            j
          
        
        −
        
          Θ
          
            j
          
        
        )
        +
        
          I
          
            i
          
        
        (
        t
        )
      
    
    {\displaystyle \tau _{i}{\dot {y}}_{i}=-y_{i}+\sum _{j=1}^{n}w_{ji}\sigma (y_{j}-\Theta _{j})+I_{i}(t)}
  Where:

  
    
      
        
          τ
          
            i
          
        
      
    
    {\displaystyle \tau _{i}}
   : Time constant of postsynaptic node

  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
   : Activation of postsynaptic node

  
    
      
        
          
            
              
                y
                ˙
              
            
          
          
            i
          
        
      
    
    {\displaystyle {\dot {y}}_{i}}
   : Rate of change of activation of postsynaptic node

  
    
      
        w
        
          

          
          
            j
            i
          
        
      
    
    {\displaystyle w{}_{ji}}
   : Weight of connection from pre to postsynaptic node

  
    
      
        σ
        (
        x
        )
      
    
    {\displaystyle \sigma (x)}
   : Sigmoid of x e.g. 
  
    
      
        σ
        (
        x
        )
        =
        1
        
          /
        
        (
        1
        +
        
          e
          
            −
            x
          
        
        )
      
    
    {\displaystyle \sigma (x)=1/(1+e^{-x})}
  .

  
    
      
        
          y
          
            j
          
        
      
    
    {\displaystyle y_{j}}
   : Activation of presynaptic node

  
    
      
        
          Θ
          
            j
          
        
      
    
    {\displaystyle \Theta _{j}}
   : Bias of presynaptic node

  
    
      
        
          I
          
            i
          
        
        (
        t
        )
      
    
    {\displaystyle I_{i}(t)}
   : Input (if any) to nodeCTRNNs have been applied to evolutionary robotics where they have been used to address vision, co-operation, and minimal cognitive behaviour.Note that, by the Shannon sampling theorem, discrete time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. This transformation can be thought of as occurring after the post-synaptic node activation functions 
  
    
      
        
          y
          
            i
          
        
        (
        t
        )
      
    
    {\displaystyle y_{i}(t)}
   have been low-pass filtered but prior to sampling.

Hierarchical
Hierarchical RNNs connect their neurons in various ways to decompose hierarchical behavior into useful subprograms. Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson, whose philosophical views have inspired hierarchical models.

Recurrent multilayer perceptron network
Generally, a recurrent multilayer perceptron network (RMLP) network consists of cascaded subnetworks, each of which contains multiple layers of nodes.  Each of these subnetworks is feed-forward except for the last layer, which can have feedback connections.  Each of these subnets is connected only by feed forward connections.

Multiple timescales model
A multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization that depends on spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties. With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book On Intelligence. Such a hierarchy also agrees with theories of memory posited by philosopher Henri Bergson, which have been incorporated into an MTRNN model.

Neural Turing machines
Neural Turing machines (NTMs) are a method of extending recurrent neural networks by coupling them to external memory resources which they can interact with by attentional processes. The combined system is analogous to a Turing machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent.

Differentiable neural computer
Differentiable neural computers (DNCs) are an extension of Neural Turing machines, allowing for usage of fuzzy amounts of each memory address and a record of chronology.

Neural network pushdown automata
Neural network pushdown automata (NNPDA) are similar to NTMs, but tapes are replaced by analogue stacks that are differentiable and that are trained. In this way, they are similar in complexity to recognizers of context free grammars (CFGs).

Memristive Networks
Greg Snider of HP Labs describes a system of cortical computing with memristive nanodevices. The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. DARPA's SyNAPSE project has funded IBM Research and HP Labs, in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS), to develop neuromorphic architectures which may be based on memristive systems.
Memristive networks are a particular type of physical neural network that have very similar properties to (Little-)Hopfield networks, as they have a continuous dynamics, have a limited memory capacity and they natural relax via the minimization of a function which is asymptotic to the Ising model. In this sense, the dynamics of a memristive circuit has the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior. From this point of view, engineering an analog memristive networks accounts to a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring, or topology.

Training
Gradient descent
Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. In neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non-linear activation functions are differentiable. Various methods for doing so were developed in the 1980s and early 1990s by Werbos, Williams, Robinson, Schmidhuber, Hochreiter, Pearlmutter and others.
The standard method is called “backpropagation through time” or BPTT, and is a generalization of back-propagation for feed-forward networks. Like that method, it is an instance of automatic differentiation in the reverse accumulation mode of Pontryagin's minimum principle. A more computationally expensive online variant is called “Real-Time Recurrent Learning” or RTRL, which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors. Unlike BPTT, this algorithm is local in time but not local in space.
In this context, local in space means that a unit's weight vector can be updated using only information stored in the connected units and the unit itself such that update complexity of a single unit is linear in the dimensionality of the weight vector. Local in time means that the updates take place continually (on-line) and depend only on the most recent time step rather than on multiple time steps within a given time horizon as in BPTT. Biological neural networks appear to be local with respect to both time and space.For recursively computing the partial derivatives, RTRL has a time-complexity of O(number of hidden x number of weights) per time step for computing the Jacobian matrices, while BPTT only takes O(number of weights) per time step, at the cost of storing all forward activations within the given time horizon. An online hybrid between BPTT and RTRL with intermediate complexity exists, along with variants for continuous time.A major problem with gradient descent for standard RNN architectures is that error gradients vanish exponentially quickly with the size of the time lag between important events. LSTM combined with a BPTT/RTRL hybrid learning method attempts to overcome these problems. This problem is also solved in the independently recurrent neural network (IndRNN) by reducing the context of a neuron to its own past state and the cross-neuron information can then be explored in the following layers. Memories of different range including long-term memory can be learned without the gradient vanishing and exploding problem.
The on-line algorithm called causal recursive backpropagation (CRBP), implements and combines BPTT and RTRL paradigms for locally recurrent networks. It works with the most general locally recurrent networks. The CRBP algorithm can minimize the global error term. This fact improves stability of the algorithm, providing a unifying view on gradient calculation techniques for recurrent networks with local feedback.
One approach to the computation of gradient information in RNNs with arbitrary architectures is based on signal-flow graphs diagrammatic derivation. It uses the BPTT batch algorithm, based on Lee's theorem for network sensitivity calculations. It was proposed by Wan and Beaufays, while its fast online version was proposed by Campolucci, Uncini and Piazza.

Global optimization methods
Training the weights in a neural network can be modeled as a non-linear global optimization problem. A target function can be formed to evaluate the fitness or error of a particular weight vector as follows: First, the weights in the network are set according to the weight vector. Next, the network is evaluated against the training sequence. Typically, the sum-squared-difference between the predictions and the target values specified in the training sequence is used to represent the error of the current weight vector. Arbitrary global optimization techniques may then be used to minimize this target function.
The most common global optimization method for training RNNs is genetic algorithms, especially in unstructured networks.Initially, the genetic algorithm is encoded with the neural network weights in a predefined manner where one gene in the chromosome represents one weight link. The whole network is represented as a single chromosome. The fitness function is evaluated as follows:

Each weight encoded in the chromosome is assigned to the respective weight link of the network.
The training set is presented to the network which propagates the input signals forward.
The mean-squared-error is returned to the fitness function.
This function drives the genetic selection process.Many chromosomes make up the population; therefore, many different neural networks are evolved until a stopping criterion is satisfied. A common stopping scheme is: 

When the neural network has learnt a certain percentage of the training data or
When the minimum value of the mean-squared-error is satisfied or
When the maximum number of training generations has been reached.The stopping criterion is evaluated by the fitness function as it gets the reciprocal of the mean-squared-error from each network during training. Therefore, the goal of the genetic algorithm is to maximize the fitness function, reducing the mean-squared-error.
Other global (and/or evolutionary) optimization techniques may be used to seek a good set of weights, such as simulated annealing or particle swarm optimization.

Related fields and models
RNNs may behave chaotically. In such cases, dynamical systems theory may be used for analysis.
They are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.
In particular, RNNs can appear as nonlinear versions of finite impulse response and infinite impulse response filters and also as a nonlinear autoregressive exogenous model (NARX).

Libraries
Apache Singa
Caffe: Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++, and has Python and MATLAB wrappers.
Chainer: The first stable deep learning library that supports dynamic, define-by-run neural networks. Fully in Python, production support for CPU, GPU, distributed training.
Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka.
Dynet: The Dynamic Neural Networks toolkit.
Flux: includes interfaces for RNNs, including GRUs and LSTMs, written in Julia.
Keras: High-level, easy to use API, providing a wrapper to many other deep learning libraries.
Microsoft Cognitive Toolkit
MXNet: a modern open-source deep learning framework used to train and deploy deep neural networks.
Paddle Paddle (https://github.com/paddlepaddle/paddle): PaddlePaddle (PArallel Distributed Deep LEarning) is a deep learning platform, which is originally developed by Baidu scientists and engineers for the purpose of applying deep learning to many products at Baidu.
PyTorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration.
TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU and Google's proprietary TPU, mobile
Theano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation.
Torch (www.torch.ch): A scientific computing framework with wide support for machine learning algorithms, written in C and lua. The main author is Ronan Collobert, and it is now used at Facebook AI Research and Twitter.

Applications
Applications of recurrent neural networks include:

Machine Translation
Robot control
Time series prediction
Speech recognition
Speech synthesis
Time series anomaly detection
Rhythm learning
Music composition
Grammar learning
Handwriting recognition
Human action recognition
Protein Homology Detection
Predicting subcellular localization of proteins
Several prediction tasks in the area of business process management
Prediction in medical care pathways

References
Further reading
Mandic, Danilo P. & Chambers, Jonathon A. (2001). Recurrent Neural Networks for Prediction: Learning Algorithms, Architectures and Stability. Wiley. ISBN 978-0-471-49517-8.

External links
Seq2SeqSharp LSTM/BiLSTM/Transformer recurrent neural networks framework running on CPUs and GPUs for sequence-to-sequence tasks (C#, .NET)
RNNSharp CRFs based on recurrent neural networks (C#, .NET)
Recurrent Neural Networks with over 60 RNN papers by Jürgen Schmidhuber's group at Dalle Molle Institute for Artificial Intelligence Research
Elman Neural Network implementation for WEKA
Recurrent Neural Nets & LSTMs in Java
an alternative try for complete RNN / Reward driven",https://en.wikipedia.org/wiki/Recurrent_neural_network,"['All articles to be expanded', 'All articles with unsourced statements', 'Articles to be expanded from August 2019', 'Articles using small message boxes', 'Articles with short description', 'Articles with unsourced statements from June 2017', 'Articles with unsourced statements from November 2016', 'Artificial intelligence', 'Artificial neural networks', 'CS1 errors: missing periodical', 'CS1 errors: missing title', 'CS1 maint: date and year', 'Short description is different from Wikidata']",Data Science
155,Relevance vector machine,"In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification.
The RVM has an identical functional form to the support vector machine, but provides probabilistic classification.
It is actually equivalent to a Gaussian process model with covariance function:

  
    
      
        k
        (
        
          x
        
        ,
        
          
            x
            ′
          
        
        )
        =
        
          ∑
          
            j
            =
            1
          
          
            N
          
        
        
          
            1
            
              α
              
                j
              
            
          
        
        φ
        (
        
          x
        
        ,
        
          
            x
          
          
            j
          
        
        )
        φ
        (
        
          
            x
          
          ′
        
        ,
        
          
            x
          
          
            j
          
        
        )
      
    
    {\displaystyle k(\mathbf {x} ,\mathbf {x'} )=\sum _{j=1}^{N}{\frac {1}{\alpha _{j}}}\varphi (\mathbf {x} ,\mathbf {x} _{j})\varphi (\mathbf {x} ',\mathbf {x} _{j})}
  where 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
   is the kernel function (usually Gaussian), 
  
    
      
        
          α
          
            j
          
        
      
    
    {\displaystyle \alpha _{j}}
   are the variances of the prior on the weight vector

  
    
      
        w
        ∼
        N
        (
        0
        ,
        
          α
          
            −
            1
          
        
        I
        )
      
    
    {\displaystyle w\sim N(0,\alpha ^{-1}I)}
  , and 
  
    
      
        
          
            x
          
          
            1
          
        
        ,
        …
        ,
        
          
            x
          
          
            N
          
        
      
    
    {\displaystyle \mathbf {x} _{1},\ldots ,\mathbf {x} _{N}}
   are the input vectors of the training set.Compared to that of support vector machines (SVM), the Bayesian formulation of the RVM avoids the set of free parameters of the SVM (that usually require cross-validation-based post-optimizations). However RVMs use an expectation maximization (EM)-like learning method and are therefore at risk of local minima. This is unlike the standard sequential minimal optimization (SMO)-based algorithms employed by SVMs, which are guaranteed to find a global optimum (of the convex problem).
The relevance vector machine is patented in the United States by Microsoft (patent expired September 4, 2019).

See also
Kernel trick
Platt scaling: turns an SVM into a probability model

References
Software
dlib C++ Library
The Kernel-Machine Library
rvmbinary: R package for binary classification
scikit-rvm
fast-scikit-rvm, rvm tutorial

External links
Tipping's webpage on Sparse Bayesian Models and the RVM
A Tutorial on RVM by Tristan Fletcher
Applied tutorial on RVM
Comparison of RVM and SVM",https://en.wikipedia.org/wiki/Relevance_vector_machine,"['Classification algorithms', 'Kernel methods for machine learning', 'Nonparametric Bayesian statistics']",Data Science
156,Reinforcement learning,"Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.
Reinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).
The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.

Introduction
Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.
Basic reinforcement is modeled as a Markov decision process (MDP):

a set of environment and agent states, S;
a set of actions, A, of the agent;

  
    
      
        
          P
          
            a
          
        
        (
        s
        ,
        
          s
          ′
        
        )
        =
        Pr
        (
        
          s
          
            t
            +
            1
          
        
        =
        
          s
          ′
        
        ∣
        
          s
          
            t
          
        
        =
        s
        ,
        
          a
          
            t
          
        
        =
        a
        )
      
    
    {\displaystyle P_{a}(s,s')=\Pr(s_{t+1}=s'\mid s_{t}=s,a_{t}=a)}
   is the probability of transition (at time 
  
    
      
        t
      
    
    {\displaystyle t}
  ) from state 
  
    
      
        s
      
    
    {\displaystyle s}
   to state 
  
    
      
        
          s
          ′
        
      
    
    {\displaystyle s'}
   under action 
  
    
      
        a
      
    
    {\displaystyle a}
  .

  
    
      
        
          R
          
            a
          
        
        (
        s
        ,
        
          s
          ′
        
        )
      
    
    {\displaystyle R_{a}(s,s')}
   is the immediate reward after transition from 
  
    
      
        s
      
    
    {\displaystyle s}
   to 
  
    
      
        
          s
          ′
        
      
    
    {\displaystyle s'}
   with action 
  
    
      
        a
      
    
    {\displaystyle a}
  .The purpose of reinforcement learning is for the agent to learn an optimal, or nearly-optimal, policy that maximizes the ""reward function"" or other user-provided reinforcement signal that accumulates from the immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals can learn to engage in behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.A basic reinforcement learning agent AI interacts with its environment in discrete time steps. At each time t, the agent receives the current state 
  
    
      
        
          s
          
            t
          
        
      
    
    {\displaystyle s_{t}}
   and reward 
  
    
      
        
          r
          
            t
          
        
      
    
    {\displaystyle r_{t}}
  . It then chooses an action 
  
    
      
        
          a
          
            t
          
        
      
    
    {\displaystyle a_{t}}
   from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state 
  
    
      
        
          s
          
            t
            +
            1
          
        
      
    
    {\displaystyle s_{t+1}}
   and the reward 
  
    
      
        
          r
          
            t
            +
            1
          
        
      
    
    {\displaystyle r_{t+1}}
   associated with the transition 
  
    
      
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        ,
        
          s
          
            t
            +
            1
          
        
        )
      
    
    {\displaystyle (s_{t},a_{t},s_{t+1})}
   is determined. The goal of a reinforcement learning agent is to learn a policy: 
  
    
      
        π
        :
        A
        ×
        S
        →
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle \pi :A\times S\rightarrow [0,1]}
  , 
  
    
      
        π
        (
        a
        ,
        s
        )
        =
        Pr
        (
        
          a
          
            t
          
        
        =
        a
        ∣
        
          s
          
            t
          
        
        =
        s
        )
      
    
    {\displaystyle \pi (a,s)=\Pr(a_{t}=a\mid s_{t}=s)}
   which maximizes the expected cumulative reward.
Formulating the problem as a MDP assumes the agent directly observes the current environmental state; in this case the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a Partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.
When the agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of regret. In order to act near optimally, the agent must reason about the long-term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.
Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including robot control, elevator scheduling, telecommunications, backgammon, checkers and Go (AlphaGo).
Two elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:

A model of the environment is known, but an analytic solution is not available;
Only a simulation model of the environment is given (the subject of simulation-based optimization);
The only way to collect information about the environment is to interact with it.The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.

Exploration
The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space MDPs in Burnetas and Katehakis (1997).Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.
One such method is 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  -greedy, where 
  
    
      
        0
        <
        ε
        <
        1
      
    
    {\displaystyle 0<\varepsilon <1}
   is a parameter controlling the amount of exploration vs. exploitation.  With probability 
  
    
      
        1
        −
        ε
      
    
    {\displaystyle 1-\varepsilon }
  , exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  , exploration is chosen, and the action is chosen uniformly at random. 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.

Algorithms for control learning
Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.

Criterion of optimality
Policy
The agent's action selection is modeled as a map called policy:

  
    
      
        π
        :
        A
        ×
        S
        →
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle \pi :A\times S\rightarrow [0,1]}
  

  
    
      
        π
        (
        a
        ,
        s
        )
        =
        Pr
        (
        
          a
          
            t
          
        
        =
        a
        ∣
        
          s
          
            t
          
        
        =
        s
        )
      
    
    {\displaystyle \pi (a,s)=\Pr(a_{t}=a\mid s_{t}=s)}
  The policy map gives the probability of taking action 
  
    
      
        a
      
    
    {\displaystyle a}
   when in state 
  
    
      
        s
      
    
    {\displaystyle s}
  . There are also non-probabilistic policies.

State-value function
Value function 
  
    
      
        
          V
          
            π
          
        
        (
        s
        )
      
    
    {\displaystyle V_{\pi }(s)}
   is defined as the expected return starting with state 
  
    
      
        s
      
    
    {\displaystyle s}
  , i.e. 
  
    
      
        
          s
          
            0
          
        
        =
        s
      
    
    {\displaystyle s_{0}=s}
  , and successively following policy 
  
    
      
        π
      
    
    {\displaystyle \pi }
  . Hence, roughly speaking, the value function estimates ""how good"" it is to be in a given state.

  
    
      
        
          V
          
            π
          
        
        (
        s
        )
        =
        E
        ⁡
        [
        R
        ]
        =
        E
        ⁡
        
          [
          
            
              ∑
              
                t
                =
                0
              
              
                ∞
              
            
            
              γ
              
                t
              
            
            
              r
              
                t
              
            
            ∣
            
              s
              
                0
              
            
            =
            s
          
          ]
        
        ,
      
    
    {\displaystyle V_{\pi }(s)=\operatorname {E} [R]=\operatorname {E} \left[\sum _{t=0}^{\infty }\gamma ^{t}r_{t}\mid s_{0}=s\right],}
  where the random variable 
  
    
      
        R
      
    
    {\displaystyle R}
   denotes the return, and is defined as the sum of future discounted rewards:

  
    
      
        R
        =
        
          ∑
          
            t
            =
            0
          
          
            ∞
          
        
        
          γ
          
            t
          
        
        
          r
          
            t
          
        
        ,
      
    
    {\displaystyle R=\sum _{t=0}^{\infty }\gamma ^{t}r_{t},}
  where 
  
    
      
        
          r
          
            t
          
        
      
    
    {\displaystyle r_{t}}
   is the reward at step 
  
    
      
        t
      
    
    {\displaystyle t}
  , 
  
    
      
        γ
        ∈
        [
        0
        ,
        1
        )
      
    
    {\displaystyle \gamma \in [0,1)}
   is the discount-rate. Gamma is less than 1, so events in the distant future are weighted less than events in the immediate future.
The algorithm must find a policy with maximum expected return. From the theory of MDPs it is known that, without loss of generality, the search can be restricted to the set of so-called stationary policies. A policy is stationary if the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted to deterministic stationary policies. A deterministic stationary policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.

Brute force
The brute force approach entails two steps:

For each possible policy, sample returns while following it
Choose the policy with the largest expected returnOne problem with this is that the number of policies can be large, or even infinite. Another is that variance of the returns may be large, which requires many samples to accurately estimate the return of each policy.
These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.

Value function
Value function approaches attempt to find a policy that maximizes the return by maintaining a set of estimates of expected returns for some policy (usually either the ""current"" [on-policy] or the optimal [off-policy] one).
These methods rely on the theory of Markov decision processes, where optimality is defined in a sense that is stronger than the above one: A policy is called optimal if it achieves the best expected return from any initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found amongst stationary policies.
To define optimality in a formal manner, define the value of a policy 
  
    
      
        π
      
    
    {\displaystyle \pi }
   by

  
    
      
        
          V
          
            π
          
        
        (
        s
        )
        =
        E
        [
        R
        ∣
        s
        ,
        π
        ]
        ,
      
    
    {\displaystyle V^{\pi }(s)=E[R\mid s,\pi ],}
  where 
  
    
      
        R
      
    
    {\displaystyle R}
   stands for the return associated with following 
  
    
      
        π
      
    
    {\displaystyle \pi }
   from the initial state 
  
    
      
        s
      
    
    {\displaystyle s}
  . Defining 
  
    
      
        
          V
          
            ∗
          
        
        (
        s
        )
      
    
    {\displaystyle V^{*}(s)}
   as the maximum possible value of 
  
    
      
        
          V
          
            π
          
        
        (
        s
        )
      
    
    {\displaystyle V^{\pi }(s)}
  , where 
  
    
      
        π
      
    
    {\displaystyle \pi }
   is allowed to change,

  
    
      
        
          V
          
            ∗
          
        
        (
        s
        )
        =
        
          max
          
            π
          
        
        
          V
          
            π
          
        
        (
        s
        )
        .
      
    
    {\displaystyle V^{*}(s)=\max _{\pi }V^{\pi }(s).}
  A policy that achieves these optimal values in each state is called optimal. Clearly, a policy that is optimal in this strong sense is also optimal in the sense that it maximizes the expected return 
  
    
      
        
          ρ
          
            π
          
        
      
    
    {\displaystyle \rho ^{\pi }}
  , since 
  
    
      
        
          ρ
          
            π
          
        
        =
        E
        [
        
          V
          
            π
          
        
        (
        S
        )
        ]
      
    
    {\displaystyle \rho ^{\pi }=E[V^{\pi }(S)]}
  , where 
  
    
      
        S
      
    
    {\displaystyle S}
   is a state randomly sampled from the distribution 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  .
Although state-values suffice to define optimality, it is useful to define action-values. Given a state 
  
    
      
        s
      
    
    {\displaystyle s}
  , an action 
  
    
      
        a
      
    
    {\displaystyle a}
   and a policy 
  
    
      
        π
      
    
    {\displaystyle \pi }
  , the action-value of the pair 
  
    
      
        (
        s
        ,
        a
        )
      
    
    {\displaystyle (s,a)}
   under 
  
    
      
        π
      
    
    {\displaystyle \pi }
   is defined by

  
    
      
        
          Q
          
            π
          
        
        (
        s
        ,
        a
        )
        =
        E
        ⁡
        [
        R
        ∣
        s
        ,
        a
        ,
        π
        ]
        ,
        
      
    
    {\displaystyle Q^{\pi }(s,a)=\operatorname {E} [R\mid s,a,\pi ],\,}
  where 
  
    
      
        R
      
    
    {\displaystyle R}
   now stands for the random return associated with first taking action 
  
    
      
        a
      
    
    {\displaystyle a}
   in state 
  
    
      
        s
      
    
    {\displaystyle s}
   and following 
  
    
      
        π
      
    
    {\displaystyle \pi }
  , thereafter.
The theory of MDPs states that if 
  
    
      
        
          π
          
            ∗
          
        
      
    
    {\displaystyle \pi ^{*}}
   is an optimal policy, we act optimally (take the optimal action) by choosing the action from 
  
    
      
        
          Q
          
            
              π
              
                ∗
              
            
          
        
        (
        s
        ,
        ⋅
        )
      
    
    {\displaystyle Q^{\pi ^{*}}(s,\cdot )}
   with the highest value at each state, 
  
    
      
        s
      
    
    {\displaystyle s}
  . The action-value function of such an optimal policy (
  
    
      
        
          Q
          
            
              π
              
                ∗
              
            
          
        
      
    
    {\displaystyle Q^{\pi ^{*}}}
  ) is called the optimal action-value function and is commonly denoted by 
  
    
      
        
          Q
          
            ∗
          
        
      
    
    {\displaystyle Q^{*}}
  . In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.
Assuming full knowledge of the MDP, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions 
  
    
      
        
          Q
          
            k
          
        
      
    
    {\displaystyle Q_{k}}
   (
  
    
      
        k
        =
        0
        ,
        1
        ,
        2
        ,
        …
      
    
    {\displaystyle k=0,1,2,\ldots }
  ) that converge to 
  
    
      
        
          Q
          
            ∗
          
        
      
    
    {\displaystyle Q^{*}}
  . Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) MDPs. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.

Monte Carlo methods
Monte Carlo methods can be used in an algorithm that mimics policy iteration. Policy iteration consists of two steps: policy evaluation and policy improvement.
Monte Carlo is used in the policy evaluation step. In this step, given a stationary, deterministic policy 
  
    
      
        π
      
    
    {\displaystyle \pi }
  , the goal is to compute the function values 
  
    
      
        
          Q
          
            π
          
        
        (
        s
        ,
        a
        )
      
    
    {\displaystyle Q^{\pi }(s,a)}
   (or a good approximation to them) for all state-action pairs 
  
    
      
        (
        s
        ,
        a
        )
      
    
    {\displaystyle (s,a)}
  . Assuming (for simplicity) that the MDP is finite, that sufficient memory is available to accommodate the action-values and that the problem is episodic and after each episode a new one starts from some random initial state. Then, the estimate of the value of a given state-action pair 
  
    
      
        (
        s
        ,
        a
        )
      
    
    {\displaystyle (s,a)}
   can be computed by averaging the sampled returns that originated from 
  
    
      
        (
        s
        ,
        a
        )
      
    
    {\displaystyle (s,a)}
   over time.  Given sufficient time, this procedure can thus construct a precise estimate 
  
    
      
        Q
      
    
    {\displaystyle Q}
   of the action-value function 
  
    
      
        
          Q
          
            π
          
        
      
    
    {\displaystyle Q^{\pi }}
  . This finishes the description of the policy evaluation step.
In the policy improvement step, the next policy is obtained by computing a greedy policy with respect to 
  
    
      
        Q
      
    
    {\displaystyle Q}
  : Given a state 
  
    
      
        s
      
    
    {\displaystyle s}
  , this new policy returns an action that maximizes 
  
    
      
        Q
        (
        s
        ,
        ⋅
        )
      
    
    {\displaystyle Q(s,\cdot )}
  . In practice lazy evaluation can defer the computation of the maximizing actions to when they are needed.
Problems with this procedure include:

The procedure may spend too much time evaluating a suboptimal policy.
It uses samples inefficiently in that a long trajectory improves the estimate only of the single state-action pair that started the trajectory.
When the returns along the trajectories have high variance, convergence is slow.
It works in episodic problems only;
It works in small, finite MDPs only.

Temporal difference methods
The first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of generalized policy iteration algorithms. Many actor critic methods belong to this category.
The second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's temporal difference (TD) methods that are based on the recursive Bellman equation. The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method, may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.
In order to address the fifth issue, function approximation methods are used. Linear function approximation starts with a mapping 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair 
  
    
      
        (
        s
        ,
        a
        )
      
    
    {\displaystyle (s,a)}
   are obtained by linearly combining the components of 
  
    
      
        ϕ
        (
        s
        ,
        a
        )
      
    
    {\displaystyle \phi (s,a)}
   with some weights 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  :

  
    
      
        Q
        (
        s
        ,
        a
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            d
          
        
        
          θ
          
            i
          
        
        
          ϕ
          
            i
          
        
        (
        s
        ,
        a
        )
        .
      
    
    {\displaystyle Q(s,a)=\sum _{i=1}^{d}\theta _{i}\phi _{i}(s,a).}
  The algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.
Value iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants.The problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency. Another problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   parameter 
  
    
      
        (
        0
        ≤
        λ
        ≤
        1
        )
      
    
    {\displaystyle (0\leq \lambda \leq 1)}
   that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.

Direct policy search
An alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.
Gradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  , let 
  
    
      
        
          π
          
            θ
          
        
      
    
    {\displaystyle \pi _{\theta }}
   denote the policy associated to 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  . Defining the performance function by

  
    
      
        ρ
        (
        θ
        )
        =
        
          ρ
          
            
              π
              
                θ
              
            
          
        
        ,
      
    
    {\displaystyle \rho (\theta )=\rho ^{\pi _{\theta }},}
  under mild conditions this function will be differentiable as a function of the parameter vector 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  . If the gradient of 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
   was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method (which is known as the likelihood ratio method in the simulation-based optimization literature). Policy search methods have been used in the robotics context. Many policy search methods may get stuck in local optima (as they are based on local search).
A large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.
Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, actor–critic methods have been proposed and performed well on various problems.

Theory
Both the asymptotic and finite-sample behavior of most algorithms is well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.
Efficient exploration of MDPs is given in  Burnetas and Katehakis (1997). Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.
For incremental algorithms, asymptotic convergence issues have been settled. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).

Research
Research topics include 

adaptive methods that work with fewer (or no) parameters under a large number of conditions
addressing the exploration problem in large MDPs
combinations with logic-based frameworks
large-scale empirical evaluations
learning and acting under partial information (e.g., using predictive state representation)
modular and hierarchical reinforcement learning
improving existing value-function and policy search methods
algorithms that work well with large (or continuous) action spaces
transfer learning
lifelong learning
efficient sample-based planning (e.g., based on Monte Carlo tree search).
bug detection in software projects
Intrinsic motivation which differentiates information-seeking, curiosity-type behaviours from task-dependent goal-directed behaviours (typically) by introducing a reward function based on maximising novel information
Cognitive modeling using reinforcement learning has been actively pursued in computational psychology 
Multiagent or distributed reinforcement learning is a topic of interest. Applications are expanding.
Actor-critic reinforcement learning
Reinforcement learning algorithms such as TD learning are under investigation as a model for dopamine-based learning in the brain. In this model, the dopaminergic projections from the substantia nigra to the basal ganglia function as the prediction error. Reinforcement learning has been used as a part of the model for human skill learning, especially in relation to the interaction between implicit and explicit learning in skill acquisition (the first publication on this application was in 1995–1996).
Occupant-centric control
Algorithmic trading and optimal execution.

Comparison of reinforcement learning algorithms
Associative reinforcement learning
Associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.

Deep reinforcement learning
This approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space. The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning.

Inverse reinforcement learning
In inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal.

Safe Reinforcement Learning
Safe Reinforcement Learning (SRL) can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes.

See also
References
Further reading
Auer, Peter; Jaksch, Thomas; Ortner, Ronald (2010). ""Near-optimal regret bounds for reinforcement learning"". Journal of Machine Learning Research. 11: 1563–1600.
Busoniu, Lucian; Babuska, Robert; De Schutter, Bart; Ernst, Damien (2010). Reinforcement Learning and Dynamic Programming using Function Approximators. Taylor & Francis CRC Press. ISBN 978-1-4398-2108-4.
François-Lavet, Vincent; Henderson, Peter; Islam, Riashat; Bellemare, Marc G.; Pineau, Joelle (2018). ""An Introduction to Deep Reinforcement Learning"". Foundations and Trends in Machine Learning. 11 (3–4): 219–354. arXiv:1811.12560. Bibcode:2018arXiv181112560F. doi:10.1561/2200000071. S2CID 54434537.
Powell, Warren (2007). Approximate dynamic programming: solving the curses of dimensionality. Wiley-Interscience. ISBN 978-0-470-17155-4.
Sutton, Richard S.; Barto, Andrew G. (2018). Reinforcement Learning: An Introduction (2 ed.). MIT Press. ISBN 978-0-262-03924-6.
Sutton, Richard S. (1988). ""Learning to predict by the method of temporal differences"". Machine Learning. 3: 9–44. doi:10.1007/BF00115009.
Szita, Istvan; Szepesvari, Csaba (2010). ""Model-based Reinforcement Learning with Nearly Tight Exploration Complexity Bounds"" (PDF). ICML 2010. Omnipress. pp. 1031–1038. Archived from the original (PDF) on 2010-07-14.

External links
Reinforcement Learning Repository
Reinforcement Learning and Artificial Intelligence (RLAI, Rich Sutton's lab at the University of Alberta)
Autonomous Learning Laboratory (ALL, Andrew Barto's lab at the University of Massachusetts Amherst)
Hybrid reinforcement learning
Real-world reinforcement learning experiments at Delft University of Technology
Stanford University Andrew Ng Lecture on Reinforcement Learning
Dissecting Reinforcement Learning Series of blog post on RL with Python code",https://en.wikipedia.org/wiki/Reinforcement_learning,"['Articles with short description', 'Belief revision', 'Harv and Sfn no-target errors', 'Markov models', 'Reinforcement learning', 'Short description matches Wikidata', 'Webarchive template wayback links', 'Wikipedia articles needing clarification from January 2020', 'Wikipedia articles needing clarification from July 2018']",Data Science
157,Regression analysis,"In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome variable') and one or more independent variables (often called 'predictors', 'covariates', or 'features'). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression).
Regression analysis is primarily used for two conceptually distinct purposes. First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. Second, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables. Importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. The latter is especially important when researchers hope to estimate causal relationships using  observational data.

History
The earliest form of regression was the method of least squares, which was published by Legendre in 1805, and by Gauss in 1809. Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the Sun (mostly comets, but also later the then newly discovered minor planets). Gauss published a further development of the theory of least squares in 1821, including a version of the Gauss–Markov theorem.
The term ""regression"" was coined by Francis Galton in the nineteenth century to describe a biological phenomenon. The phenomenon was that the heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as regression toward the mean).
For Galton, regression had only this biological meaning, but his work was later extended by Udny Yule and Karl Pearson to a more general statistical context. In the work of Yule and Pearson, the joint distribution of the response and explanatory variables is assumed to be Gaussian. This assumption was weakened by R.A. Fisher in his works of 1922 and 1925. Fisher assumed that the conditional distribution of the response variable is Gaussian, but the joint distribution need not be. In this respect, Fisher's assumption is closer to Gauss's formulation of 1821.
In the 1950s and 1960s, economists used electromechanical desk ""calculators"" to calculate regressions. Before 1970, it sometimes took up to 24 hours to receive the result from one regression.Regression methods continue to be an area of active research. In recent decades, new methods have been developed for robust regression, regression involving correlated responses such as time series and growth curves, regression in which the predictor (independent variable) or response variables are curves, images, graphs, or other complex data objects, regression methods accommodating various types of missing data, nonparametric regression, Bayesian methods for regression, regression in which the predictor variables are measured with error, regression with more predictor variables than observations, and causal inference with regression.

Regression model
In practice, researchers first select a model they would like to estimate and then use their chosen method (e.g., ordinary least squares) to estimate the parameters of that model. Regression models involve the following components:

The unknown parameters, often denoted as a scalar or vector 
  
    
      
        β
      
    
    {\displaystyle \beta }
  .
The independent variables, which are observed in data and are often denoted as a vector 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
   (where 
  
    
      
        i
      
    
    {\displaystyle i}
   denotes a row of data).
The dependent variable, which are observed in data and often denoted using the scalar 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
  .
The error terms, which are not directly observed in data and are often denoted using the scalar 
  
    
      
        
          e
          
            i
          
        
      
    
    {\displaystyle e_{i}}
  .In various fields of application, different terminologies are used in place of dependent and independent variables.
Most regression models propose that 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
   is a function of 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
   and 
  
    
      
        β
      
    
    {\displaystyle \beta }
  , with 
  
    
      
        
          e
          
            i
          
        
      
    
    {\displaystyle e_{i}}
   representing an additive error term that may stand in for un-modeled determinants of 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
   or random statistical noise:

  
    
      
        
          Y
          
            i
          
        
        =
        f
        (
        
          X
          
            i
          
        
        ,
        β
        )
        +
        
          e
          
            i
          
        
      
    
    {\displaystyle Y_{i}=f(X_{i},\beta )+e_{i}}
  The researchers' goal is to estimate the function 
  
    
      
        f
        (
        
          X
          
            i
          
        
        ,
        β
        )
      
    
    {\displaystyle f(X_{i},\beta )}
   that most closely fits the data. To carry out regression analysis, the form of the function 
  
    
      
        f
      
    
    {\displaystyle f}
   must be specified. Sometimes the form of this function is based on knowledge about the relationship between 
  
    
      
        
          Y
          
            i
          
        
      
    
    {\displaystyle Y_{i}}
   and 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
   that does not rely on the data. If no such knowledge is available, a flexible or convenient form for 
  
    
      
        f
      
    
    {\displaystyle f}
   is chosen. For example, a simple univariate regression may propose 
  
    
      
        f
        (
        
          X
          
            i
          
        
        ,
        β
        )
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          X
          
            i
          
        
      
    
    {\displaystyle f(X_{i},\beta )=\beta _{0}+\beta _{1}X_{i}}
  , suggesting that the researcher believes 
  
    
      
        
          Y
          
            i
          
        
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          X
          
            i
          
        
        +
        
          e
          
            i
          
        
      
    
    {\displaystyle Y_{i}=\beta _{0}+\beta _{1}X_{i}+e_{i}}
   to be a reasonable approximation for the statistical process generating the data.  
Once researchers determine their preferred statistical model, different forms of regression analysis provide tools to estimate the parameters 
  
    
      
        β
      
    
    {\displaystyle \beta }
  . For example,  least squares (including its most common variant, ordinary least squares) finds the value of 
  
    
      
        β
      
    
    {\displaystyle \beta }
   that minimizes the sum of squared errors 
  
    
      
        
          ∑
          
            i
          
        
        (
        
          Y
          
            i
          
        
        −
        f
        (
        
          X
          
            i
          
        
        ,
        β
        )
        
          )
          
            2
          
        
      
    
    {\displaystyle \sum _{i}(Y_{i}-f(X_{i},\beta ))^{2}}
  . A given regression method will ultimately provide an estimate of 
  
    
      
        β
      
    
    {\displaystyle \beta }
  , usually denoted 
  
    
      
        
          
            
              β
              ^
            
          
        
      
    
    {\displaystyle {\hat {\beta }}}
   to distinguish the estimate from the true (unknown) parameter value that generated the data. Using this estimate, the researcher can then use the fitted value 
  
    
      
        
          
            
              
                Y
                
                  i
                
              
              ^
            
          
        
        =
        f
        (
        
          X
          
            i
          
        
        ,
        
          
            
              β
              ^
            
          
        
        )
      
    
    {\displaystyle {\hat {Y_{i}}}=f(X_{i},{\hat {\beta }})}
   for prediction or to assess the accuracy of the model in explaining the data. Whether the researcher is intrinsically interested in the estimate 
  
    
      
        
          
            
              β
              ^
            
          
        
      
    
    {\displaystyle {\hat {\beta }}}
   or the predicted value 
  
    
      
        
          
            
              
                Y
                
                  i
                
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {Y_{i}}}}
   will depend on context and their goals. As described in ordinary least squares, least squares is widely used because the estimated function 
  
    
      
        f
        (
        
          X
          
            i
          
        
        ,
        
          
            
              β
              ^
            
          
        
        )
      
    
    {\displaystyle f(X_{i},{\hat {\beta }})}
   approximates the conditional expectation 
  
    
      
        E
        (
        
          Y
          
            i
          
        
        
          |
        
        
          X
          
            i
          
        
        )
      
    
    {\displaystyle E(Y_{i}|X_{i})}
  . However, alternative variants (e.g., least absolute deviations or quantile regression) are useful when researchers want to model other functions 
  
    
      
        f
        (
        
          X
          
            i
          
        
        ,
        β
        )
      
    
    {\displaystyle f(X_{i},\beta )}
  . 
It is important to note that there must be sufficient data to estimate a regression model. For example, suppose that a researcher has access to 
  
    
      
        N
      
    
    {\displaystyle N}
   rows of data with one dependent and two independent variables: 
  
    
      
        (
        
          Y
          
            i
          
        
        ,
        
          X
          
            1
            i
          
        
        ,
        
          X
          
            2
            i
          
        
        )
      
    
    {\displaystyle (Y_{i},X_{1i},X_{2i})}
  . Suppose further that the researcher wants to estimate a bivariate linear model via least squares: 
  
    
      
        
          Y
          
            i
          
        
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          X
          
            1
            i
          
        
        +
        
          β
          
            2
          
        
        
          X
          
            2
            i
          
        
        +
        
          e
          
            i
          
        
      
    
    {\displaystyle Y_{i}=\beta _{0}+\beta _{1}X_{1i}+\beta _{2}X_{2i}+e_{i}}
  . If the researcher only has access to 
  
    
      
        N
        =
        2
      
    
    {\displaystyle N=2}
   data points, then they could find infinitely many combinations 
  
    
      
        (
        
          
            
              
                β
                ^
              
            
          
          
            0
          
        
        ,
        
          
            
              
                β
                ^
              
            
          
          
            1
          
        
        ,
        
          
            
              
                β
                ^
              
            
          
          
            2
          
        
        )
      
    
    {\displaystyle ({\hat {\beta }}_{0},{\hat {\beta }}_{1},{\hat {\beta }}_{2})}
   that explain the data equally well: any combination can be chosen that satisfies 
  
    
      
        
          
            
              
                Y
                ^
              
            
          
          
            i
          
        
        =
        
          
            
              
                β
                ^
              
            
          
          
            0
          
        
        +
        
          
            
              
                β
                ^
              
            
          
          
            1
          
        
        
          X
          
            1
            i
          
        
        +
        
          
            
              
                β
                ^
              
            
          
          
            2
          
        
        
          X
          
            2
            i
          
        
      
    
    {\displaystyle {\hat {Y}}_{i}={\hat {\beta }}_{0}+{\hat {\beta }}_{1}X_{1i}+{\hat {\beta }}_{2}X_{2i}}
  , all of which lead to 
  
    
      
        
          ∑
          
            i
          
        
        
          
            
              
                e
                ^
              
            
          
          
            i
          
          
            2
          
        
        =
        
          ∑
          
            i
          
        
        (
        
          
            
              
                Y
                ^
              
            
          
          
            i
          
        
        −
        (
        
          
            
              
                β
                ^
              
            
          
          
            0
          
        
        +
        
          
            
              
                β
                ^
              
            
          
          
            1
          
        
        
          X
          
            1
            i
          
        
        +
        
          
            
              
                β
                ^
              
            
          
          
            2
          
        
        
          X
          
            2
            i
          
        
        )
        
          )
          
            2
          
        
        =
        0
      
    
    {\displaystyle \sum _{i}{\hat {e}}_{i}^{2}=\sum _{i}({\hat {Y}}_{i}-({\hat {\beta }}_{0}+{\hat {\beta }}_{1}X_{1i}+{\hat {\beta }}_{2}X_{2i}))^{2}=0}
   and are therefore valid solutions that minimize the sum of squared residuals. To understand why there are infinitely many options, note that the system of 
  
    
      
        N
        =
        2
      
    
    {\displaystyle N=2}
   equations is to be solved for 3 unknowns, which makes the system underdetermined. Alternatively, one can visualize infinitely many 3-dimensional planes that go through 
  
    
      
        N
        =
        2
      
    
    {\displaystyle N=2}
   fixed points. 
More generally, to estimate a least squares model with 
  
    
      
        k
      
    
    {\displaystyle k}
   distinct parameters, one must have 
  
    
      
        N
        ≥
        k
      
    
    {\displaystyle N\geq k}
   distinct data points. If 
  
    
      
        N
        <
        k
      
    
    {\displaystyle N<k}
  , then there does not generally exist a set of parameters that will perfectly fit the data. The quantity 
  
    
      
        N
        −
        k
      
    
    {\displaystyle N-k}
   appears often in regression analysis, and is referred to as the degrees of freedom in the model. Moreover, to estimate a least squares model, the independent variables 
  
    
      
        (
        
          X
          
            1
            i
          
        
        ,
        
          X
          
            2
            i
          
        
        ,
        .
        .
        .
        ,
        
          X
          
            k
            i
          
        
        )
      
    
    {\displaystyle (X_{1i},X_{2i},...,X_{ki})}
   must be linearly independent: one must not be able to reconstruct any of the independent variables by adding and multiplying the remaining independent variables. As discussed in ordinary least squares, this condition ensures that 
  
    
      
        
          X
          
            T
          
        
        X
      
    
    {\displaystyle X^{T}X}
   is an invertible matrix and therefore that a unique solution 
  
    
      
        
          
            
              β
              ^
            
          
        
      
    
    {\displaystyle {\hat {\beta }}}
   exists.

Underlying assumptions
By itself, a regression is simply a calculation using the data. In order to interpret the output of a regression as a meaningful statistical quantity that measures real-world relationships, researchers often rely on a number of classical assumptions. These often include:

The sample is representative of the population at large.
The independent variables are measured with no error.
Deviations from the model have an expected value of zero, conditional on covariates: 
  
    
      
        E
        (
        
          e
          
            i
          
        
        
          |
        
        
          X
          
            i
          
        
        )
        =
        0
      
    
    {\displaystyle E(e_{i}|X_{i})=0}
  
The variance of the residuals 
  
    
      
        
          e
          
            i
          
        
      
    
    {\displaystyle e_{i}}
   is constant across observations (homoscedasticity).
The residuals 
  
    
      
        
          e
          
            i
          
        
      
    
    {\displaystyle e_{i}}
   are uncorrelated with one another. Mathematically, the variance–covariance matrix of the errors is diagonal.A handful of conditions are sufficient for the least-squares estimator to possess desirable properties: in particular, the Gauss–Markov assumptions imply that the parameter estimates will be unbiased, consistent, and efficient in the class of linear unbiased estimators. Practitioners have developed a variety of methods to maintain some or all of these desirable properties in real-world settings, because these classical assumptions are unlikely to hold exactly. For example, modeling errors-in-variables can lead to reasonable estimates independent variables are measured with errors. Heteroscedasticity-consistent standard errors allow the variance of 
  
    
      
        
          e
          
            i
          
        
      
    
    {\displaystyle e_{i}}
   to change across values of 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
  . Correlated errors that exist within subsets of the data or follow specific patterns can be handled using clustered standard errors, geographic weighted regression, or Newey–West standard errors, among other techniques. When rows of data correspond to locations in space, the choice of how to model 
  
    
      
        
          e
          
            i
          
        
      
    
    {\displaystyle e_{i}}
   within geographic units can have important consequences. The subfield of econometrics is largely focused on developing techniques that allow researchers to make reasonable real-world conclusions in real-world settings, where classical assumptions do not hold exactly.

Linear regression
In linear regression, the model specification is that the dependent variable, 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
   is a linear combination of the parameters (but need not be linear in the independent variables). For example, in simple linear regression for modeling 
  
    
      
        n
      
    
    {\displaystyle n}
   data points there is one independent variable: 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  , and two parameters, 
  
    
      
        
          β
          
            0
          
        
      
    
    {\displaystyle \beta _{0}}
   and 
  
    
      
        
          β
          
            1
          
        
      
    
    {\displaystyle \beta _{1}}
  :

straight line: 
  
    
      
        
          y
          
            i
          
        
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          x
          
            i
          
        
        +
        
          ε
          
            i
          
        
        ,
        
        i
        =
        1
        ,
        …
        ,
        n
        .
        
      
    
    {\displaystyle y_{i}=\beta _{0}+\beta _{1}x_{i}+\varepsilon _{i},\quad i=1,\dots ,n.\!}
  In multiple linear regression, there are several independent variables or functions of independent variables.
Adding a term in 
  
    
      
        
          x
          
            i
          
          
            2
          
        
      
    
    {\displaystyle x_{i}^{2}}
   to the preceding regression gives:

parabola: 
  
    
      
        
          y
          
            i
          
        
        =
        
          β
          
            0
          
        
        +
        
          β
          
            1
          
        
        
          x
          
            i
          
        
        +
        
          β
          
            2
          
        
        
          x
          
            i
          
          
            2
          
        
        +
        
          ε
          
            i
          
        
        ,
         
        i
        =
        1
        ,
        …
        ,
        n
        .
        
      
    
    {\displaystyle y_{i}=\beta _{0}+\beta _{1}x_{i}+\beta _{2}x_{i}^{2}+\varepsilon _{i},\ i=1,\dots ,n.\!}
  This is still linear regression; although the expression on the right hand side is quadratic in the independent variable 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  , it is linear in the parameters 
  
    
      
        
          β
          
            0
          
        
      
    
    {\displaystyle \beta _{0}}
  , 
  
    
      
        
          β
          
            1
          
        
      
    
    {\displaystyle \beta _{1}}
   and 
  
    
      
        
          β
          
            2
          
        
        .
      
    
    {\displaystyle \beta _{2}.}
  
In both cases, 
  
    
      
        
          ε
          
            i
          
        
      
    
    {\displaystyle \varepsilon _{i}}
   is an error term and the subscript 
  
    
      
        i
      
    
    {\displaystyle i}
   indexes a particular observation.
Returning our attention to the straight line case: Given a random sample from the population, we estimate the population parameters and obtain the sample linear regression model:

  
    
      
        
          
            
              
                y
                ^
              
            
          
          
            i
          
        
        =
        
          
            
              
                β
                ^
              
            
          
          
            0
          
        
        +
        
          
            
              
                β
                ^
              
            
          
          
            1
          
        
        
          x
          
            i
          
        
        .
      
    
    {\displaystyle {\widehat {y}}_{i}={\widehat {\beta }}_{0}+{\widehat {\beta }}_{1}x_{i}.}
  The residual, 
  
    
      
        
          e
          
            i
          
        
        =
        
          y
          
            i
          
        
        −
        
          
            
              
                y
                ^
              
            
          
          
            i
          
        
      
    
    {\displaystyle e_{i}=y_{i}-{\widehat {y}}_{i}}
  , is the difference between the value of the dependent variable predicted by the model, 
  
    
      
        
          
            
              
                y
                ^
              
            
          
          
            i
          
        
      
    
    {\displaystyle {\widehat {y}}_{i}}
  , and the true value of the dependent variable, 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
  . One method of estimation is ordinary least squares. This method obtains parameter estimates that minimize the sum of squared residuals, SSR:

  
    
      
        S
        S
        R
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          e
          
            i
          
          
            2
          
        
        .
        
      
    
    {\displaystyle SSR=\sum _{i=1}^{n}e_{i}^{2}.\,}
  Minimization of this function results in a set of normal equations, a set of simultaneous linear equations in the parameters, which are solved to yield the parameter estimators, 
  
    
      
        
          
            
              
                β
                ^
              
            
          
          
            0
          
        
        ,
        
          
            
              
                β
                ^
              
            
          
          
            1
          
        
      
    
    {\displaystyle {\widehat {\beta }}_{0},{\widehat {\beta }}_{1}}
  .

In the case of simple regression, the formulas for the least squares estimates are

  
    
      
        
          
            
              
                β
                ^
              
            
          
          
            1
          
        
        =
        
          
            
              ∑
              (
              
                x
                
                  i
                
              
              −
              
                
                  
                    x
                    ¯
                  
                
              
              )
              (
              
                y
                
                  i
                
              
              −
              
                
                  
                    y
                    ¯
                  
                
              
              )
            
            
              ∑
              (
              
                x
                
                  i
                
              
              −
              
                
                  
                    x
                    ¯
                  
                
              
              
                )
                
                  2
                
              
            
          
        
      
    
    {\displaystyle {\widehat {\beta }}_{1}={\frac {\sum (x_{i}-{\bar {x}})(y_{i}-{\bar {y}})}{\sum (x_{i}-{\bar {x}})^{2}}}}
  

  
    
      
        
          
            
              
                β
                ^
              
            
          
          
            0
          
        
        =
        
          
            
              y
              ¯
            
          
        
        −
        
          
            
              
                β
                ^
              
            
          
          
            1
          
        
        
          
            
              x
              ¯
            
          
        
      
    
    {\displaystyle {\widehat {\beta }}_{0}={\bar {y}}-{\widehat {\beta }}_{1}{\bar {x}}}
  where 
  
    
      
        
          
            
              x
              ¯
            
          
        
      
    
    {\displaystyle {\bar {x}}}
   is the mean (average) of the 
  
    
      
        x
      
    
    {\displaystyle x}
   values and 
  
    
      
        
          
            
              y
              ¯
            
          
        
      
    
    {\displaystyle {\bar {y}}}
   is the mean of the 
  
    
      
        y
      
    
    {\displaystyle y}
   values.
Under the assumption that the population error term has a constant variance, the estimate of that variance is given by:

  
    
      
        
          
            
              
                σ
                ^
              
            
          
          
            ε
          
          
            2
          
        
        =
        
          
            
              S
              S
              R
            
            
              n
              −
              2
            
          
        
        .
        
      
    
    {\displaystyle {\hat {\sigma }}_{\varepsilon }^{2}={\frac {SSR}{n-2}}.\,}
  This is called the mean square error (MSE) of the regression. The denominator is the sample size reduced by the number of model parameters estimated from the same data, 
  
    
      
        (
        n
        −
        p
        )
      
    
    {\displaystyle (n-p)}
   for 
  
    
      
        p
      
    
    {\displaystyle p}
   regressors or 
  
    
      
        (
        n
        −
        p
        −
        1
        )
      
    
    {\displaystyle (n-p-1)}
   if an intercept is used. In this case, 
  
    
      
        p
        =
        1
      
    
    {\displaystyle p=1}
   so the denominator is 
  
    
      
        n
        −
        2
      
    
    {\displaystyle n-2}
  .
The standard errors of the parameter estimates are given by

  
    
      
        
          
            
              
                σ
                ^
              
            
          
          
            
              β
              
                1
              
            
          
        
        =
        
          
            
              
                σ
                ^
              
            
          
          
            ε
          
        
        
          
            
              1
              
                ∑
                (
                
                  x
                  
                    i
                  
                
                −
                
                  
                    
                      x
                      ¯
                    
                  
                
                
                  )
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle {\hat {\sigma }}_{\beta _{1}}={\hat {\sigma }}_{\varepsilon }{\sqrt {\frac {1}{\sum (x_{i}-{\bar {x}})^{2}}}}}
  
  
    
      
        
          
            
              
                σ
                ^
              
            
          
          
            
              β
              
                0
              
            
          
        
        =
        
          
            
              
                σ
                ^
              
            
          
          
            ε
          
        
        
          
            
              
                1
                n
              
            
            +
            
              
                
                  
                    
                      
                        x
                        ¯
                      
                    
                  
                  
                    2
                  
                
                
                  ∑
                  (
                  
                    x
                    
                      i
                    
                  
                  −
                  
                    
                      
                        x
                        ¯
                      
                    
                  
                  
                    )
                    
                      2
                    
                  
                
              
            
          
        
        =
        
          
            
              
                σ
                ^
              
            
          
          
            
              β
              
                1
              
            
          
        
        
          
            
              
                ∑
                
                  x
                  
                    i
                  
                  
                    2
                  
                
              
              n
            
          
        
        .
      
    
    {\displaystyle {\hat {\sigma }}_{\beta _{0}}={\hat {\sigma }}_{\varepsilon }{\sqrt {{\frac {1}{n}}+{\frac {{\bar {x}}^{2}}{\sum (x_{i}-{\bar {x}})^{2}}}}}={\hat {\sigma }}_{\beta _{1}}{\sqrt {\frac {\sum x_{i}^{2}}{n}}}.}
  Under the further assumption that the population error term is normally distributed, the researcher can use these estimated standard errors to create confidence intervals and conduct hypothesis tests about the population parameters.

General linear model
In the more general multiple regression model, there are 
  
    
      
        p
      
    
    {\displaystyle p}
   independent variables:

  
    
      
        
          y
          
            i
          
        
        =
        
          β
          
            1
          
        
        
          x
          
            i
            1
          
        
        +
        
          β
          
            2
          
        
        
          x
          
            i
            2
          
        
        +
        ⋯
        +
        
          β
          
            p
          
        
        
          x
          
            i
            p
          
        
        +
        
          ε
          
            i
          
        
        ,
        
      
    
    {\displaystyle y_{i}=\beta _{1}x_{i1}+\beta _{2}x_{i2}+\cdots +\beta _{p}x_{ip}+\varepsilon _{i},\,}
  where 
  
    
      
        
          x
          
            i
            j
          
        
      
    
    {\displaystyle x_{ij}}
   is the 
  
    
      
        i
      
    
    {\displaystyle i}
  -th observation on the 
  
    
      
        j
      
    
    {\displaystyle j}
  -th independent variable.
If the first independent variable takes the value 1 for all 
  
    
      
        i
      
    
    {\displaystyle i}
  , 
  
    
      
        
          x
          
            i
            1
          
        
        =
        1
      
    
    {\displaystyle x_{i1}=1}
  , then 
  
    
      
        
          β
          
            1
          
        
      
    
    {\displaystyle \beta _{1}}
   is called the regression intercept.
The least squares parameter estimates are obtained from 
  
    
      
        p
      
    
    {\displaystyle p}
   normal equations. The residual can be written as

  
    
      
        
          ε
          
            i
          
        
        =
        
          y
          
            i
          
        
        −
        
          
            
              
                β
                ^
              
            
          
          
            1
          
        
        
          x
          
            i
            1
          
        
        −
        ⋯
        −
        
          
            
              
                β
                ^
              
            
          
          
            p
          
        
        
          x
          
            i
            p
          
        
        .
      
    
    {\displaystyle \varepsilon _{i}=y_{i}-{\hat {\beta }}_{1}x_{i1}-\cdots -{\hat {\beta }}_{p}x_{ip}.}
  The normal equations are

  
    
      
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          ∑
          
            k
            =
            1
          
          
            p
          
        
        
          x
          
            i
            j
          
        
        
          x
          
            i
            k
          
        
        
          
            
              
                β
                ^
              
            
          
          
            k
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          x
          
            i
            j
          
        
        
          y
          
            i
          
        
        ,
         
        j
        =
        1
        ,
        …
        ,
        p
        .
        
      
    
    {\displaystyle \sum _{i=1}^{n}\sum _{k=1}^{p}x_{ij}x_{ik}{\hat {\beta }}_{k}=\sum _{i=1}^{n}x_{ij}y_{i},\ j=1,\dots ,p.\,}
  In matrix notation, the normal equations are written as

  
    
      
        
          (
          
            X
            
              ⊤
            
          
          X
          )
          
            
              
                β
                ^
              
            
          
          =
          

          
          
            X
            
              ⊤
            
          
          Y
        
        ,
        
      
    
    {\displaystyle \mathbf {(X^{\top }X){\hat {\boldsymbol {\beta }}}={}X^{\top }Y} ,\,}
  where the 
  
    
      
        i
        j
      
    
    {\displaystyle ij}
   element of 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
   is 
  
    
      
        
          x
          
            i
            j
          
        
      
    
    {\displaystyle x_{ij}}
  , the 
  
    
      
        i
      
    
    {\displaystyle i}
   element of the column vector 
  
    
      
        Y
      
    
    {\displaystyle Y}
   is 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
  , and the 
  
    
      
        j
      
    
    {\displaystyle j}
   element of 
  
    
      
        
          
            
              β
              ^
            
          
        
      
    
    {\displaystyle {\hat {\boldsymbol {\beta }}}}
   is 
  
    
      
        
          
            
              
                β
                ^
              
            
          
          
            j
          
        
      
    
    {\displaystyle {\hat {\beta }}_{j}}
  . Thus 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
   is 
  
    
      
        n
        ×
        p
      
    
    {\displaystyle n\times p}
  , 
  
    
      
        Y
      
    
    {\displaystyle Y}
   is 
  
    
      
        n
        ×
        1
      
    
    {\displaystyle n\times 1}
  , and 
  
    
      
        
          
            
              β
              ^
            
          
        
      
    
    {\displaystyle {\hat {\boldsymbol {\beta }}}}
   is 
  
    
      
        p
        ×
        1
      
    
    {\displaystyle p\times 1}
  . The solution is

  
    
      
        
          
            
              
                β
                ^
              
            
          
          =
          (
          
            X
            
              ⊤
            
          
          X
          
            )
            
              −
              1
            
          
          
            X
            
              ⊤
            
          
          Y
        
        .
        
      
    
    {\displaystyle \mathbf {{\hat {\boldsymbol {\beta }}}=(X^{\top }X)^{-1}X^{\top }Y} .\,}

Diagnostics
Once a regression model has been constructed, it may be important to confirm the goodness of fit of the model and the statistical significance of the estimated parameters. Commonly used checks of goodness of fit include the R-squared, analyses of the pattern of residuals and hypothesis testing. Statistical significance can be checked by an F-test of the overall fit, followed by t-tests of individual parameters.
Interpretations of these diagnostic tests rest heavily on the model's assumptions. Although examination of the residuals can be used to invalidate a model, the results of a t-test or F-test are sometimes more difficult to interpret if the model's assumptions are violated. For example, if the error term does not have a normal distribution, in small samples the estimated parameters will not follow normal distributions and complicate inference. With relatively large samples, however, a central limit theorem can be invoked such that hypothesis testing may proceed using asymptotic approximations.

Limited dependent variables
Limited dependent variables, which are response variables that are categorical variables or are variables constrained to fall only in a certain range, often arise in econometrics.
The response variable may be non-continuous (""limited"" to lie on some subset of the real line). For binary (zero or one) variables, if analysis proceeds with least-squares linear regression, the model is called the linear probability model. Nonlinear models for binary dependent variables include the probit and logit model. The multivariate probit model is a standard method of estimating a joint relationship between several binary dependent variables and some independent variables. For categorical variables with more than two values there is the multinomial logit. For ordinal variables with more than two values, there are the ordered logit and ordered probit models. Censored regression models may be used when the dependent variable is only sometimes observed, and Heckman correction type models may be used when the sample is not randomly selected from the population of interest. An alternative to such procedures is linear regression based on polychoric correlation (or polyserial correlations) between the categorical variables. Such procedures differ in the assumptions made about the distribution of the variables in the population. If the variable is positive with low values and represents the repetition of the occurrence of an event, then count models like the Poisson regression or the negative binomial model may be used.

Nonlinear regression
When the model function is not linear in the parameters, the sum of squares must be minimized by an iterative procedure. This introduces many complications which are summarized in Differences between linear and non-linear least squares.

Interpolation and extrapolation
Regression models predict a value of the Y variable given known values of the X variables. Prediction within the range of values in the dataset used for model-fitting is known informally as interpolation. Prediction outside this range of the data is known as extrapolation. Performing extrapolation relies strongly on the regression assumptions. The further the extrapolation goes outside the data, the more room there is for the model to fail due to differences between the assumptions and the sample data or the true values.
It is generally advised that when performing extrapolation, one should accompany the estimated value of the dependent variable with a prediction interval that represents the uncertainty. Such intervals tend to expand rapidly as the values of the independent variable(s) moved outside the range covered by the observed data.
For such reasons and others, some tend to say that it might be unwise to undertake extrapolation.However, this does not cover the full set of modeling errors that may be made: in particular, the assumption of a particular form for the relation between Y and X. A properly conducted regression analysis will include an assessment of how well the assumed form is matched by the observed data, but it can only do so within the range of values of the independent variables actually available. This means that any extrapolation is particularly reliant on the assumptions being made about the structural form of the regression relationship. Best-practice advice here is that a linear-in-variables and linear-in-parameters relationship should not be chosen simply for computational convenience, but that all available knowledge should be deployed in constructing a regression model. If this knowledge includes the fact that the dependent variable cannot go outside a certain range of values, this can be made use of in selecting the model – even if the observed dataset has no values particularly near such bounds. The implications of this step of choosing an appropriate functional form for the regression can be great when extrapolation is considered. At a minimum, it can ensure that any extrapolation arising from a fitted model is ""realistic"" (or in accord with what is known).

Power and sample size calculations
There are no generally agreed methods for relating the number of observations versus the number of independent variables in the model. One rule of thumb conjectured by Good and Hardin is 
  
    
      
        N
        =
        
          m
          
            n
          
        
      
    
    {\displaystyle N=m^{n}}
  , where 
  
    
      
        N
      
    
    {\displaystyle N}
   is the sample size, 
  
    
      
        n
      
    
    {\displaystyle n}
   is the number of independent variables and 
  
    
      
        m
      
    
    {\displaystyle m}
   is the number of observations needed to reach the desired precision if the model had only one independent variable. For example, a researcher is building a linear regression model using a dataset that contains 1000 patients (
  
    
      
        N
      
    
    {\displaystyle N}
  ). If the researcher decides that five observations are needed to precisely define a straight line (
  
    
      
        m
      
    
    {\displaystyle m}
  ), then the maximum number of independent variables the model can support is 4, because

  
    
      
        
          
            
              log
              ⁡
              1000
            
            
              log
              ⁡
              5
            
          
        
        =
        4.29.
      
    
    {\displaystyle {\frac {\log 1000}{\log 5}}=4.29.}

Other methods
Although the parameters of a regression model are usually estimated using the method of least squares, other methods which have been used include:

Bayesian methods, e.g. Bayesian linear regression
Percentage regression, for situations where reducing percentage errors is deemed more appropriate.
Least absolute deviations, which is more robust in the presence of outliers, leading to quantile regression
Nonparametric regression, requires a large number of observations and is computationally intensive
Scenario optimization, leading to  interval predictor models
Distance metric learning, which is learned by the search of a meaningful distance metric in a given input space.

Software
All major statistical software packages perform least squares regression analysis and inference. Simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators. While many statistical software packages can perform various types of nonparametric and robust regression, these methods are less standardized; different software packages implement different methods, and a method with a given name may be implemented differently in different packages. Specialized regression software has been developed for use in fields such as survey analysis and neuroimaging.

See also
References
Further reading
William H. Kruskal and Judith M. Tanur, ed. (1978), ""Linear Hypotheses,"" International Encyclopedia of Statistics. Free Press,  v. 1,Evan J. Williams, ""I. Regression,"" pp. 523–41.
Julian C. Stanley, ""II. Analysis of Variance,"" pp. 541–554.Lindley, D.V. (1987). ""Regression and correlation analysis,"" New Palgrave: A Dictionary of Economics, v. 4, pp. 120–23.
Birkes, David and Dodge, Y., Alternative Methods of Regression. ISBN 0-471-56881-3
Chatfield, C. (1993) ""Calculating Interval Forecasts,"" Journal of Business and Economic Statistics, 11. pp. 121–135.
Draper, N.R.; Smith, H. (1998). Applied Regression Analysis (3rd ed.). John Wiley. ISBN 978-0-471-17082-2.
Fox, J. (1997).  Applied Regression Analysis, Linear Models and Related Methods. Sage
Hardle, W., Applied Nonparametric Regression (1990), ISBN 0-521-42950-1
Meade, Nigel; Islam, Towhidul (1995). ""Prediction intervals for growth curve forecasts"". Journal of Forecasting. 14 (5): 413–430. doi:10.1002/for.3980140502.
A. Sen, M. Srivastava, Regression Analysis — Theory, Methods, and Applications, Springer-Verlag, Berlin, 2011 (4th printing).
T. Strutz: Data Fitting and Uncertainty (A practical introduction to weighted least squares and beyond). Vieweg+Teubner, ISBN 978-3-8348-1022-9.
Malakooti, B. (2013). Operations and Production Systems with Multiple Objectives. John Wiley & Sons.

External links
""Regression analysis"", Encyclopedia of Mathematics, EMS Press, 2001 [1994]
Earliest Uses: Regression – basic history and references
Regression of Weakly Correlated Data – how linear regression mistakes can appear when Y-range is much smaller than X-range",https://en.wikipedia.org/wiki/Regression_analysis,"['Actuarial science', 'All articles needing additional references', 'All articles with unsourced statements', 'Articles needing additional references from December 2020', 'Articles with short description', 'Articles with unsourced statements from February 2010', 'Articles with unsourced statements from March 2011', 'Commons category link is on Wikidata', 'Estimation theory', 'Regression analysis', 'Short description matches Wikidata', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NDL identifiers']",Data Science
158,Restricted Boltzmann machine,"A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.
RBMs were initially invented under the name Harmonium by Paul Smolensky in 1986,
and rose to prominence after Geoffrey Hinton and collaborators invented fast learning algorithms for them in the mid-2000. RBMs have found applications in dimensionality reduction,classification,collaborative filtering,  feature learning,topic modelling
and even many body quantum mechanics. They can be trained in either supervised or unsupervised ways, depending on the task.
As their name implies, RBMs are a variant of Boltzmann machines, with the restriction that their neurons must form a bipartite graph: 
a pair of nodes from each of the two groups of units (commonly referred to as the ""visible"" and ""hidden"" units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group. By contrast, ""unrestricted"" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based contrastive divergence algorithm.Restricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by ""stacking"" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation.

Structure
The standard type of RBM has binary-valued (Boolean/Bernoulli) hidden and visible units, and consists of a matrix of weights 
  
    
      
        W
        =
        (
        
          w
          
            i
            ,
            j
          
        
        )
      
    
    {\displaystyle W=(w_{i,j})}
   (size m×n) associated with the connection between hidden unit 
  
    
      
        
          h
          
            j
          
        
      
    
    {\displaystyle h_{j}}
   and visible unit 
  
    
      
        
          v
          
            i
          
        
      
    
    {\displaystyle v_{i}}
  , as well as bias weights (offsets) 
  
    
      
        
          a
          
            i
          
        
      
    
    {\displaystyle a_{i}}
   for the visible units and 
  
    
      
        
          b
          
            j
          
        
      
    
    {\displaystyle b_{j}}
   for the hidden units. Given these, the energy of a configuration (pair of boolean vectors) (v,h) is defined as

  
    
      
        E
        (
        v
        ,
        h
        )
        =
        −
        
          ∑
          
            i
          
        
        
          a
          
            i
          
        
        
          v
          
            i
          
        
        −
        
          ∑
          
            j
          
        
        
          b
          
            j
          
        
        
          h
          
            j
          
        
        −
        
          ∑
          
            i
          
        
        
          ∑
          
            j
          
        
        
          v
          
            i
          
        
        
          w
          
            i
            ,
            j
          
        
        
          h
          
            j
          
        
      
    
    {\displaystyle E(v,h)=-\sum _{i}a_{i}v_{i}-\sum _{j}b_{j}h_{j}-\sum _{i}\sum _{j}v_{i}w_{i,j}h_{j}}
  or, in matrix notation,

  
    
      
        E
        (
        v
        ,
        h
        )
        =
        −
        
          a
          
            
              T
            
          
        
        v
        −
        
          b
          
            
              T
            
          
        
        h
        −
        
          v
          
            
              T
            
          
        
        W
        h
      
    
    {\displaystyle E(v,h)=-a^{\mathrm {T} }v-b^{\mathrm {T} }h-v^{\mathrm {T} }Wh}
  This energy function is analogous to that of a Hopfield network. As in general Boltzmann machines, probability distributions over hidden and/or visible vectors are defined in terms of the energy function:

  
    
      
        P
        (
        v
        ,
        h
        )
        =
        
          
            1
            Z
          
        
        
          e
          
            −
            E
            (
            v
            ,
            h
            )
          
        
      
    
    {\displaystyle P(v,h)={\frac {1}{Z}}e^{-E(v,h)}}
  where 
  
    
      
        Z
      
    
    {\displaystyle Z}
   is a partition function defined as the sum of 
  
    
      
        
          e
          
            −
            E
            (
            v
            ,
            h
            )
          
        
      
    
    {\displaystyle e^{-E(v,h)}}
   over all possible configurations (in other words, just a normalizing constant to ensure the probability distribution sums to 1). Similarly, the (marginal) probability of a visible (input) vector of booleans is the sum over all possible hidden layer configurations:

  
    
      
        P
        (
        v
        )
        =
        
          
            1
            Z
          
        
        
          ∑
          
            h
          
        
        
          e
          
            −
            E
            (
            v
            ,
            h
            )
          
        
      
    
    {\displaystyle P(v)={\frac {1}{Z}}\sum _{h}e^{-E(v,h)}}
  Since the RBM has the shape of a bipartite graph, with no intra-layer connections, the hidden unit activations are mutually independent given the visible unit activations and conversely, the visible unit activations are mutually independent given the hidden unit activations. That is, for 
  
    
      
        m
      
    
    {\displaystyle m}
   visible units and 
  
    
      
        n
      
    
    {\displaystyle n}
   hidden units, the conditional probability of a configuration of the visible units v, given a configuration of the hidden units h, is

  
    
      
        P
        (
        v
        
          |
        
        h
        )
        =
        
          ∏
          
            i
            =
            1
          
          
            m
          
        
        P
        (
        
          v
          
            i
          
        
        
          |
        
        h
        )
      
    
    {\displaystyle P(v|h)=\prod _{i=1}^{m}P(v_{i}|h)}
  .Conversely, the conditional probability of h given v is

  
    
      
        P
        (
        h
        
          |
        
        v
        )
        =
        
          ∏
          
            j
            =
            1
          
          
            n
          
        
        P
        (
        
          h
          
            j
          
        
        
          |
        
        v
        )
      
    
    {\displaystyle P(h|v)=\prod _{j=1}^{n}P(h_{j}|v)}
  .The individual activation probabilities are given by

  
    
      
        P
        (
        
          h
          
            j
          
        
        =
        1
        
          |
        
        v
        )
        =
        σ
        
          (
          
            
              b
              
                j
              
            
            +
            
              ∑
              
                i
                =
                1
              
              
                m
              
            
            
              w
              
                i
                ,
                j
              
            
            
              v
              
                i
              
            
          
          )
        
      
    
    {\displaystyle P(h_{j}=1|v)=\sigma \left(b_{j}+\sum _{i=1}^{m}w_{i,j}v_{i}\right)}
   and 
  
    
      
        
        P
        (
        
          v
          
            i
          
        
        =
        1
        
          |
        
        h
        )
        =
        σ
        
          (
          
            
              a
              
                i
              
            
            +
            
              ∑
              
                j
                =
                1
              
              
                n
              
            
            
              w
              
                i
                ,
                j
              
            
            
              h
              
                j
              
            
          
          )
        
      
    
    {\displaystyle \,P(v_{i}=1|h)=\sigma \left(a_{i}+\sum _{j=1}^{n}w_{i,j}h_{j}\right)}
  where 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
   denotes the logistic sigmoid.
The visible units of Restricted Boltzmann Machine can be multinomial, although the hidden units are Bernoulli. In this case, the logistic function for visible units is replaced by the softmax function

  
    
      
        P
        (
        
          v
          
            i
          
          
            k
          
        
        =
        1
        
          |
        
        h
        )
        =
        
          
            
              exp
              ⁡
              (
              
                a
                
                  i
                
                
                  k
                
              
              +
              
                Σ
                
                  j
                
              
              
                W
                
                  i
                  j
                
                
                  k
                
              
              
                h
                
                  j
                
              
              )
            
            
              
                Σ
                
                  
                    k
                    ′
                  
                  =
                  1
                
                
                  K
                
              
              exp
              ⁡
              (
              
                a
                
                  i
                
                
                  
                    k
                    ′
                  
                
              
              +
              
                Σ
                
                  j
                
              
              
                W
                
                  i
                  j
                
                
                  
                    k
                    ′
                  
                
              
              
                h
                
                  j
                
              
              )
            
          
        
      
    
    {\displaystyle P(v_{i}^{k}=1|h)={\frac {\exp(a_{i}^{k}+\Sigma _{j}W_{ij}^{k}h_{j})}{\Sigma _{k'=1}^{K}\exp(a_{i}^{k'}+\Sigma _{j}W_{ij}^{k'}h_{j})}}}
  where K is the number of discrete values that the visible values have. They are applied in topic modeling, and recommender systems.

Relation to other models
Restricted Boltzmann machines are a special case of Boltzmann machines and Markov random fields.
Their graphical model corresponds to that of factor analysis.

Training algorithm
Restricted Boltzmann machines are trained to maximize the product of probabilities assigned to some training set 
  
    
      
        V
      
    
    {\displaystyle V}
   (a matrix, each row of which is treated as a visible vector 
  
    
      
        v
      
    
    {\displaystyle v}
  ),

  
    
      
        arg
        ⁡
        
          max
          
            W
          
        
        
          ∏
          
            v
            ∈
            V
          
        
        P
        (
        v
        )
      
    
    {\displaystyle \arg \max _{W}\prod _{v\in V}P(v)}
  or equivalently, to maximize the expected log probability of a training sample 
  
    
      
        v
      
    
    {\displaystyle v}
   selected randomly from 
  
    
      
        V
      
    
    {\displaystyle V}
  :

  
    
      
        arg
        ⁡
        
          max
          
            W
          
        
        
          E
        
        
          [
          
            log
            ⁡
            P
            (
            v
            )
          
          ]
        
      
    
    {\displaystyle \arg \max _{W}\mathbb {E} \left[\log P(v)\right]}
  The algorithm most often used to train RBMs, that is, to optimize the weight vector 
  
    
      
        W
      
    
    {\displaystyle W}
  , is the contrastive divergence (CD) algorithm due to Hinton, originally developed to train PoE (product of experts) models.
The algorithm performs Gibbs sampling and is used inside a gradient descent procedure (similar to the way backpropagation is used inside such a procedure when training feedforward neural nets) to compute weight update.
The basic, single-step contrastive divergence (CD-1) procedure for a single sample can be summarized as follows:

Take a training sample v, compute the probabilities of the hidden units and sample a hidden activation vector h from this probability distribution.
Compute the outer product of v and h and call this the positive gradient.
From h, sample a reconstruction v' of the visible units, then resample the hidden activations h' from this. (Gibbs sampling step)
Compute the outer product of v' and h' and call this the negative gradient.
Let the update to the weight matrix 
  
    
      
        W
      
    
    {\displaystyle W}
   be the positive gradient minus the negative gradient, times some learning rate: 
  
    
      
        Δ
        W
        =
        ϵ
        (
        v
        
          h
          
            
              T
            
          
        
        −
        
          v
          ′
        
        
          h
          
            ′
            
              
                T
              
            
          
        
        )
      
    
    {\displaystyle \Delta W=\epsilon (vh^{\mathsf {T}}-v'h'^{\mathsf {T}})}
  .
Update the biases a and b analogously: 
  
    
      
        Δ
        a
        =
        ϵ
        (
        v
        −
        
          v
          ′
        
        )
      
    
    {\displaystyle \Delta a=\epsilon (v-v')}
  , 
  
    
      
        Δ
        b
        =
        ϵ
        (
        h
        −
        
          h
          ′
        
        )
      
    
    {\displaystyle \Delta b=\epsilon (h-h')}
  .A Practical Guide to Training RBMs written by Hinton can be found on his homepage.

See also
Autoencoder
Helmholtz machine

References
External links
Introduction to Restricted Boltzmann Machines. Edwin Chen's blog, July 18, 2011.
""A Beginner's Guide to Restricted Boltzmann Machines"". Archived from the original on February 11, 2017. Retrieved November 15, 2018.CS1 maint: bot: original URL status unknown (link). Deeplearning4j Documentation
""Understanding RBMs"". Archived from the original on September 20, 2016. Retrieved December 29, 2014.. Deeplearning4j Documentation
Python implementation of Bernoulli RBM and tutorial
SimpleRBM is a very small RBM code (24kB) useful for you to learn about how RBMs learn and work.",https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine,"['All articles with dead external links', 'Articles with dead external links from April 2018', 'Articles with permanently dead external links', 'Articles with short description', 'Artificial neural networks', 'CS1 maint: bot: original URL status unknown', 'Short description matches Wikidata', 'Stochastic models', 'Supervised learning', 'Unsupervised learning', 'Webarchive template wayback links']",Data Science
159,S2CID (identifier),"Semantic Scholar is an artificial-intelligence backed search engine for academic publications that was developed at the Allen Institute for Artificial Intelligence and publicly released in November 2015. It uses recent advances in natural language processing to provide summaries for scholarly papers.

Technology
Semantic Scholar provides one-sentence summary of scientific literature. One of its aims was to address the challenge of reading numerous titles and lengthy abstracts on mobile devices. It also seeks to ensure that the three million scientific papers published yearly reach readers since it is estimated that only half of this literature are ever read.Artificial intelligence is used to capture the essence of a paper, generating it through an ""abstractive"" technique. The project uses a combination of machine learning, natural language processing, and machine vision to add a layer of semantic analysis to the traditional methods of citation analysis, and to extract relevant figures, entities, and venues from papers. In comparison to Google Scholar and PubMed, Semantic Scholar is designed to highlight the most important and influential papers, and to identify the connections between them. Each paper hosted by Semantic Scholar is assigned a unique identifier called the Semantic Scholar Corpus ID (or S2CID for short), for example

Liu, Ying; Gayle, Albert A; Wilder-Smith, Annelies; Rocklöv, Joacim (March 2020). ""The reproductive number of COVID-19 is higher compared to SARS coronavirus"". Journal of Travel Medicine. 27 (2). doi:10.1093/jtm/taaa021. PMC 7074654. PMID 32052846. S2CID 211099356.As of January 2018, following a 2017 project that added biomedical papers and topic summaries, the Semantic Scholar corpus included more than 40 million papers from computer science and biomedicine. In March 2018, Doug Raymond, who developed machine learning initiatives for the Amazon Alexa platform, was hired to lead the Semantic Scholar project. As of August 2019, the number of included papers had grown to more than 173 million after the addition of the Microsoft Academic Graph records.In 2020, users of Semantic Scholar reached seven million a month.

See also
Citation analysis
Citation index
Knowledge extraction
List of academic databases and search engines
Scientometrics

References
External links
Official website",https://en.wikipedia.org/wiki/Semantic_Scholar,"['All stub articles', 'Articles with short description', 'Bibliographic databases in computer science', 'Internet search engines', 'Scholarly search services', 'Search engine website stubs', 'Short description matches Wikidata']",Data Science
160,Semi-supervised learning,"Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). It is a special instance of weak supervision.Unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render large, fully labeled training sets infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.
A set of 
  
    
      
        l
      
    
    {\displaystyle l}
   independently identically distributed examples 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            l
          
        
        ∈
        X
      
    
    {\displaystyle x_{1},\dots ,x_{l}\in X}
   with corresponding labels 
  
    
      
        
          y
          
            1
          
        
        ,
        …
        ,
        
          y
          
            l
          
        
        ∈
        Y
      
    
    {\displaystyle y_{1},\dots ,y_{l}\in Y}
   and 
  
    
      
        u
      
    
    {\displaystyle u}
   unlabeled examples 
  
    
      
        
          x
          
            l
            +
            1
          
        
        ,
        …
        ,
        
          x
          
            l
            +
            u
          
        
        ∈
        X
      
    
    {\displaystyle x_{l+1},\dots ,x_{l+u}\in X}
   are processed. Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.
Semi-supervised learning may refer to either transductive learning or inductive learning. The goal of transductive learning is to infer the correct labels for the given unlabeled data 
  
    
      
        
          x
          
            l
            +
            1
          
        
        ,
        …
        ,
        
          x
          
            l
            +
            u
          
        
      
    
    {\displaystyle x_{l+1},\dots ,x_{l+u}}
   only. The goal of inductive learning is to infer the correct mapping from 
  
    
      
        X
      
    
    {\displaystyle X}
   to 
  
    
      
        Y
      
    
    {\displaystyle Y}
  .
Intuitively, the learning problem can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam.
It is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.

Assumptions
In order to make any use of unlabeled data, some relationship to the underlying distribution of data must exist. Semi-supervised learning algorithms make use of at least one of the following assumptions:

Continuity assumption
Points that are close to each other are more likely to share a label. This is also generally assumed in supervised learning and yields a preference for geometrically simple decision boundaries. In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes.

Cluster assumption
The data tend to form discrete clusters, and points in the same cluster are more likely to share a label (although data that shares a label may spread across multiple clusters). This is a special case of the smoothness assumption and gives rise to feature learning with clustering algorithms.

Manifold assumption
The data lie approximately on a manifold of much lower dimension than the input space. In this case learning the manifold using both the labeled and unlabeled data can avoid the curse of dimensionality. Then learning can proceed using distances and densities defined on the manifold.
The manifold assumption is practical when high-dimensional data are generated by some process that may be hard to model directly, but which has only a few degrees of freedom. For instance, human voice is controlled by a few vocal folds, and images of various facial expressions are controlled by a few muscles. In these cases distances and smoothness in the natural space of the generating problem, is superior to considering the space of all possible acoustic waves or images, respectively.

History
The heuristic approach of self-training (also known as self-learning or self-labeling) is historically the oldest approach to semi-supervised learning, with examples of applications starting in the 1960s.The transductive learning framework was formally introduced by Vladimir Vapnik in the 1970s. Interest in inductive learning using generative models also began in the 1970s. A probably approximately correct learning bound for semi-supervised learning of a Gaussian mixture was demonstrated by Ratsaby and Venkatesh in 1995.Semi-supervised learning has recently become more popular and practically relevant due to the variety of problems for which vast quantities of unlabeled data are available—e.g. text on websites, protein sequences, or images.

Methods
Generative models
Generative approaches to statistical learning first seek to estimate 
  
    
      
        p
        (
        x
        
          |
        
        y
        )
      
    
    {\displaystyle p(x|y)}
  , the distribution of data points belonging to each class. The probability 
  
    
      
        p
        (
        y
        
          |
        
        x
        )
      
    
    {\displaystyle p(y|x)}
   that a given point 
  
    
      
        x
      
    
    {\displaystyle x}
   has label 
  
    
      
        y
      
    
    {\displaystyle y}
   is then proportional to 
  
    
      
        p
        (
        x
        
          |
        
        y
        )
        p
        (
        y
        )
      
    
    {\displaystyle p(x|y)p(y)}
   by Bayes' rule. Semi-supervised learning with generative models can be viewed either as an extension of supervised learning (classification plus information about 
  
    
      
        p
        (
        x
        )
      
    
    {\displaystyle p(x)}
  ) or as an extension of unsupervised learning (clustering plus some labels).
Generative models assume that the distributions take some particular form 
  
    
      
        p
        (
        x
        
          |
        
        y
        ,
        θ
        )
      
    
    {\displaystyle p(x|y,\theta )}
   parameterized by the vector 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  . If these assumptions are incorrect, the unlabeled data may actually decrease the accuracy of the solution relative to what would have been obtained from labeled data alone. 
However, if the assumptions are correct, then the unlabeled data necessarily improves performance.The unlabeled data are distributed according to a mixture of individual-class distributions. In order to learn the mixture distribution from the unlabeled data, it must be identifiable, that is, different parameters must yield different summed distributions. Gaussian mixture distributions are identifiable and commonly used for generative models.
The parameterized joint distribution can be written as 
  
    
      
        p
        (
        x
        ,
        y
        
          |
        
        θ
        )
        =
        p
        (
        y
        
          |
        
        θ
        )
        p
        (
        x
        
          |
        
        y
        ,
        θ
        )
      
    
    {\displaystyle p(x,y|\theta )=p(y|\theta )p(x|y,\theta )}
   by using the chain rule. Each parameter vector 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   is associated with a decision function 
  
    
      
        
          f
          
            θ
          
        
        (
        x
        )
        =
        
          
            argmax
            y
          
        
         
        p
        (
        y
        
          |
        
        x
        ,
        θ
        )
      
    
    {\displaystyle f_{\theta }(x)={\underset {y}{\operatorname {argmax} }}\ p(y|x,\theta )}
  . 
The parameter is then chosen based on fit to both the labeled and unlabeled data, weighted by 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
  :

  
    
      
        
          
            argmax
            Θ
          
        
        
          (
          
            log
            ⁡
            p
            (
            {
            
              x
              
                i
              
            
            ,
            
              y
              
                i
              
            
            
              }
              
                i
                =
                1
              
              
                l
              
            
            
              |
            
            θ
            )
            +
            λ
            log
            ⁡
            p
            (
            {
            
              x
              
                i
              
            
            
              }
              
                i
                =
                l
                +
                1
              
              
                l
                +
                u
              
            
            
              |
            
            θ
            )
          
          )
        
      
    
    {\displaystyle {\underset {\Theta }{\operatorname {argmax} }}\left(\log p(\{x_{i},y_{i}\}_{i=1}^{l}|\theta )+\lambda \log p(\{x_{i}\}_{i=l+1}^{l+u}|\theta )\right)}

Low-density separation
Another major class of methods attempts to place boundaries in regions with few data points (labeled or unlabeled). One of the most commonly used algorithms is the transductive support vector machine, or TSVM (which, despite its name, may be used for inductive learning as well). Whereas support vector machines for supervised learning seek a decision boundary with maximal margin over the labeled data, the goal of TSVM is a labeling of the unlabeled data such that the decision boundary has maximal margin over all of the data. In addition to the standard hinge loss 
  
    
      
        (
        1
        −
        y
        f
        (
        x
        )
        
          )
          
            +
          
        
      
    
    {\displaystyle (1-yf(x))_{+}}
   for labeled data, a loss function 
  
    
      
        (
        1
        −
        
          |
        
        f
        (
        x
        )
        
          |
        
        
          )
          
            +
          
        
      
    
    {\displaystyle (1-|f(x)|)_{+}}
   is introduced over the unlabeled data by letting 
  
    
      
        y
        =
        sign
        ⁡
        
          f
          (
          x
          )
        
      
    
    {\displaystyle y=\operatorname {sign} {f(x)}}
  . TSVM then selects 
  
    
      
        
          f
          
            ∗
          
        
        (
        x
        )
        =
        
          h
          
            ∗
          
        
        (
        x
        )
        +
        b
      
    
    {\displaystyle f^{*}(x)=h^{*}(x)+b}
   from a reproducing kernel Hilbert space 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
   by minimizing the regularized empirical risk:

  
    
      
        
          f
          
            ∗
          
        
        =
        
          
            argmin
            f
          
        
        
          (
          
            
              ∑
              
                i
                =
                1
              
              
                l
              
            
            (
            1
            −
            
              y
              
                i
              
            
            f
            (
            
              x
              
                i
              
            
            )
            
              )
              
                +
              
            
            +
            
              λ
              
                1
              
            
            ‖
            h
            
              ‖
              
                
                  H
                
              
              
                2
              
            
            +
            
              λ
              
                2
              
            
            
              ∑
              
                i
                =
                l
                +
                1
              
              
                l
                +
                u
              
            
            (
            1
            −
            
              |
            
            f
            (
            
              x
              
                i
              
            
            )
            
              |
            
            
              )
              
                +
              
            
          
          )
        
      
    
    {\displaystyle f^{*}={\underset {f}{\operatorname {argmin} }}\left(\displaystyle \sum _{i=1}^{l}(1-y_{i}f(x_{i}))_{+}+\lambda _{1}\|h\|_{\mathcal {H}}^{2}+\lambda _{2}\sum _{i=l+1}^{l+u}(1-|f(x_{i})|)_{+}\right)}
  An exact solution is intractable due to the non-convex term 
  
    
      
        (
        1
        −
        
          |
        
        f
        (
        x
        )
        
          |
        
        
          )
          
            +
          
        
      
    
    {\displaystyle (1-|f(x)|)_{+}}
  , so research focuses on useful approximations.Other approaches that implement low-density separation include Gaussian process models, information regularization, and entropy minimization (of which TSVM is a special case).

Graph-based methods
Graph-based methods for semi-supervised learning use a graph representation of the data, with a node for each labeled and unlabeled example. The graph may be constructed using domain knowledge or similarity of examples; two common methods are to connect each data point to its 
  
    
      
        k
      
    
    {\displaystyle k}
   nearest neighbors or to examples within some distance 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
  . The weight 
  
    
      
        
          W
          
            i
            j
          
        
      
    
    {\displaystyle W_{ij}}
   of an edge between 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   and 
  
    
      
        
          x
          
            j
          
        
      
    
    {\displaystyle x_{j}}
   is then set to 
  
    
      
        
          e
          
            
              
                −
                ‖
                
                  x
                  
                    i
                  
                
                −
                
                  x
                  
                    j
                  
                
                
                  ‖
                  
                    2
                  
                
              
              ϵ
            
          
        
      
    
    {\displaystyle e^{\frac {-\|x_{i}-x_{j}\|^{2}}{\epsilon }}}
  .
Within the framework of manifold regularization, the graph serves as a proxy for the manifold. A term is added to the standard Tikhonov regularization problem to enforce smoothness of the solution relative to the manifold (in the intrinsic space of the problem) as well as relative to the ambient input space. The minimization problem becomes

  
    
      
        
          
            argmin
            
              f
              ∈
              
                
                  H
                
              
            
          
        
        
          (
          
            
              
                1
                l
              
            
            
              
                ∑
                
                  i
                  =
                  1
                
                
                  l
                
              
              V
              (
              f
              (
              
                x
                
                  i
                
              
              )
              ,
              
                y
                
                  i
                
              
              )
              +
              
                λ
                
                  A
                
              
              ‖
              f
              
                ‖
                
                  
                    H
                  
                
                
                  2
                
              
              +
              
                λ
                
                  I
                
              
              
                ∫
                
                  
                    M
                  
                
              
              ‖
              
                ∇
                
                  
                    M
                  
                
              
              f
              (
              x
              )
              
                ‖
                
                  2
                
              
              d
              p
              (
              x
              )
            
          
          )
        
      
    
    {\displaystyle {\underset {f\in {\mathcal {H}}}{\operatorname {argmin} }}\left({\frac {1}{l}}\displaystyle \sum _{i=1}^{l}V(f(x_{i}),y_{i})+\lambda _{A}\|f\|_{\mathcal {H}}^{2}+\lambda _{I}\int _{\mathcal {M}}\|\nabla _{\mathcal {M}}f(x)\|^{2}dp(x)\right)}
  where 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
   is a reproducing kernel Hilbert space and 
  
    
      
        
          
            M
          
        
      
    
    {\displaystyle {\mathcal {M}}}
   is the manifold on which the data lie. The regularization parameters 
  
    
      
        
          λ
          
            A
          
        
      
    
    {\displaystyle \lambda _{A}}
   and 
  
    
      
        
          λ
          
            I
          
        
      
    
    {\displaystyle \lambda _{I}}
   control smoothness in the ambient and intrinsic spaces respectively. The graph is used to approximate the intrinsic regularization term. Defining the graph Laplacian 
  
    
      
        L
        =
        D
        −
        W
      
    
    {\displaystyle L=D-W}
   where 
  
    
      
        
          D
          
            i
            i
          
        
        =
        
          ∑
          
            j
            =
            1
          
          
            l
            +
            u
          
        
        
          W
          
            i
            j
          
        
      
    
    {\displaystyle D_{ii}=\sum _{j=1}^{l+u}W_{ij}}
   and 
  
    
      
        
          f
        
      
    
    {\displaystyle \mathbf {f} }
   the vector 
  
    
      
        [
        f
        (
        
          x
          
            1
          
        
        )
        …
        f
        (
        
          x
          
            l
            +
            u
          
        
        )
        ]
      
    
    {\displaystyle [f(x_{1})\dots f(x_{l+u})]}
  , we have

  
    
      
        
          
            f
          
          
            T
          
        
        L
        
          f
        
        =
        
          
            ∑
            
              i
              ,
              j
              =
              1
            
            
              l
              +
              u
            
          
          
            W
            
              i
              j
            
          
          (
          
            f
            
              i
            
          
          −
          
            f
            
              j
            
          
          
            )
            
              2
            
          
          ≈
          
            ∫
            
              
                M
              
            
          
          ‖
          
            ∇
            
              
                M
              
            
          
          f
          (
          x
          )
          
            ‖
            
              2
            
          
          d
          p
          (
          x
          )
        
      
    
    {\displaystyle \mathbf {f} ^{T}L\mathbf {f} =\displaystyle \sum _{i,j=1}^{l+u}W_{ij}(f_{i}-f_{j})^{2}\approx \int _{\mathcal {M}}\|\nabla _{\mathcal {M}}f(x)\|^{2}dp(x)}
  .The Laplacian can also be used to extend the supervised learning algorithms: regularized least squares and support vector machines (SVM) to semi-supervised versions Laplacian regularized least squares and Laplacian SVM.

Heuristic approaches
Some methods for semi-supervised learning are not intrinsically geared to learning from both unlabeled and labeled data, but instead make use of unlabeled data within a supervised learning framework. For instance, the labeled and unlabeled examples 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            l
            +
            u
          
        
      
    
    {\displaystyle x_{1},\dots ,x_{l+u}}
   may inform a choice of representation, distance metric, or kernel for the data in an unsupervised first step. Then supervised learning proceeds from only the labeled examples. In this vein, some methods learn a low-dimensional representation using the supervised data and then apply either low-density separation or graph-based methods to the learned representation. Iteratively refining the representation and then performing semi-supervised learning on said representation may further improve performance.
Self-training is a wrapper method for semi-supervised learning. First a supervised learning algorithm is trained based on the labeled data only. This classifier is then applied to the unlabeled data to generate more labeled examples as input for the supervised learning algorithm. Generally only the labels the classifier is most confident in are added at each step.Co-training is an extension of self-training in which multiple classifiers are trained on different (ideally disjoint) sets of features and generate labeled examples for one another.

In human cognition
Human responses to formal semi-supervised learning problems have yielded varying conclusions about the degree of influence of the unlabeled data. More natural learning problems may also be viewed as instances of semi-supervised learning. Much of human concept learning involves a small amount of direct instruction (e.g. parental labeling of objects during childhood) combined with large amounts of unlabeled experience (e.g. observation of objects without naming or counting them, or at least without feedback).
Human infants are sensitive to the structure of unlabeled natural categories such as images of dogs and cats or male and female faces. Infants and children take into account not only unlabeled examples, but the sampling process from which labeled examples arise.

See also
PU learning
Weak supervision

References
Sources
Chapelle, Olivier; Schölkopf, Bernhard; Zien, Alexander (2006). Semi-supervised learning. Cambridge, Mass.: MIT Press. ISBN 978-0-262-03358-9.

External links
Manifold Regularization A freely available MATLAB implementation of the graph-based semi-supervised algorithms Laplacian support vector machines and Laplacian regularized least squares.
KEEL: A software tool to assess evolutionary algorithms for Data Mining problems (regression, classification, clustering, pattern mining and so on) KEEL module for semi-supervised learning.
Semi-Supervised Learning Software Semi-Supervised Learning Software
1.14. Semi-Supervised — scikit-learn 0.22.1 documentation Semi-Supervised algorithms in scikit-learn .",https://en.wikipedia.org/wiki/Semi-supervised_learning,"['All accuracy disputes', 'Articles with disputed statements from November 2017', 'CS1 Russian-language sources (ru)', 'CS1 errors: missing periodical', 'CS1 maint: multiple names: authors list', 'Harv and Sfn no-target errors', 'Machine learning']",Data Science
161,Self-organizing map,"A self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.

This makes SOMs useful for visualization by creating low-dimensional views of high-dimensional data, akin to multidimensional scaling. The artificial neural network introduced by the Finnish professor Teuvo Kohonen in the 1980s is sometimes called a Kohonen map or network. The Kohonen net is a computationally convenient abstraction building on biological models of neural systems from the 1970s and morphogenesis models dating back to Alan Turing in the  1950s.While it is typical to consider this type of network structure as related to feedforward networks where the nodes are visualized as being attached, this type of architecture is fundamentally different in arrangement and motivation.
Useful extensions include using toroidal grids where opposite edges are connected and using large numbers of nodes.
It is also common to use the U-Matrix. The U-Matrix value of a particular node is the average distance between the node's weight vector and that of its closest neighbors. In a square grid, for instance, the closest 4 or 8 nodes might be considered (the Von Neumann and Moore neighborhoods, respectively), or six nodes in a hexagonal grid.
Large SOMs display emergent properties. In maps consisting of thousands of nodes, it is possible to perform cluster operations on the map itself.

Structure and operations
Like most artificial neural networks, SOMs operate in two modes: training and mapping. ""Training"" builds the map using input examples (a competitive process, also called vector quantization), while ""mapping"" automatically classifies a new input vector.
The visible part of a self-organizing map is the map space, which consists of components called nodes or neurons.  The map space is defined beforehand, usually as a finite two-dimensional region where nodes are arranged in a regular hexagonal or rectangular grid.  Each node is associated with a ""weight"" vector, which is a position in the input space; that is, it has the same dimension as each input vector.  While nodes in the map space stay fixed, training consists in moving weight vectors toward the input data (reducing a distance metric) without spoiling the topology induced from the map space.  Thus, the self-organizing map describes a mapping from a higher-dimensional input space to a lower-dimensional map space.  Once trained, the map can classify a vector from the input space by finding the node with the closest (smallest distance metric) weight vector to the input space vector.

Learning algorithm
The goal of learning in the self-organizing map is to cause different parts of the network to respond similarly to certain input patterns. This is partly motivated by how visual, auditory or other sensory information is handled in separate parts of the cerebral cortex in the human brain.

The weights of the neurons are initialized either to small random values or sampled evenly from the subspace spanned by the two largest principal component eigenvectors. With the latter alternative, learning is much faster because the initial weights already give a good approximation of SOM weights.The network must be fed a large number of example vectors that represent, as close as possible, the kinds of vectors expected during mapping. The examples are usually administered several times as iterations.
The training utilizes competitive learning. When a training example is fed to the network, its Euclidean distance to all weight vectors is computed. The neuron whose weight vector is most similar to the input is called the best matching unit (BMU). The weights of the BMU and neurons close to it in the SOM grid are adjusted towards the input vector. The magnitude of the change decreases with time and with the grid-distance from the BMU. The update formula for a neuron v with weight vector Wv(s) is

  
    
      
        
          W
          
            v
          
        
        (
        s
        +
        1
        )
        =
        
          W
          
            v
          
        
        (
        s
        )
        +
        θ
        (
        u
        ,
        v
        ,
        s
        )
        ⋅
        α
        (
        s
        )
        ⋅
        (
        D
        (
        t
        )
        −
        
          W
          
            v
          
        
        (
        s
        )
        )
      
    
    {\displaystyle W_{v}(s+1)=W_{v}(s)+\theta (u,v,s)\cdot \alpha (s)\cdot (D(t)-W_{v}(s))}
  ,where s is the step index, t an index into the training sample, u is the index of the BMU for the input vector D(t), α(s) is a monotonically decreasing learning coefficient; Θ(u, v, s) is the neighborhood function which gives the distance between the neuron u and the neuron v in step s. Depending on the implementations, t can scan the training data set systematically (t is 0, 1, 2...T-1, then repeat, T being the training sample's size), be randomly drawn from the data set (bootstrap sampling), or implement some other sampling method (such as jackknifing).
The neighborhood function Θ(u, v, s) (also called function of lateral interaction) depends on the grid-distance between the BMU (neuron u) and neuron v. In the simplest form, it is 1 for all neurons close enough to BMU and 0 for others, but the Gaussian and mexican-hat functions are common choices, too. Regardless of the functional form, the neighborhood function shrinks with time. At the beginning when the neighborhood is broad, the self-organizing takes place on the global scale. When the neighborhood has shrunk to just a couple of neurons, the weights are converging to local estimates. In some implementations, the learning coefficient α and the neighborhood function Θ decrease steadily with increasing s, in others (in particular those where t scans the training data set) they decrease in step-wise fashion, once every T steps.

This process is repeated for each input vector for a (usually large) number of cycles λ. The network winds up associating output nodes with groups or patterns in the input data set. If these patterns can be named, the names can be attached to the associated nodes in the trained net.
During mapping, there will be one single winning neuron: the neuron whose weight vector lies closest to the input vector. This can be simply determined by calculating the Euclidean distance between input vector and weight vector.
While representing input data as vectors has been emphasized in this article, any kind of object which can be represented digitally, which has an appropriate distance measure associated with it, and in which the necessary operations for training are possible can be used to construct a self-organizing map. This includes matrices, continuous functions or even other self-organizing maps.

Variables
These are the variables needed, with vectors in bold,

  
    
      
        s
      
    
    {\displaystyle s}
   is the current iteration

  
    
      
        λ
      
    
    {\displaystyle \lambda }
   is the iteration limit

  
    
      
        t
      
    
    {\displaystyle t}
   is the index of the target input data vector in the input data set 
  
    
      
        
          D
        
      
    
    {\displaystyle \mathbf {D} }
  

  
    
      
        
          D
        
        (
        t
        )
      
    
    {\displaystyle {D}(t)}
   is a target input data vector

  
    
      
        v
      
    
    {\displaystyle v}
   is the index of the node in the map

  
    
      
        
          
            W
          
          
            v
          
        
      
    
    {\displaystyle \mathbf {W} _{v}}
   is the current weight vector of node 
  
    
      
        v
      
    
    {\displaystyle v}
  

  
    
      
        u
      
    
    {\displaystyle u}
   is the index of the best matching unit (BMU) in the map

  
    
      
        θ
        (
        u
        ,
        v
        ,
        s
        )
      
    
    {\displaystyle \theta (u,v,s)}
   is a restraint due to distance from BMU, usually called the neighbourhood function, and

  
    
      
        α
        (
        s
        )
      
    
    {\displaystyle \alpha (s)}
   is a learning restraint due to iteration progress.

Algorithm
Randomize the  node weight vectors in a map
Randomly pick an input vector 
  
    
      
        
          D
        
        (
        t
        )
      
    
    {\displaystyle {D}(t)}
  
Traverse each node in the map
Use the Euclidean distance formula to find the similarity between the input vector and the map's node's weight vector
Track the node that produces the smallest distance (this node is the best matching unit, BMU)
Update the weight vectors of the nodes in the neighborhood of the BMU (including the BMU itself) by pulling them closer to the input vector

  
    
      
        
          W
          
            v
          
        
        (
        s
        +
        1
        )
        =
        
          W
          
            v
          
        
        (
        s
        )
        +
        θ
        (
        u
        ,
        v
        ,
        s
        )
        ⋅
        α
        (
        s
        )
        ⋅
        (
        D
        (
        t
        )
        −
        
          W
          
            v
          
        
        (
        s
        )
        )
      
    
    {\displaystyle W_{v}(s+1)=W_{v}(s)+\theta (u,v,s)\cdot \alpha (s)\cdot (D(t)-W_{v}(s))}
  
Increase 
  
    
      
        s
      
    
    {\displaystyle s}
   and repeat from step 2 while 
  
    
      
        s
        <
        λ
      
    
    {\displaystyle s<\lambda }
  A variant algorithm:

Randomize the map's nodes' weight vectors
Traverse each input vector in the input data set
Traverse each node in the map
Use the Euclidean distance formula to find the similarity between the input vector and the map's node's weight vector
Track the node that produces the smallest distance (this node is the best matching unit, BMU)
Update the nodes in the neighborhood of the BMU (including the BMU itself) by pulling them closer to the input vector

  
    
      
        
          W
          
            v
          
        
        (
        s
        +
        1
        )
        =
        
          W
          
            v
          
        
        (
        s
        )
        +
        θ
        (
        u
        ,
        v
        ,
        s
        )
        ⋅
        α
        (
        s
        )
        ⋅
        (
        D
        (
        t
        )
        −
        
          W
          
            v
          
        
        (
        s
        )
        )
      
    
    {\displaystyle W_{v}(s+1)=W_{v}(s)+\theta (u,v,s)\cdot \alpha (s)\cdot (D(t)-W_{v}(s))}
  
Increase 
  
    
      
        s
      
    
    {\displaystyle s}
   and repeat from step 2 while 
  
    
      
        s
        <
        λ
      
    
    {\displaystyle s<\lambda }

SOM Initialization
Selection of a good initial approximation is a well-known problem for all iterative methods of learning neural networks. Kohonen used random initiation of SOM weights. Recently, principal component initialization, in which initial map weights are chosen from the space of the first principal components, has become popular due to the exact reproducibility of the results.Careful comparison of the random initiation approach to principal component initialization for one-dimensional SOM (models of principal curves) demonstrated that the advantages of principal component SOM initialization are not universal. The best initialization method depends on the geometry of the specific dataset. Principal component initialization is preferable (in dimension one) if the principal curve approximating the dataset can be univalently and linearly projected on the first principal component (quasilinear sets). For nonlinear datasets, however, random initiation performs better.

Examples
Fisher's Iris Flower Data
Consider an n×m array of nodes, each of which contains a weight vector and is aware of its location in the array. Each weight vector is of the same dimension as the node's input vector. The weights may initially be set to random values.
Now we need input to feed the map.  Colors can be represented by their red, green, and blue components. Consequently, we will represent colors as vectors in the unit cube of the free vector space over ℝ generated by the basis:

R = <255, 0, 0>
G = <0, 255, 0>
B = <0, 0, 255>
The diagram shown  compares the results of training on the data setsthreeColors = [255, 0, 0], [0, 255, 0], [0, 0, 255]
eightColors = [0, 0, 0], [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0], [0, 255, 255], [255, 0, 255], [255, 255, 255]and the original images.  Note the striking resemblance between the two.

Similarly, after training a 40×40 grid of neurons for 250 iterations with a learning rate of 0.1 on Fisher's Iris, the map can already detect the main differences between species.

Interpretation
There are two ways to interpret a SOM. Because in the training phase weights of the whole neighborhood are moved in the same direction, similar items tend to excite adjacent neurons. Therefore, SOM forms a semantic map where similar samples are mapped close together and dissimilar ones apart. This may be visualized by a U-Matrix (Euclidean distance between weight vectors of neighboring cells) of the SOM.The other way is to think of neuronal weights as pointers to the input space. They form a discrete approximation of the distribution of training samples. More neurons point to regions with high training sample concentration and fewer where the samples are scarce.
SOM may be considered a nonlinear generalization of Principal components analysis (PCA). It has been shown, using both artificial and real geophysical data, that SOM has many advantages over the conventional feature extraction methods such as Empirical Orthogonal Functions (EOF) or PCA.
Originally, SOM was not formulated as a solution to an optimisation problem. Nevertheless, there have been several attempts to modify the definition of SOM and to formulate an optimisation problem which gives similar results. For example, Elastic maps use the mechanical metaphor of elasticity to approximate principal manifolds: the analogy is an elastic membrane and plate.

Alternatives
The generative topographic map (GTM) is a potential alternative to SOMs. In the sense that a GTM explicitly requires a smooth and continuous mapping from the input space to the map space, it is topology preserving. However, in a practical sense, this measure of topological preservation is lacking.
The time adaptive self-organizing map (TASOM) network is an extension of the basic SOM. The TASOM employs adaptive learning rates and neighborhood functions. It also includes a scaling parameter to make the network invariant to scaling, translation and rotation of the input space. The TASOM and its variants have been used in several applications including adaptive clustering, multilevel thresholding, input space approximation, and active contour modeling. Moreover, a Binary Tree TASOM or BTASOM, resembling a binary natural tree having nodes composed of TASOM networks has been proposed where the number of its levels and the number of its nodes are adaptive with its environment.
The growing self-organizing map (GSOM) is a growing variant of the self-organizing map. The GSOM was developed to address the issue of identifying a suitable map size in the SOM. It starts with a minimal number of nodes (usually four) and grows new nodes on the boundary based on a heuristic. By using a value called the spread factor, the data analyst has the ability to control the growth of the GSOM.
The elastic maps approach borrows from the spline interpolation the idea of minimization of the elastic energy. In learning, it minimizes the sum of quadratic bending and stretching energy with the least squares approximation error.
The conformal approach  that uses conformal mapping to interpolate each training sample between grid nodes in a continuous surface. A one-to-one smooth mapping is possible in this approach.
The oriented and scalable map (OS-Map) generalises the neighborhood function and the winner selection. The homogeneous Gaussian neighborhood function is replaced with the matrix exponential. Thus one can specify the orientation either in the map space or in the data space. SOM has a fixed scale (=1), so that the maps ""optimally describe the domain of observation"". But what about a map covering the domain twice or in n-folds? This entails the conception of scaling. The OS-Map regards the scale as a statistical description of how many best-matching nodes an input has in the map.

Applications
Project prioritization and selection 
Seismic facies analysis for oil and gas exploration 
Failure mode and effects analysis 
Creation of artwork

See also
Neural gas
Learning Vector Quantization
Liquid state machine
Hybrid Kohonen SOM
Sparse coding
Sparse distributed memory
Deep learning
Neocognitron
Topological data analysis

Notes


== References ==",https://en.wikipedia.org/wiki/Self-organizing_map,"['All articles needing additional references', 'All articles that may contain original research', 'All pages needing cleanup', 'Articles needing additional references from February 2010', 'Articles needing cleanup from June 2011', 'Articles that may contain original research from June 2017', 'Articles with short description', 'Artificial neural networks', 'CS1: long volume value', 'Cleanup tagged articles without a reason field from June 2011', 'Cluster analysis algorithms', 'Commons category link from Wikidata', 'Dimension reduction', 'Finnish inventions', 'Short description matches Wikidata', 'Unsupervised learning', 'Wikipedia pages needing cleanup from June 2011']",Data Science
162,Space telescope,"A space telescope or space observatory is a telescope located in outer space to observe distant planets, galaxies and other astronomical objects. Space telescopes avoid the filtering of ultraviolet frequencies, X-rays and gamma rays; the distortion (scintillation) of electromagnetic radiation; as well as light pollution which ground-based observatories encounter.Suggested by Lyman Spitzer in 1946, the first operational space telescopes were the American Orbiting Astronomical Observatory, OAO-2 launched in 1968, and the Soviet Orion 1 ultraviolet telescope aboard space station Salyut 1 in 1971.
Space telescopes are distinct from Earth imaging satellites, that point toward Earth for satellite imaging, applied for espionage, weather analysis and other types of information gathering. 
Space observatories are divided into two types: Astronomical survey satellites to map the entire sky, and satellites which focus on selected astronomical objects or parts of the sky and beyond.

History
Wilhelm Beer and Johann Heinrich Mädler in 1837 discussed the advantages of an observatory on the Moon. In 1946, American theoretical astrophysicist Lyman Spitzer proposed a telescope in space. Spitzer's proposal called for a large telescope that would not be hindered by Earth's atmosphere. After lobbying in the 1960s and 70s for such a system to be built, Spitzer's vision ultimately materialized into the Hubble Space Telescope, which was launched on April 24, 1990 by the Space Shuttle Discovery (STS-31).The first operational space telescopes were the American Orbiting Astronomical Observatory, OAO-2 launched in 1968, and the Soviet Orion 1 ultraviolet telescope aboard space station Salyut 1 in 1971.

Advantages
Performing astronomy from ground-based observatories on Earth is limited by the filtering and distortion of electromagnetic radiation (scintillation or twinkling) due to the atmosphere. A telescope orbiting Earth outside the atmosphere is subject neither to twinkling nor to light pollution from artificial light sources on Earth. As a result, the angular resolution of space telescopes is often much higher than a ground-based telescope with a similar aperture. Many larger terrestrial telescopes, however, reduce atmospheric effects with adaptive optics.

Space-based astronomy is more important for frequency ranges which are outside the optical window and the radio window, the only two wavelength ranges of the electromagnetic spectrum that are not severely attenuated by the atmosphere. For example, X-ray astronomy is nearly impossible when done from Earth, and has reached its current importance in astronomy only due to orbiting X-ray telescopes such as the Chandra observatory and the XMM-Newton observatory. Infrared and ultraviolet are also largely blocked.

Disadvantages
Space telescopes are much more expensive to build than ground-based telescopes.  Due to their location, space telescopes are also extremely difficult to maintain.  The Hubble Space Telescope was serviced by the Space Shuttle, but most space telescopes cannot be serviced at all.

Future of space observatories
Satellites have been launched and operated by NASA, ISRO, ESA, CNSA, JAXA and the Soviet space program later succeeded by Roscosmos of Russia.  As of 2018, many space observatories have already completed their missions, while others continue operating on extended time.  However, the future availability of space telescopes and observatories depends on timely and sufficient funding.  While future space observatories are planned by NASA, JAXA and the CNSA, scientists fear that there would be gaps in coverage that would not be covered immediately by future projects and this would affect research in fundamental science.

List of space telescopes
See also
Airborne observatory
Earth observation satellite
List of telescope types
Observatory
Timeline of artificial satellites and space probes
Timeline of telescopes, observatories, and observing technology
Ultraviolet astronomy
X-ray astronomy satellite

Further reading
Neil English: Space Telescopes - Capturing the Rays of the Electromagnetic Spectrum. Springer, Cham 2017, ISBN 978-3-319-27812-4.


== References ==",https://en.wikipedia.org/wiki/Space_telescope,"['All articles to be expanded', 'American inventions', 'Articles to be expanded from September 2017', 'Articles with short description', 'Astronomical observatories', 'Commons category link is on Wikidata', 'Short description matches Wikidata', 'Space telescopes', 'Telescope types', 'Uncrewed spacecraft', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LNB identifiers', 'Wikipedia articles with MA identifiers']",Data Science
163,Spiking neural network,"Spiking neural networks (SNNs) are artificial neural networks that more closely mimic natural neural networks. In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather transmit information only when a membrane potential – an intrinsic quality of the neuron related to its membrane electrical charge – reaches a specific value, called the threshold. When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model  that fires at the moment of threshold crossing is also called a spiking neuron model.The most prominent spiking neuron model is the leaky integrate-and-fire model. In the integrate-and-fire model, the momentary activation level (modeled as a differential equation) is normally considered to be the neuron's state, with incoming spikes pushing this value higher or lower, until the state eventually either decays or  - if the firing threshold is reached - the neuron fires. After firing the state variable is reset to a lower value.
Various decoding methods exist for interpreting the outgoing spike train as a real-value number, relying on either the frequency of spikes (rate-code), the time-to-first-spike after stimulation, or the interval between spikes.

History
Multi-layer artificial neural networks are usually fully connected, receiving input from every neuron in the previous layer and signalling every neuron in the subsequent layer. Although these networks have achieved breakthroughs in many fields, they are biologically inaccurate and do not mimic the operation mechanism of neurons in the brain of a living thing.
The biologically-inspired Hodgkin–Huxley model of a spiking neuron was proposed  in 1952. This model describes how action potentials are initiated and propagated. Communication between neurons, which requires the exchange of chemical neurotransmitters in the synaptic gap, is described in various models, such as the integrate-and-fire model, FitzHugh–Nagumo model (1961–1962), and Hindmarsh–Rose model (1984). The leaky integrate-and-fire model (or a derivative) is commonly used as it is easier to compute than the Hodgkin–Huxley model.In July 2019 at the DARPA Electronics Resurgence Initiative summit, Intel unveiled an 8-million-neuron neuromorphic system comprising 64 Loihi research chips.

Underpinnings
From the information theory perspective, the problem is to explain how information is encoded and decoded by a series of trains of pulses, i.e. action potentials. Thus, a fundamental question of neuroscience is to determine whether neurons communicate by a rate or temporal code. Temporal coding suggests that a single spiking neuron can replace hundreds of hidden units on a sigmoidal neural net.A spiking neural network considers temporal information. The idea is that not all neurons are activated in every iteration of propagation (as is the case in a typical multilayer perceptron network), but only when its membrane potential reaches a certain value. When a neuron is activated, it produces a signal that is passed to connected neurons, raising or lowering their membrane potential.
In a spiking neural network, the neuron's current state is defined as its level of activation (modeled as a differential equation). An input pulse causes the current state value to rise for a period of time and then gradually decline. Encoding schemes have been constructed to interpret these output pulse sequences as a number, taking into account both pulse frequency and pulse interval. A neural network model based on pulse generation time can be established accurately. Spike coding is adopted in this new neural network. Using the exact time of pulse occurrence, a neural network can employ more information and offer stronger computing power.
Pulse-coupled neural networks (PCNN) are often confused with SNNs. A PCNN can be seen as a kind of SNN.
The SNN approach uses a binary output (signal/no signal) instead of the continuous output of traditional ANNs. Further, pulse trainings are not easily interpretable. But pulse training increases the ability to process spatiotemporal data (or real-world sensory data). Space refers to the fact that neurons connect only to nearby neurons so that they can process input blocks separately (similar to CNN using filters). Time refers to the fact that pulse training occurs over time so that the information lost in binary coding can be retrieved from the time information. This avoids the additional complexity of a recurrent neural network (RNN). It turns out that impulse neurons are more powerful computational units than traditional artificial neurons. SNN is theoretically more powerful than second-generation networks, however SNN training issues and hardware requirements limit their use. Although unsupervised biological learning methods are available, such as Hebbian learning and STDP, no effective supervised training method is suitable for SNN that can provide better performance than second-generation networks. Spike based activation of SNNs is not differentiable thus making it hard to develop gradient descent based training methods to perform error backpropagation, though a few recent algorithms such as NormAD and multilayer NormAD have demonstrated good training performance through suitable approximation of the gradient of spike based activation.
SNNs have much larger computational costs for simulating realistic neural models than traditional ANNs.

Applications
SNNs can in principle apply to the same applications as traditional ANNs. In addition, SNNs can model the central nervous system of biological organisms, such as an insect seeking food without prior knowledge of the environment. Due to their relative realism, they can be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function, recordings of this circuit can be compared to the output of the corresponding SNN, evaluating the plausibility of the hypothesis. However, there is a lack of effective training mechanisms for SNNs, which can be inhibitory for some applications, including computer vision tasks.
As of 2019 SNNs lag ANNs in terms of accuracy, but the gap is decreasing, and has vanished on some tasks.

Software
A diverse range of application software can simulate SNNs. This software can be classified according to its uses:

SNN simulation
These simulate complex neural models with a high level of detail and accuracy. Large networks usually require lengthy processing. Candidates include:GENESIS (the GEneral NEural SImulation System) – developed in James Bower's laboratory at Caltech;
NEURON – mainly developed by Michael Hines, John W. Moore and Ted Carnevale in Yale University and Duke University;
Brian – developed by Romain Brette and Dan Goodman at the École Normale Supérieure;
NEST – developed by the NEST Initiative;
BindsNET – developed by the Biologically Inspired Neural and Dynamical Systems (BINDS) lab at the University of Massachusetts Amherst.
SpykeTorch - a framework based on PyTorch optimized specifically for convolutional SNNs with at most one spike per neuron. Runs on GPUs.

Hardware
Future neuromorphic architectures will comprise billions of such nanosynapses, which require a clear understanding of the physical mechanisms responsible for plasticity. Experimental systems based on ferroelectric tunnel junctions have been used to show that STDP can be harnessed from heterogeneous polarization switching. Through combined scanning probe imaging, electrical transport and atomic-scale molecular dynamics, conductance variations can be modelled by nucleation-dominated reversal of domains. Simulations show that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way, opening the path towards unsupervised learning.

Brainchip's Akida NSoC claims to have effectively 1.2 million neurons and 10 billion synapses
Neurogrid is a board that can simulate spiking neural networks directly in hardware. (Stanford University)
SpiNNaker (Spiking Neural Network Architecture) uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model. (University of Manchester) The SpiNNaker system is based on numerical models running in real time on custom digital multicore chips using the ARM architecture. It provides custom digital chips, each with eighteen cores and a shared local 128 Mbyte RAM, with a total of over 1,000,000 cores. A single chip can simulate 16,000 neurons with eight million plastic synapses running in real time.
TrueNorth is a processor that contains 5.4 billion transistors that consumes only 70 milliwatts; most processors in personal computers contain about 1.4 billion transistors and require 35 watts or more. IBM refers to the design principle behind TrueNorth as neuromorphic computing. Its primary purpose is pattern recognition. While critics say the chip isn't powerful enough, its supporters point out that this is only the first generation, and the capabilities of improved iterations will become clear. (IBM)
Dynamic Neuromorphic Asynchronous Processor (DYNAP) combines slow, low-power, inhomogeneous sub-threshold analog circuits, and fast programmable digital circuits. It supports reconfigurable, general-purpose, real-time neural networks of spiking neurons. This allows the implementation of real-time spike-based neural processing architectures in which memory and computation are co-localized. It solves the von Neumann bottleneck problem and enables real-time multiplexed communication of spiking events for realising massive networks. Recurrent networks, feed-forward networks, convolutional networks, attractor networks, echo-state networks, deep networks, and sensor fusion networks are a few of the possibilities.
Loihi is a 14-nm Intel chip that offers 128 cores and 130,000 neurons on a 60-mm package. It integrates a wide range of features, such as hierarchical connectivity, dendritic compartments, synaptic delays and programmable synaptic learning rules. Running a spiking convolutional form of the Locally Competitive Algorithm, Loihi can solve LASSO optimization problems with over three orders of magnitude superior energy-delay product compared to conventional solvers running on a CPU isoprocess/voltage/area. A 64 Loihi research system offers 8-million-neuron neuromorphic system. Loihi is about 1,000 times as fast as a CPU and 10,000 times as energy efficient.
BrainScaleS is based on physical emulations of neuron, synapse and plasticity models with digital connectivity, running up to ten thousand times faster than real time. It was developed by the European Human Brain Project. The BrainScaleS system contains 20 8-inch silicon wafers in 180 nm process technology. Each wafer incorporates 50 x 106 plastic synapses and 200,000 biologically realistic neurons. The system does not execute pre-programmed code but evolves according to the physical properties of the electronic devices, running at up to 10 thousand times faster than real time.

Benchmarks
Classification capabilities of spiking networks trained according to unsupervised learning methods have been tested on the common benchmark datasets, such as, Iris, Wisconsin Breast Cancer or Statlog Landsat dataset. Various approaches to information encoding and network design have been used. For example, a 2-layer feedforward network for data clustering and classification. Based on the idea proposed in Hopfield (1995) the authors implemented models of local receptive fields combining the properties of radial basis functions (RBF) and spiking neurons to convert input signals (classified data) having a floating-point representation into a spiking representation.

See also
CoDi
Cognitive architecture
Cognitive map
Cognitive computer
Computational neuroscience
Neural coding
Neural correlate
Neural decoding
Neuroethology
Neuroinformatics
Models of neural computation
Motion perception
Systems neuroscience

References
External links
Full text of the book Spiking Neuron Models. Single Neurons, Populations, Plasticity by Wulfram Gerstner and Werner M. Kistler (ISBN 0-521-89079-9)",https://en.wikipedia.org/wiki/Spiking_neural_network,"['All articles needing additional references', 'All articles with unsourced statements', 'Articles containing video clips', 'Articles needing additional references from December 2018', 'Articles to be expanded from October 2020', 'Articles with multiple maintenance issues', 'Articles with unsourced statements from May 2019', 'Artificial neural networks', 'CS1 errors: missing periodical', 'Computational neuroscience', 'Computational statistics']",Data Science
164,State–action–reward–state–action,"State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was proposed by Rummery and Niranjan in a technical note with the name ""Modified Connectionist Q-Learning"" (MCQ-L). The alternative name SARSA, proposed by Rich Sutton, was only mentioned as a footnote.
This name simply reflects the fact that the main function for updating the Q-value depends on the current state of the agent ""S1"", the action the agent chooses ""A1"", the reward ""R"" the agent gets for choosing this action, the state ""S2"" that the agent enters after taking that action, and finally the next action ""A2"" the agent chooses in its new state. The acronym for the quintuple (st, at, rt, st+1, at+1) is SARSA. Some authors use a slightly different convention and write the quintuple (st, at, rt+1, st+1, at+1), depending to which time step the reward is formally assigned. The rest of the article uses the former convention.

Algorithm
Q
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
        ←
        Q
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
        +
        α
        
        [
        
          r
          
            t
          
        
        +
        γ
        
        Q
        (
        
          s
          
            t
            +
            1
          
        
        ,
        
          a
          
            t
            +
            1
          
        
        )
        −
        Q
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
        ]
      
    
    {\displaystyle Q(s_{t},a_{t})\leftarrow Q(s_{t},a_{t})+\alpha \,[r_{t}+\gamma \,Q(s_{t+1},a_{t+1})-Q(s_{t},a_{t})]}
  A SARSA agent interacts with the environment and updates the policy based on actions taken, hence this is known as an on-policy learning algorithm. The Q value for a state-action is updated by an error, adjusted by the learning rate alpha. Q values represent the possible reward received in the next time step for taking action a in state s, plus the discounted future reward received from the next state-action observation.
Watkin's Q-learning updates an estimate of the optimal state-action value function 
  
    
      
        
          Q
          
            ∗
          
        
      
    
    {\displaystyle Q^{*}}
   based on the maximum reward of available actions. While SARSA learns the Q values associated with taking the policy it follows itself, Watkin's Q-learning learns the Q values associated with taking the optimal policy while following an exploration/exploitation policy.
Some optimizations of Watkin's Q-learning may be applied to SARSA.

Hyperparameters
Learning rate (alpha)
The learning rate determines to what extent newly acquired information overrides old information. A factor of 0 will make the agent not learn anything, while a factor of 1 would make the agent consider only the most recent information.

Discount factor (gamma)
The discount factor determines the importance of future rewards. A factor of 0 makes the agent ""opportunistic"" by only considering current rewards, while a factor approaching 1 will make it strive for a long-term high reward. If the discount factor meets or exceeds 1, the 
  
    
      
        Q
      
    
    {\displaystyle Q}
   values may diverge.

Initial conditions (Q(s0, a0))
Since SARSA is an iterative algorithm, it implicitly assumes an initial condition before the first update occurs. A low (infinite) initial value, also known as ""optimistic initial conditions"", can encourage exploration: no matter what action takes place, the update rule causes it to have higher values than the other alternative, thus increasing their choice probability. In 2013 it was suggested that the first reward r could be used to reset the initial conditions. According to this idea, the first time an action is taken the reward is used to set the value of Q. This allows immediate learning in case of fixed deterministic rewards. This resetting-of-initial-conditions (RIC) approach seems to be consistent with human behavior in repeated binary choice experiments.


== References ==",https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action,['Machine learning algorithms'],Data Science
165,Statistical classification,"In statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.  Examples are assigning a given email to the ""spam"" or ""non-spam"" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).  Classification is an example of pattern recognition.
In the terminology of machine learning, classification is considered an instance of supervised learning, i.e., learning where a training set of correctly identified observations is available.  The corresponding unsupervised procedure is known as clustering, and involves grouping data into categories based on some measure of inherent similarity or distance.
Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features.  These properties may variously be categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type), ordinal (e.g. ""large"", ""medium"" or ""small""), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.
An algorithm that implements classification, especially in a concrete implementation, is known as a classifier.  The term ""classifier"" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.
Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable.  In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes.  Other fields may use different terminology: e.g. in community ecology, the term ""classification"" normally refers to cluster analysis, i.e., a type of unsupervised learning, rather than the supervised learning described in this article.

Relation to other problems
Classification and clustering are examples of the more general problem of pattern recognition, which is the assignment of some sort of output value to a given input value.  Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence; etc.
A common subclass of classification is probabilistic classification.  Algorithms of this nature use statistical inference to find the best class for a given instance.  Unlike other algorithms, which simply output a ""best"" class, probabilistic algorithms output a probability of the instance being a member of each of the possible classes.  The best class is normally then selected as the one with the highest probability.  However, such an algorithm has numerous advantages over non-probabilistic classifiers:

It can output a confidence value associated with its choice (in general, a classifier that can do this is known as a confidence-weighted classifier).
Correspondingly, it can abstain when its confidence of choosing any particular output is too low.
Because of the probabilities which are generated, probabilistic classifiers can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of error propagation.

Frequentist procedures
Early work on statistical classification was undertaken by Fisher, in the context of two-group problems, leading to Fisher's linear discriminant function as the rule for assigning a group to a new observation. This early work assumed that data-values within each of the two groups had a multivariate normal distribution. The extension of this same context to more than two-groups has also been considered with a restriction imposed that the classification rule should be linear. Later work for the multivariate normal distribution allowed the classifier to be nonlinear: several classification rules can be derived based on different adjustments of the Mahalanobis distance, with a new observation being assigned to the group whose centre has the lowest adjusted distance from the observation.

Bayesian procedures
Unlike frequentist procedures, Bayesian classification procedures provide a natural way of taking into account any available information about the relative sizes of the different groups within the overall population. Bayesian procedures tend to be computationally expensive and, in the days before Markov chain Monte Carlo computations were developed, approximations for Bayesian clustering rules were devised.Some Bayesian procedures involve the calculation of  group membership probabilities: these provide a more informative outcome than a simple attribution of a single group-label to each new observation.

Binary and multiclass classification
Classification can be thought of as two separate problems – binary classification and multiclass classification. In binary classification, a better understood task, only two classes are involved, whereas multiclass classification involves assigning an object to one of several classes. Since many classification methods have been developed specifically for binary classification, multiclass classification often requires the combined use of multiple binary classifiers.

Feature vectors
Most algorithms describe an individual instance whose category is to be predicted using a feature vector of individual, measurable properties of the instance.  Each property is termed a feature, also known in statistics as an explanatory variable (or independent variable, although features may or may not be statistically independent).  Features may variously be binary (e.g. ""on"" or ""off""); categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type); ordinal (e.g. ""large"", ""medium"" or ""small""); integer-valued (e.g. the number of occurrences of a particular word in an email); or real-valued (e.g. a measurement of blood pressure).  If the instance is an image, the feature values might correspond to the pixels of an image; if the instance is a piece of text, the feature values might be occurrence frequencies of different words.  Some algorithms work only in terms of discrete data and require that real-valued or integer-valued data be discretized into groups (e.g. less than 5, between 5 and 10, or greater than 10).

Linear classifiers
A large number of algorithms for classification can be phrased in terms of a linear function that assigns a score to each possible category k by combining the feature vector of an instance with a vector of weights, using a dot product.  The predicted category is the one with the highest score.  This type of score function is known as a linear predictor function and has the following general form:

  
    
      
        score
        ⁡
        (
        
          
            X
          
          
            i
          
        
        ,
        k
        )
        =
        
          
            β
          
          
            k
          
        
        ⋅
        
          
            X
          
          
            i
          
        
        ,
      
    
    {\displaystyle \operatorname {score} (\mathbf {X} _{i},k)={\boldsymbol {\beta }}_{k}\cdot \mathbf {X} _{i},}
  where Xi is the feature vector for instance i, βk is the vector of weights corresponding to category k, and score(Xi, k) is the score associated with assigning instance i to category k.  In discrete choice theory, where instances represent people and categories represent choices, the score is considered the utility associated with person i choosing category k.
Algorithms with this basic setup are known as linear classifiers.  What distinguishes them is the procedure for determining (training) the optimal weights/coefficients and the way that the score is interpreted.
Examples of such algorithms are

Logistic regression and Multinomial logistic regression
Probit regression
The perceptron algorithm
Support vector machines
Linear discriminant analysis.

Algorithms
In unsupervised learning, classifiers form the backbone of cluster analysis and in supervised or semi-supervised learning, classifiers are how the system characterizes and evaluates unlabeled data. In all cases though, classifiers have a specific set of dynamic rules, which includes an interpretation procedure to handle vague or unknown values, all tailored to the type of inputs being examined.Since no single form of classification is appropriate for all data sets, a large toolkit of classification algorithms have been developed. The most commonly used include:
Linear classifiers
Fisher's linear discriminant
Logistic regression
Naive Bayes classifier
Perceptron
Support vector machines
Least squares support vector machines
Quadratic classifiers
Kernel estimation
k-nearest neighbor
Boosting (meta-algorithm)
Decision trees
Random forests
Neural networks
Learning vector quantization

Evaluation
Classifier performance depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems (a phenomenon that may be explained by the no-free-lunch theorem). Various empirical tests have been performed to compare classifier performance and to find the characteristics of data that determine classifier performance. Determining a suitable classifier for a given problem is however still more an art than a science.
The measures precision and recall are popular metrics used to evaluate the quality of a classification system. More recently, receiver operating characteristic (ROC) curves have been used to evaluate the tradeoff between true- and false-positive rates of classification algorithms.
As a performance metric, the uncertainty coefficient has the advantage over simple accuracy in that it is not affected by the relative sizes of the different classes.

Further, it will not penalize an algorithm for simply rearranging the classes.

Application domains
Classification has many applications. In some of these it is employed as a data mining procedure, while in others more detailed statistical modeling is undertaken.

Computer vision
Medical imaging and medical image analysis
Optical character recognition
Video tracking
Drug discovery and development
Toxicogenomics
Quantitative structure-activity relationship
Geostatistics
Speech recognition
Handwriting recognition
Biometric identification
Biological classification
Statistical natural language processing
Document classification
Internet search engines
Credit scoring
Pattern recognition
Recommender system
Micro-array classification

See also


== References ==",https://en.wikipedia.org/wiki/Statistical_classification,"['All articles lacking in-text citations', 'Articles lacking in-text citations from January 2010', 'Articles with short description', 'Classification algorithms', 'Commons category link from Wikidata', 'Machine learning', 'Short description matches Wikidata', 'Statistical classification']",Data Science
166,Statistical learning theory,"Statistical learning theory is a framework for machine learning
drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics.

Introduction
The goals of learning are understanding and prediction. Learning falls into many categories, including supervised learning, unsupervised learning, online learning, and reinforcement learning. From the perspective of statistical learning theory, supervised learning is best understood. Supervised learning involves learning from a training set of data. Every point in the training is an input-output pair, where the input maps to an output. The learning problem consists of inferring the function that maps between the input and the output, such that the learned function can be used to predict the output from future input.
Depending on the type of output, supervised learning problems are either problems of regression or problems of classification. If the output takes a continuous range of values, it is a regression problem. Using Ohm's Law as an example, a regression could be performed with voltage as input and current as an output. The regression would find the functional relationship between voltage and current to be 
  
    
      
        R
      
    
    {\displaystyle R}
  , such that

  
    
      
        V
        =
        I
        R
      
    
    {\displaystyle V=IR}
  Classification problems are those for which the output will be an element from a discrete set of labels. Classification is very common for machine learning applications. In facial recognition, for instance, a picture of a person's face would be the input, and the output label would be that person's name. The input would be represented by a large multidimensional vector whose elements represent pixels in the picture.
After learning a function based on the training set data, that function is validated on a test set of data, data that did not appear in the training set.

Formal description
Take 
  
    
      
        X
      
    
    {\displaystyle X}
   to be the vector space of all possible inputs, and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   to be
the vector space of all possible outputs. Statistical learning theory takes the perspective that there is some unknown probability distribution over the product space 
  
    
      
        Z
        =
        X
        ×
        Y
      
    
    {\displaystyle Z=X\times Y}
  , i.e. there exists some unknown 
  
    
      
        p
        (
        z
        )
        =
        p
        (
        
          
            
              x
              →
            
          
        
        ,
        y
        )
      
    
    {\displaystyle p(z)=p({\vec {x}},y)}
  . The training set is made up of 
  
    
      
        n
      
    
    {\displaystyle n}
   samples from this probability distribution, and is notated 

  
    
      
        S
        =
        {
        (
        
          
            
              
                x
                →
              
            
          
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        ,
        …
        ,
        (
        
          
            
              
                x
                →
              
            
          
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
        }
        =
        {
        
          
            
              
                z
                →
              
            
          
          
            1
          
        
        ,
        …
        ,
        
          
            
              
                z
                →
              
            
          
          
            n
          
        
        }
      
    
    {\displaystyle S=\{({\vec {x}}_{1},y_{1}),\dots ,({\vec {x}}_{n},y_{n})\}=\{{\vec {z}}_{1},\dots ,{\vec {z}}_{n}\}}
  Every 
  
    
      
        
          
            
              
                x
                →
              
            
          
          
            i
          
        
      
    
    {\displaystyle {\vec {x}}_{i}}
   is an input vector from the training data, and 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
  
is the output that corresponds to it.
In this formalism, the inference problem consists of finding a function 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
   such that 
  
    
      
        f
        (
        
          
            
              x
              →
            
          
        
        )
        ∼
        y
      
    
    {\displaystyle f({\vec {x}})\sim y}
  . Let 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
   be a space of functions 
  
    
      
        f
        :
        X
        →
        Y
      
    
    {\displaystyle f:X\to Y}
   called the hypothesis space. The hypothesis space is the space of functions the algorithm will search through. Let 
  
    
      
        V
        (
        f
        (
        
          
            
              x
              →
            
          
        
        )
        ,
        y
        )
      
    
    {\displaystyle V(f({\vec {x}}),y)}
   be the loss function, a metric for the difference between the predicted value 
  
    
      
        f
        (
        
          
            
              x
              →
            
          
        
        )
      
    
    {\displaystyle f({\vec {x}})}
   and the actual value 
  
    
      
        y
      
    
    {\displaystyle y}
  . The expected risk is defined to be

  
    
      
        I
        [
        f
        ]
        =
        
          
            ∫
            
              X
              ×
              Y
            
          
          V
          (
          f
          (
          
            
              
                x
                →
              
            
          
          )
          ,
          y
          )
          
          p
          (
          
            
              
                x
                →
              
            
          
          ,
          y
          )
          
          d
          
            
              
                x
                →
              
            
          
          
          d
          y
        
      
    
    {\displaystyle I[f]=\displaystyle \int _{X\times Y}V(f({\vec {x}}),y)\,p({\vec {x}},y)\,d{\vec {x}}\,dy}
  The target function, the best possible function 
  
    
      
        f
      
    
    {\displaystyle f}
   that can be
chosen, is given by the 
  
    
      
        f
      
    
    {\displaystyle f}
   that satisfies

  
    
      
        f
        =
        
          inf
          
            h
            ∈
            
              
                H
              
            
          
        
        I
        [
        h
        ]
      
    
    {\displaystyle f=\inf _{h\in {\mathcal {H}}}I[h]}
  Because the probability distribution 
  
    
      
        p
        (
        
          
            
              x
              →
            
          
        
        ,
        y
        )
      
    
    {\displaystyle p({\vec {x}},y)}
   is unknown, a
proxy measure for the expected risk must be used. This measure is based on the training set, a sample from this unknown probability distribution. It is called the empirical risk

  
    
      
        
          I
          
            S
          
        
        [
        f
        ]
        =
        
          
            1
            n
          
        
        
          
            ∑
            
              i
              =
              1
            
            
              n
            
          
          V
          (
          f
          (
          
            
              
                
                  x
                  →
                
              
            
            
              i
            
          
          )
          ,
          
            y
            
              i
            
          
          )
        
      
    
    {\displaystyle I_{S}[f]={\frac {1}{n}}\displaystyle \sum _{i=1}^{n}V(f({\vec {x}}_{i}),y_{i})}
  A learning algorithm that chooses the function 
  
    
      
        
          f
          
            S
          
        
      
    
    {\displaystyle f_{S}}
   that minimizes
the empirical risk is called empirical risk minimization.

Loss functions
The choice of loss function is a determining factor on the function 
  
    
      
        
          f
          
            S
          
        
      
    
    {\displaystyle f_{S}}
   that will be chosen by the learning algorithm. The loss function
also affects the convergence rate for an algorithm. It is important for the loss function to be convex.Different loss functions are used depending on whether the problem is
one of regression or one of classification.

Regression
The most common loss function for regression is the square loss function (also known as the L2-norm). This familiar loss function is used in Ordinary Least Squares regression. The form is:

  
    
      
        V
        (
        f
        (
        
          
            
              x
              →
            
          
        
        )
        ,
        y
        )
        =
        (
        y
        −
        f
        (
        
          
            
              x
              →
            
          
        
        )
        
          )
          
            2
          
        
      
    
    {\displaystyle V(f({\vec {x}}),y)=(y-f({\vec {x}}))^{2}}
  The absolute value loss (also known as the L1-norm) is also sometimes used:

  
    
      
        V
        (
        f
        (
        
          
            
              x
              →
            
          
        
        )
        ,
        y
        )
        =
        
          |
        
        y
        −
        f
        (
        
          
            
              x
              →
            
          
        
        )
        
          |
        
      
    
    {\displaystyle V(f({\vec {x}}),y)=|y-f({\vec {x}})|}

Classification
In some sense the 0-1 indicator function is the most natural loss function for classification. It takes the value 0 if the predicted output is the same as the actual output, and it takes the value 1 if the predicted output is different from the actual output. For binary classification with 
  
    
      
        Y
        =
        {
        −
        1
        ,
        1
        }
      
    
    {\displaystyle Y=\{-1,1\}}
  , this is:

  
    
      
        V
        (
        f
        (
        
          
            
              x
              →
            
          
        
        )
        ,
        y
        )
        =
        θ
        (
        −
        y
        f
        (
        
          
            
              x
              →
            
          
        
        )
        )
      
    
    {\displaystyle V(f({\vec {x}}),y)=\theta (-yf({\vec {x}}))}
  where 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   is the Heaviside step function.

Regularization
In machine learning problems, a major problem that arises is that of overfitting. Because learning is a prediction problem, the goal is not to find a function that most closely fits the (previously observed) data, but to find one that will most accurately predict output from future input. Empirical risk minimization runs this risk of overfitting: finding a function that matches the data exactly but does not predict future output well.
Overfitting is symptomatic of unstable solutions; a small perturbation in the training set data would cause a large variation in the learned function. It can be shown that if the stability for the solution can be guaranteed, generalization and consistency are guaranteed as well. Regularization can solve the overfitting problem and give
the problem stability.
Regularization can be accomplished by restricting the hypothesis space 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
  . A common example would be restricting 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
   to linear functions: this can be seen as a reduction to the standard problem of linear regression. 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
   could also be restricted to polynomial of degree 
  
    
      
        p
      
    
    {\displaystyle p}
  , exponentials, or bounded functions on L1. Restriction of the hypothesis space avoids overfitting because the form of the potential functions are limited, and so does not allow for the choice of a function that gives empirical risk arbitrarily close to zero.
One example of regularization is Tikhonov regularization. This consists of minimizing

  
    
      
        
          
            1
            n
          
        
        
          
            ∑
            
              i
              =
              1
            
            
              n
            
          
          V
          (
          f
          (
          
            
              
                
                  x
                  →
                
              
            
            
              i
            
          
          )
          ,
          
            y
            
              i
            
          
          )
          +
          γ
          ‖
          f
          
            ‖
            
              
                H
              
            
            
              2
            
          
        
      
    
    {\displaystyle {\frac {1}{n}}\displaystyle \sum _{i=1}^{n}V(f({\vec {x}}_{i}),y_{i})+\gamma \|f\|_{\mathcal {H}}^{2}}
  where 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
   is a fixed and positive parameter, the regularization parameter. Tikhonov regularization ensures existence, uniqueness, and stability of the solution.

See also
Reproducing kernel Hilbert spaces are a useful choice for 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
  .
Proximal gradient methods for learning


== References ==",https://en.wikipedia.org/wiki/Statistical_learning_theory,"['Estimation theory', 'Machine learning']",Data Science
167,Supervised learning,"Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.The parallel task in human and animal psychology is often referred to as concept learning.

Steps
To solve a given problem of supervised learning, one has to perform the following steps:

Determine the type of training examples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, or an entire line of handwriting.
Gather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements.
Determine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output.
Determine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose to use support-vector machines or decision trees.
Complete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation.
Evaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set.

Algorithm choice
A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).
There are four major issues to consider in supervised learning:

Bias-variance tradeoff
A first issue is the tradeoff between bias and variance. Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input 
  
    
      
        x
      
    
    {\displaystyle x}
   if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for 
  
    
      
        x
      
    
    {\displaystyle x}
  . A learning algorithm has high variance for a particular input 
  
    
      
        x
      
    
    {\displaystyle x}
   if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be ""flexible"" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).

Function complexity and amount of training data
The second issue is the amount of training data available relative to the complexity of the ""true"" function (classifier or regression function). If the true function is simple, then an ""inflexible"" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn from a very large amount of training data and using a ""flexible"" learning algorithm with low bias and high variance. There is a clear demarcation between the input and the desired output.

Dimensionality of the input space
A third issue is the dimensionality of the input space. If the input feature vectors have very high dimension, the learning problem can be difficult even if the true function only depends on a small number of those features. This is because the many ""extra"" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, high input dimensional typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, this is likely to improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.

Noise in the output values
A fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled ""corrupts"" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.
In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.

Other factors to consider
Other factors to consider when choosing and applying a learning algorithm include the following:

Heterogeneity of the data. If the feature vectors include features of many different kinds (discrete, discrete ordered, counts, continuous values), some algorithms are easier to apply than others. Many algorithms, including support-vector machines, linear regression, logistic regression, neural networks, and nearest neighbor methods, require that the input features be numerical and scaled to similar ranges (e.g., to the [-1,1] interval). Methods that employ a distance function, such as nearest neighbor methods and support-vector machines with Gaussian kernels, are particularly sensitive to this. An advantage of decision trees is that they easily handle heterogeneous data.
Redundancy in the data. If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g., linear regression, logistic regression, and distance based methods) will perform poorly because of numerical instabilities. These problems can often be solved by imposing some form of regularization.
Presence of interactions and non-linearities. If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g., linear regression, logistic regression, support-vector machines, naive Bayes) and distance functions (e.g., nearest neighbor methods, support-vector machines with Gaussian kernels) generally perform well. However, if there are complex interactions among features, then algorithms such as decision trees and neural networks work better, because they are specifically designed to discover these interactions. Linear methods can also be applied, but the engineer must manually specify the interactions when using them.When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see cross validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.

Algorithms
The most widely used learning algorithms are: 

Support-vector machines
Linear regression
Logistic regression
Naive Bayes
Linear discriminant analysis
Decision trees
K-nearest neighbor algorithm
Neural networks (Multilayer perceptron)
Similarity learning

How supervised learning algorithms work
Given a set of 
  
    
      
        N
      
    
    {\displaystyle N}
   training examples of the form 
  
    
      
        {
        (
        
          x
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        ,
        .
        .
        .
        ,
        (
        
          x
          
            N
          
        
        ,
        
        
          y
          
            N
          
        
        )
        }
      
    
    {\displaystyle \{(x_{1},y_{1}),...,(x_{N},\;y_{N})\}}
   such that 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   is the feature vector of the 
  
    
      
        i
      
    
    {\displaystyle i}
  -th example and 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
   is its label (i.e., class), a learning algorithm seeks a function 
  
    
      
        g
        :
        X
        →
        Y
      
    
    {\displaystyle g:X\to Y}
  , where 
  
    
      
        X
      
    
    {\displaystyle X}
   is the input space and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   is the output space. The function 
  
    
      
        g
      
    
    {\displaystyle g}
   is an element of some space of possible functions 
  
    
      
        G
      
    
    {\displaystyle G}
  , usually called the hypothesis space. It is sometimes convenient to represent 
  
    
      
        g
      
    
    {\displaystyle g}
   using a scoring function 
  
    
      
        f
        :
        X
        ×
        Y
        →
        
          R
        
      
    
    {\displaystyle f:X\times Y\to \mathbb {R} }
   such that 
  
    
      
        g
      
    
    {\displaystyle g}
   is defined as returning the 
  
    
      
        y
      
    
    {\displaystyle y}
   value that gives the highest score: 
  
    
      
        g
        (
        x
        )
        =
        
          
            
              arg
              ⁡
              max
            
            y
          
        
        
        f
        (
        x
        ,
        y
        )
      
    
    {\displaystyle g(x)={\underset {y}{\arg \max }}\;f(x,y)}
  . Let 
  
    
      
        F
      
    
    {\displaystyle F}
   denote the space of scoring functions.
Although 
  
    
      
        G
      
    
    {\displaystyle G}
   and 
  
    
      
        F
      
    
    {\displaystyle F}
   can be any space of functions, many learning algorithms are probabilistic models where 
  
    
      
        g
      
    
    {\displaystyle g}
   takes the form of a conditional probability model 
  
    
      
        g
        (
        x
        )
        =
        P
        (
        y
        
          |
        
        x
        )
      
    
    {\displaystyle g(x)=P(y|x)}
  , or 
  
    
      
        f
      
    
    {\displaystyle f}
   takes the form of a joint probability model 
  
    
      
        f
        (
        x
        ,
        y
        )
        =
        P
        (
        x
        ,
        y
        )
      
    
    {\displaystyle f(x,y)=P(x,y)}
  . For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model.
There are two basic approaches to choosing 
  
    
      
        f
      
    
    {\displaystyle f}
   or 
  
    
      
        g
      
    
    {\displaystyle g}
  : empirical risk minimization and structural risk minimization. Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a penalty function that controls the bias/variance tradeoff.
In both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, 
  
    
      
        (
        
          x
          
            i
          
        
        ,
        
        
          y
          
            i
          
        
        )
      
    
    {\displaystyle (x_{i},\;y_{i})}
  . In order to measure how well a function fits the training data, a loss function 
  
    
      
        L
        :
        Y
        ×
        Y
        →
        
          
            R
          
          
            ≥
            0
          
        
      
    
    {\displaystyle L:Y\times Y\to \mathbb {R} ^{\geq 0}}
   is defined. For training example 
  
    
      
        (
        
          x
          
            i
          
        
        ,
        
        
          y
          
            i
          
        
        )
      
    
    {\displaystyle (x_{i},\;y_{i})}
  , the loss of predicting the value 
  
    
      
        
          
            
              y
              ^
            
          
        
      
    
    {\displaystyle {\hat {y}}}
   is 
  
    
      
        L
        (
        
          y
          
            i
          
        
        ,
        
          
            
              y
              ^
            
          
        
        )
      
    
    {\displaystyle L(y_{i},{\hat {y}})}
  .
The risk 
  
    
      
        R
        (
        g
        )
      
    
    {\displaystyle R(g)}
   of function 
  
    
      
        g
      
    
    {\displaystyle g}
   is defined as the expected loss of 
  
    
      
        g
      
    
    {\displaystyle g}
  . This can be estimated from the training data as

  
    
      
        
          R
          
            e
            m
            p
          
        
        (
        g
        )
        =
        
          
            1
            N
          
        
        
          ∑
          
            i
          
        
        L
        (
        
          y
          
            i
          
        
        ,
        g
        (
        
          x
          
            i
          
        
        )
        )
      
    
    {\displaystyle R_{emp}(g)={\frac {1}{N}}\sum _{i}L(y_{i},g(x_{i}))}
  .

Empirical risk minimization
In empirical risk minimization, the supervised learning algorithm seeks the function 
  
    
      
        g
      
    
    {\displaystyle g}
   that minimizes 
  
    
      
        R
        (
        g
        )
      
    
    {\displaystyle R(g)}
  . Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find 
  
    
      
        g
      
    
    {\displaystyle g}
  .
When 
  
    
      
        g
      
    
    {\displaystyle g}
   is a conditional probability distribution 
  
    
      
        P
        (
        y
        
          |
        
        x
        )
      
    
    {\displaystyle P(y|x)}
   and the loss function is the negative log likelihood: 
  
    
      
        L
        (
        y
        ,
        
          
            
              y
              ^
            
          
        
        )
        =
        −
        log
        ⁡
        P
        (
        y
        
          |
        
        x
        )
      
    
    {\displaystyle L(y,{\hat {y}})=-\log P(y|x)}
  , then empirical risk minimization is equivalent to maximum likelihood estimation.
When 
  
    
      
        G
      
    
    {\displaystyle G}
   contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called overfitting.

Structural risk minimization
Structural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization. The regularization penalty can be viewed as implementing a form of Occam's razor that prefers simpler functions over more complex ones.
A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function 
  
    
      
        g
      
    
    {\displaystyle g}
   is a linear function of the form

  
    
      
        g
        (
        x
        )
        =
        
          ∑
          
            j
            =
            1
          
          
            d
          
        
        
          β
          
            j
          
        
        
          x
          
            j
          
        
      
    
    {\displaystyle g(x)=\sum _{j=1}^{d}\beta _{j}x_{j}}
  .A popular regularization penalty is 
  
    
      
        
          ∑
          
            j
          
        
        
          β
          
            j
          
          
            2
          
        
      
    
    {\displaystyle \sum _{j}\beta _{j}^{2}}
  , which is the squared Euclidean norm of the weights, also known as the 
  
    
      
        
          L
          
            2
          
        
      
    
    {\displaystyle L_{2}}
   norm. Other norms include the 
  
    
      
        
          L
          
            1
          
        
      
    
    {\displaystyle L_{1}}
   norm, 
  
    
      
        
          ∑
          
            j
          
        
        
          |
        
        
          β
          
            j
          
        
        
          |
        
      
    
    {\displaystyle \sum _{j}|\beta _{j}|}
  , and the 
  
    
      
        
          L
          
            0
          
        
      
    
    {\displaystyle L_{0}}
   ""norm"", which is the number of non-zero 
  
    
      
        
          β
          
            j
          
        
      
    
    {\displaystyle \beta _{j}}
  s. The penalty will be denoted by 
  
    
      
        C
        (
        g
        )
      
    
    {\displaystyle C(g)}
  .
The supervised learning optimization problem is to find the function 
  
    
      
        g
      
    
    {\displaystyle g}
   that minimizes

  
    
      
        J
        (
        g
        )
        =
        
          R
          
            e
            m
            p
          
        
        (
        g
        )
        +
        λ
        C
        (
        g
        )
        .
      
    
    {\displaystyle J(g)=R_{emp}(g)+\lambda C(g).}
  The parameter 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   controls the bias-variance tradeoff. When 
  
    
      
        λ
        =
        0
      
    
    {\displaystyle \lambda =0}
  , this gives empirical risk minimization with low bias and high variance. When 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   is large, the learning algorithm will have high bias and low variance. The value of 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   can be chosen empirically via cross validation.
The complexity penalty has a Bayesian interpretation as the negative log prior probability of 
  
    
      
        g
      
    
    {\displaystyle g}
  , 
  
    
      
        −
        log
        ⁡
        P
        (
        g
        )
      
    
    {\displaystyle -\log P(g)}
  , in which case 
  
    
      
        J
        (
        g
        )
      
    
    {\displaystyle J(g)}
   is the posterior probability of 
  
    
      
        g
      
    
    {\displaystyle g}
  .

Generative training
The training methods described above are discriminative training methods, because they seek to find a function 
  
    
      
        g
      
    
    {\displaystyle g}
   that discriminates well between the different output values (see discriminative model). For the special case where 
  
    
      
        f
        (
        x
        ,
        y
        )
        =
        P
        (
        x
        ,
        y
        )
      
    
    {\displaystyle f(x,y)=P(x,y)}
   is a joint probability distribution and the loss function is the negative log likelihood 
  
    
      
        −
        
          ∑
          
            i
          
        
        log
        ⁡
        P
        (
        
          x
          
            i
          
        
        ,
        
          y
          
            i
          
        
        )
        ,
      
    
    {\displaystyle -\sum _{i}\log P(x_{i},y_{i}),}
   a risk minimization algorithm is said to perform generative training, because 
  
    
      
        f
      
    
    {\displaystyle f}
   can be regarded as a generative model that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis.

Generalizations
There are several ways in which the standard supervised learning problem can be generalized:

Semi-supervised learning: In this setting, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled.
Weak supervision: In this setting, noisy, limited, or imprecise sources are used to provide supervision signal for labeling training data.
Active learning: Instead of assuming that all of the training examples are given at the start, active learning algorithms interactively collect new examples, typically by making queries to a human user. Often, the queries are based on unlabeled data, which is a scenario that combines semi-supervised learning with active learning.
Structured prediction: When the desired output value is a complex object, such as a parse tree or a labeled graph, then standard methods must be extended.
Learning to rank: When the input is a set of objects and the desired output is a ranking of those objects, then again the standard methods must be extended.

Approaches and algorithms
Analytical learning
Artificial neural network
Backpropagation
Boosting (meta-algorithm)
Bayesian statistics
Case-based reasoning
Decision tree learning
Inductive logic programming
Gaussian process regression
Genetic programming
Group method of data handling
Kernel estimators
Learning automata
Learning classifier systems
Minimum message length (decision trees, decision graphs, etc.)
Multilinear subspace learning
Naive Bayes classifier
Maximum entropy classifier
Conditional random field
Nearest neighbor algorithm
Probably approximately correct learning (PAC) learning
Ripple down rules, a knowledge acquisition methodology
Symbolic machine learning algorithms
Subsymbolic machine learning algorithms
Support-vector machines
Minimum complexity machines (MCM)
Random forests
Ensembles of classifiers
Ordinal classification
Data pre-processing
Handling imbalanced datasets
Statistical relational learning
Proaftn, a multicriteria classification algorithm

Applications
Bioinformatics
Cheminformatics
Quantitative structure–activity relationship
Database marketing
Handwriting recognition
Information retrieval
Learning to rank
Information extraction
Object recognition in computer vision
Optical character recognition
Spam detection
Pattern recognition
Speech recognition
Supervised learning is a special case of downward causation in biological systems
Landform classification using satellite imagery

General issues
Computational learning theory
Inductive bias
Overfitting (machine learning)
(Uncalibrated) Class membership probabilities
Unsupervised learning
Version spaces

See also
List of datasets for machine learning research

References
External links
Machine Learning Open Source Software (MLOSS)",https://en.wikipedia.org/wiki/Supervised_learning,"['Articles with long short description', 'Articles with short description', 'Short description matches Wikidata', 'Supervised learning', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers']",Data Science
168,T-distributed stochastic neighbor embedding,"t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. It is based on Stochastic Neighbor Embedding originally developed by Sam Roweis and Geoffrey Hinton, where Laurens van der Maaten proposed the t-distributed variant. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.
The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map. While the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this can be changed as appropriate.
t-SNE has been used for visualization in a wide range of applications, including genomics, computer security research, music analysis, cancer research, bioinformatics, and biomedical signal processing.While t-SNE plots often seem to display clusters, the visual clusters can be influenced strongly by the chosen parameterization and therefore a good understanding of the parameters for t-SNE is necessary. Such ""clusters"" can be shown to even appear in non-clustered data, and thus may be false findings. Interactive exploration may thus be necessary to choose parameters and validate results. It has been demonstrated that t-SNE is often able to recover well-separated clusters, and with special parameter choices, approximates a simple form of spectral clustering.

Details
Given a set of 
  
    
      
        N
      
    
    {\displaystyle N}
   high-dimensional objects 
  
    
      
        
          
            x
          
          
            1
          
        
        ,
        …
        ,
        
          
            x
          
          
            N
          
        
      
    
    {\displaystyle \mathbf {x} _{1},\dots ,\mathbf {x} _{N}}
  , t-SNE first computes probabilities 
  
    
      
        
          p
          
            i
            j
          
        
      
    
    {\displaystyle p_{ij}}
   that are proportional to the similarity of objects 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   and 
  
    
      
        
          
            x
          
          
            j
          
        
      
    
    {\displaystyle \mathbf {x} _{j}}
  , as follows. 
For 
  
    
      
        i
        ≠
        j
      
    
    {\displaystyle i\neq j}
  , define 

  
    
      
        
          p
          
            j
            ∣
            i
          
        
        =
        
          
            
              exp
              ⁡
              (
              −
              ‖
              
                
                  x
                
                
                  i
                
              
              −
              
                
                  x
                
                
                  j
                
              
              
                ‖
                
                  2
                
              
              
                /
              
              2
              
                σ
                
                  i
                
                
                  2
                
              
              )
            
            
              
                ∑
                
                  k
                  ≠
                  i
                
              
              exp
              ⁡
              (
              −
              ‖
              
                
                  x
                
                
                  i
                
              
              −
              
                
                  x
                
                
                  k
                
              
              
                ‖
                
                  2
                
              
              
                /
              
              2
              
                σ
                
                  i
                
                
                  2
                
              
              )
            
          
        
      
    
    {\displaystyle p_{j\mid i}={\frac {\exp(-\lVert \mathbf {x} _{i}-\mathbf {x} _{j}\rVert ^{2}/2\sigma _{i}^{2})}{\sum _{k\neq i}\exp(-\lVert \mathbf {x} _{i}-\mathbf {x} _{k}\rVert ^{2}/2\sigma _{i}^{2})}}}
  and set 
  
    
      
        
          p
          
            i
            ∣
            i
          
        
        =
        0
      
    
    {\displaystyle p_{i\mid i}=0}
  . 
Note that 
  
    
      
        
          ∑
          
            j
          
        
        
          p
          
            j
            ∣
            i
          
        
        =
        1
      
    
    {\displaystyle \sum _{j}p_{j\mid i}=1}
   for all 
  
    
      
        i
      
    
    {\displaystyle i}
  . 
As Van der Maaten and Hinton explained:  ""The similarity of datapoint 
  
    
      
        
          x
          
            j
          
        
      
    
    {\displaystyle x_{j}}
   to datapoint 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   is the conditional probability, 
  
    
      
        
          p
          
            j
            
              |
            
            i
          
        
      
    
    {\displaystyle p_{j|i}}
  , that 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   would pick 
  
    
      
        
          x
          
            j
          
        
      
    
    {\displaystyle x_{j}}
   as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  .""Now define 

  
    
      
        
          p
          
            i
            j
          
        
        =
        
          
            
              
                p
                
                  j
                  ∣
                  i
                
              
              +
              
                p
                
                  i
                  ∣
                  j
                
              
            
            
              2
              N
            
          
        
      
    
    {\displaystyle p_{ij}={\frac {p_{j\mid i}+p_{i\mid j}}{2N}}}
  and note that 
  
    
      
        
          p
          
            i
            j
          
        
        =
        
          p
          
            j
            i
          
        
      
    
    {\displaystyle p_{ij}=p_{ji}}
  , 
  
    
      
        
          p
          
            i
            i
          
        
        =
        0
      
    
    {\displaystyle p_{ii}=0}
  , and 
  
    
      
        
          ∑
          
            i
            ,
            j
          
        
        
          p
          
            i
            j
          
        
        =
        1
      
    
    {\displaystyle \sum _{i,j}p_{ij}=1}
  . 
The bandwidth of the Gaussian kernels 
  
    
      
        
          σ
          
            i
          
        
      
    
    {\displaystyle \sigma _{i}}
   is set in such a way that the perplexity of the conditional distribution equals a predefined perplexity using the bisection method. As a result, the bandwidth is adapted to the density of the data: smaller values of 
  
    
      
        
          σ
          
            i
          
        
      
    
    {\displaystyle \sigma _{i}}
   are used in denser parts of the data space.
Since the Gaussian kernel uses the Euclidean distance 
  
    
      
        ‖
        
          x
          
            i
          
        
        −
        
          x
          
            j
          
        
        ‖
      
    
    {\displaystyle \lVert x_{i}-x_{j}\rVert }
  , it is affected by the curse of dimensionality, and in high dimensional data when distances lose the ability to discriminate, the 
  
    
      
        
          p
          
            i
            j
          
        
      
    
    {\displaystyle p_{ij}}
   become too similar (asymptotically, they would converge to a constant). It has been proposed to adjust the distances with a power transform, based on the intrinsic dimension of each point, to alleviate this.t-SNE aims to learn a 
  
    
      
        d
      
    
    {\displaystyle d}
  -dimensional map 
  
    
      
        
          
            y
          
          
            1
          
        
        ,
        …
        ,
        
          
            y
          
          
            N
          
        
      
    
    {\displaystyle \mathbf {y} _{1},\dots ,\mathbf {y} _{N}}
   (with 
  
    
      
        
          
            y
          
          
            i
          
        
        ∈
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle \mathbf {y} _{i}\in \mathbb {R} ^{d}}
  ) that reflects the similarities  
  
    
      
        
          p
          
            i
            j
          
        
      
    
    {\displaystyle p_{ij}}
   as well as possible. To this end, it measures similarities 
  
    
      
        
          q
          
            i
            j
          
        
      
    
    {\displaystyle q_{ij}}
   between two points in the map 
  
    
      
        
          
            y
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {y} _{i}}
   and 
  
    
      
        
          
            y
          
          
            j
          
        
      
    
    {\displaystyle \mathbf {y} _{j}}
  , using a very similar approach. 
Specifically, for 
  
    
      
        i
        ≠
        j
      
    
    {\displaystyle i\neq j}
  , define 
  
    
      
        
          q
          
            i
            j
          
        
      
    
    {\displaystyle q_{ij}}
   as

  
    
      
        
          q
          
            i
            j
          
        
        =
        
          
            
              (
              1
              +
              ‖
              
                
                  y
                
                
                  i
                
              
              −
              
                
                  y
                
                
                  j
                
              
              
                ‖
                
                  2
                
              
              
                )
                
                  −
                  1
                
              
            
            
              
                ∑
                
                  k
                
              
              
                ∑
                
                  l
                  ≠
                  k
                
              
              (
              1
              +
              ‖
              
                
                  y
                
                
                  k
                
              
              −
              
                
                  y
                
                
                  l
                
              
              
                ‖
                
                  2
                
              
              
                )
                
                  −
                  1
                
              
            
          
        
      
    
    {\displaystyle q_{ij}={\frac {(1+\lVert \mathbf {y} _{i}-\mathbf {y} _{j}\rVert ^{2})^{-1}}{\sum _{k}\sum _{l\neq k}(1+\lVert \mathbf {y} _{k}-\mathbf {y} _{l}\rVert ^{2})^{-1}}}}
  and set 
  
    
      
        
          q
          
            i
            i
          
        
        =
        0
      
    
    {\displaystyle q_{ii}=0}
  . 
Herein a heavy-tailed Student t-distribution (with one-degree of freedom, which is the same as a Cauchy distribution) is used to measure similarities between low-dimensional points in order to allow dissimilar objects to be modeled far apart in the map. 
The locations of the points 
  
    
      
        
          
            y
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {y} _{i}}
   in the map are determined by minimizing the (non-symmetric) Kullback–Leibler divergence of the distribution 
  
    
      
        P
      
    
    {\displaystyle P}
   from the distribution 
  
    
      
        Q
      
    
    {\displaystyle Q}
  , that is:

  
    
      
        
          K
          L
        
        
          (
          
            P
            ∥
            Q
          
          )
        
        =
        
          ∑
          
            i
            ≠
            j
          
        
        
          p
          
            i
            j
          
        
        log
        ⁡
        
          
            
              p
              
                i
                j
              
            
            
              q
              
                i
                j
              
            
          
        
      
    
    {\displaystyle \mathrm {KL} \left(P\parallel Q\right)=\sum _{i\neq j}p_{ij}\log {\frac {p_{ij}}{q_{ij}}}}
  The minimization of the Kullback–Leibler divergence with respect to the points 
  
    
      
        
          
            y
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {y} _{i}}
   is performed using gradient descent. 
The result of this optimization is a map that reflects the similarities between the high-dimensional inputs.

Software
ELKI contains tSNE, also with Barnes-Hut approximation
Scikit-learn, a popular machine learning toolkit in python implements t-SNE with both exact solutions and the Barnes-Hut approximation.

References
External links
Visualizing Data Using t-SNE, Google Tech Talk about t-SNE
Implementations of t-SNE in various languages, A link collection maintained by Laurens van der Maaten",https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding,"['Articles with short description', 'Dimension reduction', 'Machine learning algorithms', 'Short description matches Wikidata']",Data Science
169,Structured prediction,"Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than scalar discrete or real values.Similar to commonly used supervised learning techniques, structured prediction models are typically trained by means of observed data in which the true prediction value is used to adjust model parameters. Due to the complexity of the model and the interrelations of predicted variables the process of prediction using a trained model and of training itself is often computationally infeasible and approximate inference and learning methods are used.

Applications
For example, the problem of translating a natural language sentence into a syntactic representation such as a parse tree can be seen as a structured prediction problem in which the structured output domain is the set of all possible parse trees.
Structured prediction is also used in a wide variety of application domains including bioinformatics, natural language processing, speech recognition, and computer vision.

Example: sequence tagging
Sequence tagging is a class of problems prevalent in natural language processing, where input data are often sequences (e.g. sentences of text). The sequence tagging problem appears in several guises, e.g. part-of-speech tagging and named entity recognition. In POS tagging, for example, each word in a sequence must receive a ""tag"" (class label) that expresses its ""type"" of word:

The main challenge of this problem is to resolve ambiguity: the word ""sentence"" can also be a verb in English, and so can ""tagged"".
While this problem can be solved by simply performing classification of individual tokens, that approach does not take into account the empirical fact that tags do not occur independently; instead, each tag displays a strong conditional dependence on the tag of the previous word. This fact can be exploited in a sequence model such as a hidden Markov model or conditional random field that predicts the entire tag sequence for a sentence, rather than just individual tags, by means of the Viterbi algorithm.

Techniques
Probabilistic graphical models form a large class of structured prediction models. In particular, Bayesian networks and random fields are popular. Other algorithms and models for structured prediction include inductive logic programming, case-based reasoning, structured SVMs, Markov logic networks and constrained conditional models. Main techniques:

Conditional random field
Structured support vector machine
Structured k-Nearest Neighbours
Recurrent neural network, in particular Elman network

Structured perceptron
One of the easiest ways to understand algorithms for general structured prediction is the structured perceptron of Collins.
This algorithm combines the perceptron algorithm for learning linear classifiers with an inference algorithm (classically the Viterbi algorithm when used on sequence data) and can be described abstractly as follows. First define a ""joint feature function"" Φ(x, y) that maps a training sample x and a candidate prediction y to a vector of length n (x and y may have any structure; n is problem-dependent, but must be fixed for each model). Let GEN be a function that generates candidate predictions. Then:

Let 
  
    
      
        w
      
    
    {\displaystyle w}
   be a weight vector of length nFor a pre-determined number of iterations:For each sample 
  
    
      
        x
      
    
    {\displaystyle x}
   in the training set with true output 
  
    
      
        t
      
    
    {\displaystyle t}
  :Make a prediction 
  
    
      
        
          
            
              y
              ^
            
          
        
        =
        
          
            a
            r
            g
            
            m
            a
            x
          
        
        
        {
        
          y
        
        ∈
        
          G
          E
          N
        
        (
        
          x
        
        )
        }
        
        (
        
          
            w
          
          
            T
          
        
        
        ϕ
        (
        
          x
        
        ,
        
          y
        
        )
        )
      
    
    {\displaystyle {\hat {y}}={\operatorname {arg\,max} }\,\{{y}\in {GEN}({x})\}\,({w}^{T}\,\phi ({x},{y}))}
  Update 
  
    
      
        w
      
    
    {\displaystyle w}
  , from 
  
    
      
        
          
            
              y
              ^
            
          
        
      
    
    {\displaystyle {\hat {y}}}
   to 
  
    
      
        t
      
    
    {\displaystyle t}
  :  
  
    
      
        
          w
        
        =
        
          w
        
        +
        
          c
        
        (
        −
        ϕ
        (
        
          x
        
        ,
        
          
            
              y
              ^
            
          
        
        )
        +
        ϕ
        (
        
          x
        
        ,
        
          t
        
        )
        )
      
    
    {\displaystyle {w}={w}+{c}(-\phi ({x},{\hat {y}})+\phi ({x},{t}))}
  , 
  
    
      
        c
      
    
    {\displaystyle c}
   is learning rateIn practice, finding the argmax over 
  
    
      
        
          G
          E
          N
        
        (
        
          x
        
        )
      
    
    {\displaystyle {GEN}({x})}
   will be done using an algorithm such as Viterbi or an algorithm such as max-sum, rather than an exhaustive search through an exponentially large set of candidates.
The idea of learning is similar to multiclass perceptron.

References
Noah Smith, Linguistic Structure Prediction, 2011.
Michael Collins, Discriminative Training Methods for Hidden Markov Models, 2002.

External links
Implementation of Collins structured perceptron",https://en.wikipedia.org/wiki/Structured_prediction,"['Articles with short description', 'CS1 maint: uses authors parameter', 'Short description matches Wikidata', 'Structured prediction']",Data Science
170,Temporal difference learning,"Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.While Monte Carlo methods only adjust their estimates once the final outcome is known, TD methods adjust predictions to match later, more accurate, predictions about the future before the final outcome is known. This is a form of bootstrapping, as illustrated with the following example:

""Suppose you wish to predict the weather for Saturday, and you have some model that predicts Saturday's weather, given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday – and thus be able to change, say, Saturday's model before Saturday arrives.""Temporal difference methods are related to the temporal difference model of animal learning.

Mathematical formulation
The tabular TD(0) method is one of the simplest TD methods. It is a special case of more general stochastic approximation methods. It estimates the state value function of a finite-state Markov decision process (MDP) under a policy 
  
    
      
        π
      
    
    {\displaystyle \pi }
  . Let 
  
    
      
        
          V
          
            π
          
        
      
    
    {\displaystyle V^{\pi }}
   denote the state value function of the MDP with states 
  
    
      
        (
        
          s
          
            t
          
        
        
          )
          
            t
            ∈
            
              N
            
          
        
      
    
    {\displaystyle (s_{t})_{t\in \mathbb {N} }}
  , rewards 
  
    
      
        (
        
          r
          
            t
          
        
        
          )
          
            t
            ∈
            
              N
            
          
        
      
    
    {\displaystyle (r_{t})_{t\in \mathbb {N} }}
   and discount rate 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
   under the policy 
  
    
      
        π
      
    
    {\displaystyle \pi }
  :

  
    
      
        
          V
          
            π
          
        
        (
        s
        )
        =
        
          E
          
            a
            ∼
            π
          
        
        
          {
          
            
              ∑
              
                t
                =
                0
              
              
                ∞
              
            
            
              γ
              
                t
              
            
            
              r
              
                t
              
            
            (
            
              a
              
                t
              
            
            )
            
              
                |
              
            
            
              s
              
                0
              
            
            =
            s
          
          }
        
        .
      
    
    {\displaystyle V^{\pi }(s)=E_{a\sim \pi }\left\{\sum _{t=0}^{\infty }\gamma ^{t}r_{t}(a_{t}){\Bigg |}s_{0}=s\right\}.}
  We drop the action from the notation for convenience. 
  
    
      
        
          V
          
            π
          
        
      
    
    {\displaystyle V^{\pi }}
   satisfies the Hamilton-Jacobi-Bellman Equation: 

  
    
      
        
          V
          
            π
          
        
        (
        s
        )
        =
        
          E
          
            π
          
        
        {
        
          r
          
            0
          
        
        +
        γ
        
          V
          
            π
          
        
        (
        
          s
          
            1
          
        
        )
        
          |
        
        
          s
          
            0
          
        
        =
        s
        }
        ,
      
    
    {\displaystyle V^{\pi }(s)=E_{\pi }\{r_{0}+\gamma V^{\pi }(s_{1})|s_{0}=s\},}
  so 
  
    
      
        
          r
          
            0
          
        
        +
        γ
        
          V
          
            π
          
        
        (
        
          s
          
            1
          
        
        )
      
    
    {\displaystyle r_{0}+\gamma V^{\pi }(s_{1})}
   is an unbiased estimate for 
  
    
      
        
          V
          
            π
          
        
        (
        s
        )
      
    
    {\displaystyle V^{\pi }(s)}
  . This observation motivates the following algorithm for estimating 
  
    
      
        
          V
          
            π
          
        
      
    
    {\displaystyle V^{\pi }}
  .
The algorithm starts by initializing a table 
  
    
      
        V
        (
        s
        )
      
    
    {\displaystyle V(s)}
   arbitrarily, with one value for each state of the MDP. A positive learning rate 
  
    
      
        α
      
    
    {\displaystyle \alpha }
   is chosen.
We then repeatedly evaluate the policy 
  
    
      
        π
      
    
    {\displaystyle \pi }
  , obtain a reward 
  
    
      
        r
      
    
    {\displaystyle r}
   and update the value function for the old state using the rule:

  
    
      
        V
        (
        s
        )
        ←
        V
        (
        s
        )
        +
        α
        (
        
          
            
              
                r
                +
                γ
                V
                (
                
                  s
                  ′
                
                )
              
              ⏞
            
          
          
            The TD target
          
        
        −
        V
        (
        s
        )
        )
      
    
    {\displaystyle V(s)\leftarrow V(s)+\alpha (\overbrace {r+\gamma V(s')} ^{\text{The TD target}}-V(s))}
  where 
  
    
      
        s
      
    
    {\displaystyle s}
   and 
  
    
      
        
          s
          ′
        
      
    
    {\displaystyle s'}
  are the old and new states, respectively. The value 
  
    
      
        r
        +
        γ
        V
        (
        
          s
          ′
        
        )
      
    
    {\displaystyle r+\gamma V(s')}
   is known as the TD target.

TD-Lambda
TD-Lambda is a learning algorithm invented by Richard S. Sutton based on earlier work on temporal difference learning by Arthur Samuel. This algorithm was famously applied by Gerald Tesauro to create TD-Gammon, a program that learned to play the game of backgammon at the level of expert human players.The lambda (
  
    
      
        λ
      
    
    {\displaystyle \lambda }
  ) parameter refers to the trace decay parameter, with 
  
    
      
        0
        ⩽
        λ
        ⩽
        1
      
    
    {\displaystyle 0\leqslant \lambda \leqslant 1}
  . Higher settings lead to longer lasting traces; that is, a larger proportion of credit from a reward can be given to more distant states and actions when 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   is higher, with 
  
    
      
        λ
        =
        1
      
    
    {\displaystyle \lambda =1}
   producing parallel learning to Monte Carlo RL algorithms.

TD algorithm in neuroscience
The TD algorithm has also received attention in the field of neuroscience. Researchers discovered that the firing rate of dopamine neurons in the ventral tegmental area (VTA) and substantia nigra (SNc) appear to mimic the error function in the algorithm. The error function reports back the difference between the estimated reward at any given state or time step and the actual reward received. The larger the error function, the larger the difference between the expected and actual reward. When this is paired with a stimulus that accurately reflects a future reward, the error can be used to associate the stimulus with the future reward.
Dopamine cells appear to behave in a similar manner. In one experiment measurements of dopamine cells were made while training a monkey to associate a stimulus with the reward of juice. Initially the dopamine cells increased firing rates when the monkey received juice, indicating a difference in expected and actual rewards. Over time this increase in firing back propagated to the earliest reliable stimulus for the reward. Once the monkey was fully trained, there was no increase in firing rate upon presentation of the predicted reward. Subsequently, the firing rate for the dopamine cells decreased below normal activation when the expected reward was not produced. This mimics closely how the error function in TD is used for reinforcement learning.
The relationship between the model and potential neurological function has produced research attempting to use TD to explain many aspects of behavioral research. It has also been used to study conditions such as schizophrenia or the consequences of pharmacological manipulations of dopamine on learning.

See also
Q-learning
SARSA
Rescorla-Wagner model
PVLV

Notes
Bibliography
Sutton, R.S., Barto A.G. (1990). ""Time Derivative Models of Pavlovian Reinforcement"" (PDF). Learning and Computational Neuroscience: Foundations of Adaptive Networks: 497–537.CS1 maint: multiple names: authors list (link)
Gerald Tesauro (March 1995). ""Temporal Difference Learning and TD-Gammon"". Communications of the ACM. 38 (3): 58–68. doi:10.1145/203330.203343.
Imran Ghory. Reinforcement Learning in Board Games.
S. P. Meyn, 2007.  Control Techniques for Complex Networks, Cambridge University Press, 2007. See final chapter, and appendix with abridged Meyn & Tweedie.

External links
Scholarpedia Temporal difference Learning
TD-Gammon
TD-Networks Research Group
Connect Four TDGravity Applet (+ mobile phone version) – self-learned using TD-Leaf method (combination of TD-Lambda with shallow tree search)
Self Learning Meta-Tic-Tac-Toe Example web app showing how temporal difference learning can be used to learn state evaluation constants for a minimax AI playing a simple board game.
Reinforcement Learning Problem, document explaining how temporal difference learning can be used to speed up Q-learning
TD-Simulator Temporal difference simulator for classical conditioning",https://en.wikipedia.org/wiki/Temporal_difference_learning,"['CS1 maint: multiple names: authors list', 'Computational neuroscience', 'Reinforcement learning', 'Subtraction', 'Webarchive template wayback links']",Data Science
171,Support vector machine,"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Vapnik et al., 1997), SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik and Chervonenkis (1974) and Vapnik (1982, 1995). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support-vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data, and is one of the most widely used clustering algorithms in industrial applications.

Motivation
Classifying data is a common task in machine learning.
Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support-vector machines, a data point is viewed as a 
  
    
      
        p
      
    
    {\displaystyle p}
  -dimensional vector (a list of 
  
    
      
        p
      
    
    {\displaystyle p}
   numbers), and we want to know whether we can separate such points with a 
  
    
      
        (
        p
        −
        1
        )
      
    
    {\displaystyle (p-1)}
  -dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability.

Definition
More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier.

Whereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products of pairs of input data vectors may be computed easily in terms of the variables in the original space, by defining them in terms of a kernel function 
  
    
      
        k
        (
        x
        ,
        y
        )
      
    
    {\displaystyle k(x,y)}
   selected to suit the problem. The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant, where such a set of vectors is an orthogonal (and thus minimal) set of vectors that defines a hyperplane. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters 
  
    
      
        
          α
          
            i
          
        
      
    
    {\displaystyle \alpha _{i}}
   of images of feature vectors 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   that occur in the data base. With this choice of a hyperplane, the points 
  
    
      
        x
      
    
    {\displaystyle x}
   in the feature space that are mapped into the hyperplane are defined by the relation 
  
    
      
        
          
            ∑
            
              i
            
          
          
            α
            
              i
            
          
          k
          (
          
            x
            
              i
            
          
          ,
          x
          )
          =
          
            constant
          
          .
        
      
    
    {\displaystyle \textstyle \sum _{i}\alpha _{i}k(x_{i},x)={\text{constant}}.}
    Note that if 
  
    
      
        k
        (
        x
        ,
        y
        )
      
    
    {\displaystyle k(x,y)}
   becomes small as 
  
    
      
        y
      
    
    {\displaystyle y}
   grows further away from 
  
    
      
        x
      
    
    {\displaystyle x}
  , each term in the sum measures the degree of closeness of the test point 
  
    
      
        x
      
    
    {\displaystyle x}
   to the corresponding data base point 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  . In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in one or the other of the sets to be discriminated. Note the fact that the set of points 
  
    
      
        x
      
    
    {\displaystyle x}
   mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets that are not convex at all in the original space.

Applications
SVMs can be used to solve various real-world problems:

SVMs are helpful in text and hypertext categorization, as their application can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings. Some methods for shallow semantic parsing are based on support vector machines.
Classification of images can also be performed using SVMs. Experimental results show that SVMs achieve significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback. This is also true for image segmentation systems, including those using a modified version SVM that uses the privileged approach as suggested by Vapnik.
Classification of satellite data like SAR data using supervised SVM.
Hand-written characters can be recognized using SVM.
The SVM algorithm has been widely applied in the biological and other sciences.  They have been used to classify proteins with up to 90% of the compounds classified correctly. Permutation tests based on SVM weights have been suggested as a mechanism for interpretation of SVM models. Support-vector machine weights have also been used to interpret SVM models in the past. Posthoc interpretation of support-vector machine models in order to identify features used by the model to make predictions is a relatively new area of research with special significance in the biological sciences.

History
The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963. In 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes. The current standard incarnation (soft margin) was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.

Linear SVM
We are given a training dataset of 
  
    
      
        n
      
    
    {\displaystyle n}
   points of the form

  
    
      
        (
        
          
            x
          
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        ,
        …
        ,
        (
        
          
            x
          
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
        ,
      
    
    {\displaystyle (\mathbf {x} _{1},y_{1}),\ldots ,(\mathbf {x} _{n},y_{n}),}
  where the 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
   are either 1 or −1, each indicating the class to which the point 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   belongs. Each 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   is a 
  
    
      
        p
      
    
    {\displaystyle p}
  -dimensional real vector. We want to find the ""maximum-margin hyperplane"" that divides the group of points 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   for which 
  
    
      
        
          y
          
            i
          
        
        =
        1
      
    
    {\displaystyle y_{i}=1}
   from the group of points for which 
  
    
      
        
          y
          
            i
          
        
        =
        −
        1
      
    
    {\displaystyle y_{i}=-1}
  , which is defined so that the distance between the hyperplane and the nearest point 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   from either group is maximized.
Any hyperplane can be written as the set of points 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   satisfying

  
    
      
        
          
            w
          
          
            T
          
        
        
          x
        
        −
        b
        =
        0
        ,
      
    
    {\displaystyle \mathbf {w} ^{T}\mathbf {x} -b=0,}
  where 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   is the (not necessarily normalized) normal vector to the hyperplane. This is much like Hesse normal form, except that 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   is not necessarily a unit vector. The parameter 
  
    
      
        
          
            
              b
              
                ‖
                
                  w
                
                ‖
              
            
          
        
      
    
    {\displaystyle {\tfrac {b}{\|\mathbf {w} \|}}}
   determines the offset of the hyperplane from the origin along the normal vector 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
  .

Hard-margin
If the training data is linearly separable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the ""margin"", and the maximum-margin hyperplane is the hyperplane that lies halfway between them. With a normalized or standardized dataset, these hyperplanes can be described by the equations

  
    
      
        
          
            w
          
          
            T
          
        
        
          x
        
        −
        b
        =
        1
      
    
    {\displaystyle \mathbf {w} ^{T}\mathbf {x} -b=1}
   (anything on or above this boundary is of one class, with label 1)and

  
    
      
        
          
            w
          
          
            T
          
        
        
          x
        
        −
        b
        =
        −
        1
      
    
    {\displaystyle \mathbf {w} ^{T}\mathbf {x} -b=-1}
   (anything on or below this boundary is of the other class, with label −1).Geometrically, the distance between these two hyperplanes is 
  
    
      
        
          
            
              2
              
                ‖
                
                  w
                
                ‖
              
            
          
        
      
    
    {\displaystyle {\tfrac {2}{\|\mathbf {w} \|}}}
  , so to maximize the distance between the planes we want to minimize 
  
    
      
        ‖
        
          w
        
        ‖
      
    
    {\displaystyle \|\mathbf {w} \|}
  . The distance is computed using the distance from a point to a plane equation. We also have to prevent data points from falling into the margin, we add the following constraint: for each 
  
    
      
        i
      
    
    {\displaystyle i}
   either

  
    
      
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
        ≥
        1
      
    
    {\displaystyle \mathbf {w} ^{T}\mathbf {x} _{i}-b\geq 1}
  , if 
  
    
      
        
          y
          
            i
          
        
        =
        1
      
    
    {\displaystyle y_{i}=1}
  ,or

  
    
      
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
        ≤
        −
        1
      
    
    {\displaystyle \mathbf {w} ^{T}\mathbf {x} _{i}-b\leq -1}
  , if 
  
    
      
        
          y
          
            i
          
        
        =
        −
        1
      
    
    {\displaystyle y_{i}=-1}
  .These constraints state that each data point must lie on the correct side of the margin.
This can be rewritten as

  
    
      
        
          y
          
            i
          
        
        (
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
        )
        ≥
        1
        ,
        
        
           for all 
        
        1
        ≤
        i
        ≤
        n
        .
        
        
        (
        1
        )
      
    
    {\displaystyle y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\geq 1,\quad {\text{ for all }}1\leq i\leq n.\qquad \qquad (1)}
  We can put this together to get the optimization problem:

""Minimize 
  
    
      
        ‖
        
          w
        
        ‖
      
    
    {\displaystyle \|\mathbf {w} \|}
   subject to 
  
    
      
        
          y
          
            i
          
        
        (
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
        )
        ≥
        1
      
    
    {\displaystyle y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\geq 1}
   for 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=1,\ldots ,n}
  .""The 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   and 
  
    
      
        b
      
    
    {\displaystyle b}
   that solve this problem determine our classifier, 
  
    
      
        
          x
        
        ↦
        sgn
        ⁡
        (
        
          
            w
          
          
            T
          
        
        
          x
        
        −
        b
        )
      
    
    {\displaystyle \mathbf {x} \mapsto \operatorname {sgn}(\mathbf {w} ^{T}\mathbf {x} -b)}
   where 
  
    
      
        sgn
        ⁡
        (
        ⋅
        )
      
    
    {\displaystyle \operatorname {sgn}(\cdot )}
   is the sign function.
An important consequence of this geometric description is that the max-margin hyperplane is completely determined by those 
  
    
      
        
          
            
              
                x
                →
              
            
          
          
            i
          
        
      
    
    {\displaystyle {\vec {x}}_{i}}
   that lie nearest to it. These 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   are called support vectors.

Soft-margin
To extend SVM to cases in which the data are not linearly separable, the hinge loss function is helpful

  
    
      
        max
        
          (
          
            0
            ,
            1
            −
            
              y
              
                i
              
            
            (
            
              
                w
              
              
                T
              
            
            
              
                x
              
              
                i
              
            
            −
            b
            )
          
          )
        
        .
      
    
    {\displaystyle \max \left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\right).}
  Note that 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
   is the i-th target (i.e., in this case, 1 or −1), and 
  
    
      
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
      
    
    {\displaystyle \mathbf {w} ^{T}\mathbf {x} _{i}-b}
   is the i-th output.
This function is zero if the constraint in (1) is satisfied, in other words, if 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   lies on the correct side of the margin. For data on the wrong side of the margin, the function's value is proportional to the distance from the margin.
The goal of the optimization then is to minimize

  
    
      
        
          [
          
            
              
                1
                n
              
            
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            max
            
              (
              
                0
                ,
                1
                −
                
                  y
                  
                    i
                  
                
                (
                
                  
                    w
                  
                  
                    T
                  
                
                
                  
                    x
                  
                  
                    i
                  
                
                −
                b
                )
              
              )
            
          
          ]
        
        +
        λ
        ‖
        
          w
        
        
          ‖
          
            2
          
        
        ,
      
    
    {\displaystyle \left[{\frac {1}{n}}\sum _{i=1}^{n}\max \left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\right)\right]+\lambda \lVert \mathbf {w} \rVert ^{2},}
  where the parameter 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   determines the trade-off between increasing the margin size and ensuring that the 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   lie on the correct side of the margin. Thus, for sufficiently small values of 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
  , the second term in the loss function will become negligible, hence, it will behave similar to the hard-margin SVM, if the input data are linearly classifiable, but will still learn if a classification rule is viable or not.

Nonlinear classification
The original maximum-margin hyperplane algorithm proposed by Vapnik in 1963 constructed a linear classifier. However, in 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.) to maximum-margin hyperplanes. The resulting algorithm is formally similar, except that every dot product is replaced by a nonlinear kernel function. This allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The transformation may be nonlinear and the transformed space high-dimensional; although the classifier is a hyperplane in the transformed feature space, it may be nonlinear in the original input space.
It is noteworthy that working in a higher-dimensional feature space increases the generalization error of support-vector machines, although given enough samples the algorithm still performs well.Some common kernels include:

Polynomial (homogeneous): 
  
    
      
        k
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ,
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        )
        =
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ⋅
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        
          )
          
            d
          
        
      
    
    {\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=({\vec {x_{i}}}\cdot {\vec {x_{j}}})^{d}}
  .
Polynomial (inhomogeneous): 
  
    
      
        k
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ,
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        )
        =
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ⋅
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        +
        1
        
          )
          
            d
          
        
      
    
    {\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=({\vec {x_{i}}}\cdot {\vec {x_{j}}}+1)^{d}}
  .
Gaussian radial basis function: 
  
    
      
        k
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ,
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        )
        =
        exp
        ⁡
        (
        −
        γ
        ‖
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        −
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        
          ‖
          
            2
          
        
        )
      
    
    {\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=\exp(-\gamma \|{\vec {x_{i}}}-{\vec {x_{j}}}\|^{2})}
   for 
  
    
      
        γ
        >
        0
      
    
    {\displaystyle \gamma >0}
  . Sometimes parametrized using 
  
    
      
        γ
        =
        1
        
          /
        
        (
        2
        
          σ
          
            2
          
        
        )
      
    
    {\displaystyle \gamma =1/(2\sigma ^{2})}
  .
Hyperbolic tangent: 
  
    
      
        k
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ,
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        )
        =
        tanh
        ⁡
        (
        κ
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ⋅
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        +
        c
        )
      
    
    {\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=\tanh(\kappa {\vec {x_{i}}}\cdot {\vec {x_{j}}}+c)}
   for some (not every) 
  
    
      
        κ
        >
        0
      
    
    {\displaystyle \kappa >0}
   and 
  
    
      
        c
        <
        0
      
    
    {\displaystyle c<0}
  .The kernel is related to the transform 
  
    
      
        φ
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        )
      
    
    {\displaystyle \varphi ({\vec {x_{i}}})}
   by the equation 
  
    
      
        k
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ,
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        )
        =
        φ
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        )
        ⋅
        φ
        (
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        )
      
    
    {\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=\varphi ({\vec {x_{i}}})\cdot \varphi ({\vec {x_{j}}})}
  . The value w is also in the transformed space, with 
  
    
      
        
          
            
              
                w
                →
              
            
          
          =
          
            ∑
            
              i
            
          
          
            α
            
              i
            
          
          
            y
            
              i
            
          
          φ
          (
          
            
              
                
                  x
                  →
                
              
            
            
              i
            
          
          )
        
      
    
    {\displaystyle \textstyle {\vec {w}}=\sum _{i}\alpha _{i}y_{i}\varphi ({\vec {x}}_{i})}
  . Dot products with w for classification can again be computed by the kernel trick, i.e. 
  
    
      
        
          
            
              
                w
                →
              
            
          
          ⋅
          φ
          (
          
            
              
                x
                →
              
            
          
          )
          =
          
            ∑
            
              i
            
          
          
            α
            
              i
            
          
          
            y
            
              i
            
          
          k
          (
          
            
              
                
                  x
                  →
                
              
            
            
              i
            
          
          ,
          
            
              
                x
                →
              
            
          
          )
        
      
    
    {\displaystyle \textstyle {\vec {w}}\cdot \varphi ({\vec {x}})=\sum _{i}\alpha _{i}y_{i}k({\vec {x}}_{i},{\vec {x}})}
  .

Computing the SVM classifier
Computing the (soft-margin) SVM classifier amounts to minimizing an expression of the form

  
    
      
        
          [
          
            
              
                1
                n
              
            
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            max
            
              (
              
                0
                ,
                1
                −
                
                  y
                  
                    i
                  
                
                (
                
                  
                    w
                  
                  
                    T
                  
                
                
                  
                    x
                  
                  
                    i
                  
                
                −
                b
                )
              
              )
            
          
          ]
        
        +
        λ
        ‖
        
          w
        
        
          ‖
          
            2
          
        
        .
        
        (
        2
        )
      
    
    {\displaystyle \left[{\frac {1}{n}}\sum _{i=1}^{n}\max \left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\right)\right]+\lambda \|\mathbf {w} \|^{2}.\qquad (2)}
  We focus on the soft-margin classifier since, as noted above, choosing a sufficiently small value for 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   yields the hard-margin classifier for linearly classifiable input data. The classical approach, which involves reducing (2) to a quadratic programming problem, is detailed below. Then, more recent approaches such as sub-gradient descent and coordinate descent will be discussed.

Primal
Minimizing (2) can be rewritten as a constrained optimization problem with a differentiable objective function in the following way.
For each 
  
    
      
        i
        ∈
        {
        1
        ,
        
        …
        ,
        
        n
        }
      
    
    {\displaystyle i\in \{1,\,\ldots ,\,n\}}
   we introduce a variable 
  
    
      
        
          ζ
          
            i
          
        
        =
        max
        
          (
          
            0
            ,
            1
            −
            
              y
              
                i
              
            
            (
            
              
                w
              
              
                T
              
            
            
              
                x
              
              
                i
              
            
            −
            b
            )
          
          )
        
      
    
    {\displaystyle \zeta _{i}=\max \left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\right)}
  . Note that 
  
    
      
        
          ζ
          
            i
          
        
      
    
    {\displaystyle \zeta _{i}}
   is the smallest nonnegative number satisfying 
  
    
      
        
          y
          
            i
          
        
        (
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
        )
        ≥
        1
        −
        
          ζ
          
            i
          
        
        .
      
    
    {\displaystyle y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\geq 1-\zeta _{i}.}
  
Thus we can rewrite the optimization problem as follows

  
    
      
        
          minimize 
        
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          ζ
          
            i
          
        
        +
        λ
        ‖
        
          w
        
        
          ‖
          
            2
          
        
      
    
    {\displaystyle {\text{minimize }}{\frac {1}{n}}\sum _{i=1}^{n}\zeta _{i}+\lambda \|\mathbf {w} \|^{2}}
  

  
    
      
        
          subject to 
        
        
          y
          
            i
          
        
        (
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
        )
        ≥
        1
        −
        
          ζ
          
            i
          
        
        
        
           and 
        
        
        
          ζ
          
            i
          
        
        ≥
        0
        ,
        
        
          for all 
        
        i
        .
      
    
    {\displaystyle {\text{subject to }}y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\geq 1-\zeta _{i}\,{\text{ and }}\,\zeta _{i}\geq 0,\,{\text{for all }}i.}
  This is called the primal problem.

Dual
By solving for the Lagrangian dual of the above problem, one obtains the simplified problem

  
    
      
        
          maximize
        
        
        
        f
        (
        
          c
          
            1
          
        
        …
        
          c
          
            n
          
        
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          c
          
            i
          
        
        −
        
          
            1
            2
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        
          y
          
            i
          
        
        
          c
          
            i
          
        
        (
        
          
            x
          
          
            i
          
          
            T
          
        
        
          
            x
          
          
            j
          
        
        )
        
          y
          
            j
          
        
        
          c
          
            j
          
        
        ,
      
    
    {\displaystyle {\text{maximize}}\,\,f(c_{1}\ldots c_{n})=\sum _{i=1}^{n}c_{i}-{\frac {1}{2}}\sum _{i=1}^{n}\sum _{j=1}^{n}y_{i}c_{i}(\mathbf {x} _{i}^{T}\mathbf {x} _{j})y_{j}c_{j},}
  

  
    
      
        
          subject to 
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          c
          
            i
          
        
        
          y
          
            i
          
        
        =
        0
        ,
        
        
          and 
        
        0
        ≤
        
          c
          
            i
          
        
        ≤
        
          
            1
            
              2
              n
              λ
            
          
        
        
        
          for all 
        
        i
        .
      
    
    {\displaystyle {\text{subject to }}\sum _{i=1}^{n}c_{i}y_{i}=0,\,{\text{and }}0\leq c_{i}\leq {\frac {1}{2n\lambda }}\;{\text{for all }}i.}
  This is called the dual problem. Since the dual maximization problem is a quadratic function of the 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
   subject to linear constraints, it is efficiently solvable by quadratic programming algorithms.
Here, the variables 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
   are defined such that

  
    
      
        
          w
        
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          c
          
            i
          
        
        
          y
          
            i
          
        
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {w} =\sum _{i=1}^{n}c_{i}y_{i}\mathbf {x} _{i}}
  .Moreover, 
  
    
      
        
          c
          
            i
          
        
        =
        0
      
    
    {\displaystyle c_{i}=0}
   exactly when 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   lies on the correct side of the margin, and 
  
    
      
        0
        <
        
          c
          
            i
          
        
        <
        (
        2
        n
        λ
        
          )
          
            −
            1
          
        
      
    
    {\displaystyle 0<c_{i}<(2n\lambda )^{-1}}
    when 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   lies on the margin's boundary. It follows that 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   can be written as a linear combination of the support vectors.
The offset, 
  
    
      
        b
      
    
    {\displaystyle b}
  , can be recovered by finding an 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   on the margin's boundary and solving

  
    
      
        
          y
          
            i
          
        
        (
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
        )
        =
        1
        
        ⟺
        
        b
        =
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        
          y
          
            i
          
        
        .
      
    
    {\displaystyle y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)=1\iff b=\mathbf {w} ^{T}\mathbf {x} _{i}-y_{i}.}
  (Note that 
  
    
      
        
          y
          
            i
          
          
            −
            1
          
        
        =
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}^{-1}=y_{i}}
   since 
  
    
      
        
          y
          
            i
          
        
        =
        ±
        1
      
    
    {\displaystyle y_{i}=\pm 1}
  .)

Kernel trick
Suppose now that we would like to learn a nonlinear classification rule which corresponds to a linear classification rule for the transformed data points 
  
    
      
        φ
        (
        
          
            x
          
          
            i
          
        
        )
        .
      
    
    {\displaystyle \varphi (\mathbf {x} _{i}).}
   Moreover, we are given a kernel function 
  
    
      
        k
      
    
    {\displaystyle k}
   which satisfies 
  
    
      
        k
        (
        
          
            x
          
          
            i
          
        
        ,
        
          
            x
          
          
            j
          
        
        )
        =
        φ
        (
        
          
            x
          
          
            i
          
        
        )
        ⋅
        φ
        (
        
          
            x
          
          
            j
          
        
        )
      
    
    {\displaystyle k(\mathbf {x} _{i},\mathbf {x} _{j})=\varphi (\mathbf {x} _{i})\cdot \varphi (\mathbf {x} _{j})}
  .
We know the classification vector 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   in the transformed space satisfies

  
    
      
        
          w
        
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          c
          
            i
          
        
        
          y
          
            i
          
        
        φ
        (
        
          
            x
          
          
            i
          
        
        )
        ,
      
    
    {\displaystyle \mathbf {w} =\sum _{i=1}^{n}c_{i}y_{i}\varphi (\mathbf {x} _{i}),}
  where, the 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
   are obtained by solving the optimization problem

  
    
      
        
          
            
              
                
                  maximize
                
                
                
                f
                (
                
                  c
                  
                    1
                  
                
                …
                
                  c
                  
                    n
                  
                
                )
              
              
                
                =
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  c
                  
                    i
                  
                
                −
                
                  
                    1
                    2
                  
                
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  ∑
                  
                    j
                    =
                    1
                  
                  
                    n
                  
                
                
                  y
                  
                    i
                  
                
                
                  c
                  
                    i
                  
                
                (
                φ
                (
                
                  
                    x
                  
                  
                    i
                  
                
                )
                ⋅
                φ
                (
                
                  
                    x
                  
                  
                    j
                  
                
                )
                )
                
                  y
                  
                    j
                  
                
                
                  c
                  
                    j
                  
                
              
            
            
              
              
                
                =
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  c
                  
                    i
                  
                
                −
                
                  
                    1
                    2
                  
                
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  ∑
                  
                    j
                    =
                    1
                  
                  
                    n
                  
                
                
                  y
                  
                    i
                  
                
                
                  c
                  
                    i
                  
                
                k
                (
                
                  
                    x
                  
                  
                    i
                  
                
                ,
                
                  
                    x
                  
                  
                    j
                  
                
                )
                
                  y
                  
                    j
                  
                
                
                  c
                  
                    j
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{maximize}}\,\,f(c_{1}\ldots c_{n})&=\sum _{i=1}^{n}c_{i}-{\frac {1}{2}}\sum _{i=1}^{n}\sum _{j=1}^{n}y_{i}c_{i}(\varphi (\mathbf {x} _{i})\cdot \varphi (\mathbf {x} _{j}))y_{j}c_{j}\\&=\sum _{i=1}^{n}c_{i}-{\frac {1}{2}}\sum _{i=1}^{n}\sum _{j=1}^{n}y_{i}c_{i}k(\mathbf {x} _{i},\mathbf {x} _{j})y_{j}c_{j}\\\end{aligned}}}
  

  
    
      
        
          subject to 
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          c
          
            i
          
        
        
          y
          
            i
          
        
        =
        0
        ,
        
        
          and 
        
        0
        ≤
        
          c
          
            i
          
        
        ≤
        
          
            1
            
              2
              n
              λ
            
          
        
        
        
          for all 
        
        i
        .
      
    
    {\displaystyle {\text{subject to }}\sum _{i=1}^{n}c_{i}y_{i}=0,\,{\text{and }}0\leq c_{i}\leq {\frac {1}{2n\lambda }}\;{\text{for all }}i.}
  The coefficients 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
   can be solved for using quadratic programming, as before. Again, we can find some index 
  
    
      
        i
      
    
    {\displaystyle i}
   such that 
  
    
      
        0
        <
        
          c
          
            i
          
        
        <
        (
        2
        n
        λ
        
          )
          
            −
            1
          
        
      
    
    {\displaystyle 0<c_{i}<(2n\lambda )^{-1}}
  , so that 
  
    
      
        φ
        (
        
          
            x
          
          
            i
          
        
        )
      
    
    {\displaystyle \varphi (\mathbf {x} _{i})}
   lies on the boundary of the margin in the transformed space, and then solve

  
    
      
        
          
            
              
                b
                =
                
                  
                    w
                  
                  
                    T
                  
                
                φ
                (
                
                  
                    x
                  
                  
                    i
                  
                
                )
                −
                
                  y
                  
                    i
                  
                
              
              
                
                =
                
                  [
                  
                    
                      ∑
                      
                        j
                        =
                        1
                      
                      
                        n
                      
                    
                    
                      c
                      
                        j
                      
                    
                    
                      y
                      
                        j
                      
                    
                    φ
                    (
                    
                      
                        x
                      
                      
                        j
                      
                    
                    )
                    ⋅
                    φ
                    (
                    
                      
                        x
                      
                      
                        i
                      
                    
                    )
                  
                  ]
                
                −
                
                  y
                  
                    i
                  
                
              
            
            
              
              
                
                =
                
                  [
                  
                    
                      ∑
                      
                        j
                        =
                        1
                      
                      
                        n
                      
                    
                    
                      c
                      
                        j
                      
                    
                    
                      y
                      
                        j
                      
                    
                    k
                    (
                    
                      
                        x
                      
                      
                        j
                      
                    
                    ,
                    
                      
                        x
                      
                      
                        i
                      
                    
                    )
                  
                  ]
                
                −
                
                  y
                  
                    i
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}b=\mathbf {w} ^{T}\varphi (\mathbf {x} _{i})-y_{i}&=\left[\sum _{j=1}^{n}c_{j}y_{j}\varphi (\mathbf {x} _{j})\cdot \varphi (\mathbf {x} _{i})\right]-y_{i}\\&=\left[\sum _{j=1}^{n}c_{j}y_{j}k(\mathbf {x} _{j},\mathbf {x} _{i})\right]-y_{i}.\end{aligned}}}
  Finally,

  
    
      
        
          z
        
        ↦
        sgn
        ⁡
        (
        
          
            w
          
          
            T
          
        
        φ
        (
        
          z
        
        )
        −
        b
        )
        =
        sgn
        ⁡
        
          (
          
            
              [
              
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  c
                  
                    i
                  
                
                
                  y
                  
                    i
                  
                
                k
                (
                
                  
                    x
                  
                  
                    i
                  
                
                ,
                
                  z
                
                )
              
              ]
            
            −
            b
          
          )
        
        .
      
    
    {\displaystyle \mathbf {z} \mapsto \operatorname {sgn}(\mathbf {w} ^{T}\varphi (\mathbf {z} )-b)=\operatorname {sgn} \left(\left[\sum _{i=1}^{n}c_{i}y_{i}k(\mathbf {x} _{i},\mathbf {z} )\right]-b\right).}

Modern methods
Recent algorithms for finding the SVM classifier include sub-gradient descent and coordinate descent. Both techniques have proven to offer significant advantages over the traditional approach when dealing with large, sparse datasets—sub-gradient methods are especially efficient when there are many training examples, and coordinate descent when the dimension of the feature space is high.

Sub-gradient descent
Sub-gradient descent algorithms for the SVM work directly with the expression

  
    
      
        f
        (
        
          w
        
        ,
        b
        )
        =
        
          [
          
            
              
                1
                n
              
            
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            max
            
              (
              
                0
                ,
                1
                −
                
                  y
                  
                    i
                  
                
                (
                
                  
                    w
                  
                  
                    T
                  
                
                
                  
                    x
                  
                  
                    i
                  
                
                −
                b
                )
              
              )
            
          
          ]
        
        +
        λ
        ‖
        
          w
        
        
          ‖
          
            2
          
        
        .
      
    
    {\displaystyle f(\mathbf {w} ,b)=\left[{\frac {1}{n}}\sum _{i=1}^{n}\max \left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\right)\right]+\lambda \|\mathbf {w} \|^{2}.}
  Note that 
  
    
      
        f
      
    
    {\displaystyle f}
   is a convex function of 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   and 
  
    
      
        b
      
    
    {\displaystyle b}
  . As such, traditional gradient descent (or SGD) methods can be adapted, where instead of taking a step in the direction of the function's gradient, a step is taken in the direction of a vector selected from the function's sub-gradient. This approach has the advantage that, for certain implementations, the number of iterations does not scale with 
  
    
      
        n
      
    
    {\displaystyle n}
  , the number of data points.

Coordinate descent
Coordinate descent algorithms for the SVM work from the dual problem

  
    
      
        
          maximize
        
        
        
        f
        (
        
          c
          
            1
          
        
        …
        
          c
          
            n
          
        
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          c
          
            i
          
        
        −
        
          
            1
            2
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        
          y
          
            i
          
        
        
          c
          
            i
          
        
        (
        
          x
          
            i
          
        
        ⋅
        
          x
          
            j
          
        
        )
        
          y
          
            j
          
        
        
          c
          
            j
          
        
        ,
      
    
    {\displaystyle {\text{maximize}}\,\,f(c_{1}\ldots c_{n})=\sum _{i=1}^{n}c_{i}-{\frac {1}{2}}\sum _{i=1}^{n}\sum _{j=1}^{n}y_{i}c_{i}(x_{i}\cdot x_{j})y_{j}c_{j},}
  

  
    
      
        
          subject to 
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          c
          
            i
          
        
        
          y
          
            i
          
        
        =
        0
        ,
        
        
          and 
        
        0
        ≤
        
          c
          
            i
          
        
        ≤
        
          
            1
            
              2
              n
              λ
            
          
        
        
        
          for all 
        
        i
        .
      
    
    {\displaystyle {\text{subject to }}\sum _{i=1}^{n}c_{i}y_{i}=0,\,{\text{and }}0\leq c_{i}\leq {\frac {1}{2n\lambda }}\;{\text{for all }}i.}
  For each 
  
    
      
        i
        ∈
        {
        1
        ,
        
        …
        ,
        
        n
        }
      
    
    {\displaystyle i\in \{1,\,\ldots ,\,n\}}
  , iteratively, the coefficient 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
   is adjusted in the direction of 
  
    
      
        ∂
        f
        
          /
        
        ∂
        
          c
          
            i
          
        
      
    
    {\displaystyle \partial f/\partial c_{i}}
  . Then, the resulting vector of coefficients 
  
    
      
        (
        
          c
          
            1
          
          ′
        
        ,
        
        …
        ,
        
        
          c
          
            n
          
          ′
        
        )
      
    
    {\displaystyle (c_{1}',\,\ldots ,\,c_{n}')}
   is projected onto the nearest vector of coefficients that satisfies the given constraints. (Typically Euclidean distances are used.) The process is then repeated until a near-optimal vector of coefficients is obtained. The resulting algorithm is extremely fast in practice, although few performance guarantees have been proven.

Empirical risk minimization
The soft-margin support vector machine described above is an example of an empirical risk minimization (ERM) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. This perspective can provide further insight into how and why SVMs work, and allow us to better analyze their statistical properties.

Risk minimization
In supervised learning, one is given a set of training examples 
  
    
      
        
          X
          
            1
          
        
        …
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{1}\ldots X_{n}}
   with labels 
  
    
      
        
          y
          
            1
          
        
        …
        
          y
          
            n
          
        
      
    
    {\displaystyle y_{1}\ldots y_{n}}
  , and wishes to predict 
  
    
      
        
          y
          
            n
            +
            1
          
        
      
    
    {\displaystyle y_{n+1}}
   given 
  
    
      
        
          X
          
            n
            +
            1
          
        
      
    
    {\displaystyle X_{n+1}}
  . To do so one forms a hypothesis, 
  
    
      
        f
      
    
    {\displaystyle f}
  , such that 
  
    
      
        f
        (
        
          X
          
            n
            +
            1
          
        
        )
      
    
    {\displaystyle f(X_{n+1})}
   is a ""good"" approximation of 
  
    
      
        
          y
          
            n
            +
            1
          
        
      
    
    {\displaystyle y_{n+1}}
  . A ""good"" approximation is usually defined with the help of a loss function, 
  
    
      
        ℓ
        (
        y
        ,
        z
        )
      
    
    {\displaystyle \ell (y,z)}
  , which characterizes how bad 
  
    
      
        z
      
    
    {\displaystyle z}
   is as a prediction of 
  
    
      
        y
      
    
    {\displaystyle y}
  . We would then like to choose a hypothesis that minimizes the expected risk:

  
    
      
        ε
        (
        f
        )
        =
        
          E
        
        
          [
          
            ℓ
            (
            
              y
              
                n
                +
                1
              
            
            ,
            f
            (
            
              X
              
                n
                +
                1
              
            
            )
            )
          
          ]
        
        .
      
    
    {\displaystyle \varepsilon (f)=\mathbb {E} \left[\ell (y_{n+1},f(X_{n+1}))\right].}
  In most cases, we don't know the joint distribution of 
  
    
      
        
          X
          
            n
            +
            1
          
        
        ,
        
        
          y
          
            n
            +
            1
          
        
      
    
    {\displaystyle X_{n+1},\,y_{n+1}}
   outright. In these cases, a common strategy is to choose the hypothesis that minimizes the empirical risk:

  
    
      
        
          
            
              ε
              ^
            
          
        
        (
        f
        )
        =
        
          
            1
            n
          
        
        
          ∑
          
            k
            =
            1
          
          
            n
          
        
        ℓ
        (
        
          y
          
            k
          
        
        ,
        f
        (
        
          X
          
            k
          
        
        )
        )
        .
      
    
    {\displaystyle {\hat {\varepsilon }}(f)={\frac {1}{n}}\sum _{k=1}^{n}\ell (y_{k},f(X_{k})).}
  Under certain assumptions about the sequence of random variables 
  
    
      
        
          X
          
            k
          
        
        ,
        
        
          y
          
            k
          
        
      
    
    {\displaystyle X_{k},\,y_{k}}
   (for example, that they are generated by a finite Markov process), if the set of hypotheses being considered is small enough, the minimizer of the empirical risk will closely approximate the minimizer of the expected risk as 
  
    
      
        n
      
    
    {\displaystyle n}
   grows large. This approach is called empirical risk minimization, or ERM.

Regularization and stability
In order for the minimization problem to have a well-defined solution, we have to place constraints on the set 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
   of hypotheses being considered. If 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
   is a normed space (as is the case for SVM), a particularly effective technique is to consider only those hypotheses 
  
    
      
        f
      
    
    {\displaystyle f}
   for which 
  
    
      
        ‖
        f
        
          ‖
          
            
              H
            
          
        
        <
        k
      
    
    {\displaystyle \lVert f\rVert _{\mathcal {H}}<k}
   . This is equivalent to imposing a regularization penalty 
  
    
      
        
          
            R
          
        
        (
        f
        )
        =
        
          λ
          
            k
          
        
        ‖
        f
        
          ‖
          
            
              H
            
          
        
      
    
    {\displaystyle {\mathcal {R}}(f)=\lambda _{k}\lVert f\rVert _{\mathcal {H}}}
  , and solving the new optimization problem

  
    
      
        
          
            
              f
              ^
            
          
        
        =
        
          a
          r
          g
        
        
          min
          
            f
            ∈
            
              
                H
              
            
          
        
        
          
            
              ε
              ^
            
          
        
        (
        f
        )
        +
        
          
            R
          
        
        (
        f
        )
        .
      
    
    {\displaystyle {\hat {f}}=\mathrm {arg} \min _{f\in {\mathcal {H}}}{\hat {\varepsilon }}(f)+{\mathcal {R}}(f).}
  This approach is called Tikhonov regularization.
More generally, 
  
    
      
        
          
            R
          
        
        (
        f
        )
      
    
    {\displaystyle {\mathcal {R}}(f)}
   can be some measure of the complexity of the hypothesis 
  
    
      
        f
      
    
    {\displaystyle f}
  , so that simpler hypotheses are preferred.

SVM and the hinge loss
Recall that the (soft-margin) SVM classifier 
  
    
      
        
          
            
              
                w
              
              ^
            
          
        
        ,
        b
        :
        
          x
        
        ↦
        sgn
        ⁡
        (
        
          
            
              
                
                  w
                
                ^
              
            
          
          
            T
          
        
        
          x
        
        −
        b
        )
      
    
    {\displaystyle {\hat {\mathbf {w} }},b:\mathbf {x} \mapsto \operatorname {sgn}({\hat {\mathbf {w} }}^{T}\mathbf {x} -b)}
   is chosen to minimize the following expression:

  
    
      
        
          [
          
            
              
                1
                n
              
            
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            max
            
              (
              
                0
                ,
                1
                −
                
                  y
                  
                    i
                  
                
                (
                
                  
                    w
                  
                  
                    T
                  
                
                
                  x
                
                −
                b
                )
              
              )
            
          
          ]
        
        +
        λ
        ‖
        
          w
        
        
          ‖
          
            2
          
        
        .
      
    
    {\displaystyle \left[{\frac {1}{n}}\sum _{i=1}^{n}\max \left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} -b)\right)\right]+\lambda \|\mathbf {w} \|^{2}.}
  In light of the above discussion, we see that the SVM technique is equivalent to empirical risk minimization with Tikhonov regularization, where in this case the loss function is the hinge loss

  
    
      
        ℓ
        (
        y
        ,
        z
        )
        =
        max
        
          (
          
            0
            ,
            1
            −
            y
            z
          
          )
        
        .
      
    
    {\displaystyle \ell (y,z)=\max \left(0,1-yz\right).}
  From this perspective, SVM is closely related to other fundamental classification algorithms such as regularized least-squares and logistic regression. The difference between the three lies in the choice of loss function: regularized least-squares amounts to empirical risk minimization with the square-loss,  
  
    
      
        
          ℓ
          
            s
            q
          
        
        (
        y
        ,
        z
        )
        =
        (
        y
        −
        z
        
          )
          
            2
          
        
      
    
    {\displaystyle \ell _{sq}(y,z)=(y-z)^{2}}
  ; logistic regression employs the log-loss,

  
    
      
        
          ℓ
          
            log
          
        
        (
        y
        ,
        z
        )
        =
        ln
        ⁡
        (
        1
        +
        
          e
          
            −
            y
            z
          
        
        )
        .
      
    
    {\displaystyle \ell _{\log }(y,z)=\ln(1+e^{-yz}).}

Target functions
The difference between the hinge loss and these other loss functions is best stated in terms of target functions - the function that minimizes expected risk for a given pair of random variables 
  
    
      
        X
        ,
        
        y
      
    
    {\displaystyle X,\,y}
  .
In particular, let 
  
    
      
        
          y
          
            x
          
        
      
    
    {\displaystyle y_{x}}
   denote 
  
    
      
        y
      
    
    {\displaystyle y}
   conditional on the event that 
  
    
      
        X
        =
        x
      
    
    {\displaystyle X=x}
  .  In the classification setting, we have:

  
    
      
        
          y
          
            x
          
        
        =
        
          
            {
            
              
                
                  1
                
                
                  
                    with probability 
                  
                  
                    p
                    
                      x
                    
                  
                
              
              
                
                  −
                  1
                
                
                  
                    with probability 
                  
                  1
                  −
                  
                    p
                    
                      x
                    
                  
                
              
            
            
          
        
      
    
    {\displaystyle y_{x}={\begin{cases}1&{\text{with probability }}p_{x}\\-1&{\text{with probability }}1-p_{x}\end{cases}}}
  The optimal classifier is therefore:

  
    
      
        
          f
          
            ∗
          
        
        (
        x
        )
        =
        
          
            {
            
              
                
                  1
                
                
                  
                    if 
                  
                  
                    p
                    
                      x
                    
                  
                  ≥
                  1
                  
                    /
                  
                  2
                
              
              
                
                  −
                  1
                
                
                  
                    otherwise
                  
                
              
            
            
          
        
      
    
    {\displaystyle f^{*}(x)={\begin{cases}1&{\text{if }}p_{x}\geq 1/2\\-1&{\text{otherwise}}\end{cases}}}
  For the square-loss, the target function is the conditional expectation function, 
  
    
      
        
          f
          
            s
            q
          
        
        (
        x
        )
        =
        
          E
        
        
          [
          
            y
            
              x
            
          
          ]
        
      
    
    {\displaystyle f_{sq}(x)=\mathbb {E} \left[y_{x}\right]}
  ; For the logistic loss, it's the logit function, 
  
    
      
        
          f
          
            log
          
        
        (
        x
        )
        =
        ln
        ⁡
        
          (
          
            
              p
              
                x
              
            
            
              /
            
            (
            
              1
              −
              
                p
                
                  x
                
              
            
            )
          
          )
        
      
    
    {\displaystyle f_{\log }(x)=\ln \left(p_{x}/({1-p_{x}})\right)}
  . While both of these target functions yield the correct classifier, as 
  
    
      
        sgn
        ⁡
        (
        
          f
          
            s
            q
          
        
        )
        =
        sgn
        ⁡
        (
        
          f
          
            log
          
        
        )
        =
        
          f
          
            ∗
          
        
      
    
    {\displaystyle \operatorname {sgn}(f_{sq})=\operatorname {sgn}(f_{\log })=f^{*}}
  , they give us more information than we need. In fact, they give us enough information to completely describe the distribution of 
  
    
      
        
          y
          
            x
          
        
      
    
    {\displaystyle y_{x}}
  .
On the other hand, one can check that the target function for the hinge loss is exactly 
  
    
      
        
          f
          
            ∗
          
        
      
    
    {\displaystyle f^{*}}
  . Thus, in a sufficiently rich hypothesis space—or equivalently, for an appropriately chosen kernel—the SVM classifier will converge to the simplest function (in terms of 
  
    
      
        
          
            R
          
        
      
    
    {\displaystyle {\mathcal {R}}}
  ) that correctly classifies the data. This extends the geometric interpretation of SVM—for linear classification, the empirical risk is minimized by any function whose margins lie between the support vectors, and the simplest of these is the max-margin classifier.

Properties
SVMs belong to a family of generalized linear classifiers and can be interpreted as an extension of the perceptron. They can also be considered a special case of Tikhonov regularization. A special property is that they simultaneously minimize the empirical classification error and maximize the geometric margin; hence they are also known as maximum margin classifiers.
A comparison of the SVM to other classifiers has been made by Meyer, Leisch and Hornik.

Parameter selection
The effectiveness of SVM depends on the selection of kernel, the kernel's parameters, and soft margin parameter C.
A common choice is a Gaussian kernel, which has a single parameter 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  . The best combination of C and 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
   is often selected by a grid search with exponentially growing sequences of C and 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  , for example, 
  
    
      
        C
        ∈
        {
        
          2
          
            −
            5
          
        
        ,
        
          2
          
            −
            3
          
        
        ,
        …
        ,
        
          2
          
            13
          
        
        ,
        
          2
          
            15
          
        
        }
      
    
    {\displaystyle C\in \{2^{-5},2^{-3},\dots ,2^{13},2^{15}\}}
  ; 
  
    
      
        γ
        ∈
        {
        
          2
          
            −
            15
          
        
        ,
        
          2
          
            −
            13
          
        
        ,
        …
        ,
        
          2
          
            1
          
        
        ,
        
          2
          
            3
          
        
        }
      
    
    {\displaystyle \gamma \in \{2^{-15},2^{-13},\dots ,2^{1},2^{3}\}}
  . Typically, each combination of parameter choices is checked using cross validation, and the parameters with best cross-validation accuracy are picked. Alternatively, recent work in Bayesian optimization can be used to select C and 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
   , often requiring the evaluation of far fewer parameter combinations than grid search. The final model, which is used for testing and for classifying new data, is then trained on the whole training set using the selected parameters.

Issues
Potential drawbacks of the SVM include the following aspects:

Requires full labeling of input data
Uncalibrated class membership probabilities—SVM stems from Vapnik's theory which avoids estimating probabilities on finite data
The SVM is only directly applicable for two-class tasks. Therefore, algorithms that reduce the multi-class task to several binary problems have to be applied; see the multi-class SVM section.
Parameters of a solved model are difficult to interpret.

Extensions
Support-vector clustering (SVC)
SVC is a similar method that also builds on kernel functions but is appropriate for unsupervised learning. It is considered a fundamental method in data science.

Multiclass SVM
Multiclass SVM aims to assign labels to instances by using support-vector machines, where the labels are drawn from a finite set of several elements.
The dominant approach for doing so is to reduce the single multiclass problem into multiple binary classification problems. Common methods for such reduction include:
Building binary classifiers that distinguish between one of the labels and the rest (one-versus-all) or between every pair of classes (one-versus-one). Classification of new instances for the one-versus-all case is done by a winner-takes-all strategy, in which the classifier with the highest-output function assigns the class (it is important that the output functions be calibrated to produce comparable scores). For the one-versus-one approach, classification is done by a max-wins voting strategy, in which every classifier assigns the instance to one of the two classes, then the vote for the assigned class is increased by one vote, and finally the class with the most votes determines the instance classification.
Directed acyclic graph SVM (DAGSVM)
Error-correcting output codesCrammer and Singer proposed a multiclass SVM method which casts the multiclass classification problem into a single optimization problem, rather than decomposing it into multiple binary classification problems. See also Lee, Lin and Wahba and Van den Burg and Groenen.

Transductive support-vector machines
Transductive support-vector machines extend SVMs in that they could also treat partially labeled data in semi-supervised learning by following the principles of transduction. Here, in addition to the training set 
  
    
      
        
          
            D
          
        
      
    
    {\displaystyle {\mathcal {D}}}
  , the learner is also given a set

  
    
      
        
          
            
              D
            
          
          
            ⋆
          
        
        =
        {
        
          
            
              
                x
                →
              
            
          
          
            i
          
          
            ⋆
          
        
        ∣
        
          
            
              
                x
                →
              
            
          
          
            i
          
          
            ⋆
          
        
        ∈
        
          
            R
          
          
            p
          
        
        
          }
          
            i
            =
            1
          
          
            k
          
        
      
    
    {\displaystyle {\mathcal {D}}^{\star }=\{{\vec {x}}_{i}^{\star }\mid {\vec {x}}_{i}^{\star }\in \mathbb {R} ^{p}\}_{i=1}^{k}}
  of test examples to be classified. Formally, a transductive support-vector machine is defined by the following primal optimization problem:Minimize (in 
  
    
      
        
          
            
              
                w
                →
              
            
          
          ,
          b
          ,
          
            
              
                
                  y
                  
                    ⋆
                  
                
                →
              
            
          
        
      
    
    {\displaystyle {{\vec {w}},b,{\vec {y^{\star }}}}}
  )

  
    
      
        
          
            1
            2
          
        
        ‖
        
          
            
              w
              →
            
          
        
        
          ‖
          
            2
          
        
      
    
    {\displaystyle {\frac {1}{2}}\|{\vec {w}}\|^{2}}
  subject to (for any 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=1,\dots ,n}
   and any 
  
    
      
        j
        =
        1
        ,
        …
        ,
        k
      
    
    {\displaystyle j=1,\dots ,k}
  )

  
    
      
        
          y
          
            i
          
        
        (
        
          
            
              w
              →
            
          
        
        ⋅
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        −
        b
        )
        ≥
        1
        ,
      
    
    {\displaystyle y_{i}({\vec {w}}\cdot {\vec {x_{i}}}-b)\geq 1,}
  
  
    
      
        
          y
          
            j
          
          
            ⋆
          
        
        (
        
          
            
              w
              →
            
          
        
        ⋅
        
          
            
              
                x
                
                  j
                
                
                  ⋆
                
              
              →
            
          
        
        −
        b
        )
        ≥
        1
        ,
      
    
    {\displaystyle y_{j}^{\star }({\vec {w}}\cdot {\vec {x_{j}^{\star }}}-b)\geq 1,}
  and

  
    
      
        
          y
          
            j
          
          
            ⋆
          
        
        ∈
        {
        −
        1
        ,
        1
        }
        .
      
    
    {\displaystyle y_{j}^{\star }\in \{-1,1\}.}
  Transductive support-vector machines were introduced by Vladimir N. Vapnik in 1998.

Structured SVM
SVMs have been generalized to structured SVMs, where the label space is structured and of possibly infinite size.

Regression
A version of SVM for regression was proposed in 1996 by Vladimir N. Vapnik, Harris Drucker, Christopher J. C. Burges, Linda Kaufman and Alexander J. Smola. This method is called support-vector regression (SVR). The model produced by support-vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by SVR depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction. Another SVM version known as least-squares support-vector machine (LS-SVM) has been proposed by Suykens and Vandewalle.Training the original SVR means solving
minimize 
  
    
      
        
          
            1
            2
          
        
        ‖
        w
        
          ‖
          
            2
          
        
      
    
    {\displaystyle {\frac {1}{2}}\|w\|^{2}}
  
subject to 
  
    
      
        
          |
        
        
          y
          
            i
          
        
        −
        ⟨
        w
        ,
        
          x
          
            i
          
        
        ⟩
        −
        b
        
          |
        
        ≤
        ε
      
    
    {\displaystyle |y_{i}-\langle w,x_{i}\rangle -b|\leq \varepsilon }
  where 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   is a training sample with target value 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
  . The inner product plus intercept 
  
    
      
        ⟨
        w
        ,
        
          x
          
            i
          
        
        ⟩
        +
        b
      
    
    {\displaystyle \langle w,x_{i}\rangle +b}
   is the prediction for that sample, and 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   is a free parameter that serves as a threshold: all predictions have to be within an 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   range of the true predictions. Slack variables are usually added into the above to allow for errors and to allow approximation in the case the above problem is infeasible.

Bayesian SVM
In 2011 it was shown by Polson and Scott that the SVM admits a Bayesian interpretation through the technique of data augmentation. In this approach the SVM is viewed as a graphical model (where the parameters are connected via probability distributions). This extended view allows the application of Bayesian techniques to SVMs, such as flexible feature modeling, automatic hyperparameter tuning, and predictive uncertainty quantification. Recently, a scalable version of the Bayesian SVM was developed by Florian Wenzel, enabling the application of Bayesian SVMs to big data. Florian Wenzel developed two different versions, a variational inference (VI) scheme for the Bayesian kernel support vector machine (SVM) and a stochastic version (SVI) for the linear Bayesian SVM.

Implementation
The parameters of the maximum-margin hyperplane are derived by solving the optimization. There exist several specialized algorithms for quickly solving the quadratic programming (QP) problem that arises from SVMs, mostly relying on heuristics for breaking the problem down into smaller, more manageable chunks.
Another approach is to use an interior-point method that uses Newton-like iterations to find a solution of the Karush–Kuhn–Tucker conditions of the primal and dual problems.
Instead of solving a sequence of broken-down problems, this approach directly solves the problem altogether. To avoid solving a linear system involving the large kernel matrix, a low-rank approximation to the matrix is often used in the kernel trick.
Another common method is Platt's sequential minimal optimization (SMO) algorithm, which breaks the problem down into 2-dimensional sub-problems that are solved analytically, eliminating the need for a numerical optimization algorithm and matrix storage. This algorithm is conceptually simple, easy to implement, generally faster, and has better scaling properties for difficult SVM problems.The special case of linear support-vector machines can be solved more efficiently by the same kind of algorithms used to optimize its close cousin, logistic regression; this class of algorithms includes sub-gradient descent (e.g., PEGASOS) and coordinate descent (e.g., LIBLINEAR). LIBLINEAR has some attractive training-time properties. Each convergence iteration takes time linear in the time taken to read the train data, and the iterations also have a Q-linear convergence property, making the algorithm extremely fast.
The general kernel SVMs can also be solved more efficiently using sub-gradient descent (e.g. P-packSVM), especially when parallelization is allowed.
Kernel SVMs are available in many machine-learning toolkits, including LIBSVM, MATLAB, SAS, SVMlight, kernlab, scikit-learn, Shogun, Weka, Shark, JKernelMachines, OpenCV and others.
Preprocessing of data (standardization) is highly recommended to enhance accuracy of classification. There are a few methods of standardization, such as min-max, normalization by decimal scaling, Z-score. Subtraction of mean and division by variance of each feature is usually used for SVM.

See also
In situ adaptive tabulation
Kernel machines
Fisher kernel
Platt scaling
Polynomial kernel
Predictive analytics
Regularization perspectives on support-vector machines
Relevance vector machine, a probabilistic sparse-kernel model identical in functional form to SVM
Sequential minimal optimization
Space mapping
Winnow (algorithm)

References
Further reading
Bennett, Kristin P.; Campbell, Colin (2000). ""Support Vector Machines: Hype or Hallelujah?"" (PDF). SIGKDD Explorations. 2 (2): 1–13. doi:10.1145/380995.380999. S2CID 207753020.
Cristianini, Nello; Shawe-Taylor, John (2000). An Introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press. ISBN 0-521-78019-5.
Fradkin, Dmitriy; Muchnik, Ilya (2006). ""Support Vector Machines for Classification"" (PDF).  In Abello, J.; Carmode, G. (eds.). Discrete Methods in Epidemiology. DIMACS Series in Discrete Mathematics and Theoretical Computer Science. 70. pp. 13–20.
Ivanciuc, Ovidiu (2007). ""Applications of Support Vector Machines in Chemistry"" (PDF). Reviews in Computational Chemistry. 23: 291–400. doi:10.1002/9780470116449.ch6. ISBN 9780470116449.
James, Gareth; Witten, Daniela; Hastie, Trevor; Tibshirani, Robert (2013). ""Support Vector Machines"" (PDF). An Introduction to Statistical Learning : with Applications in R. New York: Springer. pp. 337–372. ISBN 978-1-4614-7137-0.
Schölkopf, Bernhard; Smola, Alexander J. (2002). Learning with Kernels. Cambridge, MA: MIT Press. ISBN 0-262-19475-9.
Steinwart, Ingo; Christmann, Andreas (2008). Support Vector Machines. New York: Springer. ISBN 978-0-387-77241-7.
Theodoridis, Sergios; Koutroumbas, Konstantinos (2009). Pattern Recognition (4th ed.). Academic Press. ISBN 978-1-59749-272-0.

External links
libsvm, LIBSVM is a popular library of SVM learners
liblinear is a library for large linear classification including some SVMs
SVM light is a collection of software tools for learning and classification using SVM
SVMJS live demo is a GUI demo for JavaScript implementation of SVMs",https://en.wikipedia.org/wiki/Support-vector_machine,"['All articles with specifically marked weasel-worded phrases', 'All articles with unsourced statements', 'Articles with short description', 'Articles with specifically marked weasel-worded phrases from May 2018', 'Articles with unsourced statements from June 2013', 'Articles with unsourced statements from March 2017', 'Articles with unsourced statements from March 2018', 'Classification algorithms', 'Short description matches Wikidata', 'Statistical classification', 'Support vector machines', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers']",Data Science
172,Statistics,"Statistics is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as ""all people living in a country"" or ""every atom composing a crystal"". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.
Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.
A standard statistical procedure involves the collection of data leading to test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a ""false positive"") and Type II errors (null hypothesis fails to be rejected and an actual relationship between populations is missed giving a ""false negative""). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.
The earliest writings on probability and statistics, statistical methods drawing from probability theory, date back to Arab mathematicians and cryptographers, notably Al-Khalil (717–786) and Al-Kindi (801–873). In the 18th century, statistics also started to draw heavily from calculus. In more recent years statistics has relied more on statistical software.

Introduction
Statistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data, or as a branch of mathematics. Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is concerned with the use of data in the context of uncertainty and decision making in the face of uncertainty.In applying statistics to a problem, it is common practice to start with a population or process to be studied. Populations can be diverse topics such as ""all people living in a country"" or ""every atom composing a crystal"". Ideally, statisticians compile data about the entire population (an operation called census). This may be organized by governmental statistical institutes. Descriptive statistics can be used to summarize the population data. Numerical descriptors include mean and standard deviation for continuous data (like income), while frequency and percentage are more useful in terms of describing categorical data (like education).
When a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, drawing the sample contains an element of randomness; hence, the numerical descriptors from the sample are also prone to uncertainty. To draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented while accounting for randomness. These inferences may take the form of answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation), and modeling relationships within the data (for example, using regression analysis).  Inference can extend to forecasting, prediction, and estimation of unobserved values either in or associated with the population being studied. It can include extrapolation and interpolation of time series or spatial data, and data mining.

Mathematical statistics
Mathematical statistics is the application of mathematics to statistics. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.

History
The earliest writings on probability and statistics date back to Arab mathematicians and cryptographers, during the Islamic Golden Age between the 8th and 13th centuries. Al-Khalil (717–786) wrote the Book of Cryptographic Messages, which contains the first use of permutations and combinations, to list all possible Arabic words with and without vowels. The earliest book on statistics is the 9th-century treatise Manuscript on Deciphering Cryptographic Messages, written by Arab scholar Al-Kindi (801–873). In his book, Al-Kindi gave a detailed description of how to use statistics and frequency analysis to decipher encrypted messages. This text laid the foundations for statistics and cryptanalysis. Al-Kindi also made the earliest known use of statistical inference, while he and later Arab cryptographers developed the early statistical methods for decoding encrypted messages. Ibn Adlan (1187–1268) later made an important contribution, on the use of sample size in frequency analysis.The earliest European writing on statistics dates back to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt. Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.
The mathematical foundations of modern statistics were laid in the 17th century with the development of the probability theory by Gerolamo Cardano, Blaise Pascal and Pierre de Fermat. Mathematical probability theory arose from the study of games of chance, although the concept of probability was already examined in medieval law and by philosophers such as Juan Caramuel. The method of least squares was first described by Adrien-Marie Legendre in 1805.

The modern field of statistics emerged in the late 19th and early 20th century in three stages. The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics—height, weight, eyelash length among others. Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment, the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things. Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.Ronald Fisher coined the term null hypothesis during the Lady tasting tea experiment, which ""is never proved or established, but is possibly disproved, in the course of experimentation"".The second wave of the 1910s and 20s was initiated by William Sealy Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance (which was the first to use the statistical term, variance), his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments, where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information. In his 1930 book The Genetical Theory of Natural Selection, he applied statistics to various biological concepts such as Fisher's principle (which A. W. F. Edwards called ""probably the most celebrated argument in evolutionary biology"") and Fisherian runaway, a concept in sexual selection about a positive feedback runaway affect found in evolution.
The final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of ""Type II"" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.Today, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research for example on the problem of how to analyze big data.

Statistical data
Data collection
Sampling
When full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting through statistical models.
To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design for experiments that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.
Sampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population.

Experimental and observational studies
A common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective.
An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data—like natural experiments and observational studies—for which a statistician would use a modified, more structured estimation method (e.g., Difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.

Experiments
The basic steps of a statistical experiment are:

Planning the research, including finding the number of replicates of the study, using the following information:  preliminary estimates regarding the size of treatment effects, alternative hypotheses, and the estimated experimental variability. Consideration of the selection of experimental subjects and the ethics of research is necessary. Statisticians recommend that experiments compare (at least) one new treatment with a standard treatment or control, to allow an unbiased estimate of the difference in treatment effects.
Design of experiments, using blocking to reduce the influence of confounding variables, and randomized assignment of treatments to subjects to allow unbiased estimates of treatment effects and experimental error. At this stage, the experimenters and statisticians write the experimental protocol that will guide the performance of the experiment and which specifies the primary analysis of the experimental data.
Performing the experiment following the experimental protocol and analyzing the data following the experimental protocol.
Further examining the data set in secondary analyses, to suggest new hypotheses for future study.
Documenting and presenting the results of the study.Experiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.

Observational study
An example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a cohort study, and then look for the number of cases of lung cancer in each group. A case-control study is another type of observational study in which people with and without the outcome of interest (e.g. lung cancer) are invited to participate and their exposure histories are collected.

Types of data
Various attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one (injective) transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.
Because variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating point computation. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.
Other categorizations have been proposed. For example, Mosteller and Tukey (1977) distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data. (See also: Chrisman (1998), van den Berg (1991).)
The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. ""The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer.""

Methods
Descriptive statistics
A descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarizes features of a collection of information, while descriptive statistics in the mass noun sense is the process of using and analyzing those statistics. Descriptive statistics is distinguished from inferential statistics (or inductive statistics), in that descriptive statistics aims to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent.

Inferential statistics
Statistical inference is the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.  It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.

Terminology and theory of inferential statistics
Statistics, estimators and pivotal quantities
Consider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables. The population being examined is described by a probability distribution that may have unknown parameters.
A statistic is a random variable that is a function of the random sample, but not a is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.
A random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value.
Between two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.
Other desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.
This still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.

Null hypothesis and alternative hypothesis
Interpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.The best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence ""beyond a reasonable doubt"". However, ""failure to reject H0"" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not ""prove"" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.
What statisticians call an alternative hypothesis is simply a hypothesis that contradicts the null hypothesis.

Error
Working from a null hypothesis, two broad categories of error are recognized:

Type I errors where the null hypothesis is falsely rejected, giving a ""false positive"".
Type II errors where the null hypothesis fails to be rejected and an actual difference between populations is missed, giving a ""false negative"".Standard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.
A statistical error is the amount by which an observation differs from its expected value, a residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).
Mean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.

Many statistical methods seek to minimize the residual sum of squares, and these are called ""methods of least squares"" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.
Measurement processes that generate statistical data are also subject to error.  Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.

Interval estimation
Most studies only sample part of a population, so results don't fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable.  Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by ""probability"", that is as a Bayesian probability.
In principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.

Significance
Statistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).

The standard approach is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.
Referring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.
Although in principle the acceptable level of statistical significance may be subject to debate, the significance level is the largest p-value that allows the test to reject the null hypothesis. This test is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the significance level, the lower the probability of committing type I error.
Some problems are usually associated with this framework (See criticism of hypothesis testing):

A difference that is highly statistically significant can still be of no practical significance, but it is possible to properly formulate tests to account for this. One response involves going beyond reporting only the significance level to include the p-value when reporting whether a hypothesis is rejected or accepted. The p-value, however, does not indicate the size or importance of the observed effect and can also seem to exaggerate the importance of minor differences in large studies. A better and increasingly common approach is to report confidence intervals. Although these are produced from the same calculations as those of hypothesis tests or p-values, they describe both the size of the effect and the uncertainty surrounding it.
Fallacy of the transposed conditional, aka prosecutor's fallacy: criticisms arise because the hypothesis testing approach forces one hypothesis (the null hypothesis) to be favored, since what is being evaluated is the probability of the observed result given the null hypothesis and not probability of the null hypothesis given the observed result. An alternative to this approach is offered by Bayesian inference, although it requires establishing a prior probability.
Rejecting the null hypothesis does not automatically prove the alternative hypothesis.
As everything in inferential statistics it relies on sample size, and therefore under fat tails p-values may be seriously mis-computed.

Examples
Some well-known statistical tests and procedures are:

Exploratory data analysis
Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.

Misuse
Misuse of statistics can produce subtle but serious errors in description and interpretation—subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.
Even when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data—which measures the extent to which a trend could be caused by random variation in the sample—may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.
There is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter. A mistrust and misunderstanding of statistics is associated with the quotation, ""There are three kinds of lies: lies, damned lies, and statistics"". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).Ways to avoid misuse of statistics include using proper diagrams and avoiding bias. Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias. Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs. Unfortunately, most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented. To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole. According to Huff, ""The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism.""To assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:
Who says so? (Does he/she have an axe to grind?)
How does he/she know? (Does he/she have the resources to know the facts?)
What's missing? (Does he/she give us a complete picture?)
Did someone change the subject? (Does he/she offer us the right answer to the wrong problem?)
Does it make sense? (Is his/her conclusion logical and consistent with what we already know?)

Misinterpretation: correlation
The concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables.

Applications
Applied statistics, theoretical statistics and mathematical statistics
Applied statistics comprises descriptive statistics and the application of inferential statistics. Theoretical statistics concerns the logical arguments underlying justification of approaches to statistical inference, as well as encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.
Statistical consultants can help organizations and companies that don't have in-house expertise relevant to their particular questions.

Machine learning and data mining
Machine learning models are statistical and probabilistic models that capture patterns in the data through use of computational algorithms.

Statistics in academia
Statistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Business statistics applies statistical methods in econometrics, auditing and production and operations, including services improvement and marketing research. A study of two journals in tropical biology found that the 12 most frequent statistical tests are: Analysis of Variance (ANOVA), Chi-Square Test, Student’s T Test, Linear Regression, Pearson’s Correlation Coefficient, Mann-Whitney U Test, Kruskal-Wallis Test, Shannon’s Diversity Index, Tukey's Test, Cluster Analysis, Spearman’s Rank Correlation Test and Principal Component Analysis.A typical statistics course covers descriptive statistics, probability, binomial and normal distributions, test of hypotheses and confidence intervals, linear regression, and correlation. Modern fundamental statistical courses for undergraduate students focus on correct test selection, results interpretation, and use of free statistics software.

Statistical computing
The rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.
Increased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with a new emphasis on ""experimental"" and ""empirical"" statistics. A large number of both general and special purpose statistical software are now available. Examples of available software capable of complex statistical computation include programs such as Mathematica, SAS, SPSS, and R.

Business statistics
In business, ""statistics"" is a widely used management- and decision support tool.
It is particularly applied in financial management, marketing management, and production, services and operations management .

Statistics is also heavily used in management accounting and auditing. 
The discipline of Management Science formalizes the use of statistics, and other mathematics, in business.
(Econometrics is the application of statistical methods to economic data in order to give empirical content to economic relationships.)
A typical ""Business Statistics"" course is intended for business majors, 
and covers  
descriptive statistics (collection, description, analysis, and summary of data), 
probability (typically the binomial and normal distributions), 
test of hypotheses and confidence intervals, linear regression, and correlation; 
(follow-on) courses may include forecasting, time series, decision trees, multiple linear regression, and other topics from business analytics more generally.
See also Business mathematics § University level.
Professional certification programs, such as the CFA, often include topics in statistics.

Statistics applied to mathematics or the arts
Traditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was ""required learning"" in most sciences. This tradition has changed with the use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically. Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.

In number theory, scatter plots of data generated by a distribution function may be transformed with familiar tools used in statistics to reveal underlying patterns, which may then lead to hypotheses.
Methods of statistics including predictive methods in forecasting are combined with chaos theory and fractal geometry to create video works that are considered to have great beauty.
The process art of Jackson Pollock relied on artistic experiments whereby underlying distributions in nature were artistically revealed. With the advent of computers, statistical methods were applied to formalize such distribution-driven natural processes to make and analyze moving video art.
Methods of statistics may be used predicatively in performance art, as in a card trick based on a Markov process that only works some of the time, the occasion of which can be predicted using statistical methodology.
Statistics can be used to predicatively create art, as in the statistical or stochastic music invented by Iannis Xenakis, where the music is performance-specific. Though this type of artistry does not always come out as expected, it does behave in ways that are predictable and tunable using statistics.

Specialized disciplines
Statistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:

In addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:

Statistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions. In these roles, it is a key tool, and perhaps the only reliable tool.

See also
Foundations and major areas of statistics

References
Further reading
Lydia Denworth, ""A Significant Problem: Standard scientific methods are under fire. Will anything change?"", Scientific American, vol. 321, no. 4 (October 2019), pp. 62–67. ""The use of p values for nearly a century [since 1925] to determine statistical significance of experimental results has contributed to an illusion of certainty and [to] reproducibility crises in many scientific fields. There is growing determination to reform statistical analysis... Some [researchers] suggest changing statistical methods, whereas others would do away with a threshold for defining ""significant"" results."" (p. 63.)
Barbara Illowsky; Susan Dean (2014). Introductory Statistics. OpenStax CNX. ISBN 9781938168208.
Stockburger, David W. ""Introductory Statistics: Concepts, Models, and Applications"". Missouri State University (3rd Web ed.). Archived from the original on 28 May 2020.
OpenIntro Statistics Archived 2019-06-16 at the Wayback Machine, 3rd edition by Diez, Barr, and Cetinkaya-Rundel
Stephen Jones, 2010. Statistics in Psychology: Explanations without Equations. Palgrave Macmillan. ISBN 9781137282392.
Cohen, J (1990). ""Things I have learned (so far)"" (PDF). American Psychologist. 45: 1304–1312. doi:10.1037/0003-066x.45.12.1304. Archived from the original (PDF) on 2017-10-18.
Gigerenzer, G (2004). ""Mindless statistics"". Journal of Socio-Economics. 33: 587–606. doi:10.1016/j.socec.2004.09.033.
Ioannidis, J.P.A. (2005). ""Why most published research findings are false"". PLoS Medicine. 2: 696–701. doi:10.1371/journal.pmed.0040168. PMC 1855693. PMID 17456002.

External links

(Electronic Version): TIBCO Software Inc. (2020). Data Science Textbook.
Online Statistics Education: An Interactive Multimedia Course of Study. Developed by Rice University (Lead Developer), University of Houston Clear Lake, Tufts University, and National Science Foundation.
UCLA Statistical Computing Resources
Philosophy of Statistics from the Stanford Encyclopedia of Philosophy",https://en.wikipedia.org/wiki/Statistics,"['All articles needing additional references', 'All articles with specifically marked weasel-worded phrases', 'All articles with unsourced statements', 'Arab inventions', 'Articles needing additional references from December 2020', 'Articles with short description', 'Articles with specifically marked weasel-worded phrases from April 2014', 'Articles with unsourced statements from April 2015', 'Articles with unsourced statements from February 2015', 'Articles with unsourced statements from March 2013', 'Articles with unsourced statements from September 2018', 'CS1 maint: extra text: authors list', 'Data', 'Formal sciences', 'Information', 'Mathematical and quantitative methods (economics)', 'Pages using Sister project links with default search', 'Research methods', 'Short description is different from Wikidata', 'Statistics', 'Webarchive template wayback links', 'Wikipedia articles needing clarification from October 2016', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers', 'Wikipedia articles with NARA identifiers', 'Wikipedia articles with NDL identifiers', 'Wikipedia articles with multiple identifiers']",Data Science
173,Support-vector machine,"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Vapnik et al., 1997), SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik and Chervonenkis (1974) and Vapnik (1982, 1995). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support-vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data, and is one of the most widely used clustering algorithms in industrial applications.

Motivation
Classifying data is a common task in machine learning.
Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support-vector machines, a data point is viewed as a 
  
    
      
        p
      
    
    {\displaystyle p}
  -dimensional vector (a list of 
  
    
      
        p
      
    
    {\displaystyle p}
   numbers), and we want to know whether we can separate such points with a 
  
    
      
        (
        p
        −
        1
        )
      
    
    {\displaystyle (p-1)}
  -dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability.

Definition
More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier.

Whereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products of pairs of input data vectors may be computed easily in terms of the variables in the original space, by defining them in terms of a kernel function 
  
    
      
        k
        (
        x
        ,
        y
        )
      
    
    {\displaystyle k(x,y)}
   selected to suit the problem. The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant, where such a set of vectors is an orthogonal (and thus minimal) set of vectors that defines a hyperplane. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters 
  
    
      
        
          α
          
            i
          
        
      
    
    {\displaystyle \alpha _{i}}
   of images of feature vectors 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   that occur in the data base. With this choice of a hyperplane, the points 
  
    
      
        x
      
    
    {\displaystyle x}
   in the feature space that are mapped into the hyperplane are defined by the relation 
  
    
      
        
          
            ∑
            
              i
            
          
          
            α
            
              i
            
          
          k
          (
          
            x
            
              i
            
          
          ,
          x
          )
          =
          
            constant
          
          .
        
      
    
    {\displaystyle \textstyle \sum _{i}\alpha _{i}k(x_{i},x)={\text{constant}}.}
    Note that if 
  
    
      
        k
        (
        x
        ,
        y
        )
      
    
    {\displaystyle k(x,y)}
   becomes small as 
  
    
      
        y
      
    
    {\displaystyle y}
   grows further away from 
  
    
      
        x
      
    
    {\displaystyle x}
  , each term in the sum measures the degree of closeness of the test point 
  
    
      
        x
      
    
    {\displaystyle x}
   to the corresponding data base point 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
  . In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in one or the other of the sets to be discriminated. Note the fact that the set of points 
  
    
      
        x
      
    
    {\displaystyle x}
   mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets that are not convex at all in the original space.

Applications
SVMs can be used to solve various real-world problems:

SVMs are helpful in text and hypertext categorization, as their application can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings. Some methods for shallow semantic parsing are based on support vector machines.
Classification of images can also be performed using SVMs. Experimental results show that SVMs achieve significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback. This is also true for image segmentation systems, including those using a modified version SVM that uses the privileged approach as suggested by Vapnik.
Classification of satellite data like SAR data using supervised SVM.
Hand-written characters can be recognized using SVM.
The SVM algorithm has been widely applied in the biological and other sciences.  They have been used to classify proteins with up to 90% of the compounds classified correctly. Permutation tests based on SVM weights have been suggested as a mechanism for interpretation of SVM models. Support-vector machine weights have also been used to interpret SVM models in the past. Posthoc interpretation of support-vector machine models in order to identify features used by the model to make predictions is a relatively new area of research with special significance in the biological sciences.

History
The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963. In 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes. The current standard incarnation (soft margin) was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.

Linear SVM
We are given a training dataset of 
  
    
      
        n
      
    
    {\displaystyle n}
   points of the form

  
    
      
        (
        
          
            x
          
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        ,
        …
        ,
        (
        
          
            x
          
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
        ,
      
    
    {\displaystyle (\mathbf {x} _{1},y_{1}),\ldots ,(\mathbf {x} _{n},y_{n}),}
  where the 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
   are either 1 or −1, each indicating the class to which the point 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   belongs. Each 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   is a 
  
    
      
        p
      
    
    {\displaystyle p}
  -dimensional real vector. We want to find the ""maximum-margin hyperplane"" that divides the group of points 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   for which 
  
    
      
        
          y
          
            i
          
        
        =
        1
      
    
    {\displaystyle y_{i}=1}
   from the group of points for which 
  
    
      
        
          y
          
            i
          
        
        =
        −
        1
      
    
    {\displaystyle y_{i}=-1}
  , which is defined so that the distance between the hyperplane and the nearest point 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   from either group is maximized.
Any hyperplane can be written as the set of points 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   satisfying

  
    
      
        
          
            w
          
          
            T
          
        
        
          x
        
        −
        b
        =
        0
        ,
      
    
    {\displaystyle \mathbf {w} ^{T}\mathbf {x} -b=0,}
  where 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   is the (not necessarily normalized) normal vector to the hyperplane. This is much like Hesse normal form, except that 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   is not necessarily a unit vector. The parameter 
  
    
      
        
          
            
              b
              
                ‖
                
                  w
                
                ‖
              
            
          
        
      
    
    {\displaystyle {\tfrac {b}{\|\mathbf {w} \|}}}
   determines the offset of the hyperplane from the origin along the normal vector 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
  .

Hard-margin
If the training data is linearly separable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the ""margin"", and the maximum-margin hyperplane is the hyperplane that lies halfway between them. With a normalized or standardized dataset, these hyperplanes can be described by the equations

  
    
      
        
          
            w
          
          
            T
          
        
        
          x
        
        −
        b
        =
        1
      
    
    {\displaystyle \mathbf {w} ^{T}\mathbf {x} -b=1}
   (anything on or above this boundary is of one class, with label 1)and

  
    
      
        
          
            w
          
          
            T
          
        
        
          x
        
        −
        b
        =
        −
        1
      
    
    {\displaystyle \mathbf {w} ^{T}\mathbf {x} -b=-1}
   (anything on or below this boundary is of the other class, with label −1).Geometrically, the distance between these two hyperplanes is 
  
    
      
        
          
            
              2
              
                ‖
                
                  w
                
                ‖
              
            
          
        
      
    
    {\displaystyle {\tfrac {2}{\|\mathbf {w} \|}}}
  , so to maximize the distance between the planes we want to minimize 
  
    
      
        ‖
        
          w
        
        ‖
      
    
    {\displaystyle \|\mathbf {w} \|}
  . The distance is computed using the distance from a point to a plane equation. We also have to prevent data points from falling into the margin, we add the following constraint: for each 
  
    
      
        i
      
    
    {\displaystyle i}
   either

  
    
      
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
        ≥
        1
      
    
    {\displaystyle \mathbf {w} ^{T}\mathbf {x} _{i}-b\geq 1}
  , if 
  
    
      
        
          y
          
            i
          
        
        =
        1
      
    
    {\displaystyle y_{i}=1}
  ,or

  
    
      
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
        ≤
        −
        1
      
    
    {\displaystyle \mathbf {w} ^{T}\mathbf {x} _{i}-b\leq -1}
  , if 
  
    
      
        
          y
          
            i
          
        
        =
        −
        1
      
    
    {\displaystyle y_{i}=-1}
  .These constraints state that each data point must lie on the correct side of the margin.
This can be rewritten as

  
    
      
        
          y
          
            i
          
        
        (
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
        )
        ≥
        1
        ,
        
        
           for all 
        
        1
        ≤
        i
        ≤
        n
        .
        
        
        (
        1
        )
      
    
    {\displaystyle y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\geq 1,\quad {\text{ for all }}1\leq i\leq n.\qquad \qquad (1)}
  We can put this together to get the optimization problem:

""Minimize 
  
    
      
        ‖
        
          w
        
        ‖
      
    
    {\displaystyle \|\mathbf {w} \|}
   subject to 
  
    
      
        
          y
          
            i
          
        
        (
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
        )
        ≥
        1
      
    
    {\displaystyle y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\geq 1}
   for 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=1,\ldots ,n}
  .""The 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   and 
  
    
      
        b
      
    
    {\displaystyle b}
   that solve this problem determine our classifier, 
  
    
      
        
          x
        
        ↦
        sgn
        ⁡
        (
        
          
            w
          
          
            T
          
        
        
          x
        
        −
        b
        )
      
    
    {\displaystyle \mathbf {x} \mapsto \operatorname {sgn}(\mathbf {w} ^{T}\mathbf {x} -b)}
   where 
  
    
      
        sgn
        ⁡
        (
        ⋅
        )
      
    
    {\displaystyle \operatorname {sgn}(\cdot )}
   is the sign function.
An important consequence of this geometric description is that the max-margin hyperplane is completely determined by those 
  
    
      
        
          
            
              
                x
                →
              
            
          
          
            i
          
        
      
    
    {\displaystyle {\vec {x}}_{i}}
   that lie nearest to it. These 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   are called support vectors.

Soft-margin
To extend SVM to cases in which the data are not linearly separable, the hinge loss function is helpful

  
    
      
        max
        
          (
          
            0
            ,
            1
            −
            
              y
              
                i
              
            
            (
            
              
                w
              
              
                T
              
            
            
              
                x
              
              
                i
              
            
            −
            b
            )
          
          )
        
        .
      
    
    {\displaystyle \max \left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\right).}
  Note that 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
   is the i-th target (i.e., in this case, 1 or −1), and 
  
    
      
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
      
    
    {\displaystyle \mathbf {w} ^{T}\mathbf {x} _{i}-b}
   is the i-th output.
This function is zero if the constraint in (1) is satisfied, in other words, if 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   lies on the correct side of the margin. For data on the wrong side of the margin, the function's value is proportional to the distance from the margin.
The goal of the optimization then is to minimize

  
    
      
        
          [
          
            
              
                1
                n
              
            
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            max
            
              (
              
                0
                ,
                1
                −
                
                  y
                  
                    i
                  
                
                (
                
                  
                    w
                  
                  
                    T
                  
                
                
                  
                    x
                  
                  
                    i
                  
                
                −
                b
                )
              
              )
            
          
          ]
        
        +
        λ
        ‖
        
          w
        
        
          ‖
          
            2
          
        
        ,
      
    
    {\displaystyle \left[{\frac {1}{n}}\sum _{i=1}^{n}\max \left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\right)\right]+\lambda \lVert \mathbf {w} \rVert ^{2},}
  where the parameter 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   determines the trade-off between increasing the margin size and ensuring that the 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   lie on the correct side of the margin. Thus, for sufficiently small values of 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
  , the second term in the loss function will become negligible, hence, it will behave similar to the hard-margin SVM, if the input data are linearly classifiable, but will still learn if a classification rule is viable or not.

Nonlinear classification
The original maximum-margin hyperplane algorithm proposed by Vapnik in 1963 constructed a linear classifier. However, in 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.) to maximum-margin hyperplanes. The resulting algorithm is formally similar, except that every dot product is replaced by a nonlinear kernel function. This allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The transformation may be nonlinear and the transformed space high-dimensional; although the classifier is a hyperplane in the transformed feature space, it may be nonlinear in the original input space.
It is noteworthy that working in a higher-dimensional feature space increases the generalization error of support-vector machines, although given enough samples the algorithm still performs well.Some common kernels include:

Polynomial (homogeneous): 
  
    
      
        k
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ,
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        )
        =
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ⋅
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        
          )
          
            d
          
        
      
    
    {\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=({\vec {x_{i}}}\cdot {\vec {x_{j}}})^{d}}
  .
Polynomial (inhomogeneous): 
  
    
      
        k
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ,
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        )
        =
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ⋅
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        +
        1
        
          )
          
            d
          
        
      
    
    {\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=({\vec {x_{i}}}\cdot {\vec {x_{j}}}+1)^{d}}
  .
Gaussian radial basis function: 
  
    
      
        k
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ,
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        )
        =
        exp
        ⁡
        (
        −
        γ
        ‖
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        −
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        
          ‖
          
            2
          
        
        )
      
    
    {\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=\exp(-\gamma \|{\vec {x_{i}}}-{\vec {x_{j}}}\|^{2})}
   for 
  
    
      
        γ
        >
        0
      
    
    {\displaystyle \gamma >0}
  . Sometimes parametrized using 
  
    
      
        γ
        =
        1
        
          /
        
        (
        2
        
          σ
          
            2
          
        
        )
      
    
    {\displaystyle \gamma =1/(2\sigma ^{2})}
  .
Hyperbolic tangent: 
  
    
      
        k
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ,
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        )
        =
        tanh
        ⁡
        (
        κ
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ⋅
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        +
        c
        )
      
    
    {\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=\tanh(\kappa {\vec {x_{i}}}\cdot {\vec {x_{j}}}+c)}
   for some (not every) 
  
    
      
        κ
        >
        0
      
    
    {\displaystyle \kappa >0}
   and 
  
    
      
        c
        <
        0
      
    
    {\displaystyle c<0}
  .The kernel is related to the transform 
  
    
      
        φ
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        )
      
    
    {\displaystyle \varphi ({\vec {x_{i}}})}
   by the equation 
  
    
      
        k
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        ,
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        )
        =
        φ
        (
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        )
        ⋅
        φ
        (
        
          
            
              
                x
                
                  j
                
              
              →
            
          
        
        )
      
    
    {\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=\varphi ({\vec {x_{i}}})\cdot \varphi ({\vec {x_{j}}})}
  . The value w is also in the transformed space, with 
  
    
      
        
          
            
              
                w
                →
              
            
          
          =
          
            ∑
            
              i
            
          
          
            α
            
              i
            
          
          
            y
            
              i
            
          
          φ
          (
          
            
              
                
                  x
                  →
                
              
            
            
              i
            
          
          )
        
      
    
    {\displaystyle \textstyle {\vec {w}}=\sum _{i}\alpha _{i}y_{i}\varphi ({\vec {x}}_{i})}
  . Dot products with w for classification can again be computed by the kernel trick, i.e. 
  
    
      
        
          
            
              
                w
                →
              
            
          
          ⋅
          φ
          (
          
            
              
                x
                →
              
            
          
          )
          =
          
            ∑
            
              i
            
          
          
            α
            
              i
            
          
          
            y
            
              i
            
          
          k
          (
          
            
              
                
                  x
                  →
                
              
            
            
              i
            
          
          ,
          
            
              
                x
                →
              
            
          
          )
        
      
    
    {\displaystyle \textstyle {\vec {w}}\cdot \varphi ({\vec {x}})=\sum _{i}\alpha _{i}y_{i}k({\vec {x}}_{i},{\vec {x}})}
  .

Computing the SVM classifier
Computing the (soft-margin) SVM classifier amounts to minimizing an expression of the form

  
    
      
        
          [
          
            
              
                1
                n
              
            
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            max
            
              (
              
                0
                ,
                1
                −
                
                  y
                  
                    i
                  
                
                (
                
                  
                    w
                  
                  
                    T
                  
                
                
                  
                    x
                  
                  
                    i
                  
                
                −
                b
                )
              
              )
            
          
          ]
        
        +
        λ
        ‖
        
          w
        
        
          ‖
          
            2
          
        
        .
        
        (
        2
        )
      
    
    {\displaystyle \left[{\frac {1}{n}}\sum _{i=1}^{n}\max \left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\right)\right]+\lambda \|\mathbf {w} \|^{2}.\qquad (2)}
  We focus on the soft-margin classifier since, as noted above, choosing a sufficiently small value for 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   yields the hard-margin classifier for linearly classifiable input data. The classical approach, which involves reducing (2) to a quadratic programming problem, is detailed below. Then, more recent approaches such as sub-gradient descent and coordinate descent will be discussed.

Primal
Minimizing (2) can be rewritten as a constrained optimization problem with a differentiable objective function in the following way.
For each 
  
    
      
        i
        ∈
        {
        1
        ,
        
        …
        ,
        
        n
        }
      
    
    {\displaystyle i\in \{1,\,\ldots ,\,n\}}
   we introduce a variable 
  
    
      
        
          ζ
          
            i
          
        
        =
        max
        
          (
          
            0
            ,
            1
            −
            
              y
              
                i
              
            
            (
            
              
                w
              
              
                T
              
            
            
              
                x
              
              
                i
              
            
            −
            b
            )
          
          )
        
      
    
    {\displaystyle \zeta _{i}=\max \left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\right)}
  . Note that 
  
    
      
        
          ζ
          
            i
          
        
      
    
    {\displaystyle \zeta _{i}}
   is the smallest nonnegative number satisfying 
  
    
      
        
          y
          
            i
          
        
        (
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
        )
        ≥
        1
        −
        
          ζ
          
            i
          
        
        .
      
    
    {\displaystyle y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\geq 1-\zeta _{i}.}
  
Thus we can rewrite the optimization problem as follows

  
    
      
        
          minimize 
        
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          ζ
          
            i
          
        
        +
        λ
        ‖
        
          w
        
        
          ‖
          
            2
          
        
      
    
    {\displaystyle {\text{minimize }}{\frac {1}{n}}\sum _{i=1}^{n}\zeta _{i}+\lambda \|\mathbf {w} \|^{2}}
  

  
    
      
        
          subject to 
        
        
          y
          
            i
          
        
        (
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
        )
        ≥
        1
        −
        
          ζ
          
            i
          
        
        
        
           and 
        
        
        
          ζ
          
            i
          
        
        ≥
        0
        ,
        
        
          for all 
        
        i
        .
      
    
    {\displaystyle {\text{subject to }}y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\geq 1-\zeta _{i}\,{\text{ and }}\,\zeta _{i}\geq 0,\,{\text{for all }}i.}
  This is called the primal problem.

Dual
By solving for the Lagrangian dual of the above problem, one obtains the simplified problem

  
    
      
        
          maximize
        
        
        
        f
        (
        
          c
          
            1
          
        
        …
        
          c
          
            n
          
        
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          c
          
            i
          
        
        −
        
          
            1
            2
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        
          y
          
            i
          
        
        
          c
          
            i
          
        
        (
        
          
            x
          
          
            i
          
          
            T
          
        
        
          
            x
          
          
            j
          
        
        )
        
          y
          
            j
          
        
        
          c
          
            j
          
        
        ,
      
    
    {\displaystyle {\text{maximize}}\,\,f(c_{1}\ldots c_{n})=\sum _{i=1}^{n}c_{i}-{\frac {1}{2}}\sum _{i=1}^{n}\sum _{j=1}^{n}y_{i}c_{i}(\mathbf {x} _{i}^{T}\mathbf {x} _{j})y_{j}c_{j},}
  

  
    
      
        
          subject to 
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          c
          
            i
          
        
        
          y
          
            i
          
        
        =
        0
        ,
        
        
          and 
        
        0
        ≤
        
          c
          
            i
          
        
        ≤
        
          
            1
            
              2
              n
              λ
            
          
        
        
        
          for all 
        
        i
        .
      
    
    {\displaystyle {\text{subject to }}\sum _{i=1}^{n}c_{i}y_{i}=0,\,{\text{and }}0\leq c_{i}\leq {\frac {1}{2n\lambda }}\;{\text{for all }}i.}
  This is called the dual problem. Since the dual maximization problem is a quadratic function of the 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
   subject to linear constraints, it is efficiently solvable by quadratic programming algorithms.
Here, the variables 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
   are defined such that

  
    
      
        
          w
        
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          c
          
            i
          
        
        
          y
          
            i
          
        
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {w} =\sum _{i=1}^{n}c_{i}y_{i}\mathbf {x} _{i}}
  .Moreover, 
  
    
      
        
          c
          
            i
          
        
        =
        0
      
    
    {\displaystyle c_{i}=0}
   exactly when 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   lies on the correct side of the margin, and 
  
    
      
        0
        <
        
          c
          
            i
          
        
        <
        (
        2
        n
        λ
        
          )
          
            −
            1
          
        
      
    
    {\displaystyle 0<c_{i}<(2n\lambda )^{-1}}
    when 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   lies on the margin's boundary. It follows that 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   can be written as a linear combination of the support vectors.
The offset, 
  
    
      
        b
      
    
    {\displaystyle b}
  , can be recovered by finding an 
  
    
      
        
          
            x
          
          
            i
          
        
      
    
    {\displaystyle \mathbf {x} _{i}}
   on the margin's boundary and solving

  
    
      
        
          y
          
            i
          
        
        (
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        b
        )
        =
        1
        
        ⟺
        
        b
        =
        
          
            w
          
          
            T
          
        
        
          
            x
          
          
            i
          
        
        −
        
          y
          
            i
          
        
        .
      
    
    {\displaystyle y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)=1\iff b=\mathbf {w} ^{T}\mathbf {x} _{i}-y_{i}.}
  (Note that 
  
    
      
        
          y
          
            i
          
          
            −
            1
          
        
        =
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}^{-1}=y_{i}}
   since 
  
    
      
        
          y
          
            i
          
        
        =
        ±
        1
      
    
    {\displaystyle y_{i}=\pm 1}
  .)

Kernel trick
Suppose now that we would like to learn a nonlinear classification rule which corresponds to a linear classification rule for the transformed data points 
  
    
      
        φ
        (
        
          
            x
          
          
            i
          
        
        )
        .
      
    
    {\displaystyle \varphi (\mathbf {x} _{i}).}
   Moreover, we are given a kernel function 
  
    
      
        k
      
    
    {\displaystyle k}
   which satisfies 
  
    
      
        k
        (
        
          
            x
          
          
            i
          
        
        ,
        
          
            x
          
          
            j
          
        
        )
        =
        φ
        (
        
          
            x
          
          
            i
          
        
        )
        ⋅
        φ
        (
        
          
            x
          
          
            j
          
        
        )
      
    
    {\displaystyle k(\mathbf {x} _{i},\mathbf {x} _{j})=\varphi (\mathbf {x} _{i})\cdot \varphi (\mathbf {x} _{j})}
  .
We know the classification vector 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   in the transformed space satisfies

  
    
      
        
          w
        
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          c
          
            i
          
        
        
          y
          
            i
          
        
        φ
        (
        
          
            x
          
          
            i
          
        
        )
        ,
      
    
    {\displaystyle \mathbf {w} =\sum _{i=1}^{n}c_{i}y_{i}\varphi (\mathbf {x} _{i}),}
  where, the 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
   are obtained by solving the optimization problem

  
    
      
        
          
            
              
                
                  maximize
                
                
                
                f
                (
                
                  c
                  
                    1
                  
                
                …
                
                  c
                  
                    n
                  
                
                )
              
              
                
                =
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  c
                  
                    i
                  
                
                −
                
                  
                    1
                    2
                  
                
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  ∑
                  
                    j
                    =
                    1
                  
                  
                    n
                  
                
                
                  y
                  
                    i
                  
                
                
                  c
                  
                    i
                  
                
                (
                φ
                (
                
                  
                    x
                  
                  
                    i
                  
                
                )
                ⋅
                φ
                (
                
                  
                    x
                  
                  
                    j
                  
                
                )
                )
                
                  y
                  
                    j
                  
                
                
                  c
                  
                    j
                  
                
              
            
            
              
              
                
                =
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  c
                  
                    i
                  
                
                −
                
                  
                    1
                    2
                  
                
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  ∑
                  
                    j
                    =
                    1
                  
                  
                    n
                  
                
                
                  y
                  
                    i
                  
                
                
                  c
                  
                    i
                  
                
                k
                (
                
                  
                    x
                  
                  
                    i
                  
                
                ,
                
                  
                    x
                  
                  
                    j
                  
                
                )
                
                  y
                  
                    j
                  
                
                
                  c
                  
                    j
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{maximize}}\,\,f(c_{1}\ldots c_{n})&=\sum _{i=1}^{n}c_{i}-{\frac {1}{2}}\sum _{i=1}^{n}\sum _{j=1}^{n}y_{i}c_{i}(\varphi (\mathbf {x} _{i})\cdot \varphi (\mathbf {x} _{j}))y_{j}c_{j}\\&=\sum _{i=1}^{n}c_{i}-{\frac {1}{2}}\sum _{i=1}^{n}\sum _{j=1}^{n}y_{i}c_{i}k(\mathbf {x} _{i},\mathbf {x} _{j})y_{j}c_{j}\\\end{aligned}}}
  

  
    
      
        
          subject to 
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          c
          
            i
          
        
        
          y
          
            i
          
        
        =
        0
        ,
        
        
          and 
        
        0
        ≤
        
          c
          
            i
          
        
        ≤
        
          
            1
            
              2
              n
              λ
            
          
        
        
        
          for all 
        
        i
        .
      
    
    {\displaystyle {\text{subject to }}\sum _{i=1}^{n}c_{i}y_{i}=0,\,{\text{and }}0\leq c_{i}\leq {\frac {1}{2n\lambda }}\;{\text{for all }}i.}
  The coefficients 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
   can be solved for using quadratic programming, as before. Again, we can find some index 
  
    
      
        i
      
    
    {\displaystyle i}
   such that 
  
    
      
        0
        <
        
          c
          
            i
          
        
        <
        (
        2
        n
        λ
        
          )
          
            −
            1
          
        
      
    
    {\displaystyle 0<c_{i}<(2n\lambda )^{-1}}
  , so that 
  
    
      
        φ
        (
        
          
            x
          
          
            i
          
        
        )
      
    
    {\displaystyle \varphi (\mathbf {x} _{i})}
   lies on the boundary of the margin in the transformed space, and then solve

  
    
      
        
          
            
              
                b
                =
                
                  
                    w
                  
                  
                    T
                  
                
                φ
                (
                
                  
                    x
                  
                  
                    i
                  
                
                )
                −
                
                  y
                  
                    i
                  
                
              
              
                
                =
                
                  [
                  
                    
                      ∑
                      
                        j
                        =
                        1
                      
                      
                        n
                      
                    
                    
                      c
                      
                        j
                      
                    
                    
                      y
                      
                        j
                      
                    
                    φ
                    (
                    
                      
                        x
                      
                      
                        j
                      
                    
                    )
                    ⋅
                    φ
                    (
                    
                      
                        x
                      
                      
                        i
                      
                    
                    )
                  
                  ]
                
                −
                
                  y
                  
                    i
                  
                
              
            
            
              
              
                
                =
                
                  [
                  
                    
                      ∑
                      
                        j
                        =
                        1
                      
                      
                        n
                      
                    
                    
                      c
                      
                        j
                      
                    
                    
                      y
                      
                        j
                      
                    
                    k
                    (
                    
                      
                        x
                      
                      
                        j
                      
                    
                    ,
                    
                      
                        x
                      
                      
                        i
                      
                    
                    )
                  
                  ]
                
                −
                
                  y
                  
                    i
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}b=\mathbf {w} ^{T}\varphi (\mathbf {x} _{i})-y_{i}&=\left[\sum _{j=1}^{n}c_{j}y_{j}\varphi (\mathbf {x} _{j})\cdot \varphi (\mathbf {x} _{i})\right]-y_{i}\\&=\left[\sum _{j=1}^{n}c_{j}y_{j}k(\mathbf {x} _{j},\mathbf {x} _{i})\right]-y_{i}.\end{aligned}}}
  Finally,

  
    
      
        
          z
        
        ↦
        sgn
        ⁡
        (
        
          
            w
          
          
            T
          
        
        φ
        (
        
          z
        
        )
        −
        b
        )
        =
        sgn
        ⁡
        
          (
          
            
              [
              
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    n
                  
                
                
                  c
                  
                    i
                  
                
                
                  y
                  
                    i
                  
                
                k
                (
                
                  
                    x
                  
                  
                    i
                  
                
                ,
                
                  z
                
                )
              
              ]
            
            −
            b
          
          )
        
        .
      
    
    {\displaystyle \mathbf {z} \mapsto \operatorname {sgn}(\mathbf {w} ^{T}\varphi (\mathbf {z} )-b)=\operatorname {sgn} \left(\left[\sum _{i=1}^{n}c_{i}y_{i}k(\mathbf {x} _{i},\mathbf {z} )\right]-b\right).}

Modern methods
Recent algorithms for finding the SVM classifier include sub-gradient descent and coordinate descent. Both techniques have proven to offer significant advantages over the traditional approach when dealing with large, sparse datasets—sub-gradient methods are especially efficient when there are many training examples, and coordinate descent when the dimension of the feature space is high.

Sub-gradient descent
Sub-gradient descent algorithms for the SVM work directly with the expression

  
    
      
        f
        (
        
          w
        
        ,
        b
        )
        =
        
          [
          
            
              
                1
                n
              
            
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            max
            
              (
              
                0
                ,
                1
                −
                
                  y
                  
                    i
                  
                
                (
                
                  
                    w
                  
                  
                    T
                  
                
                
                  
                    x
                  
                  
                    i
                  
                
                −
                b
                )
              
              )
            
          
          ]
        
        +
        λ
        ‖
        
          w
        
        
          ‖
          
            2
          
        
        .
      
    
    {\displaystyle f(\mathbf {w} ,b)=\left[{\frac {1}{n}}\sum _{i=1}^{n}\max \left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} _{i}-b)\right)\right]+\lambda \|\mathbf {w} \|^{2}.}
  Note that 
  
    
      
        f
      
    
    {\displaystyle f}
   is a convex function of 
  
    
      
        
          w
        
      
    
    {\displaystyle \mathbf {w} }
   and 
  
    
      
        b
      
    
    {\displaystyle b}
  . As such, traditional gradient descent (or SGD) methods can be adapted, where instead of taking a step in the direction of the function's gradient, a step is taken in the direction of a vector selected from the function's sub-gradient. This approach has the advantage that, for certain implementations, the number of iterations does not scale with 
  
    
      
        n
      
    
    {\displaystyle n}
  , the number of data points.

Coordinate descent
Coordinate descent algorithms for the SVM work from the dual problem

  
    
      
        
          maximize
        
        
        
        f
        (
        
          c
          
            1
          
        
        …
        
          c
          
            n
          
        
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          c
          
            i
          
        
        −
        
          
            1
            2
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          ∑
          
            j
            =
            1
          
          
            n
          
        
        
          y
          
            i
          
        
        
          c
          
            i
          
        
        (
        
          x
          
            i
          
        
        ⋅
        
          x
          
            j
          
        
        )
        
          y
          
            j
          
        
        
          c
          
            j
          
        
        ,
      
    
    {\displaystyle {\text{maximize}}\,\,f(c_{1}\ldots c_{n})=\sum _{i=1}^{n}c_{i}-{\frac {1}{2}}\sum _{i=1}^{n}\sum _{j=1}^{n}y_{i}c_{i}(x_{i}\cdot x_{j})y_{j}c_{j},}
  

  
    
      
        
          subject to 
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          c
          
            i
          
        
        
          y
          
            i
          
        
        =
        0
        ,
        
        
          and 
        
        0
        ≤
        
          c
          
            i
          
        
        ≤
        
          
            1
            
              2
              n
              λ
            
          
        
        
        
          for all 
        
        i
        .
      
    
    {\displaystyle {\text{subject to }}\sum _{i=1}^{n}c_{i}y_{i}=0,\,{\text{and }}0\leq c_{i}\leq {\frac {1}{2n\lambda }}\;{\text{for all }}i.}
  For each 
  
    
      
        i
        ∈
        {
        1
        ,
        
        …
        ,
        
        n
        }
      
    
    {\displaystyle i\in \{1,\,\ldots ,\,n\}}
  , iteratively, the coefficient 
  
    
      
        
          c
          
            i
          
        
      
    
    {\displaystyle c_{i}}
   is adjusted in the direction of 
  
    
      
        ∂
        f
        
          /
        
        ∂
        
          c
          
            i
          
        
      
    
    {\displaystyle \partial f/\partial c_{i}}
  . Then, the resulting vector of coefficients 
  
    
      
        (
        
          c
          
            1
          
          ′
        
        ,
        
        …
        ,
        
        
          c
          
            n
          
          ′
        
        )
      
    
    {\displaystyle (c_{1}',\,\ldots ,\,c_{n}')}
   is projected onto the nearest vector of coefficients that satisfies the given constraints. (Typically Euclidean distances are used.) The process is then repeated until a near-optimal vector of coefficients is obtained. The resulting algorithm is extremely fast in practice, although few performance guarantees have been proven.

Empirical risk minimization
The soft-margin support vector machine described above is an example of an empirical risk minimization (ERM) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. This perspective can provide further insight into how and why SVMs work, and allow us to better analyze their statistical properties.

Risk minimization
In supervised learning, one is given a set of training examples 
  
    
      
        
          X
          
            1
          
        
        …
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{1}\ldots X_{n}}
   with labels 
  
    
      
        
          y
          
            1
          
        
        …
        
          y
          
            n
          
        
      
    
    {\displaystyle y_{1}\ldots y_{n}}
  , and wishes to predict 
  
    
      
        
          y
          
            n
            +
            1
          
        
      
    
    {\displaystyle y_{n+1}}
   given 
  
    
      
        
          X
          
            n
            +
            1
          
        
      
    
    {\displaystyle X_{n+1}}
  . To do so one forms a hypothesis, 
  
    
      
        f
      
    
    {\displaystyle f}
  , such that 
  
    
      
        f
        (
        
          X
          
            n
            +
            1
          
        
        )
      
    
    {\displaystyle f(X_{n+1})}
   is a ""good"" approximation of 
  
    
      
        
          y
          
            n
            +
            1
          
        
      
    
    {\displaystyle y_{n+1}}
  . A ""good"" approximation is usually defined with the help of a loss function, 
  
    
      
        ℓ
        (
        y
        ,
        z
        )
      
    
    {\displaystyle \ell (y,z)}
  , which characterizes how bad 
  
    
      
        z
      
    
    {\displaystyle z}
   is as a prediction of 
  
    
      
        y
      
    
    {\displaystyle y}
  . We would then like to choose a hypothesis that minimizes the expected risk:

  
    
      
        ε
        (
        f
        )
        =
        
          E
        
        
          [
          
            ℓ
            (
            
              y
              
                n
                +
                1
              
            
            ,
            f
            (
            
              X
              
                n
                +
                1
              
            
            )
            )
          
          ]
        
        .
      
    
    {\displaystyle \varepsilon (f)=\mathbb {E} \left[\ell (y_{n+1},f(X_{n+1}))\right].}
  In most cases, we don't know the joint distribution of 
  
    
      
        
          X
          
            n
            +
            1
          
        
        ,
        
        
          y
          
            n
            +
            1
          
        
      
    
    {\displaystyle X_{n+1},\,y_{n+1}}
   outright. In these cases, a common strategy is to choose the hypothesis that minimizes the empirical risk:

  
    
      
        
          
            
              ε
              ^
            
          
        
        (
        f
        )
        =
        
          
            1
            n
          
        
        
          ∑
          
            k
            =
            1
          
          
            n
          
        
        ℓ
        (
        
          y
          
            k
          
        
        ,
        f
        (
        
          X
          
            k
          
        
        )
        )
        .
      
    
    {\displaystyle {\hat {\varepsilon }}(f)={\frac {1}{n}}\sum _{k=1}^{n}\ell (y_{k},f(X_{k})).}
  Under certain assumptions about the sequence of random variables 
  
    
      
        
          X
          
            k
          
        
        ,
        
        
          y
          
            k
          
        
      
    
    {\displaystyle X_{k},\,y_{k}}
   (for example, that they are generated by a finite Markov process), if the set of hypotheses being considered is small enough, the minimizer of the empirical risk will closely approximate the minimizer of the expected risk as 
  
    
      
        n
      
    
    {\displaystyle n}
   grows large. This approach is called empirical risk minimization, or ERM.

Regularization and stability
In order for the minimization problem to have a well-defined solution, we have to place constraints on the set 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
   of hypotheses being considered. If 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
   is a normed space (as is the case for SVM), a particularly effective technique is to consider only those hypotheses 
  
    
      
        f
      
    
    {\displaystyle f}
   for which 
  
    
      
        ‖
        f
        
          ‖
          
            
              H
            
          
        
        <
        k
      
    
    {\displaystyle \lVert f\rVert _{\mathcal {H}}<k}
   . This is equivalent to imposing a regularization penalty 
  
    
      
        
          
            R
          
        
        (
        f
        )
        =
        
          λ
          
            k
          
        
        ‖
        f
        
          ‖
          
            
              H
            
          
        
      
    
    {\displaystyle {\mathcal {R}}(f)=\lambda _{k}\lVert f\rVert _{\mathcal {H}}}
  , and solving the new optimization problem

  
    
      
        
          
            
              f
              ^
            
          
        
        =
        
          a
          r
          g
        
        
          min
          
            f
            ∈
            
              
                H
              
            
          
        
        
          
            
              ε
              ^
            
          
        
        (
        f
        )
        +
        
          
            R
          
        
        (
        f
        )
        .
      
    
    {\displaystyle {\hat {f}}=\mathrm {arg} \min _{f\in {\mathcal {H}}}{\hat {\varepsilon }}(f)+{\mathcal {R}}(f).}
  This approach is called Tikhonov regularization.
More generally, 
  
    
      
        
          
            R
          
        
        (
        f
        )
      
    
    {\displaystyle {\mathcal {R}}(f)}
   can be some measure of the complexity of the hypothesis 
  
    
      
        f
      
    
    {\displaystyle f}
  , so that simpler hypotheses are preferred.

SVM and the hinge loss
Recall that the (soft-margin) SVM classifier 
  
    
      
        
          
            
              
                w
              
              ^
            
          
        
        ,
        b
        :
        
          x
        
        ↦
        sgn
        ⁡
        (
        
          
            
              
                
                  w
                
                ^
              
            
          
          
            T
          
        
        
          x
        
        −
        b
        )
      
    
    {\displaystyle {\hat {\mathbf {w} }},b:\mathbf {x} \mapsto \operatorname {sgn}({\hat {\mathbf {w} }}^{T}\mathbf {x} -b)}
   is chosen to minimize the following expression:

  
    
      
        
          [
          
            
              
                1
                n
              
            
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            max
            
              (
              
                0
                ,
                1
                −
                
                  y
                  
                    i
                  
                
                (
                
                  
                    w
                  
                  
                    T
                  
                
                
                  x
                
                −
                b
                )
              
              )
            
          
          ]
        
        +
        λ
        ‖
        
          w
        
        
          ‖
          
            2
          
        
        .
      
    
    {\displaystyle \left[{\frac {1}{n}}\sum _{i=1}^{n}\max \left(0,1-y_{i}(\mathbf {w} ^{T}\mathbf {x} -b)\right)\right]+\lambda \|\mathbf {w} \|^{2}.}
  In light of the above discussion, we see that the SVM technique is equivalent to empirical risk minimization with Tikhonov regularization, where in this case the loss function is the hinge loss

  
    
      
        ℓ
        (
        y
        ,
        z
        )
        =
        max
        
          (
          
            0
            ,
            1
            −
            y
            z
          
          )
        
        .
      
    
    {\displaystyle \ell (y,z)=\max \left(0,1-yz\right).}
  From this perspective, SVM is closely related to other fundamental classification algorithms such as regularized least-squares and logistic regression. The difference between the three lies in the choice of loss function: regularized least-squares amounts to empirical risk minimization with the square-loss,  
  
    
      
        
          ℓ
          
            s
            q
          
        
        (
        y
        ,
        z
        )
        =
        (
        y
        −
        z
        
          )
          
            2
          
        
      
    
    {\displaystyle \ell _{sq}(y,z)=(y-z)^{2}}
  ; logistic regression employs the log-loss,

  
    
      
        
          ℓ
          
            log
          
        
        (
        y
        ,
        z
        )
        =
        ln
        ⁡
        (
        1
        +
        
          e
          
            −
            y
            z
          
        
        )
        .
      
    
    {\displaystyle \ell _{\log }(y,z)=\ln(1+e^{-yz}).}

Target functions
The difference between the hinge loss and these other loss functions is best stated in terms of target functions - the function that minimizes expected risk for a given pair of random variables 
  
    
      
        X
        ,
        
        y
      
    
    {\displaystyle X,\,y}
  .
In particular, let 
  
    
      
        
          y
          
            x
          
        
      
    
    {\displaystyle y_{x}}
   denote 
  
    
      
        y
      
    
    {\displaystyle y}
   conditional on the event that 
  
    
      
        X
        =
        x
      
    
    {\displaystyle X=x}
  .  In the classification setting, we have:

  
    
      
        
          y
          
            x
          
        
        =
        
          
            {
            
              
                
                  1
                
                
                  
                    with probability 
                  
                  
                    p
                    
                      x
                    
                  
                
              
              
                
                  −
                  1
                
                
                  
                    with probability 
                  
                  1
                  −
                  
                    p
                    
                      x
                    
                  
                
              
            
            
          
        
      
    
    {\displaystyle y_{x}={\begin{cases}1&{\text{with probability }}p_{x}\\-1&{\text{with probability }}1-p_{x}\end{cases}}}
  The optimal classifier is therefore:

  
    
      
        
          f
          
            ∗
          
        
        (
        x
        )
        =
        
          
            {
            
              
                
                  1
                
                
                  
                    if 
                  
                  
                    p
                    
                      x
                    
                  
                  ≥
                  1
                  
                    /
                  
                  2
                
              
              
                
                  −
                  1
                
                
                  
                    otherwise
                  
                
              
            
            
          
        
      
    
    {\displaystyle f^{*}(x)={\begin{cases}1&{\text{if }}p_{x}\geq 1/2\\-1&{\text{otherwise}}\end{cases}}}
  For the square-loss, the target function is the conditional expectation function, 
  
    
      
        
          f
          
            s
            q
          
        
        (
        x
        )
        =
        
          E
        
        
          [
          
            y
            
              x
            
          
          ]
        
      
    
    {\displaystyle f_{sq}(x)=\mathbb {E} \left[y_{x}\right]}
  ; For the logistic loss, it's the logit function, 
  
    
      
        
          f
          
            log
          
        
        (
        x
        )
        =
        ln
        ⁡
        
          (
          
            
              p
              
                x
              
            
            
              /
            
            (
            
              1
              −
              
                p
                
                  x
                
              
            
            )
          
          )
        
      
    
    {\displaystyle f_{\log }(x)=\ln \left(p_{x}/({1-p_{x}})\right)}
  . While both of these target functions yield the correct classifier, as 
  
    
      
        sgn
        ⁡
        (
        
          f
          
            s
            q
          
        
        )
        =
        sgn
        ⁡
        (
        
          f
          
            log
          
        
        )
        =
        
          f
          
            ∗
          
        
      
    
    {\displaystyle \operatorname {sgn}(f_{sq})=\operatorname {sgn}(f_{\log })=f^{*}}
  , they give us more information than we need. In fact, they give us enough information to completely describe the distribution of 
  
    
      
        
          y
          
            x
          
        
      
    
    {\displaystyle y_{x}}
  .
On the other hand, one can check that the target function for the hinge loss is exactly 
  
    
      
        
          f
          
            ∗
          
        
      
    
    {\displaystyle f^{*}}
  . Thus, in a sufficiently rich hypothesis space—or equivalently, for an appropriately chosen kernel—the SVM classifier will converge to the simplest function (in terms of 
  
    
      
        
          
            R
          
        
      
    
    {\displaystyle {\mathcal {R}}}
  ) that correctly classifies the data. This extends the geometric interpretation of SVM—for linear classification, the empirical risk is minimized by any function whose margins lie between the support vectors, and the simplest of these is the max-margin classifier.

Properties
SVMs belong to a family of generalized linear classifiers and can be interpreted as an extension of the perceptron. They can also be considered a special case of Tikhonov regularization. A special property is that they simultaneously minimize the empirical classification error and maximize the geometric margin; hence they are also known as maximum margin classifiers.
A comparison of the SVM to other classifiers has been made by Meyer, Leisch and Hornik.

Parameter selection
The effectiveness of SVM depends on the selection of kernel, the kernel's parameters, and soft margin parameter C.
A common choice is a Gaussian kernel, which has a single parameter 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  . The best combination of C and 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
   is often selected by a grid search with exponentially growing sequences of C and 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  , for example, 
  
    
      
        C
        ∈
        {
        
          2
          
            −
            5
          
        
        ,
        
          2
          
            −
            3
          
        
        ,
        …
        ,
        
          2
          
            13
          
        
        ,
        
          2
          
            15
          
        
        }
      
    
    {\displaystyle C\in \{2^{-5},2^{-3},\dots ,2^{13},2^{15}\}}
  ; 
  
    
      
        γ
        ∈
        {
        
          2
          
            −
            15
          
        
        ,
        
          2
          
            −
            13
          
        
        ,
        …
        ,
        
          2
          
            1
          
        
        ,
        
          2
          
            3
          
        
        }
      
    
    {\displaystyle \gamma \in \{2^{-15},2^{-13},\dots ,2^{1},2^{3}\}}
  . Typically, each combination of parameter choices is checked using cross validation, and the parameters with best cross-validation accuracy are picked. Alternatively, recent work in Bayesian optimization can be used to select C and 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
   , often requiring the evaluation of far fewer parameter combinations than grid search. The final model, which is used for testing and for classifying new data, is then trained on the whole training set using the selected parameters.

Issues
Potential drawbacks of the SVM include the following aspects:

Requires full labeling of input data
Uncalibrated class membership probabilities—SVM stems from Vapnik's theory which avoids estimating probabilities on finite data
The SVM is only directly applicable for two-class tasks. Therefore, algorithms that reduce the multi-class task to several binary problems have to be applied; see the multi-class SVM section.
Parameters of a solved model are difficult to interpret.

Extensions
Support-vector clustering (SVC)
SVC is a similar method that also builds on kernel functions but is appropriate for unsupervised learning. It is considered a fundamental method in data science.

Multiclass SVM
Multiclass SVM aims to assign labels to instances by using support-vector machines, where the labels are drawn from a finite set of several elements.
The dominant approach for doing so is to reduce the single multiclass problem into multiple binary classification problems. Common methods for such reduction include:
Building binary classifiers that distinguish between one of the labels and the rest (one-versus-all) or between every pair of classes (one-versus-one). Classification of new instances for the one-versus-all case is done by a winner-takes-all strategy, in which the classifier with the highest-output function assigns the class (it is important that the output functions be calibrated to produce comparable scores). For the one-versus-one approach, classification is done by a max-wins voting strategy, in which every classifier assigns the instance to one of the two classes, then the vote for the assigned class is increased by one vote, and finally the class with the most votes determines the instance classification.
Directed acyclic graph SVM (DAGSVM)
Error-correcting output codesCrammer and Singer proposed a multiclass SVM method which casts the multiclass classification problem into a single optimization problem, rather than decomposing it into multiple binary classification problems. See also Lee, Lin and Wahba and Van den Burg and Groenen.

Transductive support-vector machines
Transductive support-vector machines extend SVMs in that they could also treat partially labeled data in semi-supervised learning by following the principles of transduction. Here, in addition to the training set 
  
    
      
        
          
            D
          
        
      
    
    {\displaystyle {\mathcal {D}}}
  , the learner is also given a set

  
    
      
        
          
            
              D
            
          
          
            ⋆
          
        
        =
        {
        
          
            
              
                x
                →
              
            
          
          
            i
          
          
            ⋆
          
        
        ∣
        
          
            
              
                x
                →
              
            
          
          
            i
          
          
            ⋆
          
        
        ∈
        
          
            R
          
          
            p
          
        
        
          }
          
            i
            =
            1
          
          
            k
          
        
      
    
    {\displaystyle {\mathcal {D}}^{\star }=\{{\vec {x}}_{i}^{\star }\mid {\vec {x}}_{i}^{\star }\in \mathbb {R} ^{p}\}_{i=1}^{k}}
  of test examples to be classified. Formally, a transductive support-vector machine is defined by the following primal optimization problem:Minimize (in 
  
    
      
        
          
            
              
                w
                →
              
            
          
          ,
          b
          ,
          
            
              
                
                  y
                  
                    ⋆
                  
                
                →
              
            
          
        
      
    
    {\displaystyle {{\vec {w}},b,{\vec {y^{\star }}}}}
  )

  
    
      
        
          
            1
            2
          
        
        ‖
        
          
            
              w
              →
            
          
        
        
          ‖
          
            2
          
        
      
    
    {\displaystyle {\frac {1}{2}}\|{\vec {w}}\|^{2}}
  subject to (for any 
  
    
      
        i
        =
        1
        ,
        …
        ,
        n
      
    
    {\displaystyle i=1,\dots ,n}
   and any 
  
    
      
        j
        =
        1
        ,
        …
        ,
        k
      
    
    {\displaystyle j=1,\dots ,k}
  )

  
    
      
        
          y
          
            i
          
        
        (
        
          
            
              w
              →
            
          
        
        ⋅
        
          
            
              
                x
                
                  i
                
              
              →
            
          
        
        −
        b
        )
        ≥
        1
        ,
      
    
    {\displaystyle y_{i}({\vec {w}}\cdot {\vec {x_{i}}}-b)\geq 1,}
  
  
    
      
        
          y
          
            j
          
          
            ⋆
          
        
        (
        
          
            
              w
              →
            
          
        
        ⋅
        
          
            
              
                x
                
                  j
                
                
                  ⋆
                
              
              →
            
          
        
        −
        b
        )
        ≥
        1
        ,
      
    
    {\displaystyle y_{j}^{\star }({\vec {w}}\cdot {\vec {x_{j}^{\star }}}-b)\geq 1,}
  and

  
    
      
        
          y
          
            j
          
          
            ⋆
          
        
        ∈
        {
        −
        1
        ,
        1
        }
        .
      
    
    {\displaystyle y_{j}^{\star }\in \{-1,1\}.}
  Transductive support-vector machines were introduced by Vladimir N. Vapnik in 1998.

Structured SVM
SVMs have been generalized to structured SVMs, where the label space is structured and of possibly infinite size.

Regression
A version of SVM for regression was proposed in 1996 by Vladimir N. Vapnik, Harris Drucker, Christopher J. C. Burges, Linda Kaufman and Alexander J. Smola. This method is called support-vector regression (SVR). The model produced by support-vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by SVR depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction. Another SVM version known as least-squares support-vector machine (LS-SVM) has been proposed by Suykens and Vandewalle.Training the original SVR means solving
minimize 
  
    
      
        
          
            1
            2
          
        
        ‖
        w
        
          ‖
          
            2
          
        
      
    
    {\displaystyle {\frac {1}{2}}\|w\|^{2}}
  
subject to 
  
    
      
        
          |
        
        
          y
          
            i
          
        
        −
        ⟨
        w
        ,
        
          x
          
            i
          
        
        ⟩
        −
        b
        
          |
        
        ≤
        ε
      
    
    {\displaystyle |y_{i}-\langle w,x_{i}\rangle -b|\leq \varepsilon }
  where 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   is a training sample with target value 
  
    
      
        
          y
          
            i
          
        
      
    
    {\displaystyle y_{i}}
  . The inner product plus intercept 
  
    
      
        ⟨
        w
        ,
        
          x
          
            i
          
        
        ⟩
        +
        b
      
    
    {\displaystyle \langle w,x_{i}\rangle +b}
   is the prediction for that sample, and 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   is a free parameter that serves as a threshold: all predictions have to be within an 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   range of the true predictions. Slack variables are usually added into the above to allow for errors and to allow approximation in the case the above problem is infeasible.

Bayesian SVM
In 2011 it was shown by Polson and Scott that the SVM admits a Bayesian interpretation through the technique of data augmentation. In this approach the SVM is viewed as a graphical model (where the parameters are connected via probability distributions). This extended view allows the application of Bayesian techniques to SVMs, such as flexible feature modeling, automatic hyperparameter tuning, and predictive uncertainty quantification. Recently, a scalable version of the Bayesian SVM was developed by Florian Wenzel, enabling the application of Bayesian SVMs to big data. Florian Wenzel developed two different versions, a variational inference (VI) scheme for the Bayesian kernel support vector machine (SVM) and a stochastic version (SVI) for the linear Bayesian SVM.

Implementation
The parameters of the maximum-margin hyperplane are derived by solving the optimization. There exist several specialized algorithms for quickly solving the quadratic programming (QP) problem that arises from SVMs, mostly relying on heuristics for breaking the problem down into smaller, more manageable chunks.
Another approach is to use an interior-point method that uses Newton-like iterations to find a solution of the Karush–Kuhn–Tucker conditions of the primal and dual problems.
Instead of solving a sequence of broken-down problems, this approach directly solves the problem altogether. To avoid solving a linear system involving the large kernel matrix, a low-rank approximation to the matrix is often used in the kernel trick.
Another common method is Platt's sequential minimal optimization (SMO) algorithm, which breaks the problem down into 2-dimensional sub-problems that are solved analytically, eliminating the need for a numerical optimization algorithm and matrix storage. This algorithm is conceptually simple, easy to implement, generally faster, and has better scaling properties for difficult SVM problems.The special case of linear support-vector machines can be solved more efficiently by the same kind of algorithms used to optimize its close cousin, logistic regression; this class of algorithms includes sub-gradient descent (e.g., PEGASOS) and coordinate descent (e.g., LIBLINEAR). LIBLINEAR has some attractive training-time properties. Each convergence iteration takes time linear in the time taken to read the train data, and the iterations also have a Q-linear convergence property, making the algorithm extremely fast.
The general kernel SVMs can also be solved more efficiently using sub-gradient descent (e.g. P-packSVM), especially when parallelization is allowed.
Kernel SVMs are available in many machine-learning toolkits, including LIBSVM, MATLAB, SAS, SVMlight, kernlab, scikit-learn, Shogun, Weka, Shark, JKernelMachines, OpenCV and others.
Preprocessing of data (standardization) is highly recommended to enhance accuracy of classification. There are a few methods of standardization, such as min-max, normalization by decimal scaling, Z-score. Subtraction of mean and division by variance of each feature is usually used for SVM.

See also
In situ adaptive tabulation
Kernel machines
Fisher kernel
Platt scaling
Polynomial kernel
Predictive analytics
Regularization perspectives on support-vector machines
Relevance vector machine, a probabilistic sparse-kernel model identical in functional form to SVM
Sequential minimal optimization
Space mapping
Winnow (algorithm)

References
Further reading
Bennett, Kristin P.; Campbell, Colin (2000). ""Support Vector Machines: Hype or Hallelujah?"" (PDF). SIGKDD Explorations. 2 (2): 1–13. doi:10.1145/380995.380999. S2CID 207753020.
Cristianini, Nello; Shawe-Taylor, John (2000). An Introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press. ISBN 0-521-78019-5.
Fradkin, Dmitriy; Muchnik, Ilya (2006). ""Support Vector Machines for Classification"" (PDF).  In Abello, J.; Carmode, G. (eds.). Discrete Methods in Epidemiology. DIMACS Series in Discrete Mathematics and Theoretical Computer Science. 70. pp. 13–20.
Ivanciuc, Ovidiu (2007). ""Applications of Support Vector Machines in Chemistry"" (PDF). Reviews in Computational Chemistry. 23: 291–400. doi:10.1002/9780470116449.ch6. ISBN 9780470116449.
James, Gareth; Witten, Daniela; Hastie, Trevor; Tibshirani, Robert (2013). ""Support Vector Machines"" (PDF). An Introduction to Statistical Learning : with Applications in R. New York: Springer. pp. 337–372. ISBN 978-1-4614-7137-0.
Schölkopf, Bernhard; Smola, Alexander J. (2002). Learning with Kernels. Cambridge, MA: MIT Press. ISBN 0-262-19475-9.
Steinwart, Ingo; Christmann, Andreas (2008). Support Vector Machines. New York: Springer. ISBN 978-0-387-77241-7.
Theodoridis, Sergios; Koutroumbas, Konstantinos (2009). Pattern Recognition (4th ed.). Academic Press. ISBN 978-1-59749-272-0.

External links
libsvm, LIBSVM is a popular library of SVM learners
liblinear is a library for large linear classification including some SVMs
SVM light is a collection of software tools for learning and classification using SVM
SVMJS live demo is a GUI demo for JavaScript implementation of SVMs",https://en.wikipedia.org/wiki/Support-vector_machine,"['All articles with specifically marked weasel-worded phrases', 'All articles with unsourced statements', 'Articles with short description', 'Articles with specifically marked weasel-worded phrases from May 2018', 'Articles with unsourced statements from June 2013', 'Articles with unsourced statements from March 2017', 'Articles with unsourced statements from March 2018', 'Classification algorithms', 'Short description matches Wikidata', 'Statistical classification', 'Support vector machines', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MA identifiers']",Data Science
174,Transformer (machine learning model),"The Transformer is a deep learning model introduced in 2017 that utilizes the mechanism of attention. It is used primarily in the field of natural language processing (NLP), but recent research has also developed its application in other tasks like video understanding.Like recurrent neural networks (RNNs), Transformers are designed to handle sequential data, such as natural language, for tasks such as translation and text summarization. However, unlike RNNs, Transformers do not require that the sequential data be processed in order. For example, if the input data is a natural language sentence, the Transformer does not need to process the beginning of it before the end. Due to this feature, the Transformer allows for much more parallelization than RNNs and therefore reduced training times.Transformers have rapidly become the model of choice for NLP problems, replacing older recurrent neural network models such as the long short-term memory (LSTM). Since the Transformer model facilitates more parallelization during training, it has enabled training on larger datasets than was possible before it was introduced. This has led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which have been trained with huge general language datasets, such as Wikipedia Corpus and Common Crawl, and can be fine-tuned to specific language tasks.

Background
Before the introduction of Transformers, most state-of-the-art NLP systems relied on gated recurrent neural networks (RNNs), such as LSTMs and gated recurrent units (GRUs), with added attention mechanisms. The Transformer built on these attention technologies without using an RNN structure, highlighting the fact that the attention mechanisms alone, without recurrent sequential processing, are powerful enough to achieve the performance of RNNs with attention.
Gated RNNs process tokens sequentially, maintaining a state vector that contains a representation of the data seen after every token. To process the 
  
    
      
        
          n
          
            t
            h
          
        
      
    
    {\textstyle n^{th}}
   token, the model combines the state representing the sentence up to token 
  
    
      
        n
        −
        1
      
    
    {\textstyle n-1}
   with the information of the new token to create a new state, representing the sentence up to token 
  
    
      
        n
      
    
    {\textstyle n}
  . Theoretically, the information from one token can propagate arbitrarily far down the sequence, if at every point the state continues to encode information about the token. But in practice this mechanism is imperfect: due in part to the vanishing gradient problem, the model's state at the end of a long sentence often does not contain precise, extractable information about early tokens.
This problem was addressed by the introduction of attention mechanisms. Attention mechanisms let a model directly look at, and draw from, the state at any earlier point in the sentence. The attention layer can access all previous states and weighs them according to some learned measure of relevancy to the current token, providing sharper information about far-away relevant tokens. A clear example of the utility of attention is in translation. In an English-to-French translation system, the first word of the French output most probably depends heavily on the beginning of the English input. However, in a classic encoder-decoder LSTM model, in order to produce the first word of the French output the model is only given the state vector of the last English word. Theoretically, this vector can encode information about the whole English sentence, giving the model all necessary knowledge, but in practice this information is often not well preserved. If an attention mechanism is introduced, the model can instead learn to attend to the states of early English tokens when producing the beginning of the French output, giving it a much better concept of what it is translating.
When added to RNNs, attention mechanisms led to large gains in performance. The introduction of the Transformer brought to light the fact that attention mechanisms were powerful in themselves, and that sequential recurrent processing of data was not necessary for achieving the performance gains of RNNs with attention. The Transformer uses an attention mechanism without being an RNN, processing all tokens at the same time and calculating attention weights between them. The fact that Transformers do not rely on sequential processing, and lend themselves very easily to parallelization, allows Transformers to be trained more efficiently on larger datasets.

Architecture
Like the models invented before it, the Transformer is an encoder-decoder architecture. The encoder consists of a set of encoding layers that processes the input iteratively one layer after another and the decoder consists of a set of decoding layers that does the same thing to the output of the encoder.
The function of each encoder layer is to process its input to generate encodings, containing information about which parts of the inputs are relevant to each other. It passes its set of encodings to the next encoder layer as inputs. Each decoder layer does the opposite, taking all the encodings and processes them, using their incorporated contextual information to generate an output sequence. To achieve this, each encoder and decoder layer makes use of an attention mechanism, which for each input, weighs the relevance of every other input and draws information from them accordingly to produce the output. Each decoder layer also has an additional attention mechanism which draws information from the outputs of previous decoders, before the decoder layer draws information from the encodings. Both the encoder and decoder layers have a feed-forward neural network for additional processing of the outputs, and contain residual connections and layer normalization steps.

Scaled dot-product attention
The basic building blocks of the Transformer are scaled dot-product attention units. When a sentence is passed into a Transformer model, attention weights are calculated between every token simultaneously. The attention unit produces embeddings for every token in context that contain information not only about the token itself, but also a weighted combination of other relevant tokens weighted by the attention weights.
Concretely, for each attention unit the Transformer model learns three weight matrices; the query weights 
  
    
      
        
          W
          
            Q
          
        
      
    
    {\displaystyle W_{Q}}
  , the key weights 
  
    
      
        
          W
          
            K
          
        
      
    
    {\displaystyle W_{K}}
  , and the value weights 
  
    
      
        
          W
          
            V
          
        
      
    
    {\displaystyle W_{V}}
  . For each token 
  
    
      
        i
      
    
    {\displaystyle i}
  , the input word embedding 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   is multiplied with each of the three weight matrices to produce a query vector 
  
    
      
        
          q
          
            i
          
        
        =
        
          x
          
            i
          
        
        
          W
          
            Q
          
        
      
    
    {\displaystyle q_{i}=x_{i}W_{Q}}
  , a key vector 
  
    
      
        
          k
          
            i
          
        
        =
        
          x
          
            i
          
        
        
          W
          
            K
          
        
      
    
    {\displaystyle k_{i}=x_{i}W_{K}}
  , and a value vector 
  
    
      
        
          v
          
            i
          
        
        =
        
          x
          
            i
          
        
        
          W
          
            V
          
        
      
    
    {\displaystyle v_{i}=x_{i}W_{V}}
  . Attention weights are calculated using the query and key vectors: the attention weight 
  
    
      
        
          a
          
            i
            j
          
        
      
    
    {\displaystyle a_{ij}}
   from token 
  
    
      
        i
      
    
    {\displaystyle i}
   to token 
  
    
      
        j
      
    
    {\displaystyle j}
   is the dot product between 
  
    
      
        
          q
          
            i
          
        
      
    
    {\displaystyle q_{i}}
   and 
  
    
      
        
          k
          
            j
          
        
      
    
    {\displaystyle k_{j}}
  . The attention weights are divided by the square root of the dimension of the key vectors, 
  
    
      
        
          
            
              d
              
                k
              
            
          
        
      
    
    {\displaystyle {\sqrt {d_{k}}}}
  , which stabilizes gradients during training, and passed through a softmax which normalizes the weights to sum to 
  
    
      
        1
      
    
    {\displaystyle 1}
  . The fact that 
  
    
      
        
          W
          
            Q
          
        
      
    
    {\displaystyle W_{Q}}
   and 
  
    
      
        
          W
          
            K
          
        
      
    
    {\displaystyle W_{K}}
   are different matrices allows attention to be non-symmetric: if token 
  
    
      
        i
      
    
    {\displaystyle i}
   attends to token 
  
    
      
        j
      
    
    {\displaystyle j}
   (i.e. 
  
    
      
        
          q
          
            i
          
        
        ⋅
        
          k
          
            j
          
        
      
    
    {\displaystyle q_{i}\cdot k_{j}}
   is large), this does not necessarily mean that token 
  
    
      
        j
      
    
    {\displaystyle j}
   will attend to token 
  
    
      
        i
      
    
    {\displaystyle i}
   (i.e. 
  
    
      
        
          q
          
            j
          
        
        ⋅
        
          k
          
            i
          
        
      
    
    {\displaystyle q_{j}\cdot k_{i}}
   is large).  The output of the attention unit for token 
  
    
      
        i
      
    
    {\displaystyle i}
   is the weighted sum of the value vectors of all tokens, weighted by 
  
    
      
        
          a
          
            i
            j
          
        
      
    
    {\displaystyle a_{ij}}
  , the attention from token 
  
    
      
        i
      
    
    {\displaystyle i}
   to each token.
The attention calculation for all tokens can be expressed as one large matrix calculation, which is useful for training due to computational matrix operation optimizations which make matrix operations fast to compute. The matrices 
  
    
      
        Q
      
    
    {\displaystyle Q}
  , 
  
    
      
        K
      
    
    {\displaystyle K}
   and 
  
    
      
        V
      
    
    {\displaystyle V}
   are defined as the matrices where the 
  
    
      
        i
      
    
    {\displaystyle i}
  th rows are vectors 
  
    
      
        
          q
          
            i
          
        
      
    
    {\displaystyle q_{i}}
  , 
  
    
      
        
          k
          
            i
          
        
      
    
    {\displaystyle k_{i}}
  , and 
  
    
      
        
          v
          
            i
          
        
      
    
    {\displaystyle v_{i}}
   respectively.

  
    
      
        
          
            
              
                
                  Attention
                
                (
                Q
                ,
                K
                ,
                V
                )
                =
                
                  softmax
                
                
                  (
                  
                    
                      
                        Q
                        
                          K
                          
                            
                              T
                            
                          
                        
                      
                      
                        
                          d
                          
                            k
                          
                        
                      
                    
                  
                  )
                
                V
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V\end{aligned}}}

Multi-head attention
One set of 
  
    
      
        
          (
          
            
              W
              
                Q
              
            
            ,
            
              W
              
                K
              
            
            ,
            
              W
              
                V
              
            
          
          )
        
      
    
    {\displaystyle \left(W_{Q},W_{K},W_{V}\right)}
   matrices is called an attention head, and each layer in a Transformer model has multiple attention heads. While one attention head attends to the tokens that are relevant to each token, with multiple attention heads the model can learn to do this for different definitions of ""relevance"". Research has shown that many attention heads in Transformers encode relevance relations that are interpretable by humans. For example there are attention heads that, for every token, attend mostly to the next word, or attention heads that mainly attend from verbs to their direct objects. Since Transformer models have multiple attention heads, they can perform computations in parallel, which allows for a fast processing of the input sequence. The multiple outputs for the multi-head attention layer are concatenated to pass into the feed-forward neural network layers.

Encoder
Each encoder consists of two major components: a self-attention mechanism and a feed-forward neural network. The self-attention mechanism takes in a set of input encodings from the previous encoder and weighs their relevance to each other to generate a set of output encodings. The feed-forward neural network then further processes each output encoding individually. These output encodings are finally passed to the next encoder as its input, as well as the decoders.
The first encoder takes positional information and embeddings of the input sequence as its input, rather than encodings. The positional information is necessary for the Transformer to make use of the order of the sequence, because no other part of the Transformer makes use of this.

Decoder
Each decoder consists of three major components: a self-attention mechanism, an attention mechanism over the encodings, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders.Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. Since the transformer should not use the current or future output to predict an output though, the output sequence must be partially masked to prevent this reverse information flow. The last decoder is followed by a final linear transformation and softmax layer, to produce the output probabilities over the vocabulary.

Pseudo code
Below is pseudo code for an implementation of the Transformer variant known as the ""vanilla"" transformer:

Alternatives
Training Transformer-based architectures can be very expensive, especially for long sentences. Alternative architectures include the Reformer, which reduces the computational load from 
  
    
      
        O
        (
        
          N
          
            2
          
        
        )
      
    
    {\displaystyle O(N^{2})}
   to 
  
    
      
        O
        (
        N
        ln
        ⁡
        N
        )
      
    
    {\displaystyle O(N\ln N)}
  , where 
  
    
      
        N
      
    
    {\displaystyle N}
   is the length of the sequence. This is done using locality-sensitive hashing and reversible layers.A benchmark for comparing different transformer architectures was introduced in late 2020.

Training
Transformers typically undergo semi-supervised learning involving unsupervised pretraining followed by supervised fine-tuning. Pretraining is typically done on a much larger dataset than fine-tuning, due to the restricted availability of labeled training data. Tasks for pretraining and fine-tuning commonly include:

next-sentence prediction
question answering
reading comprehension
sentiment analysis
paraphrasing

Applications
The Transformer finds most of its applications in the field of natural language processing (NLP), for example the tasks of machine translation and time series prediction.  Many pretrained models such as GPT-2, GPT-3, BERT, XLNet, and RoBERTa demonstrate the ability of Transformers to perform a wide variety of such NLP-related tasks, and have the potential to find real-world applications. These may include:

machine translation
document summarization
document generation
named entity recognition (NER)
biological sequence analysisIn 2020, it was shown that the transformer architecture, more specifically GPT-2, could be fine-tuned to play chess. Transformers have also been applied to image processing with results showing their ability to compete with convolutional neural networks.

Implementations
The Transformer model has been implemented in major deep learning frameworks such as TensorFlow and PyTorch.
Transformers is a library produced by Hugging Face which supplies Transformer-based architectures and pretrained models. The library is free software and available on GitHub. Its models are available both in PyTorch and TensorFlow format.

References
Further reading
Hubert Ramsauer et al (2020), ""Hopfield Networks is All You Need"", preprint submitted for ICLR 2021. arXiv:2008.02217; see also authors' blog– Discussion of the effect of a transformer layer as equivalent to a Hopfield update, bringing the input closer to one of the fixed points (representable patterns) of a continuous-valued Hopfield network

External links
Alexander Rush, The Annotated Transformer, Harvard NLP group, 3 April 2018",https://en.wikipedia.org/wiki/Transformer_(machine_learning_model),"['Articles with short description', 'Artificial neural networks', 'CS1 errors: missing periodical', 'CS1 maint: multiple names: authors list', 'Short description is different from Wikidata']",Data Science
175,Turing award,"The ACM A. M. Turing Award is an annual prize given by the Association for Computing Machinery (ACM) for contributions ""of lasting and major technical importance to the computer field"". It is generally recognized as the highest distinction in computer science, or the ""Nobel Prize of Computing"".The award is named after Alan Turing, who was a British mathematician and reader in mathematics at the University of Manchester. Turing is often credited as being the key founder of theoretical computer science and artificial intelligence. From 2007 to 2013, the award was accompanied by an additional prize of US$250,000, with financial support provided by Intel and Google. Since 2014, the award has been accompanied by a prize of US$1 million, with financial support provided by Google.The first recipient, in 1966, was Alan Perlis, of Carnegie Mellon University. The first female recipient was Frances E. Allen of IBM in 2006.

Recipients
See also
List of ACM Awards
List of computer science awards
List of prizes known as the Nobel of a field
List of prizes named after people
IEEE John von Neumann Medal
Turing Lecture
Nobel Prize
Schock Prize
Nevanlinna Prize
Kanellakis Award
Millennium Technology Prize

References
External links
ACM Chronological listing of Turing Laureates
Visualizing Turing Award Laureates
ACM A.M. Turing Award Centenary Celebration
ACM A.M. Turing Award Laureate Interviews
Celebration of 50 Years of the ACM A.M. Turing Award",https://en.wikipedia.org/wiki/Turing_Award,"['Alan Turing', 'Articles with short description', 'Association for Computing Machinery', 'Awards established in 1966', 'Commons category link is on Wikidata', 'Computer science awards', 'International awards', 'Short description is different from Wikidata', 'Systems sciences awards']",Data Science
176,U-Net,"U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg. The network is based on the fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. Segmentation of a 512 × 512 image takes less than a second on a modern GPU.

Description
The U-Net architecture stems from the so-called “fully convolutional network” first proposed by Long, Shelhamer, and Darrell.The main idea is to supplement a usual contracting network by successive layers, where pooling operations are replaced by upsampling operators. Hence these layers increase the resolution of the output. What's more, a successive convolutional layer can then learn to assemble a precise output based on this information.One important modification in U-Net is that there are a large number of feature channels in the upsampling part, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting part, and yields a u-shaped architecture. The network only uses the valid part of each convolution without any fully connected layers. To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.

History
U-Net was created by Olaf Ronneberger, Philipp Fischer, Thomas Brox in 2015 at the paper “U-Net: Convolutional Networks for Biomedical Image Segmentation”. It's an improvement and development of FCN: Evan Shelhamer, Jonathan Long, Trevor Darrell (2014). ""Fully convolutional networks for semantic segmentation"".

Network architecture
The network consists of a contracting path and an expansive path, which gives it the u-shaped architecture. The contracting path is a typical convolutional network that consists of repeated application of convolutions, each followed by a rectified linear unit (ReLU) and a max pooling operation. During the contraction, the spatial information is reduced while feature information is increased. The expansive pathway combines the feature and spatial information through a sequence of up-convolutions and concatenations with high-resolution features from the contracting path.

Applications
There are many applications of U-Net in biomedical image segmentation, such as brain image segmentation (''BRATS'') and liver image segmentation (""siliver07""). Variations of the U-Net have also been applied for medical image reconstruction. Here are some variants and applications of U-Net as follows:

Pixel-wise regression using U-Net and its application on pansharpening;
3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation;
TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation.

Implementations
jakeret (2017): ""Tensorflow Unet""U-Net source code from Pattern Recognition and Image Processing at Computer Science Department of the University of Freiburg, Germany.The basic articles on the system have been cited 3693, 7049, 442 and 22 times respectively on Google Scholar as of December 24, 2018.


== References ==",https://en.wikipedia.org/wiki/U-Net,"['Articles with short description', 'Artificial neural networks', 'Deep learning', 'Short description is different from Wikidata', 'University of Freiburg']",Data Science
177,Unstructured data,"Unstructured data (or unstructured information) is information that either does not have a pre-defined data model or is not organized in a pre-defined manner. Unstructured information is typically text-heavy, but may contain data such as dates, numbers, and facts as well. This results in irregularities and ambiguities that make it difficult to understand using traditional programs as compared to data stored in fielded form in databases or annotated (semantically tagged) in documents.
In 1998, Merrill Lynch said ""unstructured data comprises the vast majority of data found in an organization, some estimates run as high as 80%."" It's unclear what the source of this number is, but nonetheless it is accepted by some. Other sources have reported similar or higher percentages of unstructured data.As of 2012, IDC and Dell EMC project that data will grow to 40 zettabytes by 2020, resulting in a 50-fold growth from the beginning of 2010. More recently, IDC and Seagate predict that the global datasphere will grow to 163 zettabytes by 2025  and majority of that will be unstructured. The Computer World magazine states that unstructured information might account for more than 70%–80% of all data in organizations.[1]

Background
The earliest research into business intelligence focused in on unstructured textual data, rather than numerical data. As early as 1958, computer science researchers like H.P. Luhn were particularly concerned with the extraction and classification of unstructured text. However, only since the turn of the century has the technology caught up with the research interest. In 2004, the SAS Institute developed the SAS Text Miner, which uses Singular Value Decomposition (SVD) to reduce a hyper-dimensional textual space into smaller dimensions for significantly more efficient machine-analysis. The mathematical and technological advances sparked by machine textual analysis prompted a number of businesses to research applications, leading to the development of fields like sentiment analysis, voice of the customer mining, and call center optimization. The emergence of Big Data in the late 2000s led to a heightened interest in the applications of unstructured data analytics in contemporary fields such as predictive analytics and root cause analysis.

Issues with terminology
The term is imprecise for several reasons:

Structure, while not formally defined, can still be implied.
Data with some form of structure may still be characterized as unstructured if its structure is not helpful for the processing task at hand.
Unstructured information might have some structure (semi-structured) or even be highly structured but in ways that are unanticipated or unannounced.

Dealing with unstructured data
Techniques such as data mining, natural language processing (NLP), and text analytics provide different methods to find patterns in, or otherwise interpret, this information. Common techniques for structuring text usually involve manual tagging with metadata or part-of-speech tagging for further text mining-based structuring. The Unstructured Information Management Architecture (UIMA) standard provided a common framework for processing this information to extract meaning and create structured data about the information.Software that creates machine-processable structure can utilize the linguistic, auditory, and visual structure that exist in all forms of human communication. Algorithms can infer this inherent structure from text, for instance, by examining word morphology, sentence syntax, and other small- and large-scale patterns. Unstructured information can then be enriched and tagged to address ambiguities and relevancy-based techniques then used to facilitate search and discovery. Examples of ""unstructured data"" may include books, journals, documents, metadata, health records, audio, video, analog data, images, files, and unstructured text such as the body of an e-mail message, Web page, or word-processor document. While the main content being conveyed does not have a defined structure, it generally comes packaged in objects (e.g. in files or documents, ...) that themselves have structure and are thus a mix of structured and unstructured data, but collectively this is still referred to as ""unstructured data"". For example, an HTML web page is tagged, but HTML mark-up typically serves solely for rendering. It does not capture the meaning or function of tagged elements in ways that support automated processing of the information content of the page. XHTML tagging does allow machine processing of elements, although it typically does not capture or convey the semantic meaning of tagged terms.
Since unstructured data commonly occurs in electronic documents, the use of a content or document management system which can categorize entire documents is often preferred over data transfer and manipulation from within the documents. Document management thus provides the means to convey structure onto document collections.
Search engines have become popular tools for indexing and searching through such data, especially text.

Approaches in natural language processing
Specific computational workflows have been developed to impose structure upon the unstructured data contained within text documents. These workflows are generally designed to handle sets of thousands or even millions of documents, or far more than manual approaches to annotation may permit. Several of these approaches are based upon the concept of online analytical processing, or OLAP, and may be supported by data models such as text cubes. Once document metadata is available through a data model, generating summaries of subsets of documents (i.e., cells within a text cube) may be performed with phrase-based approaches.

Approaches in medicine and biomedical research
Biomedical research generates one major source of unstructured data as researchers often publish their findings in scholarly journals. Though the language in these documents is challenging to derive structural elements from (e.g., due to the complicated technical vocabulary contained within and the domain knowledge required to fully contextualize observations), the results of these activities may yield links between technical and medical studies and clues regarding new disease therapies. Recent efforts to enforce structure upon biomedical documents include self-organizing map approaches for identifying topics among documents, general-purpose unsupervised algorithms, and an application of the CaseOLAP workflow to determine associations between protein names and cardiovascular disease topics in the literature. CaseOLAP defines phrase-category relationships in an accurate (identifies relationships), consistent (highly reproducible), and efficient manner. This platform offers enhanced accessibility and empowers the biomedical community with phrase-mining tools for widespread biomedical research applications.

See also
Clustering
Pattern recognition
List of text mining software

Notes
^  Today's Challenge in Government:  What to do with Unstructured Information and Why Doing Nothing Isn't An Option,  Noel Yuhanna, Principal Analyst, Forrester Research, Nov 2010

References
External links
Matching Unstructured Data and Structured Data
a brief description for Structured Data",https://en.wikipedia.org/wiki/Unstructured_data,"['All articles containing potentially dated statements', 'Articles containing potentially dated statements from 2012', 'Articles with short description', 'Business intelligence', 'Data', 'Information technology management', 'Short description matches Wikidata']",Data Science
178,Vapnik–Chervonenkis theory,"Vapnik–Chervonenkis theory (also known as VC theory) was developed during 1960–1990 by Vladimir Vapnik and Alexey Chervonenkis. The theory is a form of computational learning theory, which attempts to explain the learning process from a statistical point of view.
VC theory is related to statistical learning theory  and to empirical processes.  Richard M. Dudley and Vladimir Vapnik, among others, have applied VC-theory to empirical processes.

Introduction
VC theory covers at least four parts (as explained in The Nature of Statistical Learning Theory[1]):

Theory of consistency of learning processes
What are (necessary and sufficient) conditions for consistency of a learning process based on the empirical risk minimization principle?
Nonasymptotic theory of the rate of convergence of learning processes
How fast is the rate of convergence of the learning process?
Theory of controlling the generalization ability of learning processes
How can one control the rate of convergence (the generalization ability) of the learning process?
Theory of constructing learning machines
How can one construct algorithms that can control the generalization ability?VC Theory is a major subbranch of statistical learning theory. One of its main applications in statistical learning theory is to provide generalization conditions for learning algorithms.  From this point of view, VC theory is related to stability, which is an alternative approach for characterizing generalization.
In addition, VC theory and VC dimension are instrumental in the theory of empirical processes, in the case of processes indexed by VC classes. Arguably these are the most important applications of the VC theory, and are employed in proving generalization. Several techniques will be introduced that are widely used in the empirical process and VC theory. The discussion is mainly based on the book Weak Convergence and Empirical Processes: With Applications to Statistics.[2]

Overview of VC theory in Empirical Processes
Background on Empirical Processes
Let 
  
    
      
        
          X
          
            1
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{1},\ldots ,X_{n}}
   be random elements defined on a measurable space 
  
    
      
        (
        
          
            X
          
        
        ,
        
          
            A
          
        
        )
      
    
    {\displaystyle ({\mathcal {X}},{\mathcal {A}})}
  . For any measure 
  
    
      
        Q
      
    
    {\displaystyle Q}
   on 
  
    
      
        (
        
          
            X
          
        
        ,
        
          
            A
          
        
        )
      
    
    {\displaystyle ({\mathcal {X}},{\mathcal {A}})}
  , and any measurable functions 
  
    
      
        f
        :
        
          
            X
          
        
        →
        
          R
        
      
    
    {\displaystyle f:{\mathcal {X}}\to \mathbf {R} }
  , define

  
    
      
        Q
        f
        =
        ∫
        f
        d
        Q
      
    
    {\displaystyle Qf=\int fdQ}
  Measurability issues will be ignored here, for more technical detail see [3]. Let 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   be a class of measurable functions 
  
    
      
        f
        :
        
          
            X
          
        
        →
        
          R
        
      
    
    {\displaystyle f:{\mathcal {X}}\to \mathbf {R} }
   and define:

  
    
      
        ‖
        Q
        
          ‖
          
            
              F
            
          
        
        =
        sup
        {
        |
        Q
        f
        |
         
        :
         
        f
        ∈
        
          
            F
          
        
        }
        .
      
    
    {\displaystyle \|Q\|_{\mathcal {F}}=\sup\{\vert Qf\vert \ :\ f\in {\mathcal {F}}\}.}
  Define the empirical measure

  
    
      
        
          
            P
          
          
            n
          
        
        =
        
          n
          
            −
            1
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          δ
          
            
              X
              
                i
              
            
          
        
        ,
      
    
    {\displaystyle \mathbb {P} _{n}=n^{-1}\sum _{i=1}^{n}\delta _{X_{i}},}
  where δ here stands for the Dirac measure. The empirical measure induces a map 
  
    
      
        
          
            F
          
        
        →
        
          R
        
      
    
    {\displaystyle {\mathcal {F}}\to \mathbf {R} }
   given by:

  
    
      
        f
        ↦
        
          
            P
          
          
            n
          
        
        f
        =
        
          
            1
            n
          
        
        (
        f
        (
        
          X
          
            1
          
        
        )
        +
        .
        .
        .
        +
        f
        (
        
          X
          
            n
          
        
        )
        )
      
    
    {\displaystyle f\mapsto \mathbb {P} _{n}f={\frac {1}{n}}(f(X_{1})+...+f(X_{n}))}
  Now suppose P is the underlying true distribution of the data, which is unknown. Empirical Processes theory aims at identifying classes 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   for which statements such as the following hold:

uniform law of large numbers:
  
    
      
        ‖
        
          
            P
          
          
            n
          
        
        −
        P
        
          ‖
          
            
              F
            
          
        
        
          
            →
            n
          
        
        0
        ,
      
    
    {\displaystyle \|\mathbb {P} _{n}-P\|_{\mathcal {F}}{\underset {n}{\to }}0,}
  That is, as 
  
    
      
        n
        →
        ∞
      
    
    {\displaystyle n\to \infty }
  ,

  
    
      
        
          |
          
            
              
                1
                n
              
            
            (
            f
            (
            
              X
              
                1
              
            
            )
            +
            .
            .
            .
            +
            f
            (
            
              X
              
                n
              
            
            )
            )
            −
            ∫
            f
            d
            P
          
          |
        
        →
        0
      
    
    {\displaystyle \left|{\frac {1}{n}}(f(X_{1})+...+f(X_{n}))-\int fdP\right|\to 0}
  
uniformly for all 
  
    
      
        f
        ∈
        
          
            F
          
        
      
    
    {\displaystyle f\in {\mathcal {F}}}
  .uniform central limit theorem:
  
    
      
        
          
            G
          
          
            n
          
        
        =
        
          
            n
          
        
        (
        
          
            P
          
          
            n
          
        
        −
        P
        )
        ⇝
        
          G
        
        ,
        
        
          in 
        
        
          ℓ
          
            ∞
          
        
        (
        
          
            F
          
        
        )
      
    
    {\displaystyle \mathbb {G} _{n}={\sqrt {n}}(\mathbb {P} _{n}-P)\rightsquigarrow \mathbb {G} ,\quad {\text{in }}\ell ^{\infty }({\mathcal {F}})}
  In the former case 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   is called Glivenko-Cantelli class, and in the latter case (under the assumption 
  
    
      
        ∀
        x
        ,
        
          sup
          
            f
            ∈
            
              
                F
              
            
          
        
        |
        f
        (
        x
        )
        −
        P
        f
        |
        <
        ∞
      
    
    {\displaystyle \forall x,\sup \nolimits _{f\in {\mathcal {F}}}\vert f(x)-Pf\vert <\infty }
  ) the class 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   is called Donsker or P-Donsker. A Donsker class is Glivenko-Cantelli in probability by an application of Slutsky's theorem .
These statements are true for a single 
  
    
      
        f
      
    
    {\displaystyle f}
  , by standard LLN, CLT arguments under regularity conditions, and the difficulty in the Empirical Processes comes in because joint statements are being made for all 
  
    
      
        f
        ∈
        
          
            F
          
        
      
    
    {\displaystyle f\in {\mathcal {F}}}
  . Intuitively then, the set 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   cannot be too large, and as it turns out that the geometry of 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   plays a very important role.
One way of measuring how big the function set 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   is to use the so-called covering numbers. The covering number

  
    
      
        N
        (
        ε
        ,
        
          
            F
          
        
        ,
        ‖
        ⋅
        ‖
        )
      
    
    {\displaystyle N(\varepsilon ,{\mathcal {F}},\|\cdot \|)}
  is the minimal number of balls 
  
    
      
        {
        g
        :
        ‖
        g
        −
        f
        ‖
        <
        ε
        }
      
    
    {\displaystyle \{g:\|g-f\|<\varepsilon \}}
   needed to cover the set 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   (here it is obviously assumed that there is an underlying norm on 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
  ).  The entropy is the logarithm of the covering number.
Two sufficient conditions are provided below, under which it can be proved that the set 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   is Glivenko-Cantelli or Donsker.
A class 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   is P-Glivenko-Cantelli if it is P-measurable with envelope F such that 
  
    
      
        
          P
          
            ∗
          
        
        F
        <
        ∞
      
    
    {\displaystyle P^{\ast }F<\infty }
   and satisfies:

  
    
      
        ∀
        ε
        >
        0
        
        
          sup
          
            Q
          
        
        N
        (
        ε
        ‖
        F
        
          ‖
          
            Q
          
        
        ,
        
          
            F
          
        
        ,
        
          L
          
            1
          
        
        (
        Q
        )
        )
        <
        ∞
        .
      
    
    {\displaystyle \forall \varepsilon >0\quad \sup \nolimits _{Q}N(\varepsilon \|F\|_{Q},{\mathcal {F}},L_{1}(Q))<\infty .}
  The next condition is a version of the celebrated Dudley's theorem. If 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   is a class of functions such that

  
    
      
        
          ∫
          
            0
          
          
            ∞
          
        
        
          sup
          
            Q
          
        
        
          
            log
            ⁡
            N
            
              (
              
                ε
                ‖
                F
                
                  ‖
                  
                    Q
                    ,
                    2
                  
                
                ,
                
                  
                    F
                  
                
                ,
                
                  L
                  
                    2
                  
                
                (
                Q
                )
              
              )
            
          
        
        d
        ε
        <
        ∞
      
    
    {\displaystyle \int _{0}^{\infty }\sup \nolimits _{Q}{\sqrt {\log N\left(\varepsilon \|F\|_{Q,2},{\mathcal {F}},L_{2}(Q)\right)}}d\varepsilon <\infty }
  then 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   is P-Donsker for every probability measure P such that 
  
    
      
        
          P
          
            ∗
          
        
        
          F
          
            2
          
        
        <
        ∞
      
    
    {\displaystyle P^{\ast }F^{2}<\infty }
  . In the last integral, the notation means

  
    
      
        ‖
        f
        
          ‖
          
            Q
            ,
            2
          
        
        =
        
          
            (
            
              ∫
              
                |
              
              f
              
                
                  |
                
                
                  2
                
              
              d
              Q
            
            )
          
          
            
              1
              2
            
          
        
      
    
    {\displaystyle \|f\|_{Q,2}=\left(\int |f|^{2}dQ\right)^{\frac {1}{2}}}
  .

Symmetrization
The majority of the arguments of how to bound the empirical process, rely on symmetrization, maximal and concentration inequalities and chaining.  Symmetrization is usually the first step of the proofs, and since it is used in many machine learning proofs on bounding empirical loss functions (including the proof of the VC inequality which is discussed in the next section) it is presented here.
Consider the empirical process:

  
    
      
        f
        ↦
        (
        
          
            P
          
          
            n
          
        
        −
        P
        )
        f
        =
        
          
            
              1
              n
            
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        (
        f
        (
        
          X
          
            i
          
        
        )
        −
        P
        f
        )
      
    
    {\displaystyle f\mapsto (\mathbb {P} _{n}-P)f={\dfrac {1}{n}}\sum _{i=1}^{n}(f(X_{i})-Pf)}
  Turns out that there is a connection between the empirical and the following symmetrized process:

  
    
      
        f
        ↦
        
          
            P
          
          
            n
          
          
            0
          
        
        f
        =
        
          
            
              1
              n
            
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          ε
          
            i
          
        
        f
        (
        
          X
          
            i
          
        
        )
      
    
    {\displaystyle f\mapsto \mathbb {P} _{n}^{0}f={\dfrac {1}{n}}\sum _{i=1}^{n}\varepsilon _{i}f(X_{i})}
  The symmetrized process is a Rademacher process, conditionally on the data 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
  . Therefore, it is a sub-Gaussian process by Hoeffding's inequality.
Lemma (Symmetrization). For every nondecreasing, convex Φ: R → R and class of measurable functions 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
  ,

  
    
      
        
          E
        
        Φ
        (
        ‖
        
          
            P
          
          
            n
          
        
        −
        P
        
          ‖
          
            
              F
            
          
        
        )
        ≤
        
          E
        
        Φ
        
          (
          
            2
            
              
                ‖
                
                  
                    P
                  
                  
                    n
                  
                  
                    0
                  
                
                ‖
              
              
                
                  F
                
              
            
          
          )
        
      
    
    {\displaystyle \mathbb {E} \Phi (\|\mathbb {P} _{n}-P\|_{\mathcal {F}})\leq \mathbb {E} \Phi \left(2\left\|\mathbb {P} _{n}^{0}\right\|_{\mathcal {F}}\right)}
  The proof of the Symmetrization lemma relies on introducing independent copies of the original variables 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
   (sometimes referred to as a ghost sample) and replacing the inner expectation of the LHS by these copies. After an application of Jensen's inequality different signs could be introduced (hence the name symmetrization) without changing the expectation. The proof can be found below because of its instructive nature.

A typical way of proving empirical CLTs, first uses symmetrization to pass the empirical process to 
  
    
      
        
          
            P
          
          
            n
          
          
            0
          
        
      
    
    {\displaystyle \mathbb {P} _{n}^{0}}
   and then argue conditionally on the data, using the fact that Rademacher processes are simple processes with nice properties.

VC Connection
It turns out that there is a fascinating connection between certain combinatorial properties of the set 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   and the entropy numbers. Uniform  covering numbers can be controlled by the notion of Vapnik-Chervonenkis classes of sets - or shortly VC sets.
Consider a collection  
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   of subsets of the sample space 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
  . 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   is said to pick out a certain subset 
  
    
      
        W
      
    
    {\displaystyle W}
   of the finite set 
  
    
      
        S
        =
        {
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        }
        ⊂
        
          
            X
          
        
      
    
    {\displaystyle S=\{x_{1},\ldots ,x_{n}\}\subset {\mathcal {X}}}
   if 
  
    
      
        W
        =
        S
        ∩
        C
      
    
    {\displaystyle W=S\cap C}
   for some 
  
    
      
        C
        ∈
        
          
            C
          
        
      
    
    {\displaystyle C\in {\mathcal {C}}}
  . 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   is said to shatter S if it picks out each of its 2n subsets. The VC-index (similar to VC dimension + 1 for an appropriately chosen classifier set) 
  
    
      
        V
        (
        
          
            C
          
        
        )
      
    
    {\displaystyle V({\mathcal {C}})}
   of 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   is the smallest n for which no set of size n is shattered by 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
  .
Sauer's lemma then states that the number 
  
    
      
        
          Δ
          
            n
          
        
        (
        
          
            C
          
        
        ,
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
      
    
    {\displaystyle \Delta _{n}({\mathcal {C}},x_{1},\ldots ,x_{n})}
   of subsets picked out by a VC-class 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   satisfies:

  
    
      
        
          max
          
            
              x
              
                1
              
            
            ,
            …
            ,
            
              x
              
                n
              
            
          
        
        
          Δ
          
            n
          
        
        (
        
          
            C
          
        
        ,
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
        ≤
        
          ∑
          
            j
            =
            0
          
          
            V
            (
            
              
                C
              
            
            )
            −
            1
          
        
        
          
            
              (
            
            
              n
              j
            
            
              )
            
          
        
        ≤
        
          
            (
            
              
                
                  n
                  e
                
                
                  V
                  (
                  
                    
                      C
                    
                  
                  )
                  −
                  1
                
              
            
            )
          
          
            V
            (
            
              
                C
              
            
            )
            −
            1
          
        
      
    
    {\displaystyle \max _{x_{1},\ldots ,x_{n}}\Delta _{n}({\mathcal {C}},x_{1},\ldots ,x_{n})\leq \sum _{j=0}^{V({\mathcal {C}})-1}{n \choose j}\leq \left({\frac {ne}{V({\mathcal {C}})-1}}\right)^{V({\mathcal {C}})-1}}
  Which is a polynomial number 
  
    
      
        O
        (
        
          n
          
            V
            (
            
              
                C
              
            
            )
            −
            1
          
        
        )
      
    
    {\displaystyle O(n^{V({\mathcal {C}})-1})}
   of subsets rather than an exponential number. Intuitively this means that a finite VC-index implies that 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   has an apparent simplistic structure.
A similar bound can be shown (with a different constant, same rate) for the so-called VC subgraph classes. For a function 
  
    
      
        f
        :
        
          
            X
          
        
        →
        
          R
        
      
    
    {\displaystyle f:{\mathcal {X}}\to \mathbf {R} }
   the subgraph is a subset of 
  
    
      
        
          
            X
          
        
        ×
        
          R
        
      
    
    {\displaystyle {\mathcal {X}}\times \mathbf {R} }
   such that: 
  
    
      
        {
        (
        x
        ,
        t
        )
        :
        t
        <
        f
        (
        x
        )
        }
      
    
    {\displaystyle \{(x,t):t<f(x)\}}
  . A collection of 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   is called a VC subgraph class if all subgraphs form a VC-class.
Consider a set of indicator functions 
  
    
      
        
          
            
              I
            
          
          
            
              C
            
          
        
        =
        {
        
          1
          
            C
          
        
        :
        C
        ∈
        
          
            C
          
        
        }
      
    
    {\displaystyle {\mathcal {I}}_{\mathcal {C}}=\{1_{C}:C\in {\mathcal {C}}\}}
   in 
  
    
      
        
          L
          
            1
          
        
        (
        Q
        )
      
    
    {\displaystyle L_{1}(Q)}
   for discrete empirical type of measure Q (or equivalently for any probability measure Q). It can then be shown that quite remarkably, for 
  
    
      
        r
        ≥
        1
      
    
    {\displaystyle r\geq 1}
  :

  
    
      
        N
        (
        ε
        ,
        
          
            
              I
            
          
          
            
              C
            
          
        
        ,
        
          L
          
            r
          
        
        (
        Q
        )
        )
        ≤
        K
        V
        (
        
          
            C
          
        
        )
        (
        4
        e
        
          )
          
            V
            (
            
              
                C
              
            
            )
          
        
        
          ε
          
            −
            r
            (
            V
            (
            
              
                C
              
            
            )
            −
            1
            )
          
        
      
    
    {\displaystyle N(\varepsilon ,{\mathcal {I}}_{\mathcal {C}},L_{r}(Q))\leq KV({\mathcal {C}})(4e)^{V({\mathcal {C}})}\varepsilon ^{-r(V({\mathcal {C}})-1)}}
  Further consider the symmetric convex hull of a set 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
  : 
  
    
      
        sconv
        ⁡
        
          
            F
          
        
      
    
    {\displaystyle \operatorname {sconv} {\mathcal {F}}}
   being the collection of functions of the form 
  
    
      
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          α
          
            i
          
        
        
          f
          
            i
          
        
      
    
    {\displaystyle \sum _{i=1}^{m}\alpha _{i}f_{i}}
   with 
  
    
      
        
          ∑
          
            i
            =
            1
          
          
            m
          
        
        
          |
        
        
          α
          
            i
          
        
        
          |
        
        ≤
        1
      
    
    {\displaystyle \sum _{i=1}^{m}|\alpha _{i}|\leq 1}
  . Then if

  
    
      
        N
        
          (
          
            ε
            ‖
            F
            
              ‖
              
                Q
                ,
                2
              
            
            ,
            
              
                F
              
            
            ,
            
              L
              
                2
              
            
            (
            Q
            )
          
          )
        
        ≤
        C
        
          ε
          
            −
            V
          
        
      
    
    {\displaystyle N\left(\varepsilon \|F\|_{Q,2},{\mathcal {F}},L_{2}(Q)\right)\leq C\varepsilon ^{-V}}
  the following is valid for the convex hull of 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
  :

  
    
      
        log
        ⁡
        N
        
          (
          
            ε
            ‖
            F
            
              ‖
              
                Q
                ,
                2
              
            
            ,
            sconv
            ⁡
            
              
                F
              
            
            ,
            
              L
              
                2
              
            
            (
            Q
            )
          
          )
        
        ≤
        K
        
          ε
          
            −
            
              
                
                  2
                  V
                
                
                  V
                  +
                  2
                
              
            
          
        
      
    
    {\displaystyle \log N\left(\varepsilon \|F\|_{Q,2},\operatorname {sconv} {\mathcal {F}},L_{2}(Q)\right)\leq K\varepsilon ^{-{\frac {2V}{V+2}}}}
  The important consequence of this fact is that

  
    
      
        
          
            
              2
              V
            
            
              V
              +
              2
            
          
        
        >
        2
        ,
      
    
    {\displaystyle {\frac {2V}{V+2}}>2,}
  which is just enough so that the entropy integral is going to converge, and therefore the class 
  
    
      
        sconv
        ⁡
        
          
            F
          
        
      
    
    {\displaystyle \operatorname {sconv} {\mathcal {F}}}
   is going to be P-Donsker.
Finally an example of a VC-subgraph class is considered. Any finite-dimensional vector space 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   of measurable functions 
  
    
      
        f
        :
        
          
            X
          
        
        →
        
          R
        
      
    
    {\displaystyle f:{\mathcal {X}}\to \mathbf {R} }
   is VC-subgraph of index smaller than or equal to 
  
    
      
        dim
        ⁡
        (
        
          
            F
          
        
        )
        +
        2
      
    
    {\displaystyle \dim({\mathcal {F}})+2}
  .

There are generalizations of the notion VC subgraph class, e.g. there is the notion of pseudo-dimension. The interested reader can look into[4].

VC Inequality
A similar setting is considered, which is more common to machine learning. Let 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
   is a feature space and 
  
    
      
        
          
            Y
          
        
        =
        {
        0
        ,
        1
        }
      
    
    {\displaystyle {\mathcal {Y}}=\{0,1\}}
  . A function 
  
    
      
        f
        :
        
          
            X
          
        
        →
        
          
            Y
          
        
      
    
    {\displaystyle f:{\mathcal {X}}\to {\mathcal {Y}}}
   is called a classifier. Let 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   be a set of classifiers. Similarly to the previous section, define the shattering coefficient (also known as growth function):

  
    
      
        S
        (
        
          
            F
          
        
        ,
        n
        )
        =
        
          max
          
            
              x
              
                1
              
            
            ,
            …
            ,
            
              x
              
                n
              
            
          
        
        
          |
        
        {
        (
        f
        (
        
          x
          
            1
          
        
        )
        ,
        …
        ,
        f
        (
        
          x
          
            n
          
        
        )
        )
        ,
        f
        ∈
        
          
            F
          
        
        }
        
          |
        
      
    
    {\displaystyle S({\mathcal {F}},n)=\max _{x_{1},\ldots ,x_{n}}|\{(f(x_{1}),\ldots ,f(x_{n})),f\in {\mathcal {F}}\}|}
  Note here that there is a 1:1 go  between each of the functions in 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   and the set on which the function is 1. We can thus define 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   to be the collection of subsets obtained from the above mapping for every 
  
    
      
        f
        ∈
        
          
            F
          
        
      
    
    {\displaystyle f\in {\mathcal {F}}}
  . Therefore, in terms of the previous section the shattering coefficient is precisely

  
    
      
        
          max
          
            
              x
              
                1
              
            
            ,
            …
            ,
            
              x
              
                n
              
            
          
        
        
          Δ
          
            n
          
        
        (
        
          
            C
          
        
        ,
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
      
    
    {\displaystyle \max _{x_{1},\ldots ,x_{n}}\Delta _{n}({\mathcal {C}},x_{1},\ldots ,x_{n})}
  .This equivalence together with Sauer's Lemma implies that 
  
    
      
        S
        (
        
          
            F
          
        
        ,
        n
        )
      
    
    {\displaystyle S({\mathcal {F}},n)}
   is going to be polynomial in n, for sufficiently large n provided that the collection 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\mathcal {C}}}
   has a finite VC-index.
Let 
  
    
      
        
          D
          
            n
          
        
        =
        {
        (
        
          X
          
            1
          
        
        ,
        
          Y
          
            1
          
        
        )
        ,
        …
        ,
        (
        
          X
          
            n
          
        
        ,
        
          Y
          
            m
          
        
        )
        }
      
    
    {\displaystyle D_{n}=\{(X_{1},Y_{1}),\ldots ,(X_{n},Y_{m})\}}
   is an observed dataset. Assume that the data is generated by an unknown probability distribution 
  
    
      
        
          P
          
            X
            Y
          
        
      
    
    {\displaystyle P_{XY}}
  . Define 
  
    
      
        R
        (
        f
        )
        =
        P
        (
        f
        (
        X
        )
        ≠
        Y
        )
      
    
    {\displaystyle R(f)=P(f(X)\neq Y)}
   to be the expected 0/1 loss. Of course since 
  
    
      
        
          P
          
            X
            Y
          
        
      
    
    {\displaystyle P_{XY}}
   is unknown in general, one has no access to 
  
    
      
        R
        (
        f
        )
      
    
    {\displaystyle R(f)}
  . However the empirical risk, given by:

  
    
      
        
          
            
              
                R
                ^
              
            
          
          
            n
          
        
        (
        f
        )
        =
        
          
            
              1
              n
            
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          I
        
        (
        f
        (
        
          X
          
            i
          
        
        )
        ≠
        
          Y
          
            i
          
        
        )
      
    
    {\displaystyle {\hat {R}}_{n}(f)={\dfrac {1}{n}}\sum _{i=1}^{n}\mathbb {I} (f(X_{i})\neq Y_{i})}
  can certainly be evaluated. Then one has the following Theorem:

Theorem (VC Inequality)
For binary classification and the 0/1 loss function we have the following generalization bounds:

  
    
      
        
          
            
              
                P
                
                  (
                  
                    
                      sup
                      
                        f
                        ∈
                        
                          
                            F
                          
                        
                      
                    
                    
                      |
                      
                        
                          
                            
                              
                                R
                                ^
                              
                            
                          
                          
                            n
                          
                        
                        (
                        f
                        )
                        −
                        R
                        (
                        f
                        )
                      
                      |
                    
                    >
                    ε
                  
                  )
                
              
              
                
                ≤
                8
                S
                (
                
                  
                    F
                  
                
                ,
                n
                )
                
                  e
                  
                    −
                    n
                    
                      ε
                      
                        2
                      
                    
                    
                      /
                    
                    32
                  
                
              
            
            
              
                
                  E
                
                
                  [
                  
                    
                      sup
                      
                        f
                        ∈
                        
                          
                            F
                          
                        
                      
                    
                    
                      |
                      
                        
                          
                            
                              
                                R
                                ^
                              
                            
                          
                          
                            n
                          
                        
                        (
                        f
                        )
                        −
                        R
                        (
                        f
                        )
                      
                      |
                    
                  
                  ]
                
              
              
                
                ≤
                2
                
                  
                    
                      
                        
                          log
                          ⁡
                          S
                          (
                          
                            
                              F
                            
                          
                          ,
                          n
                          )
                          +
                          log
                          ⁡
                          2
                        
                        n
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}P\left(\sup _{f\in {\mathcal {F}}}\left|{\hat {R}}_{n}(f)-R(f)\right|>\varepsilon \right)&\leq 8S({\mathcal {F}},n)e^{-n\varepsilon ^{2}/32}\\\mathbb {E} \left[\sup _{f\in {\mathcal {F}}}\left|{\hat {R}}_{n}(f)-R(f)\right|\right]&\leq 2{\sqrt {\dfrac {\log S({\mathcal {F}},n)+\log 2}{n}}}\end{aligned}}}
  In words the VC inequality is saying that as the sample increases, provided that 
  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   has a finite VC dimension, the empirical 0/1 risk becomes a good proxy for the expected 0/1 risk. Note that both RHS of the two inequalities will converge to 0, provided that 
  
    
      
        S
        (
        
          
            F
          
        
        ,
        n
        )
      
    
    {\displaystyle S({\mathcal {F}},n)}
   grows polynomially in n.
The connection between this framework and the Empirical Process framework is evident. Here one is dealing with a modified empirical process

  
    
      
        
          
            |
            
              
                
                  
                    
                      R
                      ^
                    
                  
                
                
                  n
                
              
              −
              R
            
            |
          
          
            
              F
            
          
        
      
    
    {\displaystyle \left|{\hat {R}}_{n}-R\right|_{\mathcal {F}}}
  but not surprisingly the ideas are the same. The proof of the (first part of) VC inequality, relies on symmetrization, and then argue conditionally on the data using concentration inequalities (in particular Hoeffding's inequality). The interested reader can check the book [5] Theorems 12.4 and 12.5.

References
^ Vapnik, Vladimir N (2000). The Nature of Statistical Learning Theory. Information Science and Statistics. Springer-Verlag. ISBN 978-0-387-98780-4.
Vapnik, Vladimir N (1989). Statistical Learning Theory. Wiley-Interscience. ISBN 978-0-471-03003-4.
^  van der Vaart, Aad W.; Wellner, Jon A. (2000). Weak Convergence and Empirical Processes: With Applications to Statistics (2nd ed.). Springer. ISBN 978-0-387-94640-5.
^ Gyorfi, L.; Devroye, L.; Lugosi, G. (1996). A probabilistic theory of pattern recognition (1st ed.). Springer. ISBN 978-0387946184.
See references in articles: Richard M. Dudley, empirical processes, Shattered set.
^ Pollard, David (1990). Empirical Processes: Theory and Applications. NSF-CBMS Regional Conference Series in Probability and Statistics Volume 2. ISBN 978-0-940600-16-4.
Bousquet, O.; Boucheron, S.; Lugosi, G. (2004). ""Introduction to Statistical Learning Theory"".  In O. Bousquet; U. von Luxburg; G. Ratsch (eds.). Advanced Lectures on Machine Learning. Lecture Notes in Artificial Intelligence. 3176. Springer. pp. 169–207.
Vapnik, V.; Chervonenkis, A. (2004). ""On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities"". Theory Probab. Appl. 16 (2): 264–280. doi:10.1137/1116025.",https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory,"['Articles with short description', 'Computational learning theory', 'Empirical process', 'Short description is different from Wikidata']",Data Science
179,Unsupervised learning,"Unsupervised learning (UL) is a type of algorithm that learns patterns from untagged data. The hope is that through mimicry, the machine is forced to build a compact internal representation of its world. In contrast to supervised learning (SL) where data is tagged by a human, e.g. as ""car"" or ""fish"" etc, UL exhibits self-organization that captures patterns as neuronal predelections or probability densities. The other levels in the supervision spectrum are reinforcement learning where the machine is given only a numerical performance score as its guidance, and semi-supervised learning where a smaller portion of the data is tagged. Two broad methods in UL are Neural Networks and Probabilistic Methods.

Probabilistic methods
Two of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships. Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.
The only requirement to be called an unsupervised learning strategy is to learn a new feature space that captures the characteristics of the original space by maximizing some objective function or minimising some loss function. Therefore, generating a covariance matrix is not unsupervised learning, but taking the eigenvectors of the covariance matrix is because the linear algebra eigendecomposition operation maximizes the variance; this is known as principal component analysis. Similarly, taking the log-transform of a dataset is not unsupervised learning, but passing input data through multiple sigmoid functions while minimising some distance function between the generated and resulting data is, and is known as an Autoencoder.
A central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It could be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution 
  
    
      
        
          p
          
            X
          
        
        (
        x
        
        
          |
        
        
        y
        )
      
    
    {\textstyle p_{X}(x\,|\,y)}
   conditioned on the label 
  
    
      
        y
      
    
    {\textstyle y}
   of input data; unsupervised learning intends to infer an a priori probability distribution 
  
    
      
        
          p
          
            X
          
        
        (
        x
        )
      
    
    {\textstyle p_{X}(x)}
  .

Approaches
Some of the most common algorithms used in unsupervised learning include: (1) Clustering, (2) Anomaly detection, (3) Neural Networks, and (4) Approaches for learning latent variable models.
Each approach uses several methods as follows:

Clustering methods include: hierarchical clustering, k-means, mixture models, DBSCAN, and OPTICS algorithm
Anomaly detection methods include: Local Outlier Factor, and Isolation Forest
Approaches for learning latent variable models such as Expectation–maximization algorithm (EM), Method of moments, and Blind signal separation techniques ( Principal component analysis, Independent component analysis, Non-negative matrix factorization, Singular value decomposition )

Method of moments
One of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays.
In particular, the method of moments is shown to be effective in learning the parameters of latent variable models.
Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.The Expectation–maximization algorithm (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. In contrast, for the method of moments, the global convergence is guaranteed under some conditions.

Neural networks
Basics
First, some vocabulary:
Tasks

UL methods usually prepare a network for generative tasks rather than recognition, but grouping tasks as supervised or not can be hazy. For example, handwriting recognition started off in the 1980s as SL. Then in 2007, UL is used to prime the network for SL afterwards. Currently, SL has regained its position as the better method.
Training
During the learning phase, an unsupervised network tries to mimic the data it's given and uses the error in its mimicked output to correct itself (eg. its weights & biases). This resembles the mimicry behavior of children as they learn a language. Sometimes the error is expressed as a low probability that the erroneous output occurs, or it might be express as an unstable high energy state in the network.
Energy
An energy function is a macroscopic measure of a network's state. This analogy with physics is inspired by Ludwig Boltzmann's analysis of a gas' macroscopic energy from the microscopic probabilities of particle motion p 
  
    
      
        ∝
      
    
    {\displaystyle \propto }
   eE/kT, where k is the Boltzmann constant and T is temperature. In the RBM network the relation is p = e-E / Z, where p & E vary over every possible activation pattern and Z = 
  
    
      
        
          ∑
          
            A
            l
            l
            P
            a
            t
            t
            e
            r
            n
            s
          
        
      
    
    {\displaystyle \sum _{AllPatterns}}
   e -E(pattern). To be more precise, p(a) = e-E(a) / Z, where a is an activation pattern of all neurons (visible and hidden). Hence, early neural networks bear the name Boltzmann Machine. Paul Smolensky calls -E the Harmony. A network seeks low energy which is high Harmony.
Networks
Boltzmann and Helmholtz came before neural networks formulations, but these networks borrowed from their analyses, so these networks bear their names. Hopfield, however, directly contributed to UL.

Intermediate
Here, distributions p(x) and q(x) will be abbreviated as p and q.
History
Some more vocabulary:
Comparison of Networks

Specific Networks
Here, we highlight some characteristics of each networks. Ferromagnetism inspired Hopfield networks, Boltzmann machines, and RBMs. A neuron correspond to an iron domain with binary magnetic moments Up and Down, and neural connections correspond to the domain's influence on each other. Symmetric connections enables a global energy formulation. During inference the network updates each state using the standard activation step function. Symmetric weights guarantees convergence to a stable activation pattern.Hopfield networks are used as CAMs and are guaranteed to settle to a some pattern. Without symmetric weights, the network is very hard to analyze. With the right energy function, a network will converge.Boltzmann machines are stochastic Hopfield nets. Their state value is sampled from this pdf as follows: suppose a binary neuron fires with the Bernoulli probability p(1) = 1/3 and rests with p(0) = 2/3. One samples from it by taking a UNIFORMLY distributed random number y, and plugging it into the inverted cumulative distribution function, which in this case is the step function thresholded at 2/3. The inverse function = { 0 if x <= 2/3, 1 if x > 2/3 }Helmholtz machines are early inspirations for the Variational Auto Encoders. It's 2 networks combined into one—forward weights operates recognition and backward weights implements imagination. It is perhaps the first network to do both. Helmholtz did not work in machine learning but he inspired the view of ""statistical inference engine whose function is to infer probable causes of sensory input"" (3). the stochastic binary neuron outputs a probability that its state is 0 or 1. The data input is normally not considered a layer, but in the Helmholtz machine generation mode, the data layer receives input from the middle layer has separate weights for this purpose, so it is considered a layer. Hence this network has 3 layers.Variational Autoencoder (VAE) are inspired by Helmholtz machines and combines probability network with neural networks. An Autoencoder is a 3-layer CAM network, where the middle layer is supposed to be some internal representation of input patterns. The weights are named phi & theta rather than W and V as in Helmholtz—a cosmetic difference. The encoder neural network is a probability distribution qφ(z|x) and the decoder network is pθ(x|z). These 2 networks here can be fully connected, or use another NN scheme.
Hebbian Learning, ART, SOM
The classical example of unsupervised learning in the study of neural networks is Donald Hebb's principle, that is, neurons that fire together wire together. In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.
Among neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used in unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing.

See also
Automated machine learning
Cluster analysis
Anomaly detection
Expectation–maximization algorithm
Generative topographic map
Meta-learning (computer science)
Multivariate analysis
Radial basis function network
Weak supervision

References
Further reading
Bousquet, O.; von Luxburg, U.; Raetsch, G., eds. (2004). Advanced Lectures on Machine Learning. Springer-Verlag. ISBN 978-3540231226.
Duda, Richard O.; Hart, Peter E.; Stork, David G. (2001). ""Unsupervised Learning and Clustering"". Pattern classification (2nd ed.). Wiley. ISBN 0-471-05669-3.
Hastie, Trevor; Tibshirani, Robert (2009). The Elements of Statistical Learning: Data mining, Inference, and Prediction. New York: Springer. pp. 485–586. doi:10.1007/978-0-387-84858-7_14. ISBN 978-0-387-84857-0.
Hinton, Geoffrey; Sejnowski, Terrence J., eds. (1999). Unsupervised Learning: Foundations of Neural Computation. MIT Press. ISBN 0-262-58168-X. (This book focuses on unsupervised learning in neural networks)",https://en.wikipedia.org/wiki/Unsupervised_learning,"['Articles with short description', 'CS1 maint: multiple names: authors list', 'Machine learning', 'Short description matches Wikidata', 'Unsupervised learning', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with MA identifiers']",Data Science
180,Wide-field Infrared Survey Explorer,"The Wide-field Infrared Survey Explorer (WISE, observatory code C51) is a NASA infrared-wavelength astronomical space telescope launched in December 2009, and placed in hibernation mode in February 2011. It was re-activated in 2013. WISE discovered thousands of minor planets and numerous star clusters. Its observations also supported the discovery of the first Y Dwarf and Earth trojan asteroid.WISE performed an all-sky astronomical survey with images in 3.4, 4.6, 12 and 22 μm wavelength range bands, over ten months using a 40 cm (16 in) diameter infrared telescope in Earth orbit. After its hydrogen coolant depleted, a four-month mission extension called NEOWISE was conducted to search for near-Earth objects (NEO) such as comets and asteroids using its remaining capability.The All-Sky data including processed images, source catalogs and raw data, was released to the public on 14 March 2012, and is available at the Infrared Science Archive. In August 2013, NASA announced it would reactivate the WISE telescope for a new three-year mission to search for asteroids that could collide with Earth. Science operations and data processing for WISE and NEOWISE take place at the Infrared Processing and Analysis Center at the California Institute of Technology in Pasadena, California.

Mission goals
The mission was planned to create infrared images of 99% of the sky, with at least eight images made of each position on the sky in order to increase accuracy. The spacecraft was placed in a 525 km (326 mi), circular, polar, Sun-synchronous orbit for its ten-month mission, during which it has taken 1.5 million images, one every 11 seconds. The satellite orbited above the terminator, its telescope pointing always to the opposite direction to the Earth, except for pointing towards the Moon, which was avoided, and its solar cells towards the Sun. Each image covers a 47 arcminute field of view, which means a 6 arcsecond resolution. Each area of the sky was scanned at least 10 times at the equator; the poles were scanned at theoretically every revolution due to the overlapping of the images. The produced image library contains data on the local Solar System, the Milky Way, and the more distant Universe. Among the objects WISE studied are asteroids, cool and dim stars such as brown dwarfs, and the most luminous infrared galaxies.

Targets within the Solar System
WISE was not able to detect Kuiper belt objects, because their temperatures are too low. Pluto is the only Kuiper belt object that was detected. It was able to detect any objects warmer than 70–100 K. A Neptune-sized object would be detectable out to 700 AU, a Jupiter-mass object out to 1 light year (63,000 AU), where it would still be within the Sun's zone of gravitational control. A larger object of 2–3 Jupiter masses would be visible at a distance of up to 7–10 light years.At the time of planning, it was estimated that WISE would detect about 300,000 main-belt asteroids, of which approximately 100,000 will be new, and some 700 Near-Earth object (NEO) including about 300 undiscovered. That translates to about 1000 new main-belt asteroids per day, and 1–3 NEOs per day. The peak of magnitude distribution for NEOs will be about 21–22 V. WISE would detect each typical Solar System object 10–12 times over about 36 hours in intervals of 3 hours.

Targets outside the Solar System
Star formation, which are covered by interstellar dust, are detectable in infrared, since at this wavelength electromagnetic radiation can penetrate the dust. Infrared measurements from the WISE astronomical survey have been particularly effective at unveiling previously undiscovered star clusters. Examples of such embedded star clusters are Camargo 18, Camargo 440, Majaess 101, and Majaess 116. In addition, galaxies of the young Universe and interacting galaxies, where star formation is intensive, are bright in infrared. On this wavelength the interstellar gas clouds are also detectable, as well as proto-planetary discs. WISE satellite was expected to find at least 1,000 of those proto-planetary discs.

Spacecraft
The WISE spacecraft bus was built by Ball Aerospace & Technologies in Boulder, Colorado. The spacecraft is derived from the Ball Aerospace & Technologies RS-300 spacecraft architecture, particularly the NEXTSat spacecraft built for the successful Orbital Express mission launched on 9 March 2007. The flight system has an estimated mass of 560 kg (1,230 lb). The spacecraft is three-axis stabilized, with body-fixed solar arrays. It uses a high-gain antenna in the Ku-band to transmit to the ground through the Tracking and Data Relay Satellite System (TDRSS) geostationary system. Ball also performed the testing and flight system integration.

Telescope
Construction of the WISE telescope was divided between Ball Aerospace & Technologies (spacecraft, operations support), SSG Precision Optronics, Inc. (telescope, optics, scan mirror), DRS Technologies and Rockwell International (focal planes), Lockheed Martin (cryostat, cooling for the telescope), and Space Dynamics Laboratory (instruments, electronics, and testing). The program was managed through the Jet Propulsion Laboratory.The WISE instrument was built by the Space Dynamics Laboratory in Logan, Utah.

Mission
WISE surveyed the sky in four wavelengths of the infrared band, at a very high sensitivity. Its design specified as goals that the full sky atlas of stacked images it produced have 5-sigma sensitivity limits of 120, 160, 650, and 2600 microjanskies (µJy) at 3.3, 4.7, 12, and 23 µm (aka microns). WISE achieved at least 68, 98, 860, and 5400 µJy; 5 sigma sensitivity at 3.4, 4.6, 12, and 22 µm for the WISE All-Sky data release. This is a factor of 1,000 times better sensitivity than the survey completed in 1983 by the IRAS satellite in the 12 and 23 µm bands, and a factor of 500,000 times better than the 1990s survey by the Cosmic Background Explorer (COBE) satellite at 3.3 and 4.7 µm. On the other hand, IRAS could also observe 60 and 100 µm wavelengths.
Band 1 – 3.4 µm (micrometre) – broad-band sensitivity to stars and galaxies
Band 2 – 4.6 µm – detect thermal radiation from the internal heat sources of sub-stellar objects like brown dwarfs
Band 3 – 12 µm – detect thermal radiation from asteroids
Band 4 – 22 µm – sensitivity to dust in star-forming regions (material with temperatures of 70–100 kelvins)The primary mission lasted 10 months: one month for checkout, six months for a full-sky survey, then an additional three months of survey until cryogenic coolant (which kept the instruments at 17 K) ran out. The partial second survey pass facilitated the study of changes (e.g. orbital movement) in observed objects.

Congressional hearing
On 8 November 2007, the House Committee on Science and Technology's Subcommittee on Space and Aeronautics held a hearing to examine the status of NASA's Near-Earth Object (NEO) survey program. The prospect of using WISE was proposed by NASA officials.NASA officials told Committee staff that NASA plans to use WISE to detect near-Earth objects in addition to performing its science goals. It was projected that WISE could detect 400 NEOs (or roughly 2% of the estimated NEO population of interest) within its one-year mission.

Results
By October 2010, over 33,500 new asteroids and comets were discovered, and nearly 154,000 Solar System objects were observed by WISE.Discovery of an ultra-cool brown dwarf, WISEPC J045853.90+643451.9, about 10 to 30 light years away from Earth, was announced in late 2010 based on early data. In July 2011, it was announced that WISE had discovered the first Earth trojan asteroid, 2010 TK7. Also, the third-closest star system, Luhman 16.
As of May 2018, WISE/NEOWISE has also discovered 290 near-Earth objects and comets (see section below).

Project milestones
The WISE Mission is led by Edward L. Wright of the University of California, Los Angeles. The mission has a long history under Wright's efforts and was first funded by NASA in 1999 as a candidate for a NASA Medium-class Explorer (MIDEX) mission under the name Next Generation Sky Survey (NGSS). The history of the program from 1999 to date is briefly summarized as follows:
January 1999 — NGSS is one of five missions selected for a Phase A study, with an expected selection in late 1999 of two of these five missions for construction and launch, one in 2003 and another in 2004. Mission cost is estimated at US$139 million at this time.
March 1999 — WIRE infrared telescope spacecraft fails within hours of reaching orbit.
October 1999 — Winners of MIDEX study are awarded, and NGSS is not selected.
October 2001 — NGSS proposal is re-submitted to NASA as a MIDEX mission.
April 2002 — NGSS proposal is accepted by the NASA Explorer office to proceed as one of four MIDEX programs for a Pre-Phase A study.
December 2002 — NGSS changes its name to Wide-field Infrared Survey Explorer (WISE).
March 2003 — NASA releases a press release announcing WISE has been selected for an Extended Phase-A study, leading to a decision in 2004 on whether to proceed with the development of the mission.
April 2003 — Ball Aerospace & Technologies is selected as the spacecraft provider for the WISE mission.
April 2004 — WISE is selected as NASA's next MIDEX mission. WISE's cost is estimated at US$208 million at this time.
November 2004 — NASA selects the Space Dynamics Laboratory at Utah State University to build the telescope for WISE.
October 2006 — WISE is confirmed for development by NASA and authorized to proceed with development. Mission cost at this time is estimated to be US$300 million.
		
		
		
#4 December 2009 — WISE successfully launched from Vandenberg Air Force Base, California.
29 December 2009 — WISE successfully jettisoned instrument cover.
6 January 2010 — WISE first light image released.
14 January 2010 — WISE begins its regular four wavelength survey scheduled for nine months duration. It is expected to cover 99% of the sky with overlapping images in the first 6 months and continuing with a second pass until the hydrogen coolant is exhausted about three months later.
25 January 2010 — WISE detects a never-before-seen near Earth asteroid, designated 2010 AB78.
11 February 2010 — WISE detects a previously unknown comet, designated P/2010 B2 (WISE).
25 February 2010 — WISE website reports it has surveyed over 25% of the sky to a depth of 7 overlapping image frames.
10 April 2010 — WISE website reports it has surveyed over 50% of the sky to a depth of 7 overlapping image frames.
26 May 2010 — WISE website reports it has surveyed over 75% of the sky to a depth of 7 overlapping image frames.
16 July 2010 — Press release announces that 100% sky coverage will be completed on 17 July 2010. About half of the sky will be mapped again before the instrument's block of solid hydrogen coolant sublimes and is exhausted.
October 2010 — WISE hydrogen coolant runs out. Start of NASA Planetary Division funded NEOWISE mission.
January 2011 — Entire sky surveyed to an image density of at least 16+ frames (i.e. second scan of sky completed).
February 17, 2011 — WISE Spacecraft transmitter turned off at 20:00 UTC (12:00 noon PST) by principal investigator Ned Wright. The spacecraft will remain in hibernation without ground contacts awaiting possible future use.
14 April 2011 — Preliminary release of data covering 57% of the sky as seen by WISE.
27 July 2011 — First Earth trojan asteroid discovered from WISE data.
23 August 2011 — WISE confirms the existence of a new class of brown dwarf, the Y dwarf. Some of these stars appear to have temperatures less than 300 K, close to room temperature at about 25C. Y dwarfs show ammonia absorption, in addition to methane and water absorption bands displayed by T dwarfs.
14 March 2012 — Release of the WISE All-Sky data to the scientific community.
29 August 2012 — WISE reveals millions of black-holes.
20 September 2012 — WISE was successfully contacted to check its status.
21 August 2013 — NASA announced it would recommission WISE with a new mission to search for asteroids.
19 December 2013 — NASA releases a new image taken by the reactivated WISE telescope, following an extended cooling down phase. The revived NeoWise mission is underway and collecting data.
7 March 2014 — NASA reports that WISE, after an exhaustive survey, has not been able to uncover any evidence of ""Planet X"", a hypothesized planet within the Solar System.
26 April 2014 — The Penn State Center for Exoplanets and Habitable Worlds reports that WISE has found the coldest known brown dwarf, between -48 and -13 °C, 7.2 light years away from the Sun.
21 May 2015 — NASA reports the discovery of WISE J224607.57-052635.0, the most luminous known galaxy in the universe.

History
Launch
The launch of the Delta II launch vehicle carrying the WISE spacecraft was originally scheduled for 11 December 2009. This attempt was scrubbed to correct a problem with a booster rocket steering engine. The launch was then rescheduled for 14 December 2009. The second attempt launched on time at 14:09:33 UTC (06:09:33 local PST) from Vandenberg Air Force Base in California. The launch vehicle successfully placed the WISE spacecraft into the planned polar orbit at an altitude of 525 km (326 mi) above the Earth.WISE avoided the problem that affected Wide Field Infrared Explorer (WIRE), which failed within hours of reaching orbit in March 1999. In addition, WISE was 1,000 times more sensitive than prior surveys such as IRAS, AKARI, and COBE's DIRBE.

""Cold"" mission
A month-long checkout after launch found all spacecraft systems functioning normally and both the low- and high-rate data links to the operations center working properly. The instrument cover was successfully jettisoned on 29 December 2009. A first light image was released on 6 January 2010: an eight-second exposure in the Carina constellation showing infrared light in false color from three of WISE's four wavelength bands: Blue, green and red corresponding to 3.4, 4.6, and 12 µm, respectively. On 14 January 2010, the WISE mission started its official sky survey.The WISE group's bid for continued funding for an extended ""warm mission"" scored low by a NASA review board, in part because of a lack of outside groups publishing on WISE data. Such a mission would have allowed use of the 3.4 and 4.6 µm detectors after the last of cryo-coolant had been exhausted, with the goal of completing a second sky survey to detect additional objects and obtain parallax data on putative brown dwarf stars. NASA extended the mission in October 2010 to search for near-Earth objects (NEO).By October 2010, over 33,500 new asteroids and comets were discovered, and over 154,000 Solar System objects were observed by WISE. While active it found dozens of previously unknown asteroids every day. In total, it captured more than 2.7 million images during its primary mission.

NEOWISE (pre-hibernation)
In October 2010, NASA extended the mission by one month with a program called Near-Earth Object WISE (NEOWISE). Due to its success, the program was extended a further three months. The focus was to look for asteroids and comets close to Earth orbit, using the remaining post-cryogenic detection capability (two of four detectors on WISE work without cryogenic). In February 2011, NASA announced that NEOWISE had discovered many new objects in the Solar System, including twenty comets. During its primary and extended missions, the spacecraft delivered characterizations of 158,000 minor planets, including more than 35,000 newly discovered objects.

Hibernation and recommissioning
After completing a full scan of the asteroid belt for the NEOWISE mission, the spacecraft was put into hibernation on 1 February 2011. The spacecraft was briefly contacted to check its status on 20 September 2012.On 21 August 2013, NASA announced it would recommission NEOWISE to continue its search for near-Earth objects (NEO) and potentially dangerous asteroids. It would additionally search for asteroids that a robotic spacecraft could intercept and redirect to orbit the Moon. The extended mission would be for three years at a cost of US$5 million per year, and was brought about in part due to calls for NASA to step up asteroid detection after the Chelyabinsk meteor exploded over Russia in February 2013.NEOWISE was successfully taken out of hibernation in September 2013. With its coolant depleted, the spacecraft's temperature was reduced from 200 K (−73 °C; −100 °F) — a relatively high temperature resulting from its hibernation — to an operating temperature of 75 K (−198.2 °C; −324.7 °F) by having the telescope stare into deep space. Its instruments were then re-calibrated, and the first post-hibernation photograph was taken on 19 December 2013.

NEOWISE (post-hibernation)
The post-hibernation NEOWISE mission was anticipated to discover 150 previously unknown near-Earth objects and to learn more about the characteristics of 2,000 known asteroids. Few objects smaller than 100 m (330 ft) in diameter were detected by NEOWISE's automated detection software, known as the WISE Moving Object Processing Software (WMOPS), because it requires five or more detections to be reported. The average albedo of asteroids larger than 100 meters discovered by NEOWISE is 0.14.The telescope was turned on again in 2013, and by December 2013 the telescope had cooled down enough and was able to resume observations. Between then and May 2017, the telescope made almost 640,000 detections of over 26,000 previously known objects including asteroids and comets. In addition, it discovered 416 new objects and about a quarter of those were near-Earth objects classification.As of May 2018, WISE / NEOWISE statistics lists a total of 290 near-Earth objects (NEOs), including 2016 WF9 and C/2016 U1, discovered by the spacecraft:
262 NEAs (subset of NEOs)
047 PHAs (subset of NEAs)
028 cometsOf the 262 near-Earth asteroids (NEAs), 47 of them are considered potentially hazardous asteroids (PHAs), a subset of the much larger family of NEOs, but particularly more likely to hit Earth and cause significant destruction. NEOs can be divided into NECs (comets only) and NEAs (asteroids only), and further into subcategories such as Atira asteroids, Aten asteroids, Apollo asteroids, Amor asteroids and the potentially hazardous asteroids (PHAs).

Data releases
On 14 April 2011, a preliminary release of WISE data was made public, covering 57% of the sky observed by the spacecraft. On 14 March 2012, a new atlas and catalog of the entire infrared sky as imaged by WISE was released to the astronomic community. On 31 July 2012, NEOWISE Post-Cryo Preliminary Data was released. A release called AllWISE, combining all data, was released on 13 November 2013. NEOWISE data is released annually.In 2018, the reliability of the data was challenged in a paper by Nathan Myhrvold, who stated that the NEOWISE data suffers from systemic errors due to the spacecraft being designed to observe very distant objects rather than asteroids in the Solar System; NASA responded that they are ""confident the processes and analyses performed by the Neowise team are valid, as verified by independent researchers"".

unWISE and CatWISE
The Allwise co-added images were intentionally blurred. This has the disadvantage that many sources are not detected in crowded regions. The unofficial, unblurred coadds of the WISE imaging (unWISE) creates sharp images and masks defects and transients. unWISE coadded images can be searched by coordinates on the unWISE website. unWISE images are used for the citizen science project Backyard Worlds.In 2019, a preliminary catalog was released. The catalog is called CatWISE. This catalog combines the WISE and NEOWISE data and provides photometry at 3.4 and 4.6 µm. It uses the unWISE images and the Allwise pipeline to detect sources. CatWISE includes fainter sources and far more accurate measurement of the motion of objects. The catalog is used to extend the number of discovered brown dwarfs, especially the cold and faint Y dwarfs. CatWISE is led by Jet Propulsion Laboratory (JPL), California Institute of Technology, with funding from NASA's Astrophysics Data Analysis Program. The CatWISE preliminary catalog can be accessed through Infrared Science Archive (IRSA).

Discovered objects
In addition to numerous comets and minor planets, WISE also discovered many brown dwarf stars including some quite close the Sun in the context of Solar Neighborhood; these bodies are sort of dim stars expected to be about the size of Jupiter just a few light years from Earth. The other extraordinary discovery, was first Earth trojan, an asteroid in a special orbital relationship common to extremely large planets like Jupiter. Many other observations across the sky lead to many detentions, such of distant galaxies also.

Brown dwarfs
The nearest brown dwarfs discovered by WISE within 20 light-years include:

Before the discovery of Luhman 16 in 2013, WISE 1506+7027 at a distance of 11.1+2.3−1.3 light-years was suspected to be closest brown dwarf on the list of nearest stars (also see § Map with nearby WISE stars).

Minor planets
WISE is credited with discovering 3,088 numbered minor planets. Examples of the mission's numbered minor planet discoveries include:

(310071) 2010 KR59
(336756) 2010 NV1
(419624) 2010 SO16

Comet C/2020 F3 (NEOWISE)
On 27 March 2020, the comet C/2020 F3 (NEOWISE) was discovered by the WISE spacecraft. It eventually became a naked-eye comet and was widely photographed by professional and amateur astronomers. It was the brightest comet visible in the northern hemisphere since comet Hale-Bopp in 1997.

Gallery
Full sky views by WISE
Selected images by WISE
Map with nearby WISE stars
See also
Infrared astronomy
Nemesis (hypothetical star)
Near-Earth Object Surveillance Mission (NEOSM), a successor to NEOWISE under development (as of 2019)
Tyche (hypothetical planet)

References
External links
""WISE mission"". NASA.gov.
""WISE mission"". NASA / JPL.
""WISE mission"". UC Berkeley.
""WISE mission"". UCLA.
""NEOWISE mission"". Caltech.",https://en.wikipedia.org/wiki/Wide-field_Infrared_Survey_Explorer,"['2009 in spaceflight', 'All Wikipedia articles written in American English', 'All articles with unsourced statements', 'Articles with short description', 'Articles with unsourced statements from April 2018', 'Commons category link from Wikidata', 'Explorers Program', 'Incomplete lists from February 2020', 'Infrared telescopes', 'NASA space probes', 'Near-Earth object tracking', 'Short description is different from Wikidata', 'Space telescopes', 'Spacecraft launched by Delta II rockets', 'Use American English from August 2019', 'Use dmy dates from March 2021', 'Wide-field Infrared Survey Explorer']",Data Science
183,William S. Cleveland,"William Swain Cleveland II (born 1943) is an American computer scientist and Professor of Statistics and Professor of Computer Science at Purdue University, known for his work on data visualization, particularly on nonparametric regression and local regression.

Biography
Cleveland obtained his AB in Mathematics mid 1960s from Princeton University, where he graduated under William Feller. For his PhD studies in Statistics he moved to Yale University, where he graduated under Leonard Jimmie Savage.After graduation Cleveland started at Bell Labs, where he was staff member of the Statistics Research Department and Department Head for 12 years. Eventually he moved to the Purdue University, where he became Professor of Statistics and Courtesy Professor of Computer Science.
In 1982 he was elected as a Fellow of the American Statistical Association.His research interests are in the fields of ""data visualization, computer networking, machine learning, data mining, time series, statistical modeling, visual perception, environmental science, and seasonal adjustment."" Cleveland is credited with defining and naming the field of data science, which he did in a 2001 publication.

Selected publications
Cleveland, William S. The elements of graphing data. Monterey, CA: Wadsworth Advanced Books and Software, 1985.
Cleveland, William S. Visualizing data. Hobart Press, 1993.Articles, a selection:
Cleveland, William S. ""Robust locally weighted regression and smoothing scatterplots."" Journal of the American statistical association 74.368 (1979): 829–836.
Cleveland, William S., and Robert McGill. ""Graphical perception: Theory, experimentation, and application to the development of graphical methods."" Journal of the American statistical association 79.387 (1984): 531–554.
Cleveland, William S., and Susan J. Devlin. ""Locally weighted regression: an approach to regression analysis by local fitting."" Journal of the American Statistical Association 83.403 (1988): 596–610.
Cleveland, William S., Eric Grosse, and William M. Shyu. ""Local regression models."" Statistical models in S (1992): 309–376.

References
External links
William S. Cleveland, Shanti S. Gupta Professor of Statistics, Courtesy Professor of Computer Science
William Cleveland at the Mathematics Genealogy Project",https://en.wikipedia.org/wiki/William_S._Cleveland,"['1943 births', 'Fellows of the American Statistical Association', 'Human–computer interaction researchers', 'Information visualization experts', 'Living people', 'Princeton University alumni', 'Purdue University faculty', 'Scientists at Bell Labs', 'Wikipedia articles with BIBSYS identifiers', 'Wikipedia articles with BNF identifiers', 'Wikipedia articles with GND identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MGP identifiers', 'Wikipedia articles with NDL identifiers', 'Wikipedia articles with NKC identifiers', 'Wikipedia articles with NTA identifiers', 'Wikipedia articles with SUDOC identifiers', 'Wikipedia articles with Trove identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers', 'Yale University alumni']",Data Science
185,Vasant Dhar,"Vasant Dhar is a professor at the Stern School of Business and the Center for Data Science at New York University, former Editor-in-Chief of the journal Big Data. and the founder of SCT Capital, one of the first machine-learning-based hedge funds in New York City in the 90s. His research focuses on building scalable decision making systems from large sources of data using techniques and principles from the disciplines of Artificial Intelligence and Machine Learning.

Early life and education
He is a graduate of the The Lawrence School, Sanawar, which he considers one of the best presents his parents gave him without realizing it. Dhar graduated from the Indian Institute of Technology Delhi in 1978 with a B.Tech in Chemical Engineering. He subsequently attended the University of Pittsburgh where he received an M. Phil and a Ph.D. in 1984. After he earned his doctorate, he joined  the faculty at New York University. He worked at Morgan Stanley between 1994 and 1997 where he created the Data Mining Group that focused on predicting financial markets and customer behavior.

Career highlights
Dhar is an Artificial Intelligence researcher and data scientist whose research addresses the following question: when do we trust AI systems with decision making? The question is particularly relevant to current-day autonomous machine-learning-based systems that learn and adapt with ongoing data.
Dhar's research has been motivated by a building predictive models in a number of domains, most notably finance, as well as areas including healthcare, sports, education, and business. Why are we willing to trust machines in some areas and not others?
Dhar's view is that there is a discontinuity when we give complete decision-making control to a machine that learns from ongoing data. This discontinuity introduces some risks, specifically those around the errors made by such systems, which directly impact our degree of trust in them.
Dhar's research breaks down trust along 2 risk-based dimensions: predictability, or how frequently a system makes mistakes (X-axis) and the associated costs of error (Y-axis) of such mistakes. The research demonstrates the existence of an “automation frontier” that expresses a tradeoff between how often a system will be wrong and the consequences of such mistakes. Trust, and hence our willingness to cede control of decision making to the machine, increases with increasing predictability and lower error costs. In other words, we are willing to trust machines if they don't make too many mistakes and their costs are tolerable. As mistakes increase, we require that their consequences be less costly.
The automation frontier provides a natural way to think about the future of work. With more and better data and algorithms, parts of existing processes become automated due to increased predictability, and cross the automation frontier into the “trust the machine” zone, whereas the parts with high error costs remain under human control. The model provides a way to think about the changing responsibilities of humans and machines as more data and better algorithms become better than humans with decisions.
Dhar also uses the framework to frame policy issues around the risks of AI-based social media platforms and issues of privacy and ethical uses and governance of data. He writes regularly in the media on Artificial Intelligence, societal risks of AI platforms, data governance, privacy, ethics, and trust. He is a frequent speaker in academic as well as industrial forums.
Professor Dhar teaches courses on Systematic Investing, Prediction, Data Science, and Foundations of FinTech. He has written over 100 research articles, funded by grants from industry and government agencies such as the National Science Foundation.

See also
Data Science
Predictive Analytics

References
External links
NYU Stern Faculty Page",https://en.wikipedia.org/wiki/Vasant_Dhar,"['All orphaned articles', 'All pages needing factual verification', 'Articles with hCards', 'IIT Delhi alumni', 'Information systems researchers', 'Living people', 'New York University Stern School of Business faculty', 'Orphaned articles from October 2015', 'Pages using infobox scientist with unknown parameters', 'University of Pittsburgh alumni', 'Wikipedia articles needing factual verification from September 2019', 'Wikipedia articles with BIBSYS identifiers', 'Wikipedia articles with ISNI identifiers', 'Wikipedia articles with LCCN identifiers', 'Wikipedia articles with MGP identifiers', 'Wikipedia articles with NTA identifiers', 'Wikipedia articles with SUDOC identifiers', 'Wikipedia articles with VIAF identifiers', 'Wikipedia articles with WORLDCATID identifiers', 'Year of birth missing (living people)']",Data Science
187,Category:Use dmy dates from December 2012,"Wikipedia articles (tagged in this month) that use dd mm yyyy date formats, whether by application of the first main contributor rule or by virtue of close national ties to the subject belong in Category:Use dmy dates. Use {{Dmy}} or {{Use dmy dates}} to add an article to this category. See Wikipedia:MOSNUM.
This system of tagging/categorisation is used as a status monitor of all articles that use dd mm yyyy date formats.",https://en.wikipedia.org/wiki/Category:Use_dmy_dates_from_December_2012,"['CatAutoTOC generates Large category TOC', 'Hidden categories', 'Template Large category TOC via CatAutoTOC on category with 2,001–5,000 pages', 'Use dmy dates']",Data Science
188,Category:CS1 maint: others,"This is a tracking category for CS1 citations that use |others= without also using |author= or |editor= or any of their aliases.
|others= is provided to record other (secondary) contributors to the cited source.  Articles are listed in this category when Module:Citation/CS1 identifies a template that does not identify primary contributors.  Pages in this category should only be added by Module:Citation/CS1.

By default, Citation Style 1 and Citation Style 2 error messages are visible to all readers and maintenance category messages are hidden from all readers.
To display maintenance messages, include the following text in your common CSS page or your specific skin's CSS page (common.css and skin.css respectively):

To display hidden-by-default error messages:

Even with this CSS installed, older pages in Wikipedia's cache may not have been updated to show these error messages even though the page is listed in one of the tracking categories. A null edit will resolve that issue.
To hide normally-displayed error messages:

You can personalize the display of these messages (such as changing the color), but you will need to ask someone who knows CSS or at the technical village pump if you do not understand how.",https://en.wikipedia.org/wiki/Category:CS1_maint:_others,"['CS1 maintenance', 'CatAutoTOC generates Large category TOC', 'Hidden categories', 'Template Large category TOC via CatAutoTOC on category with over 20,000 pages', 'Tracking categories']",Data Science
